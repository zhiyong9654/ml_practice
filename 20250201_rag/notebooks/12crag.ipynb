{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb\n",
    "\n",
    "some deviations from the source code because i dont wanna pay for embeddings from openai, or hit openai models. All openAI integration is replaced with ollama.\n",
    "\n",
    "I also removed langsmith integration. don't think it's needed. just a frontend for LLM debugging which i can achieve with `langchain.debug = True`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding= OllamaEmbeddings(\n",
    "        model=\"nomic-embed-text:v1.5\"\n",
    "    ),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to write it yourself\n",
    "\n",
    "Grader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhiyong/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/pydantic/json_schema.py:2279: PydanticJsonSchemaWarning: Default value typing.Literal['Yes', 'No'] is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='{ \"binary_score\": \"yes\" }' additional_kwargs={} response_metadata={'model': 'llama3.2:3b-instruct-q8_0', 'created_at': '2025-02-17T15:03:48.357281973Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13088469335, 'load_duration': 5657294925, 'prompt_eval_count': 279, 'prompt_eval_duration': 5816000000, 'eval_count': 10, 'eval_duration': 836000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-6363d100-469e-4d38-b389-81d9a7b6623d-0' usage_metadata={'input_tokens': 279, 'output_tokens': 10, 'total_tokens': 289}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": docs[1].page_content}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "generation_prompt_template = \"\"\"You are a RAG system tasked with answering a user query given a few documents for reference.\n",
    "\n",
    "{documents}\n",
    "\n",
    "user query:\n",
    "{user_query}\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_template(generation_prompt_template)\n",
    "\n",
    "generation_and_rewrite_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b-instruct-q8_0\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't really wanna set up Tavilysearch. Will hold off for now.\n",
    "\n",
    "Define graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. retrieve\n",
    "2. grade\n",
    "   1. rewrite -> web search\n",
    "3. generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key point to note is that we always manipulate state, reminds me of ansible and idempotence\n",
    "def retrieve(state):\n",
    "    # Note that retriever is a global obj\n",
    "    user_query = state['user_query']\n",
    "    return {\n",
    "        'user_query': user_query, \n",
    "        'docs': retriever.get_relevant_documents(user_query)\n",
    "    }\n",
    "\n",
    "def grading(state):\n",
    "    # Data model\n",
    "    class GradeDocuments(BaseModel):\n",
    "        \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "        binary_score: str = Literal['Yes', \"No\"]\n",
    "\n",
    "    structured_llm_grader = ChatOllama(\n",
    "        model=\"llama3.2:3b-instruct-q8_0\",\n",
    "        temperature=0,\n",
    "        format=GradeDocuments.model_json_schema())\n",
    "\n",
    "    # Prompt\n",
    "    grade_prompt_template = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "\n",
    "question:\n",
    "{user_query}\n",
    "\n",
    "document:\n",
    "{document}\n",
    "\"\"\"\n",
    "    grade_prompt = ChatPromptTemplate.from_template(grade_prompt_template)\n",
    "\n",
    "\n",
    "    user_query = state['user_query']\n",
    "    docs = state['docs']\n",
    "    filtered_docs = []\n",
    "    search = False\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        relevant = (grade_prompt | structured_llm_grader | StrOutputParser()).invoke({\n",
    "            'user_query': user_query,\n",
    "            'document': doc.page_content.strip()\n",
    "        })\n",
    "        if relevant == 'Yes':\n",
    "            filtered_docs.append(doc)\n",
    "            print(f'Doc {i+1} is relevant.')\n",
    "        else:\n",
    "            search = True\n",
    "            print(f'Doc {i+1} is not relevant. Search will be done.')\n",
    "    return {\n",
    "        'user_query': user_query, \n",
    "        'docs': docs,\n",
    "        'filtered_docs': filtered_docs,\n",
    "        'search': search\n",
    "    }\n",
    "\n",
    "def rewrite(state):\n",
    "    user_query = state['user_query']\n",
    "    docs = state['docs']\n",
    "    filtered_docs = state['filtered_docs']\n",
    "    search = state['search']\n",
    "\n",
    "    # rewrite question for the purposes of search\n",
    "    rewrite_template = \"\"\"You are a master of search engines and are tasked to rewrite a user query for optimum web search.\n",
    "\n",
    "    user query:\n",
    "    {user_query}\"\"\"\n",
    "    rewrite_prompt = ChatPromptTemplate.from_template(rewrite_template)\n",
    "\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.2:3b-instruct-q8_0\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    rewritten_question = (rewrite_prompt | llm | StrOutputParser()).invoke({'user_query': user_query})\n",
    "    return {\n",
    "        'user_query': user_query, \n",
    "        'docs': docs,\n",
    "        'filtered_docs': filtered_docs,\n",
    "        'search': search,\n",
    "        'rewritten_question': rewritten_question\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
