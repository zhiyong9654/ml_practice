{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb\n",
    "\n",
    "some deviations from the source code because i dont wanna pay for embeddings from openai, or hit openai models. All openAI integration is replaced with ollama.\n",
    "\n",
    "I also removed langsmith integration. don't think it's needed. just a frontend for LLM debugging which i can achieve with `langchain.debug = True`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding= OllamaEmbeddings(\n",
    "        model=\"nomic-embed-text:v1.5\"\n",
    "    ),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to write it yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't really wanna set up Tavilysearch. Will hold off for now.\n",
    "\n",
    "Define graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        user_query: question\n",
    "        generation: LLM generation\n",
    "        search: whether to add search\n",
    "        docs: list of documents\n",
    "        filtered_docs: list of documents that passed LLM grader\n",
    "    \"\"\"\n",
    "    user_query: str\n",
    "    generation: str\n",
    "    search: bool\n",
    "    docs: List[str]\n",
    "    filtered_docs: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. retrieve\n",
    "2. grade\n",
    "   1. rewrite -> web search\n",
    "3. generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key point to note is that we always manipulate state, reminds me of ansible and idempotence\n",
    "def retrieve(state):\n",
    "    # Note that retriever is a global obj\n",
    "    user_query = state['user_query']\n",
    "    return {\n",
    "        'user_query': user_query, \n",
    "        'docs': retriever.get_relevant_documents(user_query)\n",
    "    }\n",
    "\n",
    "def grading(state):\n",
    "    # Data model\n",
    "    class GradeDocuments(BaseModel):\n",
    "        \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "        binary_score: str = Literal['Yes', \"No\"]\n",
    "\n",
    "    structured_llm_grader = ChatOllama(\n",
    "        model=\"llama3.2:3b-instruct-q8_0\",\n",
    "        temperature=0,\n",
    "        format=GradeDocuments.model_json_schema())\n",
    "\n",
    "    # Prompt\n",
    "    grade_prompt_template = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Give a binary score 'Yes' or 'No' to indicate whether the document is relevant to the question.\n",
    "\n",
    "question:\n",
    "{user_query}\n",
    "\n",
    "document:\n",
    "{document}\n",
    "\"\"\"\n",
    "    grade_prompt = ChatPromptTemplate.from_template(grade_prompt_template)\n",
    "\n",
    "\n",
    "    user_query = state['user_query']\n",
    "    docs = state['docs']\n",
    "    filtered_docs = []\n",
    "    search = False\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        relevant = (grade_prompt | structured_llm_grader | StrOutputParser()).invoke({\n",
    "            'user_query': user_query,\n",
    "            'document': doc.page_content.strip()\n",
    "        })\n",
    "        if relevant == 'Yes':\n",
    "            filtered_docs.append(doc)\n",
    "            print(f'Doc {i+1} is relevant.')\n",
    "        else:\n",
    "            search = True\n",
    "            print(f'Doc {i+1} is not relevant. Search will be done. Sample:\\n{doc.page_content.strip()[:500]}\\n')\n",
    "    return {\n",
    "        'user_query': user_query, \n",
    "        'docs': docs,\n",
    "        'filtered_docs': filtered_docs,\n",
    "        'search': search\n",
    "    }\n",
    "\n",
    "def decide_to_search(state):\n",
    "    search = state['search']\n",
    "    if search:\n",
    "        return \"rewrite_and_search\"\n",
    "    else:\n",
    "        return \"generate\"\n",
    "\n",
    "def rewrite_and_search(state):\n",
    "    user_query = state['user_query']\n",
    "    docs = state['docs']\n",
    "    filtered_docs = state['filtered_docs']\n",
    "    search = state['search']\n",
    "\n",
    "    # rewrite question for the purposes of search\n",
    "    rewrite_template = \"\"\"You are a master of search engines and are tasked to rewrite a user query for optimum web search.\n",
    "    You are only to respond with the rewritten user query, without any additional instructions or explanations.\n",
    "\n",
    "    Rewrite the following user query:\n",
    "    {user_query}\"\"\"\n",
    "    rewrite_prompt = ChatPromptTemplate.from_template(rewrite_template)\n",
    "\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.2:3b-instruct-q8_0\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    rewritten_question = (rewrite_prompt | llm | StrOutputParser()).invoke({'user_query': user_query})\n",
    "    print(f'Do a websearch here, using the rewritten question: {rewritten_question}')\n",
    "    return {\n",
    "        'user_query': user_query, \n",
    "        'docs': docs,\n",
    "        'filtered_docs': filtered_docs + [Document(page_content='sample websearch results')],\n",
    "        'search': search\n",
    "    }\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    user_query = state['user_query']\n",
    "    docs = state['docs']\n",
    "    filtered_docs = state['filtered_docs']\n",
    "    search = state['search']\n",
    "\n",
    "    gen_template = \"\"\"You are a helpful assistant and your job is to answer a user query given the following context.\n",
    "\n",
    "    context:\n",
    "    {documents}\n",
    "\n",
    "    user query:\n",
    "    {user_query}\"\"\"\n",
    "    gen_prompt = ChatPromptTemplate.from_template(gen_template)\n",
    "\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.2:3b-instruct-q8_0\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    gen_chain = (\n",
    "        {\n",
    "            'documents': itemgetter('documents') | RunnableLambda(lambda docs: '\\n---next document---\\n'.join([d.page_content.strip() for d in docs])),\n",
    "            'user_query': RunnablePassthrough()\n",
    "        }\n",
    "        | gen_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    generation = gen_chain.invoke({'user_query': user_query, 'documents': filtered_docs})\n",
    "\n",
    "    return {\n",
    "        'user_query': user_query, \n",
    "        'docs': docs,\n",
    "        'filtered_docs': filtered_docs + ['sample search result'],\n",
    "        'search': search,\n",
    "        'generation': generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grading\", grading)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"rewrite_and_search\", rewrite_and_search)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grading\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grading\",\n",
    "    decide_to_search,\n",
    "    {\n",
    "        \"rewrite_and_search\": \"rewrite_and_search\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"rewrite_and_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhiyong/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/pydantic/json_schema.py:2279: PydanticJsonSchemaWarning: Default value typing.Literal['Yes', 'No'] is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1 is not relevant. Search will be done. Sample:\n",
      "Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. Iâ€™ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Memory can be defined as the p\n",
      "\n",
      "Doc 2 is not relevant. Search will be done. Sample:\n",
      "They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "Generative Agents Simulati\n",
      "\n",
      "Doc 3 is not relevant. Search will be done. Sample:\n",
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Co\n",
      "\n",
      "Doc 4 is not relevant. Search will be done. Sample:\n",
      "Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Expl\n",
      "\n",
      "Node 'grading':\n",
      "\n",
      "---\n",
      "\n",
      "Do a websearch here, using the rewritten question: What are the different types of artificial agent memory models?\n",
      "Node 'rewrite_and_search':\n",
      "\n",
      "---\n",
      "\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"user_query\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that i should adjust the prompt for rewriting a bit better, so that it doesn't spit out so much unrelated content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can help answer the user's query about the types of agent memory.\n",
      "\n",
      "According to my knowledge, there are several types of memory in artificial intelligence and robotics, which can be categorized into two main types: declarative memory and procedural memory.\n",
      "\n",
      "1. **Declarative Memory**: This type of memory stores factual information, such as knowledge about the world, rules, and concepts. It is also known as \"what\" memory because it answers questions like \"What is this?\" or \"What does this do?\"\n",
      "\n",
      "Examples of declarative memories include:\n",
      "\n",
      "* Knowledge graphs\n",
      "* Ontologies\n",
      "* Expert systems\n",
      "\n",
      "2. **Procedural Memory**: This type of memory stores procedures, skills, and habits. It is also known as \"how\" memory because it answers questions like \"How do I do that?\" or \"What's the best way to do this?\"\n",
      "\n",
      "Examples of procedural memories include:\n",
      "\n",
      "* Neural networks\n",
      "* Rule-based systems\n",
      "* Sensorimotor integration\n",
      "\n",
      "Additionally, there are other types of agent memory, such as:\n",
      "\n",
      "* **Episodic Memory**: Stores specific events and experiences.\n",
      "* **Semantic Memory**: Stores general knowledge and concepts.\n",
      "* **Working Memory**: Temporarily holds information for processing.\n",
      "\n",
      "These types of memories can be used in various applications, including robotics, natural language processing, and decision-making systems.\n",
      "\n",
      "I hope this answers the user's query!\n"
     ]
    }
   ],
   "source": [
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
