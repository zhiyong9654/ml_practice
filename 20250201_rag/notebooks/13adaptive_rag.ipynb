{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb\n",
    "\n",
    "some deviations from the source code because i dont wanna pay for embeddings from openai, or hit openai models. All openAI integration is replaced with ollama.\n",
    "\n",
    "I also removed langsmith integration. don't think it's needed. just a frontend for LLM debugging which i can achieve with `langchain.debug = True`\n",
    "\n",
    "Try to roll my own adaptive RAG for my own use case. I already know how to use langgraph so time to practice. Don't wanna implement the same graph as what's done in the video cause i don't wanna do do a websearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question --> Query Analysis --> Related to documents --> Vectorstore --> Relevant to question? --Yes--> LLM Generation --> Hallucination?\n",
    "                                                                                   | No  \n",
    "                                                                                   v  \n",
    "                                                                                 HyDE  \n",
    "                                                                                   |  \n",
    "                                                                                   v  \n",
    "                                                                              LLM Generation --> Hallucination?  \n",
    "\n",
    "                               Query Analysis --> Anything else --> LLM Generation --> Hallucination?  \n",
    "\n",
    "Hallucination? --Yes--> LLM Generation  \n",
    "                 | No  \n",
    "                 v  \n",
    "          Answer Question? --Yes--> Give LLM response (End)  \n",
    "                          | No  \n",
    "                          v  \n",
    "                   Give default response (End)  \n",
    "\n",
    "\n",
    "\n",
    "The states i'll need:\n",
    "1. question\n",
    "2. documents (if vector store is hit)\n",
    "3. generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        documents: documents\n",
    "        generation: LLM generation\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load docs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding= OllamaEmbeddings(\n",
    "        model=\"nomic-embed-text:v1.5\"\n",
    "    ),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_analysis(state):\n",
    "    # Data model\n",
    "    class web_search(BaseModel):\n",
    "        \"\"\"\n",
    "        The internet. Use web_search for questions that are related to anything else than agents, prompt engineering, and adversarial attacks.\n",
    "        \"\"\"\n",
    "        query: str = Field(description=\"The query to use when searching the internet.\")\n",
    "\n",
    "    class vectorstore(BaseModel):\n",
    "        \"\"\"\n",
    "        A vectorstore containing documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics.\n",
    "        \"\"\"\n",
    "        query: str = Field(description=\"The query to use when searching the vectorstore.\")\n",
    "\n",
    "    question = state['question']\n",
    "    llm_router = ChatOllama(\n",
    "        model=\"command-r7b\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    preamble=\"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "    llm_router.bind_tools(\n",
    "        tools=[web_search, vectorstore], preamble=preamble\n",
    "    )\n",
    "    route = llm_router.invoke(question)\n",
    "\n",
    "    # Route accordingly\n",
    "    if 'tool_calls' not in route.additional_kwargs:\n",
    "        raise \"Router could not decide source\"\n",
    "    if len(route.additional_kwargs[\"tool_calls\"]) == 0:\n",
    "        raise \"Router could not decide source\"\n",
    "    \n",
    "    path = route.additional_kwargs['tool_calls'][0]['function']['name']\n",
    "    if path == 'web_search':\n",
    "        print(\"Not implementing websearch. End here.\")\n",
    "        return\n",
    "    \n",
    "    elif path == 'vectorstore':\n",
    "        print(\"Routing to vectorstore\")\n",
    "        return 'vectorstore'\n",
    "    \n",
    "    else:\n",
    "        print('Route to llm generation')\n",
    "        return 'llm_generation'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
