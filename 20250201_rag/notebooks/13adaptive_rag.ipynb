{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb\n",
    "\n",
    "some deviations from the source code because i dont wanna pay for embeddings from openai, or hit openai models. All openAI integration is replaced with ollama.\n",
    "\n",
    "I also removed langsmith integration. don't think it's needed. just a frontend for LLM debugging which i can achieve with `langchain.debug = True`\n",
    "\n",
    "Try to roll my own adaptive RAG for my own use case. I already know how to use langgraph so time to practice. Don't wanna implement the same graph as what's done in the video cause i don't wanna do do a websearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question --> query_analysis -> Related to documents --> retrieve_and_grade, relevant?  --Yes--> llm_generation --> hallucinate_check\n",
    "                                                                                   | No  \n",
    "                                                                                   v  \n",
    "                                                                                 hyde_retrieval \n",
    "                                                                                   |  \n",
    "                                                                                   v  \n",
    "                                                                              llm_generation --> hallucinate_check  \n",
    "\n",
    "            query_analysis --> Anything else --> llm_generation --> hallucinate_check \n",
    "\n",
    "hallucinate_check --Yes--> llm_generation  \n",
    "                 | No  \n",
    "                 v  \n",
    "          answer_qns_check --Yes--> Give LLM response (End)  \n",
    "                          | No  \n",
    "                          v  \n",
    "                   Give default response (End)  \n",
    "\n",
    "\n",
    "\n",
    "The states i'll need:\n",
    "1. question\n",
    "2. documents (if vector store is hit)\n",
    "3. prompt (store prompt so that we can check prompt and hallucination to see if anything is hallucinated)\n",
    "4. generation\n",
    "5. retries (count the total number of tries that's occurred for hallucination check to prevent too many loops)\n",
    "\n",
    "pts to note:\n",
    "1. query_analysis takes question --llm--> decide 1 of 2 paths (retrieve_and_grade or llm_generation)\n",
    "2. retrieve_and_grade takes question, does retrieval --llm grader--> if there isn't 1 doc that's related, send to Hyde retrieval, else store docs to documents and send to llm_generation\n",
    "3. hyde_retrieval takes question --hyde rewrite--> retrieval --llm grader--> if there isn't 1 doc that's related, send to END, else store docs to documents and send to llm_generation\n",
    "4. llm_generation takes question and docs (if any) and answers the question (store the prompt in state)--> hallucinate_check\n",
    "5. hallucinate_check checks if there's hallucination, by taking in the prompt and generation and grading whether it hallucinates, if yes, and retries < X, send it to llm_generation again (store retries: 1, +=1 etc and set logic ), if yes but retries >= X, overwrite generation with default response and send to END. If no, send to answer_qns_check\n",
    "6. answer_qns_check takes prompt and generation and grade whether it answers the question. If yes, send to END. If no overwrite generation with default response and send to END."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import Literal\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        documents: documents\n",
    "        prompt: prompt for llm generation\n",
    "        generation: LLM generation\n",
    "        tries: number of llm generation attempted\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    prompt: str\n",
    "    generation: str\n",
    "    tries: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Load docs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding= OllamaEmbeddings(\n",
    "        model=\"nomic-embed-text:v1.5\"\n",
    "    ),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOllama(\n",
    "#     model=\"command-r7b\",\n",
    "#     temperature=0.1,\n",
    "# )\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:3b-instruct-q8_0\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_analysis(state):\n",
    "    # Data model\n",
    "    class web_search(BaseModel):\n",
    "        \"\"\"\n",
    "        The internet. Use web_search for questions that are related to anything else than agents, prompt engineering, and adversarial attacks.\n",
    "        \"\"\"\n",
    "        query: str = Field(description=\"The query to use when searching the internet.\")\n",
    "\n",
    "    class vectorstore(BaseModel):\n",
    "        \"\"\"\n",
    "        A vectorstore containing documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics.\n",
    "        \"\"\"\n",
    "        query: str = Field(description=\"The query to use when searching the vectorstore.\")\n",
    "\n",
    "    question = state['question']\n",
    "    llm_router = ChatOllama(\n",
    "        model=\"llama3.1:8b\", # \"command-r7b\", command r7b doesn't support tool calling, refer to debug section below.\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "    llm_router = llm_router.bind_tools(\n",
    "        tools=[web_search, vectorstore] # , preamble=preamble\n",
    "    )\n",
    "    route = llm_router.invoke(question)\n",
    "    print(route.tool_calls)\n",
    "\n",
    "    # Route accordingly\n",
    "    if route.tool_calls == []:\n",
    "        raise ValueError(\"Router could not decide source\")\n",
    "    if len(route.tool_calls) != 1:\n",
    "        raise ValueError(\"Router could not decide routing\")\n",
    "\n",
    "    path = route.tool_calls[0]['name']\n",
    "    if path == 'web_search':\n",
    "        print(\"Not implementing websearch. End here.\")\n",
    "        state['generation'] = 'Should have been a wbsearch.'\n",
    "        return 'END'\n",
    "\n",
    "    elif path == 'vectorstore':\n",
    "        print(\"Routing to retrieve_and_grade\")\n",
    "        return 'retrieve_and_grade'\n",
    "\n",
    "    else:\n",
    "        print('Route to llm generation')\n",
    "        return 'llm_generation'\n",
    "\n",
    "def grader(question, docs):\n",
    "    # Data model\n",
    "    class GradeDocuments(BaseModel):\n",
    "        \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "        binary_score: Literal['Yes', \"No\"]\n",
    "    grader_llm = llm.with_structured_output(GradeDocuments)\n",
    "    # Prompt\n",
    "    grade_prompt_template = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Give a binary score 'Yes' or 'No' to indicate whether the document is relevant to the question.\n",
    "\n",
    "question:\n",
    "{question}\n",
    "\n",
    "document:\n",
    "{document}\n",
    "\"\"\"\n",
    "    grade_prompt = ChatPromptTemplate.from_template(grade_prompt_template)\n",
    "\n",
    "    filtered_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        relevant = (grade_prompt | grader_llm).invoke({\n",
    "            'question': question,\n",
    "            'document': doc.page_content.strip()\n",
    "        })\n",
    "        print(f\"Doc {i+1}, relevant?: {relevant.binary_score}\")\n",
    "        if relevant.binary_score == 'Yes':\n",
    "            filtered_docs.append(doc)\n",
    "    return filtered_docs\n",
    "\n",
    "def retrieve_and_grade(state):\n",
    "    # retrieve_and_grade takes question, does retrieval --llm grader--> if there isn't 1 doc that's related, send to Hyde retrieval, else store docs to documents and send to llm_generation\n",
    "    question = state['question']\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    filtered_docs = grader(question, docs)\n",
    "    return {\n",
    "        'question': question,\n",
    "        'documents': filtered_docs\n",
    "    }\n",
    "\n",
    "def retrieve_and_grade_check(state):\n",
    "    if len(state['documents']) == 0:\n",
    "        print('No relevant documents retrieved, send to HyDE')\n",
    "        return 'hyde_retrieval'\n",
    "    else:\n",
    "        return 'llm_generation'\n",
    "\n",
    "def hyde_retrieval(state):\n",
    "    # hyde_retrieval takes question --hyde rewrite--> retrieval --llm grader--> if there isn't 1 doc that's related, send to END, else store docs to documents and send to llm_generation\n",
    "    question = state['question']\n",
    "    # Prompt\n",
    "    hyde_prompt_template = \"\"\"Please write a paper/document to answer the question:\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "    hyde_prompt = ChatPromptTemplate.from_template(hyde_prompt_template)\n",
    "    hypothetical_doc = (hyde_prompt | llm | StrOutputParser()).invoke({\n",
    "            'question': question,\n",
    "    })\n",
    "    docs = retriever.get_relevant_documents(hypothetical_doc)\n",
    "    filtered_docs = grader(question, docs)\n",
    "    return {\n",
    "        'question': question,\n",
    "        'documents': filtered_docs\n",
    "    }\n",
    "\n",
    "def hyde_retrieval_check(state):\n",
    "    if len(state['documents']) == 0:\n",
    "        print('No relevant documents retrieved from HyDE, send to end')\n",
    "        state['generation'] = \"Sorry LLM could not retrieve any documents that answers your question.\"\n",
    "        return 'END'\n",
    "    else:\n",
    "        return 'llm_generation'\n",
    "    \n",
    "\n",
    "def llm_generation(state):\n",
    "    # llm_generation takes question and docs (if any) and answers the question (store the prompt in state)--> hallucinate_check\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    if 'tries' not in state:\n",
    "        tries = 1\n",
    "    else:\n",
    "        tries = state['tries'] + 1\n",
    "\n",
    "    if len(documents) == 0:\n",
    "        template = \"You are a helpful assistant, answer the following question and do not hallucinate facts.\\n{question}\"\n",
    "        gen_prompt = ChatPromptTemplate.from_template(template)\n",
    "        generation = (\n",
    "            gen_prompt \n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        ).invoke({'question': question})\n",
    "    else:\n",
    "        template = \"You are a helpful RAG system, answer the following question with the given documents as context.\\ndocuments:\\n{documents}\\n--END OF CONTEXT--\\nquestion:{question}\"\n",
    "        gen_prompt = ChatPromptTemplate.from_template(template)\n",
    "        generation = (\n",
    "            {\n",
    "                'question': itemgetter('question'),\n",
    "                'documents': itemgetter('documents') | RunnableLambda(lambda docs: '\\n\\ndocument:\\n'.join([doc.page_content.strip() for doc in docs]))\n",
    "            }\n",
    "            | gen_prompt \n",
    "            | llm \n",
    "            | StrOutputParser()\n",
    "        ).invoke({\n",
    "                'question': question,\n",
    "                'documents': documents\n",
    "        })\n",
    "    \n",
    "        \n",
    "    return {\n",
    "        'question': question,\n",
    "        'documents': documents,\n",
    "        'prompt': gen_prompt,\n",
    "        'generation': generation,\n",
    "        'tries': tries\n",
    "    }\n",
    "\n",
    "def hallucinate_check(state):\n",
    "    # hallucinate_check checks if there's hallucination, by taking in the prompt and generation and grading whether it hallucinates, if yes, and retries < X, send it to llm_generation again (store retries: 1, +=1 etc and set logic ), if yes but retries >= X, overwrite generation with default response and send to END. If no, send to answer_qns_check\n",
    "    class HallucinationCheck(BaseModel):\n",
    "        \"\"\"Binary score for relevance check on whether theres hallucinations.\"\"\"\n",
    "        binary_score: str = Literal['Yes', \"No\"]\n",
    "    hallucinate_llm = llm.with_structured_output(HallucinationCheck)\n",
    "\n",
    "    prompt = state['prompt']\n",
    "    generation = state['generation']\n",
    "    tries = state['tries']\n",
    "\n",
    "    hallucinate_check_prompt_template = \"\"\"You are an LLM hallucination checker. Your job is to evaluate whether the given response to a question has anything non-factual, or made up, containing hallucinations. If there are hallucinations, you should respond 'Yes', and vice versa.\n",
    "\\nquestion:{prompt}\\n\\n{generation}\"\"\"\n",
    "    hallucinate_check_prompt = ChatPromptTemplate.from_template(hallucinate_check_prompt_template)\n",
    "    hallucinate = (hallucinate_check_prompt | hallucinate_llm | StrOutputParser()).invoke({\n",
    "            'prompt': prompt,\n",
    "            'generation': generation\n",
    "    })\n",
    "    if hallucinate == 'Yes' and tries < 2:\n",
    "        print('LLM generation has hallucinations, attempt re-generation, tries: {tries}.')\n",
    "        return 'llm_generation'\n",
    "    elif hallucinate == 'Yes' and tries >= 2:\n",
    "        state['generation'] = 'Sorry LLM could not come up with generations that were hallucination free.'\n",
    "        return 'END'\n",
    "    else:\n",
    "        return 'answer_qns_check'\n",
    "    \n",
    "def answer_question_check(state):\n",
    "    # answer_qns_check takes prompt and generation and grade whether it answers the question. If yes, send to END. If no overwrite generation with default response and send to END.\n",
    "    class AnswerCheck(BaseModel):\n",
    "        \"\"\"Binary score for relevance check on whether the answer, answers the question.\"\"\"\n",
    "        binary_score: str = Literal['Yes', \"No\"]\n",
    "    answer_llm = llm.with_structured_output(AnswerCheck)\n",
    "    answer_check_prompt_template = \"\"\"You are an answer checker. Your job is to evaluate whether the given response to a question answers the question. If the answer answers the question, respond with 'Yes', and vice versa.\n",
    "\\nquestion:{nquestion}\\n\\n{generation}\"\"\"\n",
    "    answer_check_prompt = ChatPromptTemplate.from_template(answer_check_prompt_template)\n",
    "    answered = (answer_check_prompt | answer_llm | StrOutputParser()).invoke({\n",
    "            'prompt': state['prompt'],\n",
    "            'generation': state['generation']\n",
    "    })\n",
    "    if answered == 'No':\n",
    "        state['generation'] = \"Sorry LLM could not come up with generations that answered the question.\"\n",
    "    return 'END'\n",
    "\n",
    "def end(state):\n",
    "    return state['generation']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. query_analysis takes question --llm--> decide 1 of 2 paths (retrieve_and_grade or llm_generation)\n",
    "2. retrieve_and_grade takes question, does retrieval --llm grader--> if there isn't 1 doc that's related, send to Hyde retrieval, else store docs to documents and send to llm_generation\n",
    "3. hyde_retrieval takes question --hyde rewrite--> retrieval --llm grader--> if there isn't 1 doc that's related, send to END, else store docs to documents and send to llm_generation\n",
    "4. llm_generation takes question and docs (if any) and answers the question (store the prompt in state)--> hallucinate_check\n",
    "5. hallucinate_check checks if there's hallucination, by taking in the prompt and generation and grading whether it hallucinates, if yes, and retries < X, send it to llm_generation again (store retries: 1, +=1 etc and set logic ), if yes but retries >= X, overwrite generation with default response and send to END. If no, send to answer_qns_check\n",
    "6. answer_qns_check takes prompt and generation and grade whether it answers the question. If yes, send to END. If no overwrite generation with default response and send to END."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"start\", lambda state: state)  # Dummy node\n",
    "workflow.add_node(\"query_analysis\", query_analysis)  # route to END, \n",
    "workflow.add_node(\"retrieve_and_grade\", retrieve_and_grade)  # grade documents\n",
    "workflow.add_node(\"retrieve_and_grade_check\", retrieve_and_grade_check)  # generatae\n",
    "workflow.add_node(\"hyde_retrieval\", hyde_retrieval)  # generatae\n",
    "workflow.add_node(\"hyde_retrieval_check\", hyde_retrieval_check)  # generatae\n",
    "workflow.add_node(\"llm_generation\", llm_generation)  # transform_query\n",
    "workflow.add_node(\"hallucinate_check\", hallucinate_check)  # transform_query\n",
    "workflow.add_node(\"answer_question_check\", answer_question_check)  # transform_query\n",
    "workflow.add_node(\"END\", end)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"start\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"start\",\n",
    "    query_analysis,\n",
    "    {\n",
    "        \"END\": \"END\",\n",
    "        \"retrieve_and_grade\": \"retrieve_and_grade\",\n",
    "        \"llm_generation\": \"llm_generation\"\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve_and_grade\",\n",
    "    retrieve_and_grade_check,\n",
    "    {\n",
    "        \"hyde_retrieval\": \"hyde_retrieval\",\n",
    "        \"llm_generation\": \"llm_generation\"\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"hyde_retrieval\",\n",
    "    hyde_retrieval_check,\n",
    "    {\n",
    "        \"END\": \"END\",\n",
    "        \"llm_generation\": \"llm_generation\"\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"llm_generation\",\n",
    "    hallucinate_check,\n",
    "    {\n",
    "        \"END\": \"END\",\n",
    "        \"answer_question_check\": \"answer_question_check\"\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"answer_question_check\", \"END\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "# workflow.add_edge(\"retrieve\", \"grading\")\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"grading\",\n",
    "#     decide_to_search,\n",
    "#     {\n",
    "#         \"rewrite_and_search\": \"rewrite_and_search\",\n",
    "#         \"generate\": \"generate\",\n",
    "#     },\n",
    "# )\n",
    "# workflow.add_edge(\"rewrite_and_search\", \"generate\")\n",
    "# workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# # Compile\n",
    "# app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'vectorstore', 'args': {'query': 'types of agent memory'}, 'id': 'ae666ea1-9777-4fb4-b750-f9ad0f3aa2c6', 'type': 'tool_call'}]\n",
      "Routing to retrieve_and_grade\n",
      "Node 'start':\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging tool calling\n",
    "My router keeps answering the question instead of routing to the appropriate tool. The debugging below shows that it's because command r7b doesn't support tool calling on ollama as of 23rd feb 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class web_search(BaseModel):\n",
    "    \"\"\"\n",
    "    The internet. Use web_search for questions that are related to anything else than agents, prompt engineering, and adversarial attacks.\n",
    "    \"\"\"\n",
    "    query: str = Field(description=\"The query to use when searching the internet.\")\n",
    "\n",
    "class vectorstore(BaseModel):\n",
    "    \"\"\"\n",
    "    A vectorstore containing documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics.\n",
    "    \"\"\"\n",
    "    query: str = Field(description=\"The query to use when searching the vectorstore.\")\n",
    "\n",
    "llm_router = ChatOllama(\n",
    "    model=\"command-r7b\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "preamble=\"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "llm_router.bind_tools(\n",
    "    tools=[web_search, vectorstore], preamble=preamble\n",
    ")\n",
    "route = llm_router.invoke('What are the types of agent memory?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Agent memory is a crucial component in artificial intelligence and robotics, enabling agents to store and retrieve information for decision-making and learning. There are several types of memory that agents can utilize, each serving different purposes:\\n\\n1. Short-Term or Working Memory: This type of memory holds temporary information that the agent needs to process or manipulate immediately. It is often used for maintaining a current state, performing calculations, or holding intermediate results during problem-solving tasks. Short-term memory has limited capacity and duration, typically lasting only a few seconds to a minute.\\n\\n2. Long-Term Memory: Long-term memory stores information over extended periods, sometimes even indefinitely. It is responsible for retaining knowledge, experiences, skills, and learned behaviors. Long-term memory can be further categorized into different types:\\n   - Episodic Memory: This type of long-term memory involves the recollection of specific events and experiences in a particular order, along with their associated context and emotions. It allows agents to remember personal experiences and sequences of actions.\\n\\n   - Semantic Memory: Semantic memory stores factual knowledge, concepts, and general understanding about the world. It includes facts, rules, procedures, and abstract ideas that are not tied to specific events or times.\\n\\n   - Procedural Memory: Procedural memory is related to motor skills, habits, and learned behaviors. It enables agents to remember how to perform tasks or follow certain procedures without consciously thinking about each step.\\n\\n3. Sensory Memory: Sensory memory captures and retains information from the agent's sensory inputs for a brief period before it fades away. This type of memory ensures that the agent can process and react to immediate stimuli, such as visual, auditory, or tactile information. Examples include iconic memory (visual) and echoic memory (auditory).\\n\\n4. External Memory: External memory refers to storage systems or databases that agents can access to store and retrieve data. This includes hard drives, solid-state drives, cloud storage, or any other external storage devices used by the agent's system. External memory provides a larger capacity for storing vast amounts of information.\\n\\n5. Short-Term Working Memory: In some contexts, short-term working memory is specifically associated with the immediate holding and manipulation of information during cognitive tasks. It involves the active processing and transformation of data before it is transferred to long-term storage.\\n\\nUnderstanding these types of agent memory is essential for designing intelligent systems that can learn, adapt, and make informed decisions based on their experiences and interactions with the environment.\", additional_kwargs={}, response_metadata={'model': 'command-r7b', 'created_at': '2025-02-23T02:04:23.69322804Z', 'done': True, 'done_reason': 'stop', 'total_duration': 80428789647, 'load_duration': 8427369317, 'prompt_eval_count': 15, 'prompt_eval_duration': 820000000, 'eval_count': 509, 'eval_duration': 71179000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-81b95a15-e057-4237-9ff0-5d692c85a1cc-0', usage_metadata={'input_tokens': 15, 'output_tokens': 509, 'total_tokens': 524})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route\n",
    "# The tool calling doesn't work properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"command-r7b\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "structured_llm_router = llm.bind_tools(\n",
    "    tools=[web_search, vectorstore], #preamble=preamble\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "response = question_router.invoke(\n",
    "    {\"question\": \"Should I use websearch or vectorstore to answer this question: Who will the Bears draft first in the NFL draft?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To answer this question, it would be best to use the 'web_search' tool as it is related to a current event and requires up-to-date information. The 'vectorstore' tool is designed for questions on agents, prompt engineering, and adversarial attacks, which are not relevant to this query.\", additional_kwargs={}, response_metadata={'model': 'command-r7b', 'created_at': '2025-02-23T02:05:38.069165443Z', 'done': True, 'done_reason': 'stop', 'total_duration': 74148531953, 'load_duration': 68528020, 'prompt_eval_count': 1351, 'prompt_eval_duration': 64765000000, 'eval_count': 65, 'eval_duration': 9313000000, 'message': Message(role='assistant', content=\"To answer this question, it would be best to use the 'web_search' tool as it is related to a current event and requires up-to-date information. The 'vectorstore' tool is designed for questions on agents, prompt engineering, and adversarial attacks, which are not relevant to this query.\", images=None, tool_calls=None)}, id='run-baa706d6-1e34-4ccc-a48b-6b9ee045c782-0', usage_metadata={'input_tokens': 1351, 'output_tokens': 65, 'total_tokens': 1416})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response\n",
    "# Still doesn't work, try an example straight from langchain docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "chat = ChatOllama(\n",
    "    model=\"command-r7b\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "class Multiply(BaseModel):\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "ans = chat.invoke(\"What is 45*67\")\n",
    "ans.tool_calls\n",
    "# yeap example from langchain doc doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='llama3.1:8b', created_at='2025-02-23T02:13:25.906459303Z', done=True, done_reason='stop', total_duration=3675175215, load_duration=75125448, prompt_eval_count=179, prompt_eval_duration=281000000, eval_count=24, eval_duration=3317000000, message=Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='add_two_numbers', arguments={'a': 10, 'b': 10}))]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try direct from ollama\n",
    "import ollama\n",
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Add two numbers\n",
    "\n",
    "  Args:\n",
    "    a: The first integer number\n",
    "    b: The second integer number\n",
    "\n",
    "  Returns:\n",
    "    int: The sum of the two numbers\n",
    "  \"\"\"\n",
    "  return a + b\n",
    "\n",
    "\n",
    "response = ollama.chat(\n",
    "  \"llama3.1:8b\",\n",
    "  messages=[{'role': 'user', 'content': 'What is 10 + 10?'}],\n",
    "  tools=[add_two_numbers], # Actual function reference\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='command-r7b', created_at='2025-02-23T02:15:18.467995693Z', done=True, done_reason='stop', total_duration=88410796397, load_duration=28001151421, prompt_eval_count=1259, prompt_eval_duration=60120000000, eval_count=3, eval_duration=284000000, message=Message(role='assistant', content='20', images=None, tool_calls=None))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try direct from ollama\n",
    "import ollama\n",
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Add two numbers\n",
    "\n",
    "  Args:\n",
    "    a: The first integer number\n",
    "    b: The second integer number\n",
    "\n",
    "  Returns:\n",
    "    int: The sum of the two numbers\n",
    "  \"\"\"\n",
    "  return a + b\n",
    "\n",
    "\n",
    "response = ollama.chat(\n",
    "  \"command-r7b\",\n",
    "  messages=[{'role': 'user', 'content': 'What is 10 + 10?'}],\n",
    "  tools=[add_two_numbers], # Actual function reference\n",
    ")\n",
    "response\n",
    "# Okay looks like it's the command-r7b model that doesn't support tool calling. It's tagged as tool currently on ollama page.. so probably some bug there. nvm let's use llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "structured_llm_router = llm.bind_tools(\n",
    "    tools=[web_search, vectorstore], #preamble=preamble\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "response = question_router.invoke(\n",
    "    {\"question\": \"Should I use websearch or vectorstore to answer this question: Who will the Bears draft first in the NFL draft?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-02-23T02:18:03.656131531Z', 'done': True, 'done_reason': 'stop', 'total_duration': 31813874514, 'load_duration': 16094691810, 'prompt_eval_count': 279, 'prompt_eval_duration': 12892000000, 'eval_count': 24, 'eval_duration': 2825000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-0e4acb95-5ce4-4343-82ec-d880d8911d17-0', tool_calls=[{'name': 'vectorstore', 'args': {'query': 'Bears first draft pick in NFL draft'}, 'id': 'aa5948d2-f962-46b9-be83-4da2213e7b92', 'type': 'tool_call'}], usage_metadata={'input_tokens': 279, 'output_tokens': 24, 'total_tokens': 303})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response\n",
    "# confirmed that llama3.1 is ok, but command r doesn't work for tool claling right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
