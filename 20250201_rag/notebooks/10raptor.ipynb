{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb\n",
    "\n",
    "I'll attempt to implement everything myself to get a better idea of how to do raptor, in case there's any edge cases i miss. Will reuse the docs they provide though.\n",
    "\n",
    "Questions have different levels of abstractions. Hence, your documents can be fed in an LLM reccursive to build summarizations (which are higher level abstractions) a certain number of times. These LLM generated abstractions can then be saved as docs back into your vectorstore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document loading part has many broken links, update to use other pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "import re\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "doc_urls = [\n",
    "    'https://scikit-learn.org/stable/modules/linear_model.html',\n",
    "    'https://scikit-learn.org/stable/modules/lda_qda.html',\n",
    "    'https://scikit-learn.org/stable/modules/kernel_ridge.html',\n",
    "    'https://scikit-learn.org/stable/modules/svm.html',\n",
    "    'https://scikit-learn.org/stable/modules/sgd.html',\n",
    "    'https://scikit-learn.org/stable/modules/neighbors.html',\n",
    "    'https://scikit-learn.org/stable/modules/gaussian_process.html',\n",
    "    'https://scikit-learn.org/stable/modules/cross_decomposition.html',\n",
    "    'https://scikit-learn.org/stable/modules/naive_bayes.html',\n",
    "    'https://scikit-learn.org/stable/modules/tree.html',\n",
    "    'https://scikit-learn.org/stable/modules/ensemble.html',\n",
    "    'https://scikit-learn.org/stable/modules/multiclass.html',\n",
    "    'https://scikit-learn.org/stable/modules/feature_selection.html',\n",
    "    'https://scikit-learn.org/stable/modules/semi_supervised.html',\n",
    "    'https://scikit-learn.org/stable/modules/isotonic.html',\n",
    "    'https://scikit-learn.org/stable/modules/calibration.html',\n",
    "    'https://scikit-learn.org/stable/modules/neural_networks_supervised.html',\n",
    "    'https://scikit-learn.org/stable/modules/mixture.html',\n",
    "    'https://scikit-learn.org/stable/modules/manifold.html',\n",
    "    'https://scikit-learn.org/stable/modules/clustering.html',\n",
    "    'https://scikit-learn.org/stable/modules/decomposition.html',\n",
    "    'https://scikit-learn.org/stable/modules/covariance.html',\n",
    "    'https://scikit-learn.org/stable/modules/outlier_detection.html',\n",
    "    'https://scikit-learn.org/stable/modules/density.html',\n",
    "    'https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html',\n",
    "    'https://scikit-learn.org/stable/modules/cross_validation.html',\n",
    "    'https://scikit-learn.org/stable/modules/grid_search.html',\n",
    "    'https://scikit-learn.org/stable/modules/classification_threshold.html',\n",
    "    'https://scikit-learn.org/stable/modules/model_evaluation.html',\n",
    "    'https://scikit-learn.org/stable/modules/learning_curve.html',\n",
    "    'https://scikit-learn.org/stable/modules/partial_dependence.html',\n",
    "    'https://scikit-learn.org/stable/modules/permutation_importance.html',\n",
    "    'https://scikit-learn.org/stable/modules/compose.html',\n",
    "    'https://scikit-learn.org/stable/modules/feature_extraction.html',\n",
    "    'https://scikit-learn.org/stable/modules/preprocessing.html',\n",
    "    'https://scikit-learn.org/stable/modules/impute.html',\n",
    "    'https://scikit-learn.org/stable/modules/unsupervised_reduction.html',\n",
    "    'https://scikit-learn.org/stable/modules/random_projection.html',\n",
    "    'https://scikit-learn.org/stable/modules/kernel_approximation.html'\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for url in doc_urls:\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url, max_depth=3, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "    )\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "def process_doc(doc):\n",
    "    doc = doc.strip()\n",
    "    preamble_match = re.search('^.+User Guide', doc, re.DOTALL)\n",
    "    try:\n",
    "        end_match = re.search('On this page.+$', doc, re.DOTALL)\n",
    "        doc = doc[preamble_match.end():end_match.start()]\n",
    "    except:\n",
    "        doc = doc[preamble_match.end():]\n",
    "    return doc.strip()\n",
    "\n",
    "# Doc texts\n",
    "docs_texts = [process_doc(d.page_content) for d in docs]\n",
    "\n",
    "# Calculate the number of tokens for each document\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(docs_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221859"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(concatenated_content, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "914"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doc texts split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 400\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=100\n",
    ")\n",
    "texts_split = text_splitter.split_text(concatenated_content)\n",
    "len(texts_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raptor's paper suggests the following method of clustering:\n",
    "1. Use UMAP to reduce dimensionality of embedding\n",
    "2. Global cluster using Gaussian mixture model \n",
    "3. Local cluster using Gaussian mixture model\n",
    "\n",
    "Try to do this dev urself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "import numpy as np\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "import langchain \n",
    "langchain.debug = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_clustering(X, K, k, cv_fold=5):\n",
    "    \"\"\"Given a 2d array, where every row is 1 sample point, run GMM clustering on it repeatedly,\n",
    "    searching for the best cluster size, based on BIC.\n",
    "    params:\n",
    "        X (2d array): size m x n, float.\n",
    "        K (int): maximum cluster size to try\n",
    "        k (int): minimum cluster size to try\n",
    "\n",
    "    return:\n",
    "        clusters (1d array): size m, containing the array of 0-k values, where k is the optimum cluster size.\n",
    "    \"\"\"\n",
    "    def gmm_bic_score(estimator, X):\n",
    "        \"\"\"Callable to pass to GridSearchCV that will use the BIC score.\"\"\"\n",
    "        # Make it negative since GridSearchCV expects a score to maximize\n",
    "        return -estimator.bic(X)\n",
    "    \n",
    "    max_cluster_trial = int(min(X.shape[0], K) * (1 - 1 / cv_fold))\n",
    "    \n",
    "    param_grid = {\n",
    "        \"n_components\": range(k, max_cluster_trial),\n",
    "        \"covariance_type\": ['full', 'tied', 'diag', 'spherical']\n",
    "    }\n",
    "    grid_search = GridSearchCV(\n",
    "        GaussianMixture(), param_grid=param_grid, scoring=gmm_bic_score, cv=cv_fold\n",
    "    )\n",
    "    grid_search.fit(X)\n",
    "    print(f\"Selected params for clustering: {grid_search.best_params_}, with BIC score: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raptor(documents, summarization_chain, desired_embedding_size=128, global_max_clusters=5, local_max_clusters=45):\n",
    "    # Does one loop of raptor, returns a list of dic, listing global cluster, local cluster, summary, documents for summary\n",
    "    return_values = []\n",
    "    documents_array = np.array(documents)\n",
    "    # Get embeddings\n",
    "    embed = OllamaEmbeddings(\n",
    "        model=\"nomic-embed-text:v1.5\"\n",
    "    )\n",
    "    full_embeddings = np.array(embed.embed_documents(documents))\n",
    "    # This removes the need for UMAP\n",
    "    compressed_embeddings = full_embeddings[:, :desired_embedding_size]\n",
    "\n",
    "    # Get clusters\n",
    "    global_clusters = gmm_clustering(compressed_embeddings, global_max_clusters, 2)\n",
    "    # Iterate over each global cluster\n",
    "    for global_cluster_index in set(global_clusters):\n",
    "        # Select all documents/embeddings of this global cluster\n",
    "        global_cluster_embeddings = compressed_embeddings[global_clusters==global_cluster_index]\n",
    "        global_cluster_documents = documents_array[global_clusters==global_cluster_index]\n",
    "        print(f\"Global cluster: {global_cluster_index}, has {len(global_cluster_documents)} documents.\")\n",
    "        # recluster into local cluster\n",
    "        local_clusters = gmm_clustering(global_cluster_embeddings, local_max_clusters, 3, cv_fold=3)\n",
    "        for local_cluster_index in set(local_clusters):\n",
    "            # Select all documents of this local cluster\n",
    "            local_cluster_documents = global_cluster_documents[local_clusters==local_cluster_index]\n",
    "            print(f\"Global cluster: {global_cluster_index}, Local cluster: {local_cluster_index}, has {len(local_cluster_documents)} documents.\")\n",
    "            # Hit LLM for summarization\n",
    "            summarization = summarization_chain.invoke(local_cluster_documents)\n",
    "            return_values.append({\n",
    "                'global_cluster': global_cluster_index,\n",
    "                'local_cluster': local_cluster_index,\n",
    "                'summarization': summarization,\n",
    "                'clustered_documents': local_cluster_documents.tolist()\n",
    "            })\n",
    "    return return_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2:3b-instruct-q8_0\", temperature=0)\n",
    "\n",
    "summarization_prompt_template = \"\"\"Your job is to give a detailed summary of the following documents:\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\"\"\"\n",
    "\n",
    "summarization_prompt = ChatPromptTemplate.from_template(summarization_prompt_template)\n",
    "\n",
    "summarization_chain = (\n",
    "    RunnableLambda(lambda docs: '\\n---------new doc---------\\n'.join(docs))\n",
    "    | summarization_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected params for clustering: {'covariance_type': 'spherical', 'n_components': 2}, with BIC score: 212464.15781760923\n",
      "Global cluster: 0, has 449 documents.\n",
      "Selected params for clustering: {'covariance_type': 'spherical', 'n_components': 3}, with BIC score: 170945.4248511313\n",
      "Global cluster: 0, Local cluster: 0, has 176 documents.\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The coefficient estimates for Ordinary Least Squares rely on the\\nindependence of the features. When features are correlated and the\\ncolumns of the design matrix \\\\(X\\\\) have an approximately linear\\ndependence, the design matrix becomes close to singular\\nand as a result, the least-squares estimate becomes highly sensitive\\nto random errors in the observed target, producing a large\\nvariance. This situation of multicollinearity can arise, for\\nexample, when data are collected without an experimental design.\\nExamples\\n\\nOrdinary Least Squares Example\\n\\n\\n1.1.1.1. Non-Negative Least Squares#\\nIt is possible to constrain all the coefficients to be non-negative, which may\\nbe useful when they represent some physical or naturally non-negative\\nquantities (e.g., frequency counts or prices of goods).\\nLinearRegression accepts a boolean positive\\nparameter: when set to True Non-Negative Least Squares are then applied.\\nExamples\\n\\nNon-negative least squares\\n\\n\\n\\n1.1.1.2. Ordinary Least Squares Complexity#\\nThe least squares solution is computed using the singular value\\ndecomposition of X. If X is a matrix of shape (n_samples, n_features)\\nthis method has a cost of\\n\\\\(O(n_{\\\\text{samples}} n_{\\\\text{features}}^2)\\\\), assuming that\\n\\\\(n_{\\\\text{samples}} \\\\geq n_{\\\\text{features}}\\\\).\\n\\n\\n\\n1.1.2. Ridge regression and classification#\\n---------new doc---------\\n1.1.16. Robustness regression: outliers and modeling errors#\\nRobust regression aims to fit a regression model in the\\npresence of corrupt data: either outliers, or error in the model.\\n\\n\\n\\n\\n\\n1.1.16.1. Different scenario and useful concepts#\\nThere are different things to keep in mind when dealing with data\\ncorrupted by outliers:\\n\\nOutliers in X or in y?\\n\\n\\nOutliers in the y direction\\nOutliers in the X direction\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFraction of outliers versus amplitude of error\\nThe number of outlying points matters, but also how much they are\\noutliers.\\n\\n\\nSmall outliers\\nLarge outliers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAn important notion of robust fitting is that of breakdown point: the\\nfraction of data that can be outlying for the fit to start missing the\\ninlying data.\\nNote that in general, robust fitting in high-dimensional setting (large\\nn_features) is very hard. The robust models here will probably not work\\nin these settings.\\n\\nTrade-offs: which estimator ?\\nScikit-learn provides 3 robust regression estimators:\\nRANSAC,\\nTheil Sen and\\nHuberRegressor.\\n---------new doc---------\\nTrade-offs: which estimator ?\\nScikit-learn provides 3 robust regression estimators:\\nRANSAC,\\nTheil Sen and\\nHuberRegressor.\\n\\nHuberRegressor should be faster than\\nRANSAC and Theil Sen\\nunless the number of samples are very large, i.e. n_samples >> n_features.\\nThis is because RANSAC and Theil Sen\\nfit on smaller subsets of the data. However, both Theil Sen\\nand RANSAC are unlikely to be as robust as\\nHuberRegressor for the default parameters.\\nRANSAC is faster than Theil Sen\\nand scales much better with the number of samples.\\nRANSAC will deal better with large\\noutliers in the y direction (most common situation).\\nTheil Sen will cope better with\\nmedium-size outliers in the X direction, but this property will\\ndisappear in high-dimensional settings.\\n\\nWhen in doubt, use RANSAC.\\n---------new doc---------\\nWhen in doubt, use RANSAC.\\n\\n\\n\\n1.1.16.2. RANSAC: RANdom SAmple Consensus#\\nRANSAC (RANdom SAmple Consensus) fits a model from random subsets of\\ninliers from the complete data set.\\nRANSAC is a non-deterministic algorithm producing only a reasonable result with\\na certain probability, which is dependent on the number of iterations (see\\nmax_trials parameter). It is typically used for linear and non-linear\\nregression problems and is especially popular in the field of photogrammetric\\ncomputer vision.\\nThe algorithm splits the complete input sample data into a set of inliers,\\nwhich may be subject to noise, and outliers, which are e.g. caused by erroneous\\nmeasurements or invalid hypotheses about the data. The resulting model is then\\nestimated only from the determined inliers.\\n\\n\\n\\n\\nExamples\\n\\nRobust linear model estimation using RANSAC\\nRobust linear estimator fitting\\n\\n\\n\\nDetails of the algorithm#\\nEach iteration performs the following steps:\\n---------new doc---------\\nExamples\\n\\nRobust linear model estimation using RANSAC\\nRobust linear estimator fitting\\n\\n\\n\\nDetails of the algorithm#\\nEach iteration performs the following steps:\\n\\nSelect min_samples random samples from the original data and check\\nwhether the set of data is valid (see is_data_valid).\\nFit a model to the random subset (estimator.fit) and check\\nwhether the estimated model is valid (see is_model_valid).\\nClassify all data as inliers or outliers by calculating the residuals\\nto the estimated model (estimator.predict(X) - y) - all data\\nsamples with absolute residuals smaller than or equal to the\\nresidual_threshold are considered as inliers.\\nSave fitted model as best model if number of inlier samples is\\nmaximal. In case the current estimated model has the same number of\\ninliers, it is only considered as the best model if it has better score.\\n\\nThese steps are performed either a maximum number of times (max_trials) or\\nuntil one of the special stop criteria are met (see stop_n_inliers and\\nstop_score). The final model is estimated using all inlier samples (consensus\\nset) of the previously determined best model.\\nThe is_data_valid and is_model_valid functions allow to identify and reject\\ndegenerate combinations of random sub-samples. If the estimated model is not\\nneeded for identifying degenerate cases, is_data_valid should be used as it\\nis called prior to fitting the model and thus leading to better computational\\nperformance.\\n\\n\\n\\nReferences#\\n---------new doc---------\\n1.4.1. Classification#\\nSVC, NuSVC and LinearSVC are classes\\ncapable of performing binary and multi-class classification on a dataset.\\n\\n\\n\\n\\nSVC and NuSVC are similar methods, but accept slightly\\ndifferent sets of parameters and have different mathematical formulations (see\\nsection Mathematical formulation). On the other hand,\\nLinearSVC is another (faster) implementation of Support Vector\\nClassification for the case of a linear kernel. It also\\nlacks some of the attributes of SVC and NuSVC, like\\nsupport_. LinearSVC uses squared_hinge loss and due to its\\nimplementation in liblinear it also regularizes the intercept, if considered.\\nThis effect can however be reduced by carefully fine tuning its\\nintercept_scaling parameter, which allows the intercept term to have a\\ndifferent regularization behavior compared to the other features. The\\nclassification results and score can therefore differ from the other two\\nclassifiers.\\nAs other classifiers, SVC, NuSVC and\\nLinearSVC take as input two arrays: an array X of shape\\n(n_samples, n_features) holding the training samples, and an array y of\\nclass labels (strings or integers), of shape (n_samples):\\n>>> from sklearn import svm\\n>>> X = [[0, 0], [1, 1]]\\n>>> y = [0, 1]\\n>>> clf = svm.SVC()\\n>>> clf.fit(X, y)\\nSVC()\\n\\n\\nAfter being fitted, the model can then be used to predict new values:\\n>>> clf.predict([[2., 2.]])\\narray([1])\\n---------new doc---------\\nAfter being fitted, the model can then be used to predict new values:\\n>>> clf.predict([[2., 2.]])\\narray([1])\\n\\n\\nSVMs decision function (detailed in the Mathematical formulation)\\ndepends on some subset of the training data, called the support vectors. Some\\nproperties of these support vectors can be found in attributes\\nsupport_vectors_, support_ and n_support_:\\n>>> # get support vectors\\n>>> clf.support_vectors_\\narray([[0., 0.],\\n       [1., 1.]])\\n>>> # get indices of support vectors\\n>>> clf.support_\\narray([0, 1]...)\\n>>> # get number of support vectors for each class\\n>>> clf.n_support_\\narray([1, 1]...)\\n\\n\\nExamples\\n\\nSVM: Maximum margin separating hyperplane\\nSVM-Anova: SVM with univariate feature selection\\nPlot classification probability\\n---------new doc---------\\nExamples\\n\\nSVM: Maximum margin separating hyperplane\\nSVM-Anova: SVM with univariate feature selection\\nPlot classification probability\\n\\n\\n1.4.1.1. Multi-class classification#\\nSVC and NuSVC implement the “one-versus-one”\\napproach for multi-class classification. In total,\\nn_classes * (n_classes - 1) / 2\\nclassifiers are constructed and each one trains data from two classes.\\nTo provide a consistent interface with other classifiers, the\\ndecision_function_shape option allows to monotonically transform the\\nresults of the “one-versus-one” classifiers to a “one-vs-rest” decision\\nfunction of shape (n_samples, n_classes), which is the default setting\\nof the parameter (default=’ovr’).\\n>>> X = [[0], [1], [2], [3]]\\n>>> Y = [0, 1, 2, 3]\\n>>> clf = svm.SVC(decision_function_shape='ovo')\\n>>> clf.fit(X, Y)\\nSVC(decision_function_shape='ovo')\\n>>> dec = clf.decision_function([[1]])\\n>>> dec.shape[1] # 6 classes: 4*3/2 = 6\\n6\\n>>> clf.decision_function_shape = \\\"ovr\\\"\\n>>> dec = clf.decision_function([[1]])\\n>>> dec.shape[1] # 4 classes\\n4\\n---------new doc---------\\nOn the other hand, LinearSVC implements “one-vs-the-rest”\\nmulti-class strategy, thus training n_classes models.\\n>>> lin_clf = svm.LinearSVC()\\n>>> lin_clf.fit(X, Y)\\nLinearSVC()\\n>>> dec = lin_clf.decision_function([[1]])\\n>>> dec.shape[1]\\n4\\n\\n\\nSee Mathematical formulation for a complete description of\\nthe decision function.\\n---------new doc---------\\nDetails on multi-class strategies#\\nNote that the LinearSVC also implements an alternative multi-class\\nstrategy, the so-called multi-class SVM formulated by Crammer and Singer\\n[16], by using the option multi_class='crammer_singer'. In practice,\\none-vs-rest classification is usually preferred, since the results are mostly\\nsimilar, but the runtime is significantly less.\\nFor “one-vs-rest” LinearSVC the attributes coef_ and intercept_\\nhave the shape (n_classes, n_features) and (n_classes,) respectively.\\nEach row of the coefficients corresponds to one of the n_classes\\n“one-vs-rest” classifiers and similar for the intercepts, in the\\norder of the “one” class.\\nIn the case of “one-vs-one” SVC and NuSVC, the layout of\\nthe attributes is a little more involved. In the case of a linear\\nkernel, the attributes coef_ and intercept_ have the shape\\n(n_classes * (n_classes - 1) / 2, n_features) and (n_classes *\\n(n_classes - 1) / 2) respectively. This is similar to the layout for\\nLinearSVC described above, with each row now corresponding\\nto a binary classifier. The order for classes\\n0 to n is “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . .\\n. “n-1 vs n”.\\nThe shape of dual_coef_ is (n_classes-1, n_SV) with\\n---------new doc---------\\n\\\\(\\\\alpha^{0}_{0,1}\\\\)\\n\\\\(\\\\alpha^{1}_{0,1}\\\\)\\n\\\\(\\\\alpha^{2}_{0,1}\\\\)\\n\\\\(\\\\alpha^{0}_{1,0}\\\\)\\n\\\\(\\\\alpha^{1}_{1,0}\\\\)\\n\\\\(\\\\alpha^{0}_{2,0}\\\\)\\n\\\\(\\\\alpha^{1}_{2,0}\\\\)\\n\\n\\\\(\\\\alpha^{0}_{0,2}\\\\)\\n\\\\(\\\\alpha^{1}_{0,2}\\\\)\\n\\\\(\\\\alpha^{2}_{0,2}\\\\)\\n\\\\(\\\\alpha^{0}_{1,2}\\\\)\\n\\\\(\\\\alpha^{1}_{1,2}\\\\)\\n\\\\(\\\\alpha^{0}_{2,1}\\\\)\\n\\\\(\\\\alpha^{1}_{2,1}\\\\)\\n\\nCoefficients\\nfor SVs of class 0\\nCoefficients\\nfor SVs of class 1\\nCoefficients\\nfor SVs of class 2\\n\\n\\n\\n\\n\\nExamples\\n\\nPlot different SVM classifiers in the iris dataset\\n\\n\\n\\n1.4.1.2. Scores and probabilities#\\nThe decision_function method of SVC and NuSVC gives\\nper-class scores for each sample (or a single score per sample in the binary\\ncase). When the constructor option probability is set to True,\\nclass membership probability estimates (from the methods predict_proba and\\npredict_log_proba) are enabled. In the binary case, the probabilities are\\ncalibrated using Platt scaling [9]: logistic regression on the SVM’s scores,\\nfit by an additional cross-validation on the training data.\\nIn the multiclass case, this is extended as per [10].\\n---------new doc---------\\nNote\\nThe same probability calibration procedure is available for all estimators\\nvia the CalibratedClassifierCV (see\\nProbability calibration). In the case of SVC and NuSVC, this\\nprocedure is builtin in libsvm which is used under the hood, so it does\\nnot rely on scikit-learn’s\\nCalibratedClassifierCV.\\n\\nThe cross-validation involved in Platt scaling\\nis an expensive operation for large datasets.\\nIn addition, the probability estimates may be inconsistent with the scores:\\n\\nthe “argmax” of the scores may not be the argmax of the probabilities\\nin binary classification, a sample may be labeled by predict as\\nbelonging to the positive class even if the output of predict_proba is\\nless than 0.5; and similarly, it could be labeled as negative even if the\\noutput of predict_proba is more than 0.5.\\n\\nPlatt’s method is also known to have theoretical issues.\\nIf confidence scores are required, but these do not have to be probabilities,\\nthen it is advisable to set probability=False\\nand use decision_function instead of predict_proba.\\nPlease note that when decision_function_shape='ovr' and n_classes > 2,\\nunlike decision_function, the predict method does not try to break ties\\nby default. You can set break_ties=True for the output of predict to be\\nthe same as np.argmax(clf.decision_function(...), axis=1), otherwise the\\nfirst class among the tied classes will always be returned; but have in mind\\nthat it comes with a computational cost. See\\nSVM Tie Breaking Example for an example on\\ntie breaking.\\n---------new doc---------\\n1.4.1.3. Unbalanced problems#\\nIn problems where it is desired to give more importance to certain\\nclasses or certain individual samples, the parameters class_weight and\\nsample_weight can be used.\\nSVC (but not NuSVC) implements the parameter\\nclass_weight in the fit method. It’s a dictionary of the form\\n{class_label : value}, where value is a floating point number > 0\\nthat sets the parameter C of class class_label to C * value.\\nThe figure below illustrates the decision boundary of an unbalanced problem,\\nwith and without weight correction.\\n\\n\\n\\n\\nSVC, NuSVC, SVR, NuSVR, LinearSVC,\\nLinearSVR and OneClassSVM implement also weights for\\nindividual samples in the fit method through the sample_weight parameter.\\nSimilar to class_weight, this sets the parameter C for the i-th\\nexample to C * sample_weight[i], which will encourage the classifier to\\nget these samples right. The figure below illustrates the effect of sample\\nweighting on the decision boundary. The size of the circles is proportional\\nto the sample weights:\\n\\n\\n\\n\\nExamples\\n\\nSVM: Separating hyperplane for unbalanced classes\\nSVM: Weighted samples\\n---------new doc---------\\nSee section Preprocessing data for more details on scaling and\\nnormalization.\\n\\n\\n\\nRegarding the shrinking parameter, quoting [12]: We found that if the\\nnumber of iterations is large, then shrinking can shorten the training\\ntime. However, if we loosely solve the optimization problem (e.g., by\\nusing a large stopping tolerance), the code without using shrinking may\\nbe much faster\\nParameter nu in NuSVC/OneClassSVM/NuSVR\\napproximates the fraction of training errors and support vectors.\\nIn SVC, if the data is unbalanced (e.g. many\\npositive and few negative), set class_weight='balanced' and/or try\\ndifferent penalty parameters C.\\nRandomness of the underlying implementations: The underlying\\nimplementations of SVC and NuSVC use a random number\\ngenerator only to shuffle the data for probability estimation (when\\nprobability is set to True). This randomness can be controlled\\nwith the random_state parameter. If probability is set to False\\nthese estimators are not random and random_state has no effect on the\\nresults. The underlying OneClassSVM implementation is similar to\\nthe ones of SVC and NuSVC. As no probability estimation\\nis provided for OneClassSVM, it is not random.\\nThe underlying LinearSVC implementation uses a random number\\ngenerator to select features when fitting the model with a dual coordinate\\ndescent (i.e. when dual is set to True). It is thus not uncommon\\nto have slightly different results for the same input data. If that\\nhappens, try with a smaller tol parameter. This randomness can also be\\ncontrolled with the random_state parameter. When dual is\\nset to False the underlying implementation of LinearSVC is\\nnot random and random_state has no effect on the results.\\n---------new doc---------\\nIn both cases, the criterion is evaluated once by epoch, and the algorithm stops\\nwhen the criterion does not improve n_iter_no_change times in a row. The\\nimprovement is evaluated with absolute tolerance tol, and the algorithm\\nstops in any case after a maximum number of iteration max_iter.\\nSee Early stopping of Stochastic Gradient Descent for an\\nexample of the effects of early stopping.\\n\\n\\n1.5.7. Tips on Practical Use#\\n\\nStochastic Gradient Descent is sensitive to feature scaling, so it\\nis highly recommended to scale your data. For example, scale each\\nattribute on the input vector X to [0,1] or [-1,+1], or standardize\\nit to have mean 0 and variance 1. Note that the same scaling must be\\napplied to the test vector to obtain meaningful results. This can be easily\\ndone using StandardScaler:\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nscaler.fit(X_train)  # Don't cheat - fit only on training data\\nX_train = scaler.transform(X_train)\\nX_test = scaler.transform(X_test)  # apply same transformation to test data\\n\\n# Or better yet: use a pipeline!\\nfrom sklearn.pipeline import make_pipeline\\nest = make_pipeline(StandardScaler(), SGDClassifier())\\nest.fit(X_train)\\nest.predict(X_test)\\n\\n\\nIf your attributes have an intrinsic scale (e.g. word frequencies or\\nindicator features) scaling is not needed.\\n---------new doc---------\\nProbability calculation#\\nThe probability of category \\\\(t\\\\) in feature \\\\(i\\\\) given class\\n\\\\(c\\\\) is estimated as:\\n\\n\\\\[P(x_i = t \\\\mid y = c \\\\: ;\\\\, \\\\alpha) = \\\\frac{ N_{tic} + \\\\alpha}{N_{c} +\\n                                       \\\\alpha n_i},\\\\]\\nwhere \\\\(N_{tic} = |\\\\{j \\\\in J \\\\mid x_{ij} = t, y_j = c\\\\}|\\\\) is the number\\nof times category \\\\(t\\\\) appears in the samples \\\\(x_{i}\\\\), which belong\\nto class \\\\(c\\\\), \\\\(N_{c} = |\\\\{ j \\\\in J\\\\mid y_j = c\\\\}|\\\\) is the number\\nof samples with class c, \\\\(\\\\alpha\\\\) is a smoothing parameter and\\n\\\\(n_i\\\\) is the number of available categories of feature \\\\(i\\\\).\\n\\nCategoricalNB assumes that the sample matrix \\\\(X\\\\) is encoded (for\\ninstance with the help of OrdinalEncoder) such\\nthat all categories for each feature \\\\(i\\\\) are represented with numbers\\n\\\\(0, ..., n_i - 1\\\\) where \\\\(n_i\\\\) is the number of available categories\\nof feature \\\\(i\\\\).\\n---------new doc---------\\nSome advantages of decision trees are:\\n\\nSimple to understand and to interpret. Trees can be visualized.\\nRequires little data preparation. Other techniques often require data\\nnormalization, dummy variables need to be created and blank values to\\nbe removed. Some tree and algorithm combinations support\\nmissing values.\\nThe cost of using the tree (i.e., predicting data) is logarithmic in the\\nnumber of data points used to train the tree.\\nAble to handle both numerical and categorical data. However, the scikit-learn\\nimplementation does not support categorical variables for now. Other\\ntechniques are usually specialized in analyzing datasets that have only one type\\nof variable. See algorithms for more\\ninformation.\\nAble to handle multi-output problems.\\nUses a white box model. If a given situation is observable in a model,\\nthe explanation for the condition is easily explained by boolean logic.\\nBy contrast, in a black box model (e.g., in an artificial neural\\nnetwork), results may be more difficult to interpret.\\nPossible to validate a model using statistical tests. That makes it\\npossible to account for the reliability of the model.\\nPerforms well even if its assumptions are somewhat violated by\\nthe true model from which the data were generated.\\n\\nThe disadvantages of decision trees include:\\n---------new doc---------\\nThe disadvantages of decision trees include:\\n\\nDecision-tree learners can create over-complex trees that do not\\ngeneralize the data well. This is called overfitting. Mechanisms\\nsuch as pruning, setting the minimum number of samples required\\nat a leaf node or setting the maximum depth of the tree are\\nnecessary to avoid this problem.\\nDecision trees can be unstable because small variations in the\\ndata might result in a completely different tree being generated.\\nThis problem is mitigated by using decision trees within an\\nensemble.\\nPredictions of decision trees are neither smooth nor continuous, but\\npiecewise constant approximations as seen in the above figure. Therefore,\\nthey are not good at extrapolation.\\nThe problem of learning an optimal decision tree is known to be\\nNP-complete under several aspects of optimality and even for simple\\nconcepts. Consequently, practical decision-tree learning algorithms\\nare based on heuristic algorithms such as the greedy algorithm where\\nlocally optimal decisions are made at each node. Such algorithms\\ncannot guarantee to return the globally optimal decision tree.  This\\ncan be mitigated by training multiple trees in an ensemble learner,\\nwhere the features and samples are randomly sampled with replacement.\\nThere are concepts that are hard to learn because decision trees\\ndo not express them easily, such as XOR, parity or multiplexer problems.\\nDecision tree learners create biased trees if some classes dominate.\\nIt is therefore recommended to balance the dataset prior to fitting\\nwith the decision tree.\\n---------new doc---------\\n1.10.1. Classification#\\nDecisionTreeClassifier is a class capable of performing multi-class\\nclassification on a dataset.\\nAs with other classifiers, DecisionTreeClassifier takes as input two arrays:\\nan array X, sparse or dense, of shape (n_samples, n_features) holding the\\ntraining samples, and an array Y of integer values, shape (n_samples,),\\nholding the class labels for the training samples:\\n>>> from sklearn import tree\\n>>> X = [[0, 0], [1, 1]]\\n>>> Y = [0, 1]\\n>>> clf = tree.DecisionTreeClassifier()\\n>>> clf = clf.fit(X, Y)\\n\\n\\nAfter being fitted, the model can then be used to predict the class of samples:\\n>>> clf.predict([[2., 2.]])\\narray([1])\\n\\n\\nIn case that there are multiple classes with the same and highest\\nprobability, the classifier will predict the class with the lowest index\\namongst those classes.\\nAs an alternative to outputting a specific class, the probability of each class\\ncan be predicted, which is the fraction of training samples of the class in a\\nleaf:\\n>>> clf.predict_proba([[2., 2.]])\\narray([[0., 1.]])\\n---------new doc---------\\nIn case that there are multiple classes with the same and highest\\nprobability, the classifier will predict the class with the lowest index\\namongst those classes.\\nAs an alternative to outputting a specific class, the probability of each class\\ncan be predicted, which is the fraction of training samples of the class in a\\nleaf:\\n>>> clf.predict_proba([[2., 2.]])\\narray([[0., 1.]])\\n\\n\\nDecisionTreeClassifier is capable of both binary (where the\\nlabels are [-1, 1]) classification and multiclass (where the labels are\\n[0, …, K-1]) classification.\\nUsing the Iris dataset, we can construct a tree as follows:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn import tree\\n>>> iris = load_iris()\\n>>> X, y = iris.data, iris.target\\n>>> clf = tree.DecisionTreeClassifier()\\n>>> clf = clf.fit(X, y)\\n\\n\\nOnce trained, you can plot the tree with the plot_tree function:\\n>>> tree.plot_tree(clf)\\n[...]\\n---------new doc---------\\nAlternatively, the tree can also be exported in textual format with the\\nfunction export_text. This method doesn’t require the installation\\nof external libraries and is more compact:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> from sklearn.tree import export_text\\n>>> iris = load_iris()\\n>>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\\n>>> decision_tree = decision_tree.fit(iris.data, iris.target)\\n>>> r = export_text(decision_tree, feature_names=iris['feature_names'])\\n>>> print(r)\\n|--- petal width (cm) <= 0.80\\n|   |--- class: 0\\n|--- petal width (cm) >  0.80\\n|   |--- petal width (cm) <= 1.75\\n|   |   |--- class: 1\\n|   |--- petal width (cm) >  1.75\\n|   |   |--- class: 2\\n\\n\\n\\nExamples\\n\\nPlot the decision surface of decision trees trained on the iris dataset\\nUnderstanding the decision tree structure\\n\\n\\n\\n1.10.2. Regression#\\n---------new doc---------\\nExamples\\n\\nPlot the decision surface of decision trees trained on the iris dataset\\nUnderstanding the decision tree structure\\n\\n\\n\\n1.10.2. Regression#\\n\\n\\n\\n\\nDecision trees can also be applied to regression problems, using the\\nDecisionTreeRegressor class.\\nAs in the classification setting, the fit method will take as argument arrays X\\nand y, only that in this case y is expected to have floating point values\\ninstead of integer values:\\n>>> from sklearn import tree\\n>>> X = [[0, 0], [2, 2]]\\n>>> y = [0.5, 2.5]\\n>>> clf = tree.DecisionTreeRegressor()\\n>>> clf = clf.fit(X, y)\\n>>> clf.predict([[1, 1]])\\narray([0.5])\\n\\n\\nExamples\\n\\nDecision Tree Regression\\n---------new doc---------\\nExamples\\n\\nDecision Tree Regression\\n\\n\\n\\n1.10.3. Multi-output problems#\\nA multi-output problem is a supervised learning problem with several outputs\\nto predict, that is when Y is a 2d array of shape (n_samples, n_outputs).\\nWhen there is no correlation between the outputs, a very simple way to solve\\nthis kind of problem is to build n independent models, i.e. one for each\\noutput, and then to use those models to independently predict each one of the n\\noutputs. However, because it is likely that the output values related to the\\nsame input are themselves correlated, an often better way is to build a single\\nmodel capable of predicting simultaneously all n outputs. First, it requires\\nlower training time since only a single estimator is built. Second, the\\ngeneralization accuracy of the resulting estimator may often be increased.\\nWith regard to decision trees, this strategy can readily be used to support\\nmulti-output problems. This requires the following changes:\\n\\nStore n output values in leaves, instead of 1;\\nUse splitting criteria that compute the average reduction across all\\nn outputs.\\n\\nThis module offers support for multi-output problems by implementing this\\nstrategy in both DecisionTreeClassifier and\\nDecisionTreeRegressor. If a decision tree is fit on an output array Y\\nof shape (n_samples, n_outputs) then the resulting estimator will:\\n\\nOutput n_output values upon predict;\\nOutput a list of n_output arrays of class probabilities upon\\npredict_proba.\\n\\nThe use of multi-output trees for regression is demonstrated in\\nDecision Tree Regression. In this example, the input\\nX is a single real value and the outputs Y are the sine and cosine of X.\\n---------new doc---------\\nOutput n_output values upon predict;\\nOutput a list of n_output arrays of class probabilities upon\\npredict_proba.\\n\\nThe use of multi-output trees for regression is demonstrated in\\nDecision Tree Regression. In this example, the input\\nX is a single real value and the outputs Y are the sine and cosine of X.\\n\\n\\n\\n\\nThe use of multi-output trees for classification is demonstrated in\\nFace completion with a multi-output estimators. In this example, the inputs\\nX are the pixels of the upper half of faces and the outputs Y are the pixels of\\nthe lower half of those faces.\\n\\n\\n\\n\\nExamples\\n\\nFace completion with a multi-output estimators\\n\\nReferences\\n\\nM. Dumont et al,  Fast multi-class image annotation with random subwindows\\nand multiple output randomized trees,\\nInternational Conference on Computer Vision Theory and Applications 2009\\n---------new doc---------\\n1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART#\\nWhat are all the various decision tree algorithms and how do they differ\\nfrom each other? Which one is implemented in scikit-learn?\\n\\n\\nVarious decision tree algorithms#\\nID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan.\\nThe algorithm creates a multiway tree, finding for each node (i.e. in\\na greedy manner) the categorical feature that will yield the largest\\ninformation gain for categorical targets. Trees are grown to their\\nmaximum size and then a pruning step is usually applied to improve the\\nability of the tree to generalize to unseen data.\\nC4.5 is the successor to ID3 and removed the restriction that features\\nmust be categorical by dynamically defining a discrete attribute (based\\non numerical variables) that partitions the continuous attribute value\\ninto a discrete set of intervals. C4.5 converts the trained trees\\n(i.e. the output of the ID3 algorithm) into sets of if-then rules.\\nThe accuracy of each rule is then evaluated to determine the order\\nin which they should be applied. Pruning is done by removing a rule’s\\nprecondition if the accuracy of the rule improves without it.\\nC5.0 is Quinlan’s latest version release under a proprietary license.\\nIt uses less memory and builds smaller rulesets than C4.5 while being\\nmore accurate.\\nCART (Classification and Regression Trees) is very similar to C4.5, but\\nit differs in that it supports numerical target variables (regression) and\\ndoes not compute rule sets. CART constructs binary trees using the feature\\nand threshold that yield the largest information gain at each node.\\n---------new doc---------\\nscikit-learn uses an optimized version of the CART algorithm; however, the\\nscikit-learn implementation does not support categorical variables for now.\\n\\n\\n1.10.7. Mathematical formulation#\\nGiven training vectors \\\\(x_i \\\\in R^n\\\\), i=1,…, l and a label vector\\n\\\\(y \\\\in R^l\\\\), a decision tree recursively partitions the feature space\\nsuch that the samples with the same labels or similar target values are grouped\\ntogether.\\nLet the data at node \\\\(m\\\\) be represented by \\\\(Q_m\\\\) with \\\\(n_m\\\\)\\nsamples. For each candidate split \\\\(\\\\theta = (j, t_m)\\\\) consisting of a\\nfeature \\\\(j\\\\) and threshold \\\\(t_m\\\\), partition the data into\\n\\\\(Q_m^{left}(\\\\theta)\\\\) and \\\\(Q_m^{right}(\\\\theta)\\\\) subsets\\n\\n\\\\[ \\\\begin{align}\\\\begin{aligned}Q_m^{left}(\\\\theta) = \\\\{(x, y) | x_j \\\\leq t_m\\\\}\\\\\\\\Q_m^{right}(\\\\theta) = Q_m \\\\setminus Q_m^{left}(\\\\theta)\\\\end{aligned}\\\\end{align} \\\\]\\nThe quality of a candidate split of node \\\\(m\\\\) is then computed using an\\nimpurity function or loss function \\\\(H()\\\\), the choice of which depends on\\nthe task being solved (classification or regression)\\n---------new doc---------\\n1.10.8. Missing Values Support#\\nDecisionTreeClassifier, DecisionTreeRegressor\\nhave built-in support for missing values using splitter='best', where\\nthe splits are determined in a greedy fashion.\\nExtraTreeClassifier, and ExtraTreeRegressor have built-in\\nsupport for missing values for splitter='random', where the splits\\nare determined randomly. For more details on how the splitter differs on\\nnon-missing values, see the Forest section.\\nThe criterion supported when there are missing-values are\\n'gini', 'entropy’, or 'log_loss', for classification or\\n'squared_error', 'friedman_mse', or 'poisson' for regression.\\nFirst we will describe how DecisionTreeClassifier, DecisionTreeRegressor\\nhandle missing-values in the data.\\nFor each potential threshold on the non-missing data, the splitter will evaluate\\nthe split with all the missing values going to the left node or the right node.\\nDecisions are made as follows:\\n\\nBy default when predicting, the samples with missing values are classified\\nwith the class used in the split found during training:\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n>>> tree.predict(X)\\narray([0, 0, 1, 1])\\n---------new doc---------\\n>>> X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n>>> tree.predict(X)\\narray([0, 0, 1, 1])\\n\\n\\n\\nIf the criterion evaluation is the same for both nodes,\\nthen the tie for missing value at predict time is broken by going to the\\nright node. The splitter also checks the split where all the missing\\nvalues go to one child and non-missing values go to the other:\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n\\n>>> X_test = np.array([np.nan]).reshape(-1, 1)\\n>>> tree.predict(X_test)\\narray([1])\\n\\n\\n\\nIf no missing values are seen during training for a given feature, then during\\nprediction missing values are mapped to the child with the most samples:\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([0, 1, 2, 3]).reshape(-1, 1)\\n>>> y = [0, 1, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n---------new doc---------\\n>>> X = np.array([0, 1, 2, 3]).reshape(-1, 1)\\n>>> y = [0, 1, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n\\n>>> X_test = np.array([np.nan]).reshape(-1, 1)\\n>>> tree.predict(X_test)\\narray([1])\\n\\n\\n\\n\\nExtraTreeClassifier, and ExtraTreeRegressor handle missing values\\nin a slightly different way. When splitting a node, a random threshold will be chosen\\nto split the non-missing values on. Then the non-missing values will be sent to the\\nleft and right child based on the randomly selected threshold, while the missing\\nvalues will also be randomly sent to the left or right child. This is repeated for\\nevery feature considered at each split. The best split among these is chosen.\\nDuring prediction, the treatment of missing-values is the same as that of the\\ndecision tree:\\n\\nBy default when predicting, the samples with missing values are classified\\nwith the class used in the split found during training.\\nIf no missing values are seen during training for a given feature, then during\\nprediction missing values are mapped to the child with the most samples.\\n\\n\\n\\n1.10.9. Minimal Cost-Complexity Pruning#\\nMinimal cost-complexity pruning is an algorithm used to prune a tree to avoid\\nover-fitting, described in Chapter 3 of [BRE]. This algorithm is parameterized\\nby \\\\(\\\\alpha\\\\ge0\\\\) known as the complexity parameter. The complexity\\nparameter is used to define the cost-complexity measure, \\\\(R_\\\\alpha(T)\\\\) of\\na given tree \\\\(T\\\\):\\n---------new doc---------\\n1.11.1. Gradient-boosted trees#\\nGradient Tree Boosting\\nor Gradient Boosted Decision Trees (GBDT) is a generalization\\nof boosting to arbitrary differentiable loss functions, see the seminal work of\\n[Friedman2001]. GBDT is an excellent model for both regression and\\nclassification, in particular for tabular data.\\n\\nGradientBoostingClassifier vs HistGradientBoostingClassifier\\nScikit-learn provides two implementations of gradient-boosted trees:\\nHistGradientBoostingClassifier vs\\nGradientBoostingClassifier for classification, and the\\ncorresponding classes for regression. The former can be orders of\\nmagnitude faster than the latter when the number of samples is\\nlarger than tens of thousands of samples.\\nMissing values and categorical data are natively supported by the\\nHist… version, removing the need for additional preprocessing such as\\nimputation.\\nGradientBoostingClassifier and\\nGradientBoostingRegressor, might be preferred for small sample\\nsizes since binning may lead to split points that are too approximate\\nin this setting.\\n---------new doc---------\\n1.11.1.1. Histogram-Based Gradient Boosting#\\nScikit-learn 0.21 introduced two new implementations of\\ngradient boosted trees, namely HistGradientBoostingClassifier\\nand HistGradientBoostingRegressor, inspired by\\nLightGBM (See [LightGBM]).\\nThese histogram-based estimators can be orders of magnitude faster\\nthan GradientBoostingClassifier and\\nGradientBoostingRegressor when the number of samples is larger\\nthan tens of thousands of samples.\\nThey also have built-in support for missing values, which avoids the need\\nfor an imputer.\\nThese fast estimators first bin the input samples X into\\ninteger-valued bins (typically 256 bins) which tremendously reduces the\\nnumber of splitting points to consider, and allows the algorithm to\\nleverage integer-based data structures (histograms) instead of relying on\\nsorted continuous values when building the trees. The API of these\\nestimators is slightly different, and some of the features from\\nGradientBoostingClassifier and GradientBoostingRegressor\\nare not yet supported, for instance some loss functions.\\nExamples\\n\\nPartial Dependence and Individual Conditional Expectation Plots\\nComparing Random Forests and Histogram Gradient Boosting models\\n\\n\\n1.11.1.1.1. Usage#\\nMost of the parameters are unchanged from\\nGradientBoostingClassifier and GradientBoostingRegressor.\\nOne exception is the max_iter parameter that replaces n_estimators, and\\ncontrols the number of iterations of the boosting process:\\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\\n>>> from sklearn.datasets import make_hastie_10_2\\n---------new doc---------\\nNote that early-stopping is enabled by default if the number of samples is\\nlarger than 10,000. The early-stopping behaviour is controlled via the\\nearly_stopping, scoring, validation_fraction,\\nn_iter_no_change, and tol parameters. It is possible to early-stop\\nusing an arbitrary scorer, or just the training or validation loss.\\nNote that for technical reasons, using a callable as a scorer is significantly slower\\nthan using the loss. By default, early-stopping is performed if there are at least\\n10,000 samples in the training set, using the validation loss.\\n\\n\\n1.11.1.1.2. Missing values support#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor have built-in support for missing\\nvalues (NaNs).\\nDuring training, the tree grower learns at each split point whether samples\\nwith missing values should go to the left or right child, based on the\\npotential gain. When predicting, samples with missing values are assigned to\\nthe left or right child consequently:\\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\\n>>> gbdt.predict(X)\\narray([0, 0, 1, 1])\\n---------new doc---------\\n>>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\\n>>> gbdt.predict(X)\\narray([0, 0, 1, 1])\\n\\n\\nWhen the missingness pattern is predictive, the splits can be performed on\\nwhether the feature value is missing or not:\\n>>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)\\n>>> y = [0, 1, 0, 0, 1]\\n>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,\\n...                                       max_depth=2,\\n...                                       learning_rate=1,\\n...                                       max_iter=1).fit(X, y)\\n>>> gbdt.predict(X)\\narray([0, 1, 0, 0, 1])\\n\\n\\nIf no missing values were encountered for a given feature during training,\\nthen samples with missing values are mapped to whichever child has the most\\nsamples.\\nExamples\\n\\nFeatures in Histogram Gradient Boosting Trees\\n---------new doc---------\\nIf no missing values were encountered for a given feature during training,\\nthen samples with missing values are mapped to whichever child has the most\\nsamples.\\nExamples\\n\\nFeatures in Histogram Gradient Boosting Trees\\n\\n\\n\\n1.11.1.1.3. Sample weight support#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor support sample weights during\\nfit.\\nThe following toy example demonstrates that samples with a sample weight of zero are ignored:\\n>>> X = [[1, 0],\\n...      [1, 0],\\n...      [1, 0],\\n...      [0, 1]]\\n>>> y = [0, 0, 1, 0]\\n>>> # ignore the first 2 training samples by setting their weight to 0\\n>>> sample_weight = [0, 0, 1, 1]\\n>>> gb = HistGradientBoostingClassifier(min_samples_leaf=1)\\n>>> gb.fit(X, y, sample_weight=sample_weight)\\nHistGradientBoostingClassifier(...)\\n>>> gb.predict([[1, 0]])\\narray([1])\\n>>> gb.predict_proba([[1, 0]])[0, 1]\\n0.99...\\n\\n\\nAs you can see, the [1, 0] is comfortably classified as 1 since the first\\ntwo samples are ignored due to their sample weights.\\nImplementation detail: taking sample weights into account amounts to\\nmultiplying the gradients (and the hessians) by the sample weights. Note that\\nthe binning stage (specifically the quantiles computation) does not take the\\nweights into account.\\n---------new doc---------\\nSplit finding with categorical features#\\nThe canonical way of considering categorical splits in a tree is to consider\\nall of the \\\\(2^{K - 1} - 1\\\\) partitions, where \\\\(K\\\\) is the number of\\ncategories. This can quickly become prohibitive when \\\\(K\\\\) is large.\\nFortunately, since gradient boosting trees are always regression trees (even\\nfor classification problems), there exist a faster strategy that can yield\\nequivalent splits. First, the categories of a feature are sorted according to\\nthe variance of the target, for each category k. Once the categories are\\nsorted, one can consider continuous partitions, i.e. treat the categories\\nas if they were ordered continuous values (see Fisher [Fisher1958] for a\\nformal proof). As a result, only \\\\(K - 1\\\\) splits need to be considered\\ninstead of \\\\(2^{K - 1} - 1\\\\). The initial sorting is a\\n\\\\(\\\\mathcal{O}(K \\\\log(K))\\\\) operation, leading to a total complexity of\\n\\\\(\\\\mathcal{O}(K \\\\log(K) + K)\\\\), instead of \\\\(\\\\mathcal{O}(2^K)\\\\).\\n\\nExamples\\n\\nCategorical Feature Support in Gradient Boosting\\n\\n\\n\\n1.11.1.1.5. Monotonic Constraints#\\nDepending on the problem at hand, you may have prior knowledge indicating\\nthat a given feature should in general have a positive (or negative) effect\\non the target value. For example, all else being equal, a higher credit\\nscore should increase the probability of getting approved for a loan.\\nMonotonic constraints allow you to incorporate such prior knowledge into the\\nmodel.\\nFor a predictor \\\\(F\\\\) with two features:\\n\\na monotonic increase constraint is a constraint of the form:\\n---------new doc---------\\n1.11.1.1.8. Why it’s faster#\\nThe bottleneck of a gradient boosting procedure is building the decision\\ntrees. Building a traditional decision tree (as in the other GBDTs\\nGradientBoostingClassifier and GradientBoostingRegressor)\\nrequires sorting the samples at each node (for\\neach feature). Sorting is needed so that the potential gain of a split point\\ncan be computed efficiently. Splitting a single node has thus a complexity\\nof \\\\(\\\\mathcal{O}(n_\\\\text{features} \\\\times n \\\\log(n))\\\\) where \\\\(n\\\\)\\nis the number of samples at the node.\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor, in contrast, do not require sorting the\\nfeature values and instead use a data-structure called a histogram, where the\\nsamples are implicitly ordered. Building a histogram has a\\n\\\\(\\\\mathcal{O}(n)\\\\) complexity, so the node splitting procedure has a\\n\\\\(\\\\mathcal{O}(n_\\\\text{features} \\\\times n)\\\\) complexity, much smaller\\nthan the previous one. In addition, instead of considering \\\\(n\\\\) split\\npoints, we consider only max_bins split points, which might be much\\nsmaller.\\nIn order to build histograms, the input data X needs to be binned into\\ninteger-valued bins. This binning procedure does require sorting the feature\\nvalues, but it only happens once at the very beginning of the boosting process\\n(not at each node, like in GradientBoostingClassifier and\\nGradientBoostingRegressor).\\nFinally, many parts of the implementation of\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor are parallelized.\\nReferences\\n---------new doc---------\\n[XGBoost]\\n(1,2,3)\\nTianqi Chen, Carlos Guestrin, “XGBoost: A Scalable Tree\\nBoosting System”\\n\\n\\n[LightGBM]\\nKe et. al. “LightGBM: A Highly Efficient Gradient\\nBoostingDecision Tree”\\n\\n\\n[Fisher1958]\\nFisher, W.D. (1958). “On Grouping for Maximum Homogeneity”\\nJournal of the American Statistical Association, 53, 789-798.\\n\\n\\n\\n\\n\\n1.11.1.2. GradientBoostingClassifier and GradientBoostingRegressor#\\nThe usage and the parameters of GradientBoostingClassifier and\\nGradientBoostingRegressor are described below. The 2 most important\\nparameters of these estimators are n_estimators and learning_rate.\\n\\n\\nClassification#\\nGradientBoostingClassifier supports both binary and multi-class\\nclassification.\\nThe following example shows how to fit a gradient boosting classifier\\nwith 100 decision stumps as weak learners:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> X_train, X_test = X[:2000], X[2000:]\\n>>> y_train, y_test = y[:2000], y[2000:]\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> X_train, X_test = X[:2000], X[2000:]\\n>>> y_train, y_test = y[:2000], y[2000:]\\n\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.913...\\n\\n\\nThe number of weak learners (i.e. regression trees) is controlled by the\\nparameter n_estimators; The size of each tree can be controlled either by setting the tree\\ndepth via max_depth or by setting the number of leaf nodes via\\nmax_leaf_nodes. The learning_rate is a hyper-parameter in the range\\n(0.0, 1.0] that controls overfitting via shrinkage .\\n\\nNote\\nClassification with more than 2 classes requires the induction\\nof n_classes regression trees at each iteration,\\nthus, the total number of induced trees equals\\nn_classes * n_estimators. For datasets with a large number\\nof classes we strongly recommend to use\\nHistGradientBoostingClassifier as an alternative to\\nGradientBoostingClassifier .\\n---------new doc---------\\nNote\\nClassification with more than 2 classes requires the induction\\nof n_classes regression trees at each iteration,\\nthus, the total number of induced trees equals\\nn_classes * n_estimators. For datasets with a large number\\nof classes we strongly recommend to use\\nHistGradientBoostingClassifier as an alternative to\\nGradientBoostingClassifier .\\n\\n\\n\\n\\nRegression#\\nGradientBoostingRegressor supports a number of\\ndifferent loss functions\\nfor regression which can be specified via the argument\\nloss; the default loss function for regression is squared error\\n('squared_error').\\n>>> import numpy as np\\n>>> from sklearn.metrics import mean_squared_error\\n>>> from sklearn.datasets import make_friedman1\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n\\n>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\\n>>> X_train, X_test = X[:200], X[200:]\\n>>> y_train, y_test = y[:200], y[200:]\\n>>> est = GradientBoostingRegressor(\\n...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\\n...     loss='squared_error'\\n... ).fit(X_train, y_train)\\n>>> mean_squared_error(y_test, est.predict(X_test))\\n5.00...\\n---------new doc---------\\nThe figure below shows the results of applying GradientBoostingRegressor\\nwith least squares loss and 500 base learners to the diabetes dataset\\n(sklearn.datasets.load_diabetes).\\nThe plot shows the train and test error at each iteration.\\nThe train error at each iteration is stored in the\\ntrain_score_ attribute of the gradient boosting model.\\nThe test error at each iterations can be obtained\\nvia the staged_predict method which returns a\\ngenerator that yields the predictions at each stage. Plots like these can be used\\nto determine the optimal number of trees (i.e. n_estimators) by early stopping.\\n\\n\\n\\n\\n\\nExamples\\n\\nGradient Boosting regression\\nGradient Boosting Out-of-Bag estimates\\n\\n\\n1.11.1.2.1. Fitting additional weak-learners#\\nBoth GradientBoostingRegressor and GradientBoostingClassifier\\nsupport warm_start=True which allows you to add more estimators to an already\\nfitted model.\\n>>> import numpy as np\\n>>> from sklearn.metrics import mean_squared_error\\n>>> from sklearn.datasets import make_friedman1\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n---------new doc---------\\n>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\\n>>> X_train, X_test = X[:200], X[200:]\\n>>> y_train, y_test = y[:200], y[200:]\\n>>> est = GradientBoostingRegressor(\\n...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\\n...     loss='squared_error'\\n... )\\n>>> est = est.fit(X_train, y_train)  # fit with 100 trees\\n>>> mean_squared_error(y_test, est.predict(X_test))\\n5.00...\\n>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and increase num of trees\\n>>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est\\n>>> mean_squared_error(y_test, est.predict(X_test))\\n3.84...\\n---------new doc---------\\nGradient Boosting regularization\\nGradient Boosting Out-of-Bag estimates\\nOOB Errors for Random Forests\\n\\n\\n\\n1.11.1.2.7. Interpretation with feature importance#\\nIndividual decision trees can be interpreted easily by simply\\nvisualizing the tree structure. Gradient boosting models, however,\\ncomprise hundreds of regression trees thus they cannot be easily\\ninterpreted by visual inspection of the individual trees. Fortunately,\\na number of techniques have been proposed to summarize and interpret\\ngradient boosting models.\\nOften features do not contribute equally to predict the target\\nresponse; in many situations the majority of the features are in fact\\nirrelevant.\\nWhen interpreting a model, the first question usually is: what are\\nthose important features and how do they contributing in predicting\\nthe target response?\\nIndividual decision trees intrinsically perform feature selection by selecting\\nappropriate split points. This information can be used to measure the\\nimportance of each feature; the basic idea is: the more often a\\nfeature is used in the split points of a tree the more important that\\nfeature is. This notion of importance can be extended to decision tree\\nensembles by simply averaging the impurity-based feature importance of each tree (see\\nFeature importance evaluation for more details).\\nThe feature importance scores of a fit gradient boosting model can be\\naccessed via the feature_importances_ property:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\n>>> clf.feature_importances_\\narray([0.10..., 0.10..., 0.11..., ...\\n\\n\\nNote that this computation of feature importance is based on entropy, and it\\nis distinct from sklearn.inspection.permutation_importance which is\\nbased on permutation of the features.\\nExamples\\n\\nGradient Boosting regression\\n\\nReferences\\n\\n\\n[Friedman2001]\\n(1,2,3,4)\\nFriedman, J.H. (2001). Greedy function approximation: A gradient\\nboosting machine.\\nAnnals of Statistics, 29, 1189-1232.\\n\\n\\n[Friedman2002]\\nFriedman, J.H. (2002). Stochastic gradient boosting..\\nComputational Statistics & Data Analysis, 38, 367-378.\\n\\n\\n[R2007]\\nG. Ridgeway (2006). Generalized Boosted Models: A guide to the gbm\\npackage\\n---------new doc---------\\nLike decision trees, forests of trees also extend to\\nmulti-output problems  (if Y is an array\\nof shape (n_samples, n_outputs)).\\n\\n1.11.2.1. Random Forests#\\nIn random forests (see RandomForestClassifier and\\nRandomForestRegressor classes), each tree in the ensemble is built\\nfrom a sample drawn with replacement (i.e., a bootstrap sample) from the\\ntraining set.\\nFurthermore, when splitting each node during the construction of a tree, the\\nbest split is found through an exhaustive search of the features values of\\neither all input features or a random subset of size max_features.\\n(See the parameter tuning guidelines for more details.)\\nThe purpose of these two sources of randomness is to decrease the variance of\\nthe forest estimator. Indeed, individual decision trees typically exhibit high\\nvariance and tend to overfit. The injected randomness in forests yield decision\\ntrees with somewhat decoupled prediction errors. By taking an average of those\\npredictions, some errors can cancel out. Random forests achieve a reduced\\nvariance by combining diverse trees, sometimes at the cost of a slight increase\\nin bias. In practice the variance reduction is often significant hence yielding\\nan overall better model.\\nIn contrast to the original publication [B2001], the scikit-learn\\nimplementation combines classifiers by averaging their probabilistic\\nprediction, instead of letting each classifier vote for a single class.\\nA competitive alternative to random forests are\\nHistogram-Based Gradient Boosting (HGBT) models:\\n---------new doc---------\\n>>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\\n...     random_state=0)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean()\\n0.98...\\n\\n>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,\\n...     min_samples_split=2, random_state=0)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean()\\n0.999...\\n\\n>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\\n...     min_samples_split=2, random_state=0)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean() > 0.999\\nTrue\\n---------new doc---------\\n1.11.2.3. Parameters#\\nThe main parameters to adjust when using these methods is n_estimators and\\nmax_features. The former is the number of trees in the forest. The larger\\nthe better, but also the longer it will take to compute. In addition, note that\\nresults will stop getting significantly better beyond a critical number of\\ntrees. The latter is the size of the random subsets of features to consider\\nwhen splitting a node. The lower the greater the reduction of variance, but\\nalso the greater the increase in bias. Empirical good default values are\\nmax_features=1.0 or equivalently max_features=None (always considering\\nall features instead of a random subset) for regression problems, and\\nmax_features=\\\"sqrt\\\" (using a random subset of size sqrt(n_features))\\nfor classification tasks (where n_features is the number of features in\\nthe data). The default value of max_features=1.0 is equivalent to bagged\\ntrees and more randomness can be achieved by setting smaller values (e.g. 0.3\\nis a typical default in the literature). Good results are often achieved when\\nsetting max_depth=None in combination with min_samples_split=2 (i.e.,\\nwhen fully developing the trees). Bear in mind though that these values are\\nusually not optimal, and might result in models that consume a lot of RAM.\\nThe best parameter values should always be cross-validated. In addition, note\\nthat in random forests, bootstrap samples are used by default\\n(bootstrap=True) while the default strategy for extra-trees is to use the\\nwhole dataset (bootstrap=False). When using bootstrap sampling the\\ngeneralization error can be estimated on the left out or out-of-bag samples.\\n---------new doc---------\\nThe best parameter values should always be cross-validated. In addition, note\\nthat in random forests, bootstrap samples are used by default\\n(bootstrap=True) while the default strategy for extra-trees is to use the\\nwhole dataset (bootstrap=False). When using bootstrap sampling the\\ngeneralization error can be estimated on the left out or out-of-bag samples.\\nThis can be enabled by setting oob_score=True.\\n---------new doc---------\\nBreiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.\\n\\n\\n\\n[B1998]\\n\\nBreiman, “Arcing Classifiers”, Annals of Statistics 1998.\\n\\n\\n\\n\\nP. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized\\ntrees”, Machine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n1.11.2.5. Feature importance evaluation#\\nThe relative rank (i.e. depth) of a feature used as a decision node in a\\ntree can be used to assess the relative importance of that feature with\\nrespect to the predictability of the target variable. Features used at\\nthe top of the tree contribute to the final prediction decision of a\\nlarger fraction of the input samples. The expected fraction of the\\nsamples they contribute to can thus be used as an estimate of the\\nrelative importance of the features. In scikit-learn, the fraction of\\nsamples a feature contributes to is combined with the decrease in impurity\\nfrom splitting them to create a normalized estimate of the predictive power\\nof that feature.\\nBy averaging the estimates of predictive ability over several randomized\\ntrees one can reduce the variance of such an estimate and use it\\nfor feature selection. This is known as the mean decrease in impurity, or MDI.\\nRefer to [L2014] for more information on MDI and feature importance\\nevaluation with Random Forests.\\n---------new doc---------\\nWarning\\nThe impurity-based feature importances computed on tree-based models suffer\\nfrom two flaws that can lead to misleading conclusions. First they are\\ncomputed on statistics derived from the training dataset and therefore do\\nnot necessarily inform us on which features are most important to make good\\npredictions on held-out dataset. Secondly, they favor high cardinality\\nfeatures, that is features with many unique values.\\nPermutation feature importance is an alternative to impurity-based feature\\nimportance that does not suffer from these flaws. These two methods of\\nobtaining feature importance are explored in:\\nPermutation Importance vs Random Forest Feature Importance (MDI).\\n\\nIn practice those estimates are stored as an attribute named\\nfeature_importances_ on the fitted model. This is an array with shape\\n(n_features,) whose values are positive and sum to 1.0. The higher\\nthe value, the more important is the contribution of the matching feature\\nto the prediction function.\\nExamples\\n\\nFeature importances with a forest of trees\\n\\nReferences\\n\\n\\n[L2014]\\nG. Louppe, “Understanding Random Forests: From Theory to\\nPractice”,\\nPhD Thesis, U. of Liege, 2014.\\n---------new doc---------\\nIn scikit-learn, bagging methods are offered as a unified\\nBaggingClassifier meta-estimator  (resp. BaggingRegressor),\\ntaking as input a user-specified estimator along with parameters\\nspecifying the strategy to draw random subsets. In particular, max_samples\\nand max_features control the size of the subsets (in terms of samples and\\nfeatures), while bootstrap and bootstrap_features control whether\\nsamples and features are drawn with or without replacement. When using a subset\\nof the available samples the generalization accuracy can be estimated with the\\nout-of-bag samples by setting oob_score=True. As an example, the\\nsnippet below illustrates how to instantiate a bagging ensemble of\\nKNeighborsClassifier estimators, each built on random\\nsubsets of 50% of the samples and 50% of the features.\\n>>> from sklearn.ensemble import BaggingClassifier\\n>>> from sklearn.neighbors import KNeighborsClassifier\\n>>> bagging = BaggingClassifier(KNeighborsClassifier(),\\n...                             max_samples=0.5, max_features=0.5)\\n\\n\\nExamples\\n\\nSingle estimator versus bagging: bias-variance decomposition\\n\\nReferences\\n\\n\\n[B1999]\\nL. Breiman, “Pasting small votes for classification in large\\ndatabases and on-line”, Machine Learning, 36(1), 85-103, 1999.\\n\\n\\n[B1996]\\nL. Breiman, “Bagging predictors”, Machine Learning, 24(2),\\n123-140, 1996.\\n---------new doc---------\\n1.11.4.1. Majority Class Labels (Majority/Hard Voting)#\\nIn majority voting, the predicted class label for a particular sample is\\nthe class label that represents the majority (mode) of the class labels\\npredicted by each individual classifier.\\nE.g., if the prediction for a given sample is\\n\\nclassifier 1 -> class 1\\nclassifier 2 -> class 1\\nclassifier 3 -> class 2\\n\\nthe VotingClassifier (with voting='hard') would classify the sample\\nas “class 1” based on the majority class label.\\nIn the cases of a tie, the VotingClassifier will select the class\\nbased on the ascending sort order. E.g., in the following scenario\\n\\nclassifier 1 -> class 2\\nclassifier 2 -> class 1\\n\\nthe class label 1 will be assigned to the sample.\\n\\n\\n1.11.4.2. Usage#\\nThe following example shows how to fit the majority rule classifier:\\n>>> from sklearn import datasets\\n>>> from sklearn.model_selection import cross_val_score\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.naive_bayes import GaussianNB\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.ensemble import VotingClassifier\\n\\n>>> iris = datasets.load_iris()\\n>>> X, y = iris.data[:, 1:3], iris.target\\n\\n>>> clf1 = LogisticRegression(random_state=1)\\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\\n>>> clf3 = GaussianNB()\\n---------new doc---------\\n>>> iris = datasets.load_iris()\\n>>> X, y = iris.data[:, 1:3], iris.target\\n\\n>>> clf1 = LogisticRegression(random_state=1)\\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\\n>>> clf3 = GaussianNB()\\n\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='hard')\\n\\n>>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\\n...     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\\n...     print(\\\"Accuracy: %0.2f (+/- %0.2f) [%s]\\\" % (scores.mean(), scores.std(), label))\\nAccuracy: 0.95 (+/- 0.04) [Logistic Regression]\\nAccuracy: 0.94 (+/- 0.04) [Random Forest]\\nAccuracy: 0.91 (+/- 0.04) [naive Bayes]\\nAccuracy: 0.95 (+/- 0.04) [Ensemble]\\n---------new doc---------\\nclassifier 1\\nw1 * 0.2\\nw1 * 0.5\\nw1 * 0.3\\n\\nclassifier 2\\nw2 * 0.6\\nw2 * 0.3\\nw2 * 0.1\\n\\nclassifier 3\\nw3 * 0.3\\nw3 * 0.4\\nw3 * 0.3\\n\\nweighted average\\n0.37\\n0.4\\n0.23\\n\\n\\n\\n\\nHere, the predicted class label is 2, since it has the\\nhighest average probability.\\nThe following example illustrates how the decision regions may change\\nwhen a soft VotingClassifier is used based on a linear Support\\nVector Machine, a Decision Tree, and a K-nearest neighbor classifier:\\n>>> from sklearn import datasets\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> from sklearn.neighbors import KNeighborsClassifier\\n>>> from sklearn.svm import SVC\\n>>> from itertools import product\\n>>> from sklearn.ensemble import VotingClassifier\\n\\n>>> # Loading some example data\\n>>> iris = datasets.load_iris()\\n>>> X = iris.data[:, [0, 2]]\\n>>> y = iris.target\\n---------new doc---------\\n>>> # Loading some example data\\n>>> iris = datasets.load_iris()\\n>>> X = iris.data[:, [0, 2]]\\n>>> y = iris.target\\n\\n>>> # Training classifiers\\n>>> clf1 = DecisionTreeClassifier(max_depth=4)\\n>>> clf2 = KNeighborsClassifier(n_neighbors=7)\\n>>> clf3 = SVC(kernel='rbf', probability=True)\\n>>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\\n...                         voting='soft', weights=[2, 1, 2])\\n\\n>>> clf1 = clf1.fit(X, y)\\n>>> clf2 = clf2.fit(X, y)\\n>>> clf3 = clf3.fit(X, y)\\n>>> eclf = eclf.fit(X, y)\\n\\n\\n\\n\\n\\n\\n\\n\\n1.11.4.4. Usage#\\nIn order to predict the class labels based on the predicted\\nclass-probabilities (scikit-learn estimators in the VotingClassifier\\nmust support predict_proba method):\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='soft'\\n... )\\n---------new doc---------\\nOptionally, weights can be provided for the individual classifiers:\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='soft', weights=[2,5,1]\\n... )\\n\\n\\n\\n\\nUsing the VotingClassifier with GridSearchCV#\\nThe VotingClassifier can also be used together with\\nGridSearchCV in order to tune the\\nhyperparameters of the individual estimators:\\n>>> from sklearn.model_selection import GridSearchCV\\n>>> clf1 = LogisticRegression(random_state=1)\\n>>> clf2 = RandomForestClassifier(random_state=1)\\n>>> clf3 = GaussianNB()\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='soft'\\n... )\\n\\n>>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\\n\\n>>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\\n>>> grid = grid.fit(iris.data, iris.target)\\n\\n\\n\\n\\n\\n\\n1.11.5. Voting Regressor#\\nThe idea behind the VotingRegressor is to combine conceptually\\ndifferent machine learning regressors and return the average predicted values.\\nSuch a regressor can be useful for a set of equally well performing models\\nin order to balance out their individual weaknesses.\\n---------new doc---------\\n1.11.5. Voting Regressor#\\nThe idea behind the VotingRegressor is to combine conceptually\\ndifferent machine learning regressors and return the average predicted values.\\nSuch a regressor can be useful for a set of equally well performing models\\nin order to balance out their individual weaknesses.\\n\\n1.11.5.1. Usage#\\nThe following example shows how to fit the VotingRegressor:\\n>>> from sklearn.datasets import load_diabetes\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>> from sklearn.ensemble import RandomForestRegressor\\n>>> from sklearn.linear_model import LinearRegression\\n>>> from sklearn.ensemble import VotingRegressor\\n\\n>>> # Loading some example data\\n>>> X, y = load_diabetes(return_X_y=True)\\n\\n>>> # Training classifiers\\n>>> reg1 = GradientBoostingRegressor(random_state=1)\\n>>> reg2 = RandomForestRegressor(random_state=1)\\n>>> reg3 = LinearRegression()\\n>>> ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\\n>>> ereg = ereg.fit(X, y)\\n\\n\\n\\n\\n\\n\\nExamples\\n\\nPlot individual and voting regression predictions\\n---------new doc---------\\nThe final_estimator will use the predictions of the estimators as input. It\\nneeds to be a classifier or a regressor when using StackingClassifier\\nor StackingRegressor, respectively:\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>> from sklearn.ensemble import StackingRegressor\\n>>> final_estimator = GradientBoostingRegressor(\\n...     n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,\\n...     random_state=42)\\n>>> reg = StackingRegressor(\\n...     estimators=estimators,\\n...     final_estimator=final_estimator)\\n\\n\\nTo train the estimators and final_estimator, the fit method needs\\nto be called on the training data:\\n>>> from sklearn.datasets import load_diabetes\\n>>> X, y = load_diabetes(return_X_y=True)\\n>>> from sklearn.model_selection import train_test_split\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\\n...                                                     random_state=42)\\n>>> reg.fit(X_train, y_train)\\nStackingRegressor(...)\\n---------new doc---------\\nDuring training, the estimators are fitted on the whole training data\\nX_train. They will be used when calling predict or predict_proba. To\\ngeneralize and avoid over-fitting, the final_estimator is trained on\\nout-samples using sklearn.model_selection.cross_val_predict internally.\\nFor StackingClassifier, note that the output of the estimators is\\ncontrolled by the parameter stack_method and it is called by each estimator.\\nThis parameter is either a string, being estimator method names, or 'auto'\\nwhich will automatically identify an available method depending on the\\navailability, tested in the order of preference: predict_proba,\\ndecision_function and predict.\\nA StackingRegressor and StackingClassifier can be used as\\nany other regressor or classifier, exposing a predict, predict_proba, or\\ndecision_function method, e.g.:\\n>>> y_pred = reg.predict(X_test)\\n>>> from sklearn.metrics import r2_score\\n>>> print('R2 score: {:.2f}'.format(r2_score(y_test, y_pred)))\\nR2 score: 0.53\\n\\n\\nNote that it is also possible to get the output of the stacked\\nestimators using the transform method:\\n>>> reg.transform(X_test[:5])\\narray([[142..., 138..., 146...],\\n       [179..., 182..., 151...],\\n       [139..., 132..., 158...],\\n       [286..., 292..., 225...],\\n       [126..., 124..., 164...]])\\n---------new doc---------\\nIn practice, a stacking predictor predicts as good as the best predictor of the\\nbase layer and even sometimes outperforms it by combining the different\\nstrengths of the these predictors. However, training a stacking predictor is\\ncomputationally expensive.\\n\\nNote\\nFor StackingClassifier, when using stack_method_='predict_proba',\\nthe first column is dropped when the problem is a binary classification\\nproblem. Indeed, both probability columns predicted by each estimator are\\nperfectly collinear.\\n---------new doc---------\\nNote\\nMultiple stacking layers can be achieved by assigning final_estimator to\\na StackingClassifier or StackingRegressor:\\n>>> final_layer_rfr = RandomForestRegressor(\\n...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\\n>>> final_layer_gbr = GradientBoostingRegressor(\\n...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\\n>>> final_layer = StackingRegressor(\\n...     estimators=[('rf', final_layer_rfr),\\n...                 ('gbrt', final_layer_gbr)],\\n...     final_estimator=RidgeCV()\\n...     )\\n>>> multi_layer_regressor = StackingRegressor(\\n...     estimators=[('ridge', RidgeCV()),\\n...                 ('lasso', LassoCV(random_state=42)),\\n...                 ('knr', KNeighborsRegressor(n_neighbors=20,\\n...                                             metric='euclidean'))],\\n...     final_estimator=final_layer\\n... )\\n---------new doc---------\\nAdaBoost can be used both for classification and regression problems:\\n\\nFor multi-class classification, AdaBoostClassifier implements\\nAdaBoost.SAMME [ZZRH2009].\\nFor regression, AdaBoostRegressor implements AdaBoost.R2 [D1997].\\n\\n\\n1.11.7.1. Usage#\\nThe following example shows how to fit an AdaBoost classifier with 100 weak\\nlearners:\\n>>> from sklearn.model_selection import cross_val_score\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.ensemble import AdaBoostClassifier\\n\\n>>> X, y = load_iris(return_X_y=True)\\n>>> clf = AdaBoostClassifier(n_estimators=100)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean()\\n0.9...\\n\\n\\nThe number of weak learners is controlled by the parameter n_estimators. The\\nlearning_rate parameter controls the contribution of the weak learners in\\nthe final combination. By default, weak learners are decision stumps. Different\\nweak learners can be specified through the estimator parameter.\\nThe main parameters to tune to obtain good results are n_estimators and\\nthe complexity of the base estimators (e.g., its depth max_depth or\\nminimum required number of samples to consider a split min_samples_split).\\nExamples\\n\\nMulti-class AdaBoosted Decision Trees shows the performance\\nof AdaBoost on a multi-class problem.\\nTwo-class AdaBoost shows the decision boundary\\nand decision function values for a non-linearly separable two-class problem\\nusing AdaBoost-SAMME.\\nDecision Tree Regression with AdaBoost demonstrates regression\\nwith the AdaBoost.R2 algorithm.\\n\\nReferences\\n---------new doc---------\\nMulticlass as One-Vs-One:\\n\\nsvm.NuSVC\\nsvm.SVC.\\ngaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_one”)\\n\\n\\nMulticlass as One-Vs-The-Rest:\\n\\nensemble.GradientBoostingClassifier\\ngaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_rest”)\\nsvm.LinearSVC (setting multi_class=”ovr”)\\nlinear_model.LogisticRegression (most solvers)\\nlinear_model.LogisticRegressionCV (most solvers)\\nlinear_model.SGDClassifier\\nlinear_model.Perceptron\\nlinear_model.PassiveAggressiveClassifier\\n\\n\\nSupport multilabel:\\n\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nneighbors.KNeighborsClassifier\\nneural_network.MLPClassifier\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\nlinear_model.RidgeClassifier\\nlinear_model.RidgeClassifierCV\\n\\n\\nSupport multiclass-multioutput:\\n\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nneighbors.KNeighborsClassifier\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\n\\n\\n\\n\\n1.12.1. Multiclass classification#\\n---------new doc---------\\nSupport multiclass-multioutput:\\n\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nneighbors.KNeighborsClassifier\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\n\\n\\n\\n\\n1.12.1. Multiclass classification#\\n\\nWarning\\nAll classifiers in scikit-learn do multiclass classification\\nout-of-the-box. You don’t need to use the sklearn.multiclass module\\nunless you want to experiment with different multiclass strategies.\\n\\nMulticlass classification is a classification task with more than two\\nclasses. Each sample can only be labeled as one class.\\nFor example, classification using features extracted from a set of images of\\nfruit, where each image may either be of an orange, an apple, or a pear.\\nEach image is one sample and is labeled as one of the 3 possible classes.\\nMulticlass classification makes the assumption that each sample is assigned\\nto one and only one label - one sample cannot, for example, be both a pear\\nand an apple.\\nWhile all scikit-learn classifiers are capable of multiclass classification,\\nthe meta-estimators offered by sklearn.multiclass\\npermit changing the way they handle more than two classes\\nbecause this may have an effect on classifier performance\\n(either in terms of generalization error or required computational resources).\\n\\n1.12.1.1. Target format#\\nValid multiclass representations for\\ntype_of_target (y) are:\\n---------new doc---------\\n1.12.1.2. OneVsRestClassifier#\\nThe one-vs-rest strategy, also known as one-vs-all, is implemented in\\nOneVsRestClassifier.  The strategy consists in\\nfitting one classifier per class. For each classifier, the class is fitted\\nagainst all the other classes. In addition to its computational efficiency\\n(only n_classes classifiers are needed), one advantage of this approach is\\nits interpretability. Since each class is represented by one and only one\\nclassifier, it is possible to gain knowledge about the class by inspecting its\\ncorresponding classifier. This is the most commonly used strategy and is a fair\\ndefault choice.\\nBelow is an example of multiclass learning using OvR:\\n>>> from sklearn import datasets\\n>>> from sklearn.multiclass import OneVsRestClassifier\\n>>> from sklearn.svm import LinearSVC\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n---------new doc---------\\n1.12.1.3. OneVsOneClassifier#\\nOneVsOneClassifier constructs one classifier per\\npair of classes. At prediction time, the class which received the most votes\\nis selected. In the event of a tie (among two classes with an equal number of\\nvotes), it selects the class with the highest aggregate classification\\nconfidence by summing over the pair-wise classification confidence levels\\ncomputed by the underlying binary classifiers.\\nSince it requires to fit n_classes * (n_classes - 1) / 2 classifiers,\\nthis method is usually slower than one-vs-the-rest, due to its\\nO(n_classes^2) complexity. However, this method may be advantageous for\\nalgorithms such as kernel algorithms which don’t scale well with\\nn_samples. This is because each individual learning problem only involves\\na small subset of the data whereas, with one-vs-the-rest, the complete\\ndataset is used n_classes times. The decision function is the result\\nof a monotonic transformation of the one-versus-one classification.\\nBelow is an example of multiclass learning using OvO:\\n>>> from sklearn import datasets\\n>>> from sklearn.multiclass import OneVsOneClassifier\\n>>> from sklearn.svm import LinearSVC\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n---------new doc---------\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n       1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\n---------new doc---------\\none-vs-the-rest. In theory, log2(n_classes) / n_classes is sufficient to\\nrepresent each class unambiguously. However, in practice, it may not lead to\\ngood accuracy since log2(n_classes) is much smaller than n_classes.\\nA number greater than 1 will require more classifiers than\\none-vs-the-rest. In this case, some classifiers will in theory correct for\\nthe mistakes made by other classifiers, hence the name “error-correcting”.\\nIn practice, however, this may not happen as classifier mistakes will\\ntypically be correlated. The error-correcting output codes have a similar\\neffect to bagging.\\nBelow is an example of multiclass learning using Output-Codes:\\n>>> from sklearn import datasets\\n>>> from sklearn.multiclass import OutputCodeClassifier\\n>>> from sklearn.svm import LinearSVC\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> clf = OutputCodeClassifier(LinearSVC(random_state=0), code_size=2, random_state=0)\\n>>> clf.fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n---------new doc---------\\nReferences\\n\\n“Solving multiclass learning problems via error-correcting output codes”,\\nDietterich T., Bakiri G., Journal of Artificial Intelligence Research 2, 1995.\\n\\n\\n\\n[3]\\n“The error coding method and PICTs”, James G., Hastie T.,\\nJournal of Computational and Graphical statistics 7, 1998.\\n\\n\\n\\n“The Elements of Statistical Learning”,\\nHastie T., Tibshirani R., Friedman J., page 606 (second-edition), 2008.\\n\\n\\n\\n\\n1.12.2. Multilabel classification#\\nMultilabel classification (closely related to multioutput\\nclassification) is a classification task labeling each sample with m\\nlabels from n_classes possible classes, where m can be 0 to\\nn_classes inclusive. This can be thought of as predicting properties of a\\nsample that are not mutually exclusive. Formally, a binary output is assigned\\nto each class, for every sample. Positive classes are indicated with 1 and\\nnegative classes with 0 or -1. It is thus comparable to running n_classes\\nbinary classification tasks, for example with\\nMultiOutputClassifier. This approach treats\\neach label independently whereas multilabel classifiers may treat the\\nmultiple classes simultaneously, accounting for correlated behavior among\\nthem.\\nFor example, prediction of the topics relevant to a text document or video.\\nThe document or video may be about one of ‘religion’, ‘politics’, ‘finance’\\nor ‘education’, several of the topic classes or all of the topic classes.\\n---------new doc---------\\n1.12.2.2. MultiOutputClassifier#\\nMultilabel classification support can be added to any classifier with\\nMultiOutputClassifier. This strategy consists of\\nfitting one classifier per target.  This allows multiple target variable\\nclassifications. The purpose of this class is to extend estimators\\nto be able to estimate a series of target functions (f1,f2,f3…,fn)\\nthat are trained on a single X predictor matrix to predict a series\\nof responses (y1,y2,y3…,yn).\\nYou can find a usage example for\\nMultiOutputClassifier\\nas part of the section on Multiclass-multioutput classification\\nsince it is a generalization of multilabel classification to\\nmulticlass outputs instead of binary outputs.\\n---------new doc---------\\n1.12.2.3. ClassifierChain#\\nClassifier chains (see ClassifierChain) are a way\\nof combining a number of binary classifiers into a single multi-label model\\nthat is capable of exploiting correlations among targets.\\nFor a multi-label classification problem with N classes, N binary\\nclassifiers are assigned an integer between 0 and N-1. These integers\\ndefine the order of models in the chain. Each classifier is then fit on the\\navailable training data plus the true labels of the classes whose\\nmodels were assigned a lower number.\\nWhen predicting, the true labels will not be available. Instead the\\npredictions of each model are passed on to the subsequent models in the\\nchain to be used as features.\\nClearly the order of the chain is important. The first model in the chain\\nhas no information about the other labels while the last model in the chain\\nhas features indicating the presence of all of the other labels. In general\\none does not know the optimal ordering of the models in the chain so\\ntypically many randomly ordered chains are fit and their predictions are\\naveraged together.\\nReferences\\n\\nJesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,\\n“Classifier Chains for Multi-label Classification”, 2009.\\n---------new doc---------\\n1.12.3. Multiclass-multioutput classification#\\nMulticlass-multioutput classification\\n(also known as multitask classification) is a\\nclassification task which labels each sample with a set of non-binary\\nproperties. Both the number of properties and the number of\\nclasses per property is greater than 2. A single estimator thus\\nhandles several joint classification tasks. This is both a generalization of\\nthe multilabel classification task, which only considers binary\\nattributes, as well as a generalization of the multiclass classification\\ntask, where only one property is considered.\\nFor example, classification of the properties “type of fruit” and “colour”\\nfor a set of images of fruit. The property “type of fruit” has the possible\\nclasses: “apple”, “pear” and “orange”. The property “colour” has the\\npossible classes: “green”, “red”, “yellow” and “orange”. Each sample is an\\nimage of a fruit, a label is output for both properties and each label is\\none of the possible classes of the corresponding property.\\nNote that all classifiers handling multiclass-multioutput (also known as\\nmultitask classification) tasks, support the multilabel classification task\\nas a special case. Multitask classification is similar to the multioutput\\nclassification task with different model formulations. For more information,\\nsee the relevant estimator documentation.\\nBelow is an example of multiclass-multioutput classification:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.multioutput import MultiOutputClassifier\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.utils import shuffle\\n>>> import numpy as np\\n---------new doc---------\\nclassification task with different model formulations. For more information,\\nsee the relevant estimator documentation.\\nBelow is an example of multiclass-multioutput classification:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.multioutput import MultiOutputClassifier\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.utils import shuffle\\n>>> import numpy as np\\n>>> X, y1 = make_classification(n_samples=10, n_features=100,\\n...                             n_informative=30, n_classes=3,\\n...                             random_state=1)\\n>>> y2 = shuffle(y1, random_state=1)\\n>>> y3 = shuffle(y1, random_state=2)\\n>>> Y = np.vstack((y1, y2, y3)).T\\n>>> n_samples, n_features = X.shape # 10,100\\n>>> n_outputs = Y.shape[1] # 3\\n>>> n_classes = 3\\n>>> forest = RandomForestClassifier(random_state=1)\\n>>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)\\n>>> multi_target_forest.fit(X, Y).predict(X)\\narray([[2, 2, 0],\\n       [1, 2, 1],\\n       [2, 1, 0],\\n       [0, 0, 2],\\n---------new doc---------\\n>>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)\\n>>> multi_target_forest.fit(X, Y).predict(X)\\narray([[2, 2, 0],\\n       [1, 2, 1],\\n       [2, 1, 0],\\n       [0, 0, 2],\\n       [0, 2, 1],\\n       [0, 0, 2],\\n       [1, 1, 0],\\n       [1, 1, 1],\\n       [0, 0, 2],\\n       [2, 0, 0]])\\n---------new doc---------\\nWarning\\nAt present, no metric in sklearn.metrics\\nsupports the multiclass-multioutput classification task.\\n\\n\\n1.12.3.1. Target format#\\nA valid representation of multioutput y is a dense matrix of shape\\n(n_samples, n_classes) of class labels. A column wise concatenation of 1d\\nmulticlass variables. An example of y for 3 samples:\\n>>> y = np.array([['apple', 'green'], ['orange', 'orange'], ['pear', 'green']])\\n>>> print(y)\\n[['apple' 'green']\\n ['orange' 'orange']\\n ['pear' 'green']]\\n\\n\\n\\n\\n\\n1.12.4. Multioutput regression#\\nMultioutput regression predicts multiple numerical properties for each\\nsample. Each property is a numerical variable and the number of properties\\nto be predicted for each sample is greater than or equal to 2. Some estimators\\nthat support multioutput regression are faster than just running n_output\\nestimators.\\nFor example, prediction of both wind speed and wind direction, in degrees,\\nusing data obtained at a certain location. Each sample would be data\\nobtained at one location and both wind speed and direction would be\\noutput for each sample.\\nThe following regressors natively support multioutput regression:\\n---------new doc---------\\n1.12.4.2. MultiOutputRegressor#\\nMultioutput regression support can be added to any regressor with\\nMultiOutputRegressor.  This strategy consists of\\nfitting one regressor per target. Since each target is represented by exactly\\none regressor it is possible to gain knowledge about the target by\\ninspecting its corresponding regressor. As\\nMultiOutputRegressor fits one regressor per\\ntarget it can not take advantage of correlations between targets.\\nBelow is an example of multioutput regression:\\n>>> from sklearn.datasets import make_regression\\n>>> from sklearn.multioutput import MultiOutputRegressor\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>> X, y = make_regression(n_samples=10, n_targets=3, random_state=1)\\n>>> MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X)\\narray([[-154.75474165, -147.03498585,  -50.03812219],\\n       [   7.12165031,    5.12914884,  -81.46081961],\\n       [-187.8948621 , -100.44373091,   13.88978285],\\n       [-141.62745778,   95.02891072, -191.48204257],\\n       [  97.03260883,  165.34867495,  139.52003279],\\n       [ 123.92529176,   21.25719016,   -7.84253   ],\\n---------new doc---------\\n1.12.4.3. RegressorChain#\\nRegressor chains (see RegressorChain) is\\nanalogous to ClassifierChain as a way of\\ncombining a number of regressions into a single multi-target model that is\\ncapable of exploiting correlations among targets.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking\\n\\n\\n\\n\\nnext\\n1.13. Feature selection\\n\\n\\n --- \\n\\n\\n1. Supervised learning\\n1.13. Feature selection\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1.13. Feature selection#\\nThe classes in the sklearn.feature_selection module can be used\\nfor feature selection/dimensionality reduction on sample sets, either to\\nimprove estimators’ accuracy scores or to boost their performance on very\\nhigh-dimensional datasets.\\n\\n1.13.1. Removing features with low variance#\\nVarianceThreshold is a simple baseline approach to feature selection.\\nIt removes all features whose variance doesn’t meet some threshold.\\nBy default, it removes all zero-variance features,\\ni.e. features that have the same value in all samples.\\nAs an example, suppose that we have a dataset with boolean features,\\nand we want to remove all features that are either one or zero (on or off)\\nin more than 80% of the samples.\\nBoolean features are Bernoulli random variables,\\nand the variance of such variables is given by\\n---------new doc---------\\n\\\\[\\\\mathrm{Var}[X] = p(1 - p)\\\\]\\nso we can select using the threshold .8 * (1 - .8):\\n>>> from sklearn.feature_selection import VarianceThreshold\\n>>> X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\\n>>> sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\\n>>> sel.fit_transform(X)\\narray([[0, 1],\\n       [1, 0],\\n       [0, 0],\\n       [1, 1],\\n       [1, 0],\\n       [1, 1]])\\n\\n\\nAs expected, VarianceThreshold has removed the first column,\\nwhich has a probability \\\\(p = 5/6 > .8\\\\) of containing a zero.\\n\\n\\n1.13.2. Univariate feature selection#\\nUnivariate feature selection works by selecting the best features based on\\nunivariate statistical tests. It can be seen as a preprocessing step\\nto an estimator. Scikit-learn exposes feature selection routines\\nas objects that implement the transform method:\\n---------new doc---------\\nAs expected, VarianceThreshold has removed the first column,\\nwhich has a probability \\\\(p = 5/6 > .8\\\\) of containing a zero.\\n\\n\\n1.13.2. Univariate feature selection#\\nUnivariate feature selection works by selecting the best features based on\\nunivariate statistical tests. It can be seen as a preprocessing step\\nto an estimator. Scikit-learn exposes feature selection routines\\nas objects that implement the transform method:\\n\\nSelectKBest removes all but the \\\\(k\\\\) highest scoring features\\nSelectPercentile removes all but a user-specified highest scoring\\npercentage of features\\nusing common univariate statistical tests for each feature:\\nfalse positive rate SelectFpr, false discovery rate\\nSelectFdr, or family wise error SelectFwe.\\nGenericUnivariateSelect allows to perform univariate feature\\nselection with a configurable strategy. This allows to select the best\\nunivariate selection strategy with hyper-parameter search estimator.\\n\\nFor instance, we can use a F-test to retrieve the two\\nbest features for a dataset as follows:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.feature_selection import SelectKBest\\n>>> from sklearn.feature_selection import f_classif\\n>>> X, y = load_iris(return_X_y=True)\\n>>> X.shape\\n(150, 4)\\n>>> X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\\n>>> X_new.shape\\n(150, 2)\\n\\n\\nThese objects take as input a scoring function that returns univariate scores\\nand p-values (or only scores for SelectKBest and\\nSelectPercentile):\\n---------new doc---------\\nWarning\\nBeware not to use a regression scoring function with a classification\\nproblem, you will get useless results.\\n\\n\\nNote\\nThe SelectPercentile and SelectKBest support unsupervised\\nfeature selection as well. One needs to provide a score_func where y=None.\\nThe score_func should use internally X to compute the scores.\\n\\nExamples\\n\\nUnivariate Feature Selection\\nComparison of F-test and mutual information\\n\\n\\n\\n1.13.3. Recursive feature elimination#\\nGiven an external estimator that assigns weights to features (e.g., the\\ncoefficients of a linear model), the goal of recursive feature elimination (RFE)\\nis to select features by recursively considering smaller and smaller sets of\\nfeatures. First, the estimator is trained on the initial set of features and\\nthe importance of each feature is obtained either through any specific attribute\\n(such as coef_, feature_importances_) or callable. Then, the least important\\nfeatures are pruned from current set of features. That procedure is recursively\\nrepeated on the pruned set until the desired number of features to select is\\neventually reached.\\nRFECV performs RFE in a cross-validation loop to find the optimal\\nnumber of features. In more details, the number of features selected is tuned\\nautomatically by fitting an RFE selector on the different\\ncross-validation splits (provided by the cv parameter). The performance\\nof the RFE selector are evaluated using scorer for different number\\nof selected features and aggregated together. Finally, the scores are averaged\\nacross folds and the number of features selected is set to the number of\\nfeatures that maximize the cross-validation score.\\nExamples\\n---------new doc---------\\nRecursive feature elimination: A recursive feature elimination example\\nshowing the relevance of pixels in a digit classification task.\\nRecursive feature elimination with cross-validation: A recursive feature\\nelimination example with automatic tuning of the number of features\\nselected with cross-validation.\\n\\n\\n\\n1.13.4. Feature selection using SelectFromModel#\\nSelectFromModel is a meta-transformer that can be used alongside any\\nestimator that assigns importance to each feature through a specific attribute (such as\\ncoef_, feature_importances_) or via an importance_getter callable after fitting.\\nThe features are considered unimportant and removed if the corresponding\\nimportance of the feature values are below the provided\\nthreshold parameter. Apart from specifying the threshold numerically,\\nthere are built-in heuristics for finding a threshold using a string argument.\\nAvailable heuristics are “mean”, “median” and float multiples of these like\\n“0.1*mean”. In combination with the threshold criteria, one can use the\\nmax_features parameter to set a limit on the number of features to select.\\nFor examples on how it is to be used refer to the sections below.\\nExamples\\n\\nModel-based and sequential feature selection\\n---------new doc---------\\n1.13.4.2. Tree-based feature selection#\\nTree-based estimators (see the sklearn.tree module and forest\\nof trees in the sklearn.ensemble module) can be used to compute\\nimpurity-based feature importances, which in turn can be used to discard irrelevant\\nfeatures (when coupled with the SelectFromModel\\nmeta-transformer):\\n>>> from sklearn.ensemble import ExtraTreesClassifier\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.feature_selection import SelectFromModel\\n>>> X, y = load_iris(return_X_y=True)\\n>>> X.shape\\n(150, 4)\\n>>> clf = ExtraTreesClassifier(n_estimators=50)\\n>>> clf = clf.fit(X, y)\\n>>> clf.feature_importances_  \\narray([ 0.04...,  0.05...,  0.4...,  0.4...])\\n>>> model = SelectFromModel(clf, prefit=True)\\n>>> X_new = model.transform(X)\\n>>> X_new.shape               \\n(150, 2)\\n\\n\\nExamples\\n\\nFeature importances with a forest of trees: example on\\nsynthetic data showing the recovery of the actually meaningful features.\\nPermutation Importance vs Random Forest Feature Importance (MDI): example\\ndiscussing the caveats of using impurity-based feature importances as a proxy for\\nfeature relevance.\\n---------new doc---------\\nExamples\\n\\nFeature importances with a forest of trees: example on\\nsynthetic data showing the recovery of the actually meaningful features.\\nPermutation Importance vs Random Forest Feature Importance (MDI): example\\ndiscussing the caveats of using impurity-based feature importances as a proxy for\\nfeature relevance.\\n\\n\\n\\n\\n1.13.5. Sequential Feature Selection#\\nSequential Feature Selection [sfs] (SFS) is available in the\\nSequentialFeatureSelector transformer.\\nSFS can be either forward or backward:\\nForward-SFS is a greedy procedure that iteratively finds the best new feature\\nto add to the set of selected features. Concretely, we initially start with\\nzero features and find the one feature that maximizes a cross-validated score\\nwhen an estimator is trained on this single feature. Once that first feature\\nis selected, we repeat the procedure by adding a new feature to the set of\\nselected features. The procedure stops when the desired number of selected\\nfeatures is reached, as determined by the n_features_to_select parameter.\\nBackward-SFS follows the same idea but works in the opposite direction:\\ninstead of starting with no features and greedily adding features, we start\\nwith all the features and greedily remove features from the set. The\\ndirection parameter controls whether forward or backward SFS is used.\\n---------new doc---------\\nDetails on Sequential Feature Selection#\\nIn general, forward and backward selection do not yield equivalent results.\\nAlso, one may be much faster than the other depending on the requested number\\nof selected features: if we have 10 features and ask for 7 selected features,\\nforward selection would need to perform 7 iterations while backward selection\\nwould only need to perform 3.\\nSFS differs from RFE and\\nSelectFromModel in that it does not\\nrequire the underlying model to expose a coef_ or feature_importances_\\nattribute. It may however be slower considering that more models need to be\\nevaluated, compared to the other approaches. For example in backward\\nselection, the iteration going from m features to m - 1 features using k-fold\\ncross-validation requires fitting m * k models, while\\nRFE would require only a single fit, and\\nSelectFromModel always just does a single\\nfit and requires no iterations.\\nReferences\\n\\n\\n[sfs]\\nFerri et al, Comparative study of techniques for\\nlarge-scale feature selection.\\n\\n\\n\\nExamples\\n\\nModel-based and sequential feature selection\\n\\n\\n\\n1.13.6. Feature selection as part of a pipeline#\\nFeature selection is usually used as a pre-processing step before doing\\nthe actual learning. The recommended way to do this in scikit-learn is\\nto use a Pipeline:\\nclf = Pipeline([\\n  ('feature_selection', SelectFromModel(LinearSVC(penalty=\\\"l1\\\"))),\\n  ('classification', RandomForestClassifier())\\n])\\nclf.fit(X, y)\\n---------new doc---------\\nIn this snippet we make use of a LinearSVC\\ncoupled with SelectFromModel\\nto evaluate feature importances and select the most relevant features.\\nThen, a RandomForestClassifier is trained on the\\ntransformed output, i.e. using only relevant features. You can perform\\nsimilar operations with the other feature selection methods and also\\nclassifiers that provide a way to evaluate feature importances of course.\\nSee the Pipeline examples for more details.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n1.12. Multiclass and multioutput algorithms\\n\\n\\n\\n\\nnext\\n1.14. Semi-supervised learning\\n\\n\\n --- \\n\\n\\n1. Supervised learning\\n1.14. Semi-supervised learning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1.14. Semi-supervised learning#\\nSemi-supervised learning is a situation\\nin which in your training data some of the samples are not labeled. The\\nsemi-supervised estimators in sklearn.semi_supervised are able to\\nmake use of this additional unlabeled data to better capture the shape of\\nthe underlying data distribution and generalize better to new samples.\\nThese algorithms can perform well when we have a very small amount of\\nlabeled points and a large amount of unlabeled points.\\n\\nUnlabeled entries in y\\nIt is important to assign an identifier to unlabeled points along with the\\nlabeled data when training the model with the fit method. The\\nidentifier that this implementation uses is the integer value \\\\(-1\\\\).\\nNote that for string labels, the dtype of y should be object so that it\\ncan contain both strings and integers.\\n\\n\\nNote\\nSemi-supervised algorithms need to make assumptions about the distribution\\nof the dataset in order to achieve performance gains. See here\\nfor more details.\\n---------new doc---------\\nNote\\nSemi-supervised algorithms need to make assumptions about the distribution\\nof the dataset in order to achieve performance gains. See here\\nfor more details.\\n\\n\\n1.14.1. Self Training#\\nThis self-training implementation is based on Yarowsky’s [1] algorithm. Using\\nthis algorithm, a given supervised classifier can function as a semi-supervised\\nclassifier, allowing it to learn from unlabeled data.\\nSelfTrainingClassifier can be called with any classifier that\\nimplements predict_proba, passed as the parameter base_classifier. In\\neach iteration, the base_classifier predicts labels for the unlabeled\\nsamples and adds a subset of these labels to the labeled dataset.\\nThe choice of this subset is determined by the selection criterion. This\\nselection can be done using a threshold on the prediction probabilities, or\\nby choosing the k_best samples according to the prediction probabilities.\\nThe labels used for the final fit as well as the iteration in which each sample\\nwas labeled are available as attributes. The optional max_iter parameter\\nspecifies how many times the loop is executed at most.\\nThe max_iter parameter may be set to None, causing the algorithm to iterate\\nuntil all samples have labels or no new samples are selected in that iteration.\\n\\nNote\\nWhen using the self-training classifier, the\\ncalibration of the classifier is important.\\n\\nExamples\\n\\nEffect of varying threshold for self-training\\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\\n\\nReferences\\n---------new doc---------\\nNote\\nWhen using the self-training classifier, the\\ncalibration of the classifier is important.\\n\\nExamples\\n\\nEffect of varying threshold for self-training\\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\\n\\nReferences\\n\\n\\n[1]\\n“Unsupervised word sense disambiguation rivaling supervised methods”\\nDavid Yarowsky, Proceedings of the 33rd annual meeting on Association for\\nComputational Linguistics (ACL ‘95). Association for Computational Linguistics,\\nStroudsburg, PA, USA, 189-196.\\n\\n\\n\\n\\n1.14.2. Label Propagation#\\nLabel propagation denotes a few variations of semi-supervised graph\\ninference algorithms.\\n\\nA few features available in this model:\\nUsed for classification tasks\\nKernel methods to project data into alternate dimensional spaces\\n\\n\\n\\nscikit-learn provides two label propagation models:\\nLabelPropagation and LabelSpreading. Both work by\\nconstructing a similarity graph over all items in the input dataset.\\n\\n\\n\\n\\nAn illustration of label-propagation: the structure of unlabeled\\nobservations is consistent with the class structure, and thus the\\nclass label can be propagated to the unlabeled observations of the\\ntraining set.#\\n---------new doc---------\\nExamples\\n\\nIsotonic Regression\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n1.14. Semi-supervised learning\\n\\n\\n\\n\\nnext\\n1.16. Probability calibration\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThis Page\\n\\nShow Source\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      © Copyright 2007 - 2025, scikit-learn developers (BSD License).\\n\\n\\n --- \\n\\n\\n1. Supervised learning\\n1.16. Probability calibration\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1.16. Probability calibration#\\nWhen performing classification you often want not only to predict the class\\nlabel, but also obtain a probability of the respective label. This probability\\ngives you some kind of confidence on the prediction. Some models can give you\\npoor estimates of the class probabilities and some even do not support\\nprobability prediction (e.g., some instances of\\nSGDClassifier).\\nThe calibration module allows you to better calibrate\\nthe probabilities of a given model, or to add support for probability\\nprediction.\\nWell calibrated classifiers are probabilistic classifiers for which the output\\nof the predict_proba method can be directly interpreted as a confidence\\nlevel.\\nFor instance, a well calibrated (binary) classifier should classify the samples such\\nthat among the samples to which it gave a predict_proba value close to, say,\\n0.8, approximately 80% actually belong to the positive class.\\nBefore we show how to re-calibrate a classifier, we first need a way to detect how\\ngood a classifier is calibrated.\\n---------new doc---------\\nLogisticRegression is more likely to return well calibrated predictions by itself as it has a\\ncanonical link function for its loss, i.e. the logit-link for the Log loss.\\nIn the unpenalized case, this leads to the so-called balance property, see [8] and Logistic regression.\\nIn the plot above, data is generated according to a linear mechanism, which is\\nconsistent with the LogisticRegression model (the model is ‘well specified’),\\nand the value of the regularization parameter C is tuned to be\\nappropriate (neither too strong nor too low). As a consequence, this model returns\\naccurate predictions from its predict_proba method.\\nIn contrast to that, the other shown models return biased probabilities; with\\ndifferent biases per model.\\nGaussianNB (Naive Bayes) tends to push probabilities to 0 or 1 (note the counts\\nin the histograms). This is mainly because it makes the assumption that\\nfeatures are conditionally independent given the class, which is not the\\ncase in this dataset which contains 2 redundant features.\\nRandomForestClassifier shows the opposite behavior: the histograms\\nshow peaks at probabilities approximately 0.2 and 0.9, while probabilities\\nclose to 0 or 1 are very rare. An explanation for this is given by\\nNiculescu-Mizil and Caruana [3]: “Methods such as bagging and random\\nforests that average predictions from a base set of models can have\\ndifficulty making predictions near 0 and 1 because variance in the\\nunderlying base models will bias predictions that should be near zero or one\\naway from these values. Because predictions are restricted to the interval\\n[0,1], errors caused by variance tend to be one-sided near zero and one. For\\n---------new doc---------\\nforests that average predictions from a base set of models can have\\ndifficulty making predictions near 0 and 1 because variance in the\\nunderlying base models will bias predictions that should be near zero or one\\naway from these values. Because predictions are restricted to the interval\\n[0,1], errors caused by variance tend to be one-sided near zero and one. For\\nexample, if a model should predict p = 0 for a case, the only way bagging\\ncan achieve this is if all bagged trees predict zero. If we add noise to the\\ntrees that bagging is averaging over, this noise will cause some trees to\\npredict values larger than 0 for this case, thus moving the average\\nprediction of the bagged ensemble away from 0. We observe this effect most\\nstrongly with random forests because the base-level trees trained with\\nrandom forests have relatively high variance due to feature subsetting.” As\\na result, the calibration curve shows a characteristic sigmoid shape, indicating that\\nthe classifier could trust its “intuition” more and return probabilities closer\\nto 0 or 1 typically.\\nLinearSVC (SVC) shows an even more sigmoid curve than the random forest, which\\nis typical for maximum-margin methods (compare Niculescu-Mizil and Caruana [3]), which\\nfocus on difficult to classify samples that are close to the decision boundary (the\\nsupport vectors).\\n---------new doc---------\\n1.16.2. Calibrating a classifier#\\nCalibrating a classifier consists of fitting a regressor (called a\\ncalibrator) that maps the output of the classifier (as given by\\ndecision_function or predict_proba) to a calibrated probability\\nin [0, 1]. Denoting the output of the classifier for a given sample by \\\\(f_i\\\\),\\nthe calibrator tries to predict the conditional event probability\\n\\\\(P(y_i = 1 | f_i)\\\\).\\nIdeally, the calibrator is fit on a dataset independent of the training data used to\\nfit the classifier in the first place.\\nThis is because performance of the classifier on its training data would be\\nbetter than for novel data. Using the classifier output of training data\\nto fit the calibrator would thus result in a biased calibrator that maps to\\nprobabilities closer to 0 and 1 than it should.\\n\\n\\n1.16.3. Usage#\\nThe CalibratedClassifierCV class is used to calibrate a classifier.\\nCalibratedClassifierCV uses a cross-validation approach to ensure\\nunbiased data is always used to fit the calibrator. The data is split into k\\n(train_set, test_set) couples (as determined by cv). When ensemble=True\\n(default), the following procedure is repeated independently for each\\ncross-validation split:\\n\\na clone of base_estimator is trained on the train subset\\nthe trained base_estimator makes predictions on the test subset\\nthe predictions are used to fit a calibrator (either a sigmoid or isotonic\\nregressor) (when the data is multiclass, a calibrator is fit for every class)\\n---------new doc---------\\nThis results in an\\nensemble of k (classifier, calibrator) couples where each calibrator maps\\nthe output of its corresponding classifier into [0, 1]. Each couple is exposed\\nin the calibrated_classifiers_ attribute, where each entry is a calibrated\\nclassifier with a predict_proba method that outputs calibrated\\nprobabilities. The output of predict_proba for the main\\nCalibratedClassifierCV instance corresponds to the average of the\\npredicted probabilities of the k estimators in the calibrated_classifiers_\\nlist. The output of predict is the class that has the highest\\nprobability.\\nIt is important to choose cv carefully when using ensemble=True.\\nAll classes should be present in both train and test subsets for every split.\\nWhen a class is absent in the train subset, the predicted probability for that\\nclass will default to 0 for the (classifier, calibrator) couple of that split.\\nThis skews the predict_proba as it averages across all couples.\\nWhen a class is absent in the test subset, the calibrator for that class\\n(within the (classifier, calibrator) couple of that split) is\\nfit on data with no positive class. This results in ineffective calibration.\\nWhen ensemble=False, cross-validation is used to obtain ‘unbiased’\\npredictions for all the data, via\\ncross_val_predict.\\nThese unbiased predictions are then used to train the calibrator. The attribute\\ncalibrated_classifiers_ consists of only one (classifier, calibrator)\\ncouple where the classifier is the base_estimator trained on all the data.\\nIn this case the output of predict_proba for\\nCalibratedClassifierCV is the predicted probabilities obtained\\nfrom the single (classifier, calibrator) couple.\\n---------new doc---------\\ncross_val_predict.\\nThese unbiased predictions are then used to train the calibrator. The attribute\\ncalibrated_classifiers_ consists of only one (classifier, calibrator)\\ncouple where the classifier is the base_estimator trained on all the data.\\nIn this case the output of predict_proba for\\nCalibratedClassifierCV is the predicted probabilities obtained\\nfrom the single (classifier, calibrator) couple.\\nThe main advantage of ensemble=True is to benefit from the traditional\\nensembling effect (similar to Bagging meta-estimator). The resulting ensemble should\\nboth be well calibrated and slightly more accurate than with ensemble=False.\\nThe main advantage of using ensemble=False is computational: it reduces the\\noverall fit time by training only a single base classifier and calibrator\\npair, decreases the final model size and increases prediction speed.\\nAlternatively an already fitted classifier can be calibrated by using a\\nFrozenEstimator as\\nCalibratedClassifierCV(estimator=FrozenEstimator(estimator)).\\nIt is up to the user to make sure that the data used for fitting the classifier\\nis disjoint from the data used for fitting the regressor.\\ndata used for fitting the regressor.\\nCalibratedClassifierCV supports the use of two regression techniques\\nfor calibration via the method parameter: \\\"sigmoid\\\" and \\\"isotonic\\\".\\n---------new doc---------\\nThe disadvantages of Multi-layer Perceptron (MLP) include:\\n\\nMLP with hidden layers have a non-convex loss function where there exists\\nmore than one local minimum. Therefore different random weight\\ninitializations can lead to different validation accuracy.\\nMLP requires tuning a number of hyperparameters such as the number of\\nhidden neurons, layers, and iterations.\\nMLP is sensitive to feature scaling.\\n\\nPlease see Tips on Practical Use section that addresses\\nsome of these disadvantages.\\n\\n\\n\\n1.17.2. Classification#\\nClass MLPClassifier implements a multi-layer perceptron (MLP) algorithm\\nthat trains using Backpropagation.\\nMLP trains on two arrays: array X of size (n_samples, n_features), which holds\\nthe training samples represented as floating point feature vectors; and array\\ny of size (n_samples,), which holds the target values (class labels) for the\\ntraining samples:\\n>>> from sklearn.neural_network import MLPClassifier\\n>>> X = [[0., 0.], [1., 1.]]\\n>>> y = [0, 1]\\n>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\\n...                     hidden_layer_sizes=(5, 2), random_state=1)\\n...\\n>>> clf.fit(X, y)\\nMLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\\n              solver='lbfgs')\\n---------new doc---------\\nMLPClassifier supports multi-class classification by\\napplying Softmax\\nas the output function.\\nFurther, the model supports multi-label classification\\nin which a sample can belong to more than one class. For each class, the raw\\noutput passes through the logistic function. Values larger or equal to 0.5\\nare rounded to 1, otherwise to 0. For a predicted output of a sample, the\\nindices where the value is 1 represents the assigned classes of that sample:\\n>>> X = [[0., 0.], [1., 1.]]\\n>>> y = [[0, 1], [1, 1]]\\n>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\\n...                     hidden_layer_sizes=(15,), random_state=1)\\n...\\n>>> clf.fit(X, y)\\nMLPClassifier(alpha=1e-05, hidden_layer_sizes=(15,), random_state=1,\\n              solver='lbfgs')\\n>>> clf.predict([[1., 2.]])\\narray([[1, 1]])\\n>>> clf.predict([[0., 0.]])\\narray([[0, 1]])\\n\\n\\nSee the examples below and the docstring of\\nMLPClassifier.fit for further information.\\nExamples\\n\\nCompare Stochastic learning strategies for MLPClassifier\\nSee Visualization of MLP weights on MNIST for\\nvisualized representation of trained weights.\\n---------new doc---------\\nThe first row of output array indicates that there are three samples whose\\ntrue cluster is “a”. Of them, two are in predicted cluster 0, one is in 1,\\nand none is in 2. And the second row indicates that there are three samples\\nwhose true cluster is “b”. Of them, none is in predicted cluster 0, one is in\\n1 and two are in 2.\\nA confusion matrix for classification is a square\\ncontingency matrix where the order of rows and columns correspond to a list\\nof classes.\\n\\nAdvantages:\\n\\nAllows to examine the spread of each true cluster across predicted clusters\\nand vice versa.\\nThe contingency table calculated is typically utilized in the calculation of\\na similarity statistic (like the others listed in this document) between the\\ntwo clusterings.\\n\\n\\n\\nDrawbacks:\\n\\nContingency matrix is easy to interpret for a small number of clusters, but\\nbecomes very hard to interpret for a large number of clusters.\\nIt doesn’t give a single metric to use as an objective for clustering\\noptimisation.\\n\\n\\n\\n\\nReferences#\\n\\nWikipedia entry for contingency matrix\\n\\n\\n\\n\\n2.3.11.9. Pair Confusion Matrix#\\nThe pair confusion matrix\\n(sklearn.metrics.cluster.pair_confusion_matrix) is a 2x2\\nsimilarity matrix\\n---------new doc---------\\n\\\\[\\\\begin{split}C = \\\\left[\\\\begin{matrix}\\nC_{00} & C_{01} \\\\\\\\\\nC_{10} & C_{11}\\n\\\\end{matrix}\\\\right]\\\\end{split}\\\\]\\nbetween two clusterings computed by considering all pairs of samples and\\ncounting pairs that are assigned into the same or into different clusters\\nunder the true and predicted clusterings.\\nIt has the following entries:\\n\\\\(C_{00}\\\\) : number of pairs with both clusterings having the samples\\nnot clustered together\\n\\\\(C_{10}\\\\) : number of pairs with the true label clustering having the\\nsamples clustered together but the other clustering not having the samples\\nclustered together\\n\\\\(C_{01}\\\\) : number of pairs with the true label clustering not having\\nthe samples clustered together but the other clustering having the samples\\nclustered together\\n\\\\(C_{11}\\\\) : number of pairs with both clusterings having the samples\\nclustered together\\nConsidering a pair of samples that is clustered together a positive pair,\\nthen as in binary classification the count of true negatives is\\n\\\\(C_{00}\\\\), false negatives is \\\\(C_{10}\\\\), true positives is\\n\\\\(C_{11}\\\\) and false positives is \\\\(C_{01}\\\\).\\nPerfectly matching labelings have all non-zero entries on the\\ndiagonal regardless of actual label values:\\n>>> from sklearn.metrics.cluster import pair_confusion_matrix\\n>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])\\narray([[8, 0],\\n       [0, 4]])\\n---------new doc---------\\n2.7. Novelty and Outlier Detection#\\nMany applications require being able to decide whether a new observation\\nbelongs to the same distribution as existing observations (it is an\\ninlier), or should be considered as different (it is an outlier).\\nOften, this ability is used to clean real data sets. Two important\\ndistinctions must be made:\\n\\noutlier detection:\\nThe training data contains outliers which are defined as observations that\\nare far from the others. Outlier detection estimators thus try to fit the\\nregions where the training data is the most concentrated, ignoring the\\ndeviant observations.\\n\\nnovelty detection:\\nThe training data is not polluted by outliers and we are interested in\\ndetecting whether a new observation is an outlier. In this context an\\noutlier is also called a novelty.\\n\\n\\nOutlier detection and novelty detection are both used for anomaly\\ndetection, where one is interested in detecting abnormal or unusual\\nobservations. Outlier detection is then also known as unsupervised anomaly\\ndetection and novelty detection as semi-supervised anomaly detection. In the\\ncontext of outlier detection, the outliers/anomalies cannot form a\\ndense cluster as available estimators assume that the outliers/anomalies are\\nlocated in low density regions. On the contrary, in the context of novelty\\ndetection, novelties/anomalies can form a dense cluster as long as they are in\\na low density region of the training data, considered as normal in this\\ncontext.\\nThe scikit-learn project provides a set of machine learning tools that\\ncan be used both for novelty or outlier detection. This strategy is\\nimplemented with objects learning in an unsupervised way from the data:\\nestimator.fit(X_train)\\n---------new doc---------\\nnew observations can then be sorted as inliers or outliers with a\\npredict method:\\nestimator.predict(X_test)\\n\\n\\nInliers are labeled 1, while outliers are labeled -1. The predict method\\nmakes use of a threshold on the raw scoring function computed by the\\nestimator. This scoring function is accessible through the score_samples\\nmethod, while the threshold can be controlled by the contamination\\nparameter.\\nThe decision_function method is also defined from the scoring function,\\nin such a way that negative values are outliers and non-negative ones are\\ninliers:\\nestimator.decision_function(X_test)\\n\\n\\nNote that neighbors.LocalOutlierFactor does not support\\npredict, decision_function and score_samples methods by default\\nbut only a fit_predict method, as this estimator was originally meant to\\nbe applied for outlier detection. The scores of abnormality of the training\\nsamples are accessible through the negative_outlier_factor_ attribute.\\nIf you really want to use neighbors.LocalOutlierFactor for novelty\\ndetection, i.e. predict labels or compute the score of abnormality of new\\nunseen data, you can instantiate the estimator with the novelty parameter\\nset to True before fitting the estimator. In this case, fit_predict is\\nnot available.\\n---------new doc---------\\nWarning\\nNovelty detection with Local Outlier Factor\\nWhen novelty is set to True be aware that you must only use\\npredict, decision_function and score_samples on new unseen data\\nand not on the training samples as this would lead to wrong results.\\nI.e., the result of predict will not be the same as fit_predict.\\nThe scores of abnormality of the training samples are always accessible\\nthrough the negative_outlier_factor_ attribute.\\n\\nThe behavior of neighbors.LocalOutlierFactor is summarized in the\\nfollowing table.\\n\\n\\nMethod\\nOutlier detection\\nNovelty detection\\n\\n\\n\\nfit_predict\\nOK\\nNot available\\n\\npredict\\nNot available\\nUse only on new data\\n\\ndecision_function\\nNot available\\nUse only on new data\\n\\nscore_samples\\nUse negative_outlier_factor_\\nUse only on new data\\n\\nnegative_outlier_factor_\\nOK\\nOK\\n\\n\\n\\n\\n\\n2.7.1. Overview of outlier detection methods#\\nA comparison of the outlier detection algorithms in scikit-learn. Local\\nOutlier Factor (LOF) does not show a decision boundary in black as it\\nhas no predict method to be applied on new data when it is used for outlier\\ndetection.\\n---------new doc---------\\nnegative_outlier_factor_\\nOK\\nOK\\n\\n\\n\\n\\n\\n2.7.1. Overview of outlier detection methods#\\nA comparison of the outlier detection algorithms in scikit-learn. Local\\nOutlier Factor (LOF) does not show a decision boundary in black as it\\nhas no predict method to be applied on new data when it is used for outlier\\ndetection.\\n\\n\\n\\n\\nensemble.IsolationForest and neighbors.LocalOutlierFactor\\nperform reasonably well on the data sets considered here.\\nThe svm.OneClassSVM is known to be sensitive to outliers and thus\\ndoes not perform very well for outlier detection. That being said, outlier\\ndetection in high-dimension, or without any assumptions on the distribution\\nof the inlying data is very challenging. svm.OneClassSVM may still\\nbe used with outlier detection but requires fine-tuning of its hyperparameter\\nnu to handle outliers and prevent overfitting.\\nlinear_model.SGDOneClassSVM provides an implementation of a\\nlinear One-Class SVM with a linear complexity in the number of samples. This\\nimplementation is here used with a kernel approximation technique to obtain\\nresults similar to svm.OneClassSVM which uses a Gaussian kernel\\nby default. Finally, covariance.EllipticEnvelope assumes the data is\\nGaussian and learns an ellipse. For more details on the different estimators\\nrefer to the example\\nComparing anomaly detection algorithms for outlier detection on toy datasets and the\\nsections hereunder.\\nExamples\\n---------new doc---------\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison of the svm.OneClassSVM, the\\nensemble.IsolationForest, the\\nneighbors.LocalOutlierFactor and\\ncovariance.EllipticEnvelope.\\nSee Evaluation of outlier detection estimators\\nfor an example showing how to evaluate outlier detection estimators,\\nthe neighbors.LocalOutlierFactor and the\\nensemble.IsolationForest, using ROC curves from\\nmetrics.RocCurveDisplay.\\n---------new doc---------\\n2.7.2. Novelty Detection#\\nConsider a data set of \\\\(n\\\\) observations from the same\\ndistribution described by \\\\(p\\\\) features.  Consider now that we\\nadd one more observation to that data set. Is the new observation so\\ndifferent from the others that we can doubt it is regular? (i.e. does\\nit come from the same distribution?) Or on the contrary, is it so\\nsimilar to the other that we cannot distinguish it from the original\\nobservations? This is the question addressed by the novelty detection\\ntools and methods.\\nIn general, it is about to learn a rough, close frontier delimiting\\nthe contour of the initial observations distribution, plotted in\\nembedding \\\\(p\\\\)-dimensional space. Then, if further observations\\nlay within the frontier-delimited subspace, they are considered as\\ncoming from the same population than the initial\\nobservations. Otherwise, if they lay outside the frontier, we can say\\nthat they are abnormal with a given confidence in our assessment.\\nThe One-Class SVM has been introduced by Schölkopf et al. for that purpose\\nand implemented in the Support Vector Machines module in the\\nsvm.OneClassSVM object. It requires the choice of a\\nkernel and a scalar parameter to define a frontier.  The RBF kernel is\\nusually chosen although there exists no exact formula or algorithm to\\nset its bandwidth parameter. This is the default in the scikit-learn\\nimplementation. The nu parameter, also known as the margin of\\nthe One-Class SVM, corresponds to the probability of finding a new,\\nbut regular, observation outside the frontier.\\nReferences\\n\\nEstimating the support of a high-dimensional distribution\\nSchölkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471.\\n---------new doc---------\\n2.7.3. Outlier Detection#\\nOutlier detection is similar to novelty detection in the sense that\\nthe goal is to separate a core of regular observations from some\\npolluting ones, called outliers. Yet, in the case of outlier\\ndetection, we don’t have a clean data set representing the population\\nof regular observations that can be used to train any tool.\\n\\n2.7.3.1. Fitting an elliptic envelope#\\nOne common way of performing outlier detection is to assume that the\\nregular data come from a known distribution (e.g. data are Gaussian\\ndistributed). From this assumption, we generally try to define the\\n“shape” of the data, and can define outlying observations as\\nobservations which stand far enough from the fit shape.\\nThe scikit-learn provides an object\\ncovariance.EllipticEnvelope that fits a robust covariance\\nestimate to the data, and thus fits an ellipse to the central data\\npoints, ignoring points outside the central mode.\\nFor instance, assuming that the inlier data are Gaussian distributed, it\\nwill estimate the inlier location and covariance in a robust way (i.e.\\nwithout being influenced by outliers). The Mahalanobis distances\\nobtained from this estimate is used to derive a measure of outlyingness.\\nThis strategy is illustrated below.\\n\\n\\n\\n\\nExamples\\n---------new doc---------\\nThe ensemble.IsolationForest supports warm_start=True which\\nallows you to add more trees to an already fitted model:\\n>>> from sklearn.ensemble import IsolationForest\\n>>> import numpy as np\\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\\n>>> clf = IsolationForest(n_estimators=10, warm_start=True)\\n>>> clf.fit(X)  # fit 10 trees  \\n>>> clf.set_params(n_estimators=20)  # add 10 more trees  \\n>>> clf.fit(X)  # fit the added trees  \\n\\n\\nExamples\\n\\nSee IsolationForest example for\\nan illustration of the use of IsolationForest.\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison of ensemble.IsolationForest with\\nneighbors.LocalOutlierFactor,\\nsvm.OneClassSVM (tuned to perform like an outlier detection\\nmethod), linear_model.SGDOneClassSVM, and a covariance-based\\noutlier detection with covariance.EllipticEnvelope.\\n\\nReferences\\n\\nLiu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”\\nData Mining, 2008. ICDM’08. Eighth IEEE International Conference on.\\n---------new doc---------\\nWhen the proportion of outliers is high (i.e. greater than 10 %, as in the\\nexample below), n_neighbors should be greater (n_neighbors=35 in the example\\nbelow).\\nThe strength of the LOF algorithm is that it takes both local and global\\nproperties of datasets into consideration: it can perform well even in datasets\\nwhere abnormal samples have different underlying densities.\\nThe question is not, how isolated the sample is, but how isolated it is\\nwith respect to the surrounding neighborhood.\\nWhen applying LOF for outlier detection, there are no predict,\\ndecision_function and score_samples methods but only a fit_predict\\nmethod. The scores of abnormality of the training samples are accessible\\nthrough the negative_outlier_factor_ attribute.\\nNote that predict, decision_function and score_samples can be used\\non new unseen data when LOF is applied for novelty detection, i.e. when the\\nnovelty parameter is set to True, but the result of predict may\\ndiffer from that of fit_predict. See Novelty detection with Local Outlier Factor.\\nThis strategy is illustrated below.\\n---------new doc---------\\nExamples\\n\\nSee Outlier detection with Local Outlier Factor (LOF)\\nfor an illustration of the use of neighbors.LocalOutlierFactor.\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison with other anomaly detection methods.\\n\\nReferences\\n\\nBreunig, Kriegel, Ng, and Sander (2000)\\nLOF: identifying density-based local outliers.\\nProc. ACM SIGMOD\\n\\n\\n\\n\\n2.7.4. Novelty detection with Local Outlier Factor#\\nTo use neighbors.LocalOutlierFactor for novelty detection, i.e.\\npredict labels or compute the score of abnormality of new unseen data, you\\nneed to instantiate the estimator with the novelty parameter\\nset to True before fitting the estimator:\\nlof = LocalOutlierFactor(novelty=True)\\nlof.fit(X_train)\\n\\n\\nNote that fit_predict is not available in this case to avoid inconsistencies.\\n\\nWarning\\nNovelty detection with Local Outlier Factor`\\nWhen novelty is set to True be aware that you must only use\\npredict, decision_function and score_samples on new unseen data\\nand not on the training samples as this would lead to wrong results.\\nI.e., the result of predict will not be the same as fit_predict.\\nThe scores of abnormality of the training samples are always accessible\\nthrough the negative_outlier_factor_ attribute.\\n\\nNovelty detection with Local Outlier Factor is illustrated below.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n2.6. Covariance estimation\\n\\n\\n\\n\\nnext\\n2.8. Density Estimation\\n\\n\\n --- \\n\\n\\n2. Unsupervised learning\\n2.8. Density Estimation\\n---------new doc---------\\nIn scikit-learn a random split into training and test sets\\ncan be quickly computed with the train_test_split helper function.\\nLet’s load the iris data set to fit a linear support vector machine on it:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn import datasets\\n>>> from sklearn import svm\\n\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> X.shape, y.shape\\n((150, 4), (150,))\\n\\n\\nWe can now quickly sample a training set while holding out 40% of the\\ndata for testing (evaluating) our classifier:\\n>>> X_train, X_test, y_train, y_test = train_test_split(\\n...     X, y, test_size=0.4, random_state=0)\\n\\n>>> X_train.shape, y_train.shape\\n((90, 4), (90,))\\n>>> X_test.shape, y_test.shape\\n((60, 4), (60,))\\n\\n>>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.96...\\n---------new doc---------\\n>>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.96...\\n\\n\\nWhen evaluating different settings (“hyperparameters”) for estimators,\\nsuch as the C setting that must be manually set for an SVM,\\nthere is still a risk of overfitting on the test set\\nbecause the parameters can be tweaked until the estimator performs optimally.\\nThis way, knowledge about the test set can “leak” into the model\\nand evaluation metrics no longer report on generalization performance.\\nTo solve this problem, yet another part of the dataset can be held out\\nas a so-called “validation set”: training proceeds on the training set,\\nafter which evaluation is done on the validation set,\\nand when the experiment seems to be successful,\\nfinal evaluation can be done on the test set.\\nHowever, by partitioning the available data into three sets,\\nwe drastically reduce the number of samples\\nwhich can be used for learning the model,\\nand the results can depend on a particular random choice for the pair of\\n(train, validation) sets.\\nA solution to this problem is a procedure called\\ncross-validation\\n(CV for short).\\nA test set should still be held out for final evaluation,\\nbut the validation set is no longer needed when doing CV.\\nIn the basic approach, called k-fold CV,\\nthe training set is split into k smaller sets\\n(other approaches are described below,\\nbut generally follow the same principles).\\nThe following procedure is followed for each of the k “folds”:\\n---------new doc---------\\nA model is trained using \\\\(k-1\\\\) of the folds as training data;\\nthe resulting model is validated on the remaining part of the data\\n(i.e., it is used as a test set to compute a performance measure\\nsuch as accuracy).\\n\\nThe performance measure reported by k-fold cross-validation\\nis then the average of the values computed in the loop.\\nThis approach can be computationally expensive,\\nbut does not waste too much data\\n(as is the case when fixing an arbitrary validation set),\\nwhich is a major advantage in problems such as inverse inference\\nwhere the number of samples is very small.\\n\\n\\n\\n3.1.1. Computing cross-validated metrics#\\nThe simplest way to use cross-validation is to call the\\ncross_val_score helper function on the estimator and the dataset.\\nThe following example demonstrates how to estimate the accuracy of a linear\\nkernel support vector machine on the iris dataset by splitting the data, fitting\\na model and computing the score 5 consecutive times (with different splits each\\ntime):\\n>>> from sklearn.model_selection import cross_val_score\\n>>> clf = svm.SVC(kernel='linear', C=1, random_state=42)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores\\narray([0.96..., 1. , 0.96..., 0.96..., 1. ])\\n\\n\\nThe mean score and the standard deviation are hence given by:\\n>>> print(\\\"%0.2f accuracy with a standard deviation of %0.2f\\\" % (scores.mean(), scores.std()))\\n0.98 accuracy with a standard deviation of 0.02\\n---------new doc---------\\nSee The scoring parameter: defining model evaluation rules for details.\\nIn the case of the Iris dataset, the samples are balanced across target\\nclasses hence the accuracy and the F1-score are almost equal.\\nWhen the cv argument is an integer, cross_val_score uses the\\nKFold or StratifiedKFold strategies by default, the latter\\nbeing used if the estimator derives from ClassifierMixin.\\nIt is also possible to use other cross validation strategies by passing a cross\\nvalidation iterator instead, for instance:\\n>>> from sklearn.model_selection import ShuffleSplit\\n>>> n_samples = X.shape[0]\\n>>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\\n>>> cross_val_score(clf, X, y, cv=cv)\\narray([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])\\n---------new doc---------\\nData transformation with held-out data#\\nJust as it is important to test a predictor on data held-out from\\ntraining, preprocessing (such as standardization, feature selection, etc.)\\nand similar data transformations similarly should\\nbe learnt from a training set and applied to held-out data for prediction:\\n>>> from sklearn import preprocessing\\n>>> X_train, X_test, y_train, y_test = train_test_split(\\n...     X, y, test_size=0.4, random_state=0)\\n>>> scaler = preprocessing.StandardScaler().fit(X_train)\\n>>> X_train_transformed = scaler.transform(X_train)\\n>>> clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\\n>>> X_test_transformed = scaler.transform(X_test)\\n>>> clf.score(X_test_transformed, y_test)\\n0.9333...\\n\\n\\nA Pipeline makes it easier to compose\\nestimators, providing this behavior under cross-validation:\\n>>> from sklearn.pipeline import make_pipeline\\n>>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\\n>>> cross_val_score(clf, X, y, cv=cv)\\narray([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])\\n\\n\\nSee Pipelines and composite estimators.\\n\\n\\n3.1.1.1. The cross_validate function and multiple metric evaluation#\\nThe cross_validate function differs from cross_val_score in\\ntwo ways:\\n---------new doc---------\\n3.1.1.2. Obtaining predictions by cross-validation#\\nThe function cross_val_predict has a similar interface to\\ncross_val_score, but returns, for each element in the input, the\\nprediction that was obtained for that element when it was in the test set. Only\\ncross-validation strategies that assign all elements to a test set exactly once\\ncan be used (otherwise, an exception is raised).\\n\\nWarning\\nNote on inappropriate usage of cross_val_predict\\nThe result of cross_val_predict may be different from those\\nobtained using cross_val_score as the elements are grouped in\\ndifferent ways. The function cross_val_score takes an average\\nover cross-validation folds, whereas cross_val_predict simply\\nreturns the labels (or probabilities) from several distinct models\\nundistinguished. Thus, cross_val_predict is not an appropriate\\nmeasure of generalization error.\\n\\n\\nThe function cross_val_predict is appropriate for:\\nVisualization of predictions obtained from different models.\\nModel blending: When predictions of one supervised estimator are used to\\ntrain another estimator in ensemble methods.\\n\\n\\n\\nThe available cross validation iterators are introduced in the following\\nsection.\\nExamples\\n\\nReceiver Operating Characteristic (ROC) with cross validation,\\nRecursive feature elimination with cross-validation,\\nCustom refit strategy of a grid search with cross-validation,\\nSample pipeline for text feature extraction and evaluation,\\nPlotting Cross-Validated Predictions,\\nNested versus non-nested cross-validation.\\n\\n\\n\\n\\n3.1.2. Cross validation iterators#\\nThe following sections list utilities to generate indices\\nthat can be used to generate dataset splits according to different cross\\nvalidation strategies.\\n---------new doc---------\\n3.1.2. Cross validation iterators#\\nThe following sections list utilities to generate indices\\nthat can be used to generate dataset splits according to different cross\\nvalidation strategies.\\n\\n3.1.2.1. Cross-validation iterators for i.i.d. data#\\nAssuming that some data is Independent and Identically Distributed (i.i.d.) is\\nmaking the assumption that all samples stem from the same generative process\\nand that the generative process is assumed to have no memory of past generated\\nsamples.\\nThe following cross-validators can be used in such cases.\\n\\nNote\\nWhile i.i.d. data is a common assumption in machine learning theory, it rarely\\nholds in practice. If one knows that the samples have been generated using a\\ntime-dependent process, it is safer to\\nuse a time-series aware cross-validation scheme.\\nSimilarly, if we know that the generative process has a group structure\\n(samples collected from different subjects, experiments, measurement\\ndevices), it is safer to use group-wise cross-validation.\\n\\n\\n3.1.2.1.1. K-fold#\\nKFold divides all the samples in \\\\(k\\\\) groups of samples,\\ncalled folds (if \\\\(k = n\\\\), this is equivalent to the Leave One\\nOut strategy), of equal sizes (if possible). The prediction function is\\nlearned using \\\\(k - 1\\\\) folds, and the fold left out is used for test.\\nExample of 2-fold cross-validation on a dataset with 4 samples:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import KFold\\n---------new doc---------\\n>>> X = [\\\"a\\\", \\\"b\\\", \\\"c\\\", \\\"d\\\"]\\n>>> kf = KFold(n_splits=2)\\n>>> for train, test in kf.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[2 3] [0 1]\\n[0 1] [2 3]\\n\\n\\nHere is a visualization of the cross-validation behavior. Note that\\nKFold is not affected by classes or groups.\\n\\n\\n\\n\\nEach fold is constituted by two arrays: the first one is related to the\\ntraining set, and the second one to the test set.\\nThus, one can create the training/test sets using numpy indexing:\\n>>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\\n>>> y = np.array([0, 1, 0, 1])\\n>>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\\n---------new doc---------\\n3.1.2.1.2. Repeated K-Fold#\\nRepeatedKFold repeats K-Fold n times. It can be used when one\\nrequires to run KFold n times, producing different splits in\\neach repetition.\\nExample of 2-fold K-Fold repeated 2 times:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import RepeatedKFold\\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\\n>>> random_state = 12883823\\n>>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\\n>>> for train, test in rkf.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n...\\n[2 3] [0 1]\\n[0 1] [2 3]\\n[0 2] [1 3]\\n[1 3] [0 2]\\n\\n\\nSimilarly, RepeatedStratifiedKFold repeats Stratified K-Fold n times\\nwith different randomization in each repetition.\\n\\n\\n3.1.2.1.3. Leave One Out (LOO)#\\nLeaveOneOut (or LOO) is a simple cross-validation. Each learning\\nset is created by taking all the samples except one, the test set being\\nthe sample left out. Thus, for \\\\(n\\\\) samples, we have \\\\(n\\\\) different\\ntraining sets and \\\\(n\\\\) different tests set. This cross-validation\\nprocedure does not waste much data as only one sample is removed from the\\ntraining set:\\n>>> from sklearn.model_selection import LeaveOneOut\\n---------new doc---------\\n>>> X = [1, 2, 3, 4]\\n>>> loo = LeaveOneOut()\\n>>> for train, test in loo.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[1 2 3] [0]\\n[0 2 3] [1]\\n[0 1 3] [2]\\n[0 1 2] [3]\\n\\n\\nPotential users of LOO for model selection should weigh a few known caveats.\\nWhen compared with \\\\(k\\\\)-fold cross validation, one builds \\\\(n\\\\) models\\nfrom \\\\(n\\\\) samples instead of \\\\(k\\\\) models, where \\\\(n > k\\\\).\\nMoreover, each is trained on \\\\(n - 1\\\\) samples rather than\\n\\\\((k-1) n / k\\\\). In both ways, assuming \\\\(k\\\\) is not too large\\nand \\\\(k < n\\\\), LOO is more computationally expensive than \\\\(k\\\\)-fold\\ncross validation.\\nIn terms of accuracy, LOO often results in high variance as an estimator for the\\ntest error. Intuitively, since \\\\(n - 1\\\\) of\\nthe \\\\(n\\\\) samples are used to build each model, models constructed from\\nfolds are virtually identical to each other and to the model built from the\\nentire training set.\\nHowever, if the learning curve is steep for the training size in question,\\nthen 5- or 10- fold cross validation can overestimate the generalization error.\\nAs a general rule, most authors, and empirical evidence, suggest that 5- or 10-\\nfold cross validation should be preferred to LOO.\\n\\n\\nReferences#\\n---------new doc---------\\nReferences#\\n\\nhttp://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html;\\nT. Hastie, R. Tibshirani, J. Friedman,  The Elements of Statistical Learning, Springer 2009\\nL. Breiman, P. Spector Submodel selection and evaluation in regression: The X-random case, International Statistical Review 1992;\\nR. Kohavi, A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, Intl. Jnt. Conf. AI\\nR. Bharat Rao, G. Fung, R. Rosales, On the Dangers of Cross-Validation. An Experimental Evaluation, SIAM 2008;\\nG. James, D. Witten, T. Hastie, R Tibshirani, An Introduction to\\nStatistical Learning, Springer 2013.\\n\\n\\n\\n\\n3.1.2.1.4. Leave P Out (LPO)#\\nLeavePOut is very similar to LeaveOneOut as it creates all\\nthe possible training/test sets by removing \\\\(p\\\\) samples from the complete\\nset. For \\\\(n\\\\) samples, this produces \\\\({n \\\\choose p}\\\\) train-test\\npairs. Unlike LeaveOneOut and KFold, the test sets will\\noverlap for \\\\(p > 1\\\\).\\nExample of Leave-2-Out on a dataset with 4 samples:\\n>>> from sklearn.model_selection import LeavePOut\\n---------new doc---------\\n>>> X = np.ones(4)\\n>>> lpo = LeavePOut(p=2)\\n>>> for train, test in lpo.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[2 3] [0 1]\\n[1 3] [0 2]\\n[1 2] [0 3]\\n[0 3] [1 2]\\n[0 2] [1 3]\\n[0 1] [2 3]\\n\\n\\n\\n\\n3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split#\\nThe ShuffleSplit iterator will generate a user defined number of\\nindependent train / test dataset splits. Samples are first shuffled and\\nthen split into a pair of train and test sets.\\nIt is possible to control the randomness for reproducibility of the\\nresults by explicitly seeding the random_state pseudo random number\\ngenerator.\\nHere is a usage example:\\n>>> from sklearn.model_selection import ShuffleSplit\\n>>> X = np.arange(10)\\n>>> ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\\n>>> for train_index, test_index in ss.split(X):\\n...     print(\\\"%s %s\\\" % (train_index, test_index))\\n[9 1 6 7 3 0 5] [2 8 4]\\n[2 9 8 0 6 7 4] [3 5 1]\\n[4 5 1 0 6 9 7] [2 3 8]\\n[2 7 5 8 0 3 4] [6 1 9]\\n[4 1 0 6 8 9 3] [5 2 7]\\n---------new doc---------\\nHere is a visualization of the cross-validation behavior. Note that\\nShuffleSplit is not affected by classes or groups.\\n\\n\\n\\n\\nShuffleSplit is thus a good alternative to KFold cross\\nvalidation that allows a finer control on the number of iterations and\\nthe proportion of samples on each side of the train / test split.\\n\\n\\n\\n3.1.2.2. Cross-validation iterators with stratification based on class labels#\\nSome classification problems can exhibit a large imbalance in the distribution\\nof the target classes: for instance there could be several times more negative\\nsamples than positive samples. In such cases it is recommended to use\\nstratified sampling as implemented in StratifiedKFold and\\nStratifiedShuffleSplit to ensure that relative class frequencies is\\napproximately preserved in each train and validation fold.\\n---------new doc---------\\n3.1.2.2.1. Stratified k-fold#\\nStratifiedKFold is a variation of k-fold which returns stratified\\nfolds: each set contains approximately the same percentage of samples of each\\ntarget class as the complete set.\\nHere is an example of stratified 3-fold cross-validation on a dataset with 50 samples from\\ntwo unbalanced classes.  We show the number of samples in each class and compare with\\nKFold.\\n>>> from sklearn.model_selection import StratifiedKFold, KFold\\n>>> import numpy as np\\n>>> X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\\n>>> skf = StratifiedKFold(n_splits=3)\\n>>> for train, test in skf.split(X, y):\\n...     print('train -  {}   |   test -  {}'.format(\\n...         np.bincount(y[train]), np.bincount(y[test])))\\ntrain -  [30  3]   |   test -  [15  2]\\ntrain -  [30  3]   |   test -  [15  2]\\ntrain -  [30  4]   |   test -  [15  1]\\n>>> kf = KFold(n_splits=3)\\n>>> for train, test in kf.split(X, y):\\n...     print('train -  {}   |   test -  {}'.format(\\n---------new doc---------\\nWe can see that StratifiedKFold preserves the class ratios\\n(approximately 1 / 10) in both train and test dataset.\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nRepeatedStratifiedKFold can be used to repeat Stratified K-Fold n times\\nwith different randomization in each repetition.\\n\\n\\n3.1.2.2.2. Stratified Shuffle Split#\\nStratifiedShuffleSplit is a variation of ShuffleSplit, which returns\\nstratified splits, i.e which creates splits by preserving the same\\npercentage for each target class as in the complete set.\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\n\\n\\n\\n3.1.2.3. Predefined fold-splits / Validation-sets#\\nFor some datasets, a pre-defined split of the data into training- and\\nvalidation fold or into several cross-validation folds already\\nexists. Using PredefinedSplit it is possible to use these folds\\ne.g. when searching for hyperparameters.\\nFor example, when using a validation set, set the test_fold to 0 for all\\nsamples that are part of the validation set, and to -1 for all other samples.\\n---------new doc---------\\n3.1.2.4. Cross-validation iterators for grouped data#\\nThe i.i.d. assumption is broken if the underlying generative process yields\\ngroups of dependent samples.\\nSuch a grouping of data is domain specific. An example would be when there is\\nmedical data collected from multiple patients, with multiple samples taken from\\neach patient. And such data is likely to be dependent on the individual group.\\nIn our example, the patient id for each sample will be its group identifier.\\nIn this case we would like to know if a model trained on a particular set of\\ngroups generalizes well to the unseen groups. To measure this, we need to\\nensure that all the samples in the validation fold come from groups that are\\nnot represented at all in the paired training fold.\\nThe following cross-validation splitters can be used to do that.\\nThe grouping identifier for the samples is specified via the groups\\nparameter.\\n\\n3.1.2.4.1. Group k-fold#\\nGroupKFold is a variation of k-fold which ensures that the same group is\\nnot represented in both testing and training sets. For example if the data is\\nobtained from different subjects with several samples per-subject and if the\\nmodel is flexible enough to learn from highly person specific features it\\ncould fail to generalize to new subjects. GroupKFold makes it possible\\nto detect this kind of overfitting situations.\\nImagine you have three subjects, each with an associated number from 1 to 3:\\n>>> from sklearn.model_selection import GroupKFold\\n---------new doc---------\\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\\n>>> y = [\\\"a\\\", \\\"b\\\", \\\"b\\\", \\\"b\\\", \\\"c\\\", \\\"c\\\", \\\"c\\\", \\\"d\\\", \\\"d\\\", \\\"d\\\"]\\n>>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\\n\\n>>> gkf = GroupKFold(n_splits=3)\\n>>> for train, test in gkf.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[0 1 2 3 4 5] [6 7 8 9]\\n[0 1 2 6 7 8 9] [3 4 5]\\n[3 4 5 6 7 8 9] [0 1 2]\\n\\n\\nEach subject is in a different testing fold, and the same subject is never in\\nboth testing and training. Notice that the folds do not have exactly the same\\nsize due to the imbalance in the data. If class proportions must be balanced\\nacross folds, StratifiedGroupKFold is a better option.\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nSimilar to KFold, the test sets from GroupKFold will form a\\ncomplete partition of all the data.\\nWhile GroupKFold attempts to place the same number of samples in each\\nfold when shuffle=False, when shuffle=True it attempts to place equal\\nnumber of distinct groups in each fold (but doesn not account for group sizes).\\n---------new doc---------\\n3.1.2.4.2. StratifiedGroupKFold#\\nStratifiedGroupKFold is a cross-validation scheme that combines both\\nStratifiedKFold and GroupKFold. The idea is to try to\\npreserve the distribution of classes in each split while keeping each group\\nwithin a single split. That might be useful when you have an unbalanced\\ndataset so that using just GroupKFold might produce skewed splits.\\nExample:\\n>>> from sklearn.model_selection import StratifiedGroupKFold\\n>>> X = list(range(18))\\n>>> y = [1] * 6 + [0] * 12\\n>>> groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]\\n>>> sgkf = StratifiedGroupKFold(n_splits=3)\\n>>> for train, test in sgkf.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]\\n[ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]\\n[ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]\\n\\n\\n\\n\\nImplementation notes#\\n\\nWith the current implementation full shuffle is not possible in most\\nscenarios. When shuffle=True, the following happens:\\n---------new doc---------\\nImplementation notes#\\n\\nWith the current implementation full shuffle is not possible in most\\nscenarios. When shuffle=True, the following happens:\\n\\nAll groups are shuffled.\\nGroups are sorted by standard deviation of classes using stable sort.\\nSorted groups are iterated over and assigned to folds.\\n\\nThat means that only groups with the same standard deviation of class\\ndistribution will be shuffled, which might be useful when each group has only\\na single class.\\n\\nThe algorithm greedily assigns each group to one of n_splits test sets,\\nchoosing the test set that minimises the variance in class distribution\\nacross test sets. Group assignment proceeds from groups with highest to\\nlowest variance in class frequency, i.e. large groups peaked on one or few\\nclasses are assigned first.\\nThis split is suboptimal in a sense that it might produce imbalanced splits\\neven if perfect stratification is possible. If you have relatively close\\ndistribution of classes in each group, using GroupKFold is better.\\n\\n\\nHere is a visualization of cross-validation behavior for uneven groups:\\n---------new doc---------\\nHere is a visualization of cross-validation behavior for uneven groups:\\n\\n\\n\\n\\n\\n\\n3.1.2.4.3. Leave One Group Out#\\nLeaveOneGroupOut is a cross-validation scheme where each split holds\\nout samples belonging to one specific group. Group information is\\nprovided via an array that encodes the group of each sample.\\nEach training set is thus constituted by all the samples except the ones\\nrelated to a specific group. This is the same as LeavePGroupsOut with\\nn_groups=1 and the same as GroupKFold with n_splits equal to the\\nnumber of unique labels passed to the groups parameter.\\nFor example, in the cases of multiple experiments, LeaveOneGroupOut\\ncan be used to create a cross-validation based on the different experiments:\\nwe create a training set using the samples of all the experiments except one:\\n>>> from sklearn.model_selection import LeaveOneGroupOut\\n\\n>>> X = [1, 5, 10, 50, 60, 70, 80]\\n>>> y = [0, 1, 1, 2, 2, 2, 2]\\n>>> groups = [1, 1, 2, 2, 3, 3, 3]\\n>>> logo = LeaveOneGroupOut()\\n>>> for train, test in logo.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[2 3 4 5 6] [0 1]\\n[0 1 4 5 6] [2 3]\\n[0 1 2 3] [4 5 6]\\n\\n\\nAnother common application is to use time information: for instance the\\ngroups could be the year of collection of the samples and thus allow\\nfor cross-validation against time-based splits.\\n---------new doc---------\\nAnother common application is to use time information: for instance the\\ngroups could be the year of collection of the samples and thus allow\\nfor cross-validation against time-based splits.\\n\\n\\n3.1.2.4.4. Leave P Groups Out#\\nLeavePGroupsOut is similar as LeaveOneGroupOut, but removes\\nsamples related to \\\\(P\\\\) groups for each training/test set. All possible\\ncombinations of \\\\(P\\\\) groups are left out, meaning test sets will overlap\\nfor \\\\(P>1\\\\).\\nExample of Leave-2-Group Out:\\n>>> from sklearn.model_selection import LeavePGroupsOut\\n\\n>>> X = np.arange(6)\\n>>> y = [1, 1, 1, 2, 2, 2]\\n>>> groups = [1, 1, 2, 2, 3, 3]\\n>>> lpgo = LeavePGroupsOut(n_groups=2)\\n>>> for train, test in lpgo.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[4 5] [0 1 2 3]\\n[2 3] [0 1 4 5]\\n[0 1] [2 3 4 5]\\n\\n\\n\\n\\n3.1.2.4.5. Group Shuffle Split#\\nThe GroupShuffleSplit iterator behaves as a combination of\\nShuffleSplit and LeavePGroupsOut, and generates a\\nsequence of randomized partitions in which a subset of groups are held\\nout for each split. Each train/test split is performed independently meaning\\nthere is no guaranteed relationship between successive test sets.\\nHere is a usage example:\\n>>> from sklearn.model_selection import GroupShuffleSplit\\n---------new doc---------\\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\\n>>> y = [\\\"a\\\", \\\"b\\\", \\\"b\\\", \\\"b\\\", \\\"c\\\", \\\"c\\\", \\\"c\\\", \\\"a\\\"]\\n>>> groups = [1, 1, 2, 2, 3, 3, 4, 4]\\n>>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\\n>>> for train, test in gss.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n...\\n[0 1 2 3] [4 5 6 7]\\n[2 3 6 7] [0 1 4 5]\\n[2 3 4 5] [0 1 6 7]\\n[4 5 6 7] [0 1 2 3]\\n\\n\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nThis class is useful when the behavior of LeavePGroupsOut is\\ndesired, but the number of groups is large enough that generating all\\npossible partitions with \\\\(P\\\\) groups withheld would be prohibitively\\nexpensive. In such a scenario, GroupShuffleSplit provides\\na random sample (with replacement) of the train / test splits\\ngenerated by LeavePGroupsOut.\\n---------new doc---------\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nThis class is useful when the behavior of LeavePGroupsOut is\\ndesired, but the number of groups is large enough that generating all\\npossible partitions with \\\\(P\\\\) groups withheld would be prohibitively\\nexpensive. In such a scenario, GroupShuffleSplit provides\\na random sample (with replacement) of the train / test splits\\ngenerated by LeavePGroupsOut.\\n\\n\\n\\n3.1.2.5. Using cross-validation iterators to split train and test#\\nThe above group cross-validation functions may also be useful for splitting a\\ndataset into training and testing subsets. Note that the convenience\\nfunction train_test_split is a wrapper around ShuffleSplit\\nand thus only allows for stratified splitting (using the class labels)\\nand cannot account for groups.\\nTo perform the train and test split, use the indices for the train and test\\nsubsets yielded by the generator output by the split() method of the\\ncross-validation splitter. For example:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import GroupShuffleSplit\\n---------new doc---------\\n3.1.2.6. Cross validation of time series data#\\nTime series data is characterized by the correlation between observations\\nthat are near in time (autocorrelation). However, classical\\ncross-validation techniques such as KFold and\\nShuffleSplit assume the samples are independent and\\nidentically distributed, and would result in unreasonable correlation\\nbetween training and testing instances (yielding poor estimates of\\ngeneralization error) on time series data. Therefore, it is very important\\nto evaluate our model for time series data on the “future” observations\\nleast like those that are used to train the model. To achieve this, one\\nsolution is provided by TimeSeriesSplit.\\n\\n3.1.2.6.1. Time Series Split#\\nTimeSeriesSplit is a variation of k-fold which\\nreturns first \\\\(k\\\\) folds as train set and the \\\\((k+1)\\\\) th\\nfold as test set. Note that unlike standard cross-validation methods,\\nsuccessive training sets are supersets of those that come before them.\\nAlso, it adds all surplus data to the first training partition, which\\nis always used to train the model.\\nThis class can be used to cross-validate time series data samples\\nthat are observed at fixed time intervals.\\nExample of 3-split time series cross-validation on a dataset with 6 samples:\\n>>> from sklearn.model_selection import TimeSeriesSplit\\n---------new doc---------\\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\\n>>> y = np.array([1, 2, 3, 4, 5, 6])\\n>>> tscv = TimeSeriesSplit(n_splits=3)\\n>>> print(tscv)\\nTimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)\\n>>> for train, test in tscv.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[0 1 2] [3]\\n[0 1 2 3] [4]\\n[0 1 2 3 4] [5]\\n\\n\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\n\\n\\n\\n\\n3.1.3. A note on shuffling#\\nIf the data ordering is not arbitrary (e.g. samples with the same class label\\nare contiguous), shuffling it first may be essential to get a meaningful cross-\\nvalidation result. However, the opposite may be true if the samples are not\\nindependently and identically distributed. For example, if samples correspond\\nto news articles, and are ordered by their time of publication, then shuffling\\nthe data will likely lead to a model that is overfit and an inflated validation\\nscore: it will be tested on samples that are artificially similar (close in\\ntime) to training samples.\\nSome cross validation iterators, such as KFold, have an inbuilt option\\nto shuffle the data indices before splitting them. Note that:\\n---------new doc---------\\nThis consumes less memory than shuffling the data directly.\\nBy default no shuffling occurs, including for the (stratified) K fold cross-\\nvalidation performed by specifying cv=some_integer to\\ncross_val_score, grid search, etc. Keep in mind that\\ntrain_test_split still returns a random split.\\nThe random_state parameter defaults to None, meaning that the\\nshuffling will be different every time KFold(..., shuffle=True) is\\niterated. However, GridSearchCV will use the same shuffling for each set\\nof parameters validated by a single call to its fit method.\\nTo get identical results for each split, set random_state to an integer.\\n\\nFor more details on how to control the randomness of cv splitters and avoid\\ncommon pitfalls, see Controlling randomness.\\n\\n\\n3.1.4. Cross validation and model selection#\\nCross validation iterators can also be used to directly perform model\\nselection using Grid Search for the optimal hyperparameters of the\\nmodel. This is the topic of the next section: Tuning the hyper-parameters of an estimator.\\n---------new doc---------\\nthe data. In the latter case, using a more appropriate classifier that\\nis able to utilize the structure in the data, would result in a lower\\np-value.\\nCross-validation provides information about how well a classifier generalizes,\\nspecifically the range of expected errors of the classifier. However, a\\nclassifier trained on a high dimensional dataset with no structure may still\\nperform better than expected on cross-validation, just by chance.\\nThis can typically happen with small datasets with less than a few hundred\\nsamples.\\npermutation_test_score provides information\\non whether the classifier has found a real class structure and can help in\\nevaluating the performance of the classifier.\\nIt is important to note that this test has been shown to produce low\\np-values even if there is only weak structure in the data because in the\\ncorresponding permutated datasets there is absolutely no structure. This\\ntest is therefore only able to show when the model reliably outperforms\\nrandom guessing.\\nFinally, permutation_test_score is computed\\nusing brute force and internally fits (n_permutations + 1) * n_cv models.\\nIt is therefore only tractable with small datasets for which fitting an\\nindividual model is very fast.\\nExamples\\n---------new doc---------\\nTest with permutations the significance of a classification score\\n\\n\\n\\nReferences#\\n\\nOjala and Garriga. Permutation Tests for Studying Classifier Performance.\\nJ. Mach. Learn. Res. 2010.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3. Model selection and evaluation\\n\\n\\n\\n\\nnext\\n3.2. Tuning the hyper-parameters of an estimator\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.2. Tuning the hyper-parameters of an estimator\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3.2. Tuning the hyper-parameters of an estimator#\\nHyper-parameters are parameters that are not directly learnt within estimators.\\nIn scikit-learn they are passed as arguments to the constructor of the\\nestimator classes. Typical examples include C, kernel and gamma\\nfor Support Vector Classifier, alpha for Lasso, etc.\\nIt is possible and recommended to search the hyper-parameter space for the\\nbest cross validation score.\\nAny parameter provided when constructing an estimator may be optimized in this\\nmanner. Specifically, to find the names and current values for all parameters\\nfor a given estimator, use:\\nestimator.get_params()\\n\\n\\nA search consists of:\\n\\nan estimator (regressor or classifier such as sklearn.svm.SVC());\\na parameter space;\\na method for searching or sampling candidates;\\na cross-validation scheme; and\\na score function.\\n---------new doc---------\\nAdvanced examples#\\n\\nSee Nested versus non-nested cross-validation\\nfor an example of Grid Search within a cross validation loop on the iris\\ndataset. This is the best practice for evaluating the performance of a\\nmodel with grid search.\\nSee Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV\\nfor an example of GridSearchCV being used to evaluate multiple\\nmetrics simultaneously.\\nSee Balance model complexity and cross-validated score\\nfor an example of using refit=callable interface in\\nGridSearchCV. The example shows how this interface adds certain\\namount of flexibility in identifying the “best” estimator. This interface\\ncan also be used in multiple metrics evaluation.\\nSee Statistical comparison of models using grid search\\nfor an example of how to do a statistical comparison on the outputs of\\nGridSearchCV.\\n\\n\\n\\n\\n3.2.2. Randomized Parameter Optimization#\\nWhile using a grid of parameter settings is currently the most widely used\\nmethod for parameter optimization, other search methods have more\\nfavorable properties.\\nRandomizedSearchCV implements a randomized search over parameters,\\nwhere each setting is sampled from a distribution over possible parameter values.\\nThis has two main benefits over an exhaustive search:\\n\\nA budget can be chosen independent of the number of parameters and possible values.\\nAdding parameters that do not influence the performance does not decrease efficiency.\\n---------new doc---------\\nA budget can be chosen independent of the number of parameters and possible values.\\nAdding parameters that do not influence the performance does not decrease efficiency.\\n\\nSpecifying how parameters should be sampled is done using a dictionary, very\\nsimilar to specifying parameters for GridSearchCV. Additionally,\\na computation budget, being the number of sampled candidates or sampling\\niterations, is specified using the n_iter parameter.\\nFor each parameter, either a distribution over possible values or a list of\\ndiscrete choices (which will be sampled uniformly) can be specified:\\n{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\\n  'kernel': ['rbf'], 'class_weight':['balanced', None]}\\n\\n\\nThis example uses the scipy.stats module, which contains many useful\\ndistributions for sampling parameters, such as expon, gamma,\\nuniform, loguniform or randint.\\nIn principle, any function can be passed that provides a rvs (random\\nvariate sample) method to sample a value. A call to the rvs function should\\nprovide independent random samples from possible parameter values on\\nconsecutive calls.\\n\\nWarning\\nThe distributions in scipy.stats prior to version scipy 0.16\\ndo not allow specifying a random state. Instead, they use the global\\nnumpy random state, that can be seeded via np.random.seed or set\\nusing np.random.set_state. However, beginning scikit-learn 0.18,\\nthe sklearn.model_selection module sets the random state provided\\nby the user if scipy >= 0.16 is also available.\\n---------new doc---------\\nChoosing a resource#\\nBy default, the resource is defined in terms of number of samples. That is,\\neach iteration will use an increasing amount of samples to train on. You can\\nhowever manually specify a parameter to use as the resource with the\\nresource parameter. Here is an example where the resource is defined in\\nterms of the number of estimators of a random forest:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\\n>>> from sklearn.model_selection import HalvingGridSearchCV\\n>>> import pandas as pd\\n>>> param_grid = {'max_depth': [3, 5, 10],\\n...               'min_samples_split': [2, 5, 10]}\\n>>> base_estimator = RandomForestClassifier(random_state=0)\\n>>> X, y = make_classification(n_samples=1000, random_state=0)\\n>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\\n...                          factor=2, resource='n_estimators',\\n...                          max_resources=30).fit(X, y)\\n>>> sh.best_estimator_\\nRandomForestClassifier(max_depth=5, n_estimators=24, random_state=0)\\n---------new doc---------\\n3.2.3.1. Aggressive elimination of candidates#\\nUsing the aggressive_elimination parameter, you can force the search\\nprocess to end up with less than factor candidates at the last\\niteration.\\n\\n\\nCode example of aggressive elimination#\\nIdeally, we want the last iteration to evaluate factor candidates. We\\nthen just have to pick the best one. When the number of available resources is\\nsmall with respect to the number of candidates, the last iteration may have to\\nevaluate more than factor candidates:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.svm import SVC\\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\\n>>> from sklearn.model_selection import HalvingGridSearchCV\\n>>> import pandas as pd\\n>>> param_grid = {'kernel': ('linear', 'rbf'),\\n...               'C': [1, 10, 100]}\\n>>> base_estimator = SVC(gamma='scale')\\n>>> X, y = make_classification(n_samples=1000)\\n>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\\n...                          factor=2, max_resources=40,\\n...                          aggressive_elimination=False).fit(X, y)\\n>>> sh.n_resources_\\n[20, 40]\\n>>> sh.n_candidates_\\n[6, 3]\\n---------new doc---------\\nNotice that we end with 2 candidates at the last iteration since we have\\neliminated enough candidates during the first iterations, using n_resources =\\nmin_resources = 20.\\n\\n\\n\\n3.2.3.2. Analyzing results with the cv_results_ attribute#\\nThe cv_results_ attribute contains useful information for analyzing the\\nresults of a search. It can be converted to a pandas dataframe with df =\\npd.DataFrame(est.cv_results_). The cv_results_ attribute of\\nHalvingGridSearchCV and HalvingRandomSearchCV is similar\\nto that of GridSearchCV and RandomizedSearchCV, with\\nadditional information related to the successive halving process.\\n\\n\\nExample of a (truncated) output dataframe:#\\n\\n\\n\\niter\\nn_resources\\nmean_test_score\\nparams\\n\\n\\n\\n0\\n0\\n125\\n0.983667\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 5}\\n\\n1\\n0\\n125\\n0.983667\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 8, ‘min_samples_split’: 7}\\n\\n2\\n0\\n125\\n0.983667\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\\n---------new doc---------\\nHere, <estimator> is the parameter name of the nested estimator,\\nin this case estimator.\\nIf the meta-estimator is constructed as a collection of estimators as in\\npipeline.Pipeline, then <estimator> refers to the name of the estimator,\\nsee Access to nested parameters. In practice, there can be several\\nlevels of nesting:\\n>>> from sklearn.pipeline import Pipeline\\n>>> from sklearn.feature_selection import SelectKBest\\n>>> pipe = Pipeline([\\n...    ('select', SelectKBest()),\\n...    ('model', calibrated_forest)])\\n>>> param_grid = {\\n...    'select__k': [1, 2],\\n...    'model__estimator__max_depth': [2, 4, 6, 8]}\\n>>> search = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)\\n\\n\\nPlease refer to Pipeline: chaining estimators for performing parameter searches over\\npipelines.\\n\\n\\n3.2.4.4. Model selection: development and evaluation#\\nModel selection by evaluating various parameter settings can be seen as a way\\nto use the labeled data to “train” the parameters of the grid.\\nWhen evaluating the resulting model it is important to do it on\\nheld-out samples that were not seen during the grid search process:\\nit is recommended to split the data into a development set (to\\nbe fed to the GridSearchCV instance) and an evaluation set\\nto compute performance metrics.\\nThis can be done by using the train_test_split\\nutility function.\\n---------new doc---------\\nensemble.RandomForestClassifier([...])\\nA random forest classifier.\\n\\nensemble.RandomForestRegressor([...])\\nA random forest regressor.\\n\\nensemble.ExtraTreesClassifier([...])\\nAn extra-trees classifier.\\n\\nensemble.ExtraTreesRegressor([n_estimators, ...])\\nAn extra-trees regressor.\\n\\nensemble.GradientBoostingClassifier(*[, ...])\\nGradient Boosting for classification.\\n\\nensemble.GradientBoostingRegressor(*[, ...])\\nGradient Boosting for regression.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.1. Cross-validation: evaluating estimator performance\\n\\n\\n\\n\\nnext\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3.3. Tuning the decision threshold for class prediction#\\nClassification is best divided into two parts:\\n\\nthe statistical problem of learning a model to predict, ideally, class probabilities;\\nthe decision problem to take concrete action based on those probability predictions.\\n---------new doc---------\\nLet’s take a straightforward example related to weather forecasting: the first point is\\nrelated to answering “what is the chance that it will rain tomorrow?” while the second\\npoint is related to answering “should I take an umbrella tomorrow?”.\\nWhen it comes to the scikit-learn API, the first point is addressed providing scores\\nusing predict_proba or decision_function. The former returns conditional\\nprobability estimates \\\\(P(y|X)\\\\) for each class, while the latter returns a decision\\nscore for each class.\\nThe decision corresponding to the labels are obtained with predict. In binary\\nclassification, a decision rule or action is then defined by thresholding the scores,\\nleading to the prediction of a single class label for each sample. For binary\\nclassification in scikit-learn, class labels predictions are obtained by hard-coded\\ncut-off rules: a positive class is predicted when the conditional probability\\n\\\\(P(y|X)\\\\) is greater than 0.5 (obtained with predict_proba) or if the\\ndecision score is greater than 0 (obtained with decision_function).\\nHere, we show an example that illustrates the relation between conditional\\nprobability estimates \\\\(P(y|X)\\\\) and class labels:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> X, y = make_classification(random_state=0)\\n>>> classifier = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y)\\n>>> classifier.predict_proba(X[:4])\\narray([[0.94     , 0.06     ],\\n---------new doc---------\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> X, y = make_classification(random_state=0)\\n>>> classifier = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y)\\n>>> classifier.predict_proba(X[:4])\\narray([[0.94     , 0.06     ],\\n       [0.94     , 0.06     ],\\n       [0.0416..., 0.9583...],\\n       [0.0416..., 0.9583...]])\\n>>> classifier.predict(X[:4])\\narray([0, 0, 1, 1])\\n---------new doc---------\\n3.3.1. Post-tuning the decision threshold#\\nOne solution to address the problem stated in the introduction is to tune the decision\\nthreshold of the classifier once the model has been trained. The\\nTunedThresholdClassifierCV tunes this threshold using\\nan internal cross-validation. The optimum threshold is chosen to maximize a given\\nmetric.\\nThe following image illustrates the tuning of the decision threshold for a gradient\\nboosting classifier. While the vanilla and tuned classifiers provide the same\\npredict_proba outputs and thus the same Receiver Operating Characteristic (ROC)\\nand Precision-Recall curves, the class label predictions differ because of the tuned\\ndecision threshold. The vanilla classifier predicts the class of interest for a\\nconditional probability greater than 0.5 while the tuned classifier predicts the class\\nof interest for a very low probability (around 0.02). This decision threshold optimizes\\na utility metric defined by the business (in this case an insurance company).\\n\\n\\n\\n\\n\\n3.3.1.1. Options to tune the decision threshold#\\nThe decision threshold can be tuned through different strategies controlled by the\\nparameter scoring.\\nOne way to tune the threshold is by maximizing a pre-defined scikit-learn metric. These\\nmetrics can be found by calling the function get_scorer_names.\\nBy default, the balanced accuracy is the metric used but be aware that one should choose\\na meaningful metric for their use case.\\n---------new doc---------\\n3.3.1.2. Important notes regarding the internal cross-validation#\\nBy default TunedThresholdClassifierCV uses a 5-fold\\nstratified cross-validation to tune the decision threshold. The parameter cv allows to\\ncontrol the cross-validation strategy. It is possible to bypass cross-validation by\\nsetting cv=\\\"prefit\\\" and providing a fitted classifier. In this case, the decision\\nthreshold is tuned on the data provided to the fit method.\\nHowever, you should be extremely careful when using this option. You should never use\\nthe same data for training the classifier and tuning the decision threshold due to the\\nrisk of overfitting. Refer to the following example section for more details (cf.\\nConsideration regarding model refitting and cross-validation). If you have limited resources, consider using\\na float number for cv to limit to an internal single train-test split.\\nThe option cv=\\\"prefit\\\" should only be used when the provided classifier was already\\ntrained, and you just want to find the best decision threshold using a new validation\\nset.\\n\\n\\n3.3.1.3. Manually setting the decision threshold#\\nThe previous sections discussed strategies to find an optimal decision threshold. It is\\nalso possible to manually set the decision threshold using the class\\nFixedThresholdClassifier. In case that you don’t want\\nto refit the model when calling fit, wrap your sub-estimator with a\\nFrozenEstimator and do\\nFixedThresholdClassifier(FrozenEstimator(estimator), ...).\\n\\n\\n3.3.1.4. Examples#\\n---------new doc---------\\nclass 0       0.67      1.00      0.80         2\\n     class 1       0.00      0.00      0.00         1\\n     class 2       1.00      0.50      0.67         2\\n\\n    accuracy                           0.60         5\\n   macro avg       0.56      0.50      0.49         5\\nweighted avg       0.67      0.60      0.59         5\\n\\n\\nExamples\\n\\nSee Recognizing hand-written digits\\nfor an example of classification report usage for\\nhand-written digits.\\nSee Custom refit strategy of a grid search with cross-validation\\nfor an example of classification report usage for\\ngrid search with nested cross-validation.\\n---------new doc---------\\nExamples\\n\\nSee Multiclass Receiver Operating Characteristic (ROC) for an example of\\nusing ROC to evaluate the quality of the output of a classifier.\\nSee Receiver Operating Characteristic (ROC) with cross validation  for an\\nexample of using ROC to evaluate classifier output quality, using cross-validation.\\nSee Species distribution modeling\\nfor an example of using ROC to model species distribution.\\n\\nReferences\\n\\n\\n[HT2001]\\n(1,2)\\nHand, D.J. and Till, R.J., (2001). A simple generalisation\\nof the area under the ROC curve for multiple class classification problems.\\nMachine learning, 45(2), pp. 171-186.\\n\\n\\n[FC2009]\\nFerri, Cèsar & Hernandez-Orallo, Jose & Modroiu, R. (2009).\\nAn Experimental Comparison of Performance Measures for Classification.\\nPattern Recognition Letters. 30. 27-38.\\n\\n\\n[PD2000]\\nProvost, F., Domingos, P. (2000). Well-trained PETs: Improving\\nprobability estimation trees\\n(Section 6.2), CeDER Working Paper #IS-00-04, Stern School of Business,\\nNew York University.\\n\\n\\n[F2006]\\nFawcett, T., 2006. An introduction to ROC analysis.\\nPattern Recognition Letters, 27(8), pp. 861-874.\\n\\n\\n[F2001]\\nFawcett, T., 2001. Using rule sets to maximize\\nROC performance\\nIn Data Mining, 2001.\\nProceedings IEEE International Conference, pp. 131-138.\\n---------new doc---------\\nSee Effect of transforming the targets in regression model for\\nan example on how to use PredictionErrorDisplay\\nto visualize the prediction quality improvement of a regression model\\nobtained by transforming the target before learning.\\n\\n\\n\\n\\n3.4.7. Clustering metrics#\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions to measure clustering performance. For more information see the\\nClustering performance evaluation section for instance clustering, and\\nBiclustering evaluation for biclustering.\\n\\n\\n3.4.8. Dummy estimators#\\nWhen doing supervised learning, a simple sanity check consists of comparing\\none’s estimator against simple rules of thumb. DummyClassifier\\nimplements several such simple strategies for classification:\\n\\nstratified generates random predictions by respecting the training\\nset class distribution.\\nmost_frequent always predicts the most frequent label in the training set.\\nprior always predicts the class that maximizes the class prior\\n(like most_frequent) and predict_proba returns the class prior.\\nuniform generates predictions uniformly at random.\\n\\nconstant always predicts a constant label that is provided by the user.A major motivation of this method is F1-scoring, when the positive class\\nis in the minority.\\n---------new doc---------\\nconstant always predicts a constant label that is provided by the user.A major motivation of this method is F1-scoring, when the positive class\\nis in the minority.\\n\\n\\n\\n\\nNote that with all these strategies, the predict method completely ignores\\nthe input data!\\nTo illustrate DummyClassifier, first let’s create an imbalanced\\ndataset:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.model_selection import train_test_split\\n>>> X, y = load_iris(return_X_y=True)\\n>>> y[y != 1] = -1\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\n\\nNext, let’s compare the accuracy of SVC and most_frequent:\\n>>> from sklearn.dummy import DummyClassifier\\n>>> from sklearn.svm import SVC\\n>>> clf = SVC(kernel='linear', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.63...\\n>>> clf = DummyClassifier(strategy='most_frequent', random_state=0)\\n>>> clf.fit(X_train, y_train)\\nDummyClassifier(random_state=0, strategy='most_frequent')\\n>>> clf.score(X_test, y_test)\\n0.57...\\n---------new doc---------\\nWe see that SVC doesn’t do much better than a dummy classifier. Now, let’s\\nchange the kernel:\\n>>> clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.94...\\n\\n\\nWe see that the accuracy was boosted to almost 100%.  A cross validation\\nstrategy is recommended for a better estimate of the accuracy, if it\\nis not too CPU costly. For more information see the Cross-validation: evaluating estimator performance\\nsection. Moreover if you want to optimize over the parameter space, it is highly\\nrecommended to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator\\nsection for details.\\nMore generally, when the accuracy of a classifier is too close to random, it\\nprobably means that something went wrong: features are not helpful, a\\nhyperparameter is not correctly tuned, the classifier is suffering from class\\nimbalance, etc…\\nDummyRegressor also implements four simple rules of thumb for regression:\\n\\nmean always predicts the mean of the training targets.\\nmedian always predicts the median of the training targets.\\nquantile always predicts a user provided quantile of the training targets.\\nconstant always predicts a constant value that is provided by the user.\\n\\nIn all these strategies, the predict method completely ignores\\nthe input data.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n\\n\\nnext\\n3.5. Validation curves: plotting scores to evaluate models\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.5. Validation curves: plotting scores to evaluate models\\n---------new doc---------\\nIn all these strategies, the predict method completely ignores\\nthe input data.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n\\n\\nnext\\n3.5. Validation curves: plotting scores to evaluate models\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.5. Validation curves: plotting scores to evaluate models\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3.5. Validation curves: plotting scores to evaluate models#\\nEvery estimator has its advantages and drawbacks. Its generalization error\\ncan be decomposed in terms of bias, variance and noise. The bias of an\\nestimator is its average error for different training sets. The variance\\nof an estimator indicates how sensitive it is to varying training sets. Noise\\nis a property of the data.\\nIn the following plot, we see a function \\\\(f(x) = \\\\cos (\\\\frac{3}{2} \\\\pi x)\\\\)\\nand some noisy samples from that function. We use three different estimators\\nto fit the function: linear regression with polynomial features of degree 1,\\n4 and 15. We see that the first estimator can at best provide only a poor fit\\nto the samples and the true function because it is too simple (high bias),\\nthe second estimator approximates it almost perfectly and the last estimator\\napproximates the training data perfectly but does not fit the true function\\nvery well, i.e. it is very sensitive to varying training data (high variance).\\n---------new doc---------\\nUnderfitting vs. Overfitting\\nEffect of model regularization on training and test error\\nPlotting Learning Curves and Checking Models’ Scalability\\n\\n\\n3.5.1. Validation curve#\\nTo validate a model we need a scoring function (see Metrics and scoring: quantifying the quality of predictions),\\nfor example accuracy for classifiers. The proper way of choosing multiple\\nhyperparameters of an estimator is of course grid search or similar methods\\n(see Tuning the hyper-parameters of an estimator) that select the hyperparameter with the maximum score\\non a validation set or multiple validation sets. Note that if we optimize\\nthe hyperparameters based on a validation score the validation score is biased\\nand not a good estimate of the generalization any longer. To get a proper\\nestimate of the generalization we have to compute the score on another test\\nset.\\nHowever, it is sometimes helpful to plot the influence of a single\\nhyperparameter on the training score and the validation score to find out\\nwhether the estimator is overfitting or underfitting for some hyperparameter\\nvalues.\\nThe function validation_curve can help in this case:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import validation_curve\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.svm import SVC\\n\\n>>> np.random.seed(0)\\n>>> X, y = load_iris(return_X_y=True)\\n>>> indices = np.arange(y.shape[0])\\n>>> np.random.shuffle(indices)\\n>>> X, y = X[indices], y[indices]\\n---------new doc---------\\n>>> np.random.seed(0)\\n>>> X, y = load_iris(return_X_y=True)\\n>>> indices = np.arange(y.shape[0])\\n>>> np.random.shuffle(indices)\\n>>> X, y = X[indices], y[indices]\\n\\n>>> train_scores, valid_scores = validation_curve(\\n...     SVC(kernel=\\\"linear\\\"), X, y, param_name=\\\"C\\\", param_range=np.logspace(-7, 3, 3),\\n... )\\n>>> train_scores\\narray([[0.90..., 0.94..., 0.91..., 0.89..., 0.92...],\\n       [0.9... , 0.92..., 0.93..., 0.92..., 0.93...],\\n       [0.97..., 1...   , 0.98..., 0.97..., 0.99...]])\\n>>> valid_scores\\narray([[0.9..., 0.9... , 0.9... , 0.96..., 0.9... ],\\n       [0.9..., 0.83..., 0.96..., 0.96..., 0.93...],\\n       [1.... , 0.93..., 1....  , 1....  , 0.9... ]])\\n---------new doc---------\\nIf you intend to plot the validation curves only, the class\\nValidationCurveDisplay is more direct than\\nusing matplotlib manually on the results of a call to validation_curve.\\nYou can use the method\\nfrom_estimator similarly\\nto validation_curve to generate and plot the validation curve:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import ValidationCurveDisplay\\nfrom sklearn.svm import SVC\\nfrom sklearn.utils import shuffle\\nX, y = load_iris(return_X_y=True)\\nX, y = shuffle(X, y, random_state=0)\\nValidationCurveDisplay.from_estimator(\\n   SVC(kernel=\\\"linear\\\"), X, y, param_name=\\\"C\\\", param_range=np.logspace(-7, 3, 10)\\n)\\n\\n\\n\\n\\n\\nIf the training score and the validation score are both low, the estimator will\\nbe underfitting. If the training score is high and the validation score is low,\\nthe estimator is overfitting and otherwise it is working very well. A low\\ntraining score and a high validation score is usually not possible.\\n---------new doc---------\\nIf the training score and the validation score are both low, the estimator will\\nbe underfitting. If the training score is high and the validation score is low,\\nthe estimator is overfitting and otherwise it is working very well. A low\\ntraining score and a high validation score is usually not possible.\\n\\n\\n3.5.2. Learning curve#\\nA learning curve shows the validation and training score of an estimator\\nfor varying numbers of training samples. It is a tool to find out how much\\nwe benefit from adding more training data and whether the estimator suffers\\nmore from a variance error or a bias error. Consider the following example\\nwhere we plot the learning curve of a naive Bayes classifier and an SVM.\\nFor the naive Bayes, both the validation score and the training score\\nconverge to a value that is quite low with increasing size of the training\\nset. Thus, we will probably not benefit much from more training data.\\nIn contrast, for small amounts of data, the training score of the SVM is\\nmuch greater than the validation score. Adding more training samples will\\nmost likely increase generalization.\\n\\n\\n\\n\\nWe can use the function learning_curve to generate the values\\nthat are required to plot such a learning curve (number of samples\\nthat have been used, the average scores on the training sets and the\\naverage scores on the validation sets):\\n>>> from sklearn.model_selection import learning_curve\\n>>> from sklearn.svm import SVC\\n---------new doc---------\\nWe can use the function learning_curve to generate the values\\nthat are required to plot such a learning curve (number of samples\\nthat have been used, the average scores on the training sets and the\\naverage scores on the validation sets):\\n>>> from sklearn.model_selection import learning_curve\\n>>> from sklearn.svm import SVC\\n\\n>>> train_sizes, train_scores, valid_scores = learning_curve(\\n...     SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)\\n>>> train_sizes\\narray([ 50, 80, 110])\\n>>> train_scores\\narray([[0.98..., 0.98 , 0.98..., 0.98..., 0.98...],\\n       [0.98..., 1.   , 0.98..., 0.98..., 0.98...],\\n       [0.98..., 1.   , 0.98..., 0.98..., 0.99...]])\\n>>> valid_scores\\narray([[1. ,  0.93...,  1. ,  1. ,  0.96...],\\n       [1. ,  0.96...,  1. ,  1. ,  0.96...],\\n       [1. ,  0.96...,  1. ,  1. ,  0.96...]])\\n---------new doc---------\\nIf you intend to plot the learning curves only, the class\\nLearningCurveDisplay will be easier to use.\\nYou can use the method\\nfrom_estimator similarly\\nto learning_curve to generate and plot the learning curve:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import LearningCurveDisplay\\nfrom sklearn.svm import SVC\\nfrom sklearn.utils import shuffle\\nX, y = load_iris(return_X_y=True)\\nX, y = shuffle(X, y, random_state=0)\\nLearningCurveDisplay.from_estimator(\\n   SVC(kernel=\\\"linear\\\"), X, y, train_sizes=[50, 80, 110], cv=5)\\n\\n\\n\\n\\n\\nExamples\\n\\nSee Plotting Learning Curves and Checking Models’ Scalability for an\\nexample of using learning curves to check the scalability of a predictive model.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.4. Metrics and scoring: quantifying the quality of predictions\\n\\n\\n\\n\\nnext\\n4. Inspection\\n\\n\\n --- \\n\\n\\n4. Inspection\\n4.1. Partial Dependence and Individual Conditional Expectation plots\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4.1. Partial Dependence and Individual Conditional Expectation plots#\\nPartial dependence plots (PDP) and individual conditional expectation (ICE)\\nplots can be used to visualize and analyze interaction between the target\\nresponse [1] and a set of input features of interest.\\nBoth PDPs [H2009] and ICEs [G2015] assume that the input features of interest\\nare independent from the complement features, and this assumption is often\\nviolated in practice. Thus, in the case of correlated features, we will\\ncreate absurd data points to compute the PDP/ICE [M2019].\\n---------new doc---------\\nOne-way PDPs tell us about the interaction between the target response and an input\\nfeature of interest (e.g. linear, non-linear). The left plot in the above figure\\nshows the effect of the temperature on the number of bike rentals; we can clearly see\\nthat a higher temperature is related with a higher number of bike rentals. Similarly, we\\ncould analyze the effect of the humidity on the number of bike rentals (middle plot).\\nThus, these interpretations are marginal, considering a feature at a time.\\nPDPs with two input features of interest show the interactions among the two features.\\nFor example, the two-variable PDP in the above figure shows the dependence of the number\\nof bike rentals on joint values of temperature and humidity. We can clearly see an\\ninteraction between the two features: with a temperature higher than 20 degrees Celsius,\\nmainly the humidity has a strong impact on the number of bike rentals. For lower\\ntemperatures, both the temperature and the humidity have an impact on the number of bike\\nrentals.\\nThe sklearn.inspection module provides a convenience function\\nfrom_estimator to create one-way and two-way partial\\ndependence plots. In the below example we show how to create a grid of\\npartial dependence plots: two one-way PDPs for the features 0 and 1\\nand a two-way PDP between the two features:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n>>> from sklearn.inspection import PartialDependenceDisplay\\n---------new doc---------\\nThe same parameter target is used to specify the target in multi-output\\nregression settings.\\n\\nIf you need the raw values of the partial dependence function rather than\\nthe plots, you can use the\\nsklearn.inspection.partial_dependence function:\\n>>> from sklearn.inspection import partial_dependence\\n\\n>>> results = partial_dependence(clf, X, [0])\\n>>> results[\\\"average\\\"]\\narray([[ 2.466...,  2.466..., ...\\n>>> results[\\\"grid_values\\\"]\\n[array([-1.624..., -1.592..., ...\\n\\n\\nThe values at which the partial dependence should be evaluated are directly\\ngenerated from X. For 2-way partial dependence, a 2D-grid of values is\\ngenerated. The values field returned by\\nsklearn.inspection.partial_dependence gives the actual values\\nused in the grid for each input feature of interest. They also correspond to\\nthe axis of the plots.\\n\\n\\n4.1.2. Individual conditional expectation (ICE) plot#\\nSimilar to a PDP, an individual conditional expectation (ICE) plot\\nshows the dependence between the target function and an input feature of\\ninterest. However, unlike a PDP, which shows the average effect of the input\\nfeature, an ICE plot visualizes the dependence of the prediction on a\\nfeature for each sample separately with one line per sample.\\nDue to the limits of human perception, only one input feature of interest is\\nsupported for ICE plots.\\nThe figures below show two ICE plots for the bike sharing dataset,\\nwith a HistGradientBoostingRegressor:.\\nThe figures plot the corresponding PD line overlaid on ICE lines.\\n---------new doc---------\\nWhile the PDPs are good at showing the average effect of the target features,\\nthey can obscure a heterogeneous relationship created by interactions.\\nWhen interactions are present the ICE plot will provide many more insights.\\nFor example, we see that the ICE for the temperature feature gives us some\\nadditional information: Some of the ICE lines are flat while some others\\nshows a decrease of the dependence for temperature above 35 degrees Celsius.\\nWe observe a similar pattern for the humidity feature: some of the ICE\\nlines show a sharp decrease when the humidity is above 80%.\\nThe sklearn.inspection module’s PartialDependenceDisplay.from_estimator\\nconvenience function can be used to create ICE plots by setting\\nkind='individual'. In the example below, we show how to create a grid of\\nICE plots:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n>>> from sklearn.inspection import PartialDependenceDisplay\\n\\n\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\n>>> features = [0, 1]\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind='individual')\\n<...>\\n---------new doc---------\\nbeing weighted by the fraction of training samples that entered that branch.\\nFinally, the partial dependence is given by a weighted average of all the\\nvisited leaves values.\\nWith the ‘brute’ method, the parameter X is used both for generating the\\ngrid of values \\\\(x_S\\\\) and the complement feature values \\\\(x_C\\\\).\\nHowever with the ‘recursion’ method, X is only used for the grid values:\\nimplicitly, the \\\\(x_C\\\\) values are those of the training data.\\nBy default, the ‘recursion’ method is used for plotting PDPs on tree-based\\nestimators that support it, and ‘brute’ is used for the rest.\\n---------new doc---------\\nNote\\nWhile both methods should be close in general, they might differ in some\\nspecific settings. The ‘brute’ method assumes the existence of the\\ndata points \\\\((x_S, x_C^{(i)})\\\\). When the features are correlated,\\nsuch artificial samples may have a very low probability mass. The ‘brute’\\nand ‘recursion’ methods will likely disagree regarding the value of the\\npartial dependence, because they will treat these unlikely\\nsamples differently. Remember, however, that the primary assumption for\\ninterpreting PDPs is that the features should be independent.\\n\\nExamples\\n\\nPartial Dependence and Individual Conditional Expectation Plots\\n\\nFootnotes\\n\\n\\n[1]\\nFor classification, the target response may be the probability of a\\nclass (the positive class for binary classification), or the decision\\nfunction.\\n\\n\\nReferences\\n\\n\\n[H2009]\\nT. Hastie, R. Tibshirani and J. Friedman,\\nThe Elements of Statistical Learning,\\nSecond Edition, Section 10.13.2, Springer, 2009.\\n\\n\\n[M2019]\\nC. Molnar,\\nInterpretable Machine Learning,\\nSection 5.1, 2019.\\n\\n\\n[G2015]\\n(1,2)\\nA. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin,\\n“Peeking Inside the Black Box: Visualizing Statistical\\nLearning With Plots of Individual Conditional Expectation”\\nJournal of Computational and Graphical Statistics,\\n24(1): 44-65, Springer, 2015.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n4. Inspection\\n\\n\\n\\n\\nnext\\n4.2. Permutation feature importance\\n\\n\\n --- \\n\\n\\n4. Inspection\\n4.2. Permutation feature importance\\n---------new doc---------\\nprevious\\n4. Inspection\\n\\n\\n\\n\\nnext\\n4.2. Permutation feature importance\\n\\n\\n --- \\n\\n\\n4. Inspection\\n4.2. Permutation feature importance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4.2. Permutation feature importance#\\nPermutation feature importance is a model inspection technique that measures the\\ncontribution of each feature to a fitted model’s statistical performance\\non a given tabular dataset. This technique is particularly useful for non-linear\\nor opaque estimators, and involves randomly shuffling the values of a\\nsingle feature and observing the resulting degradation of the model’s score\\n[1]. By breaking the relationship between the feature and the target, we\\ndetermine how much the model relies on such particular feature.\\nIn the following figures, we observe the effect of permuting features on the correlation\\nbetween the feature and the target and consequently on the model statistical\\nperformance.\\n---------new doc---------\\nOn the top figure, we observe that permuting a predictive feature breaks the\\ncorrelation between the feature and the target, and consequently the model\\nstatistical performance decreases. On the bottom figure, we observe that permuting\\na non-predictive feature does not significantly degrade the model statistical performance.\\nOne key advantage of permutation feature importance is that it is\\nmodel-agnostic, i.e. it can be applied to any fitted estimator. Moreover, it can\\nbe calculated multiple times with different permutations of the feature, further\\nproviding a measure of the variance in the estimated feature importances for the\\nspecific trained model.\\nThe figure below shows the permutation feature importance of a\\nRandomForestClassifier trained on an augmented\\nversion of the titanic dataset that contains a random_cat and a random_num\\nfeatures, i.e. a categrical and a numerical feature that are not correlated in\\nany way with the target variable:\\n\\n\\n\\n\\n\\nWarning\\nFeatures that are deemed of low importance for a bad model (low\\ncross-validation score) could be very important for a good model.\\nTherefore it is always important to evaluate the predictive power of a model\\nusing a held-out set (or better with cross-validation) prior to computing\\nimportances. Permutation importance does not reflect to the intrinsic\\npredictive value of a feature by itself but how important this feature is\\nfor a particular model.\\n---------new doc---------\\nThe permutation_importance function calculates the feature importance\\nof estimators for a given dataset. The n_repeats parameter sets the\\nnumber of times a feature is randomly shuffled and returns a sample of feature\\nimportances.\\nLet’s consider the following trained regression model:\\n>>> from sklearn.datasets import load_diabetes\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn.linear_model import Ridge\\n>>> diabetes = load_diabetes()\\n>>> X_train, X_val, y_train, y_val = train_test_split(\\n...     diabetes.data, diabetes.target, random_state=0)\\n...\\n>>> model = Ridge(alpha=1e-2).fit(X_train, y_train)\\n>>> model.score(X_val, y_val)\\n0.356...\\n---------new doc---------\\nIts validation performance, measured via the \\\\(R^2\\\\) score, is\\nsignificantly larger than the chance level. This makes it possible to use the\\npermutation_importance function to probe which features are most\\npredictive:\\n>>> from sklearn.inspection import permutation_importance\\n>>> r = permutation_importance(model, X_val, y_val,\\n...                            n_repeats=30,\\n...                            random_state=0)\\n...\\n>>> for i in r.importances_mean.argsort()[::-1]:\\n...     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\\n...         print(f\\\"{diabetes.feature_names[i]:<8}\\\"\\n...               f\\\"{r.importances_mean[i]:.3f}\\\"\\n...               f\\\" +/- {r.importances_std[i]:.3f}\\\")\\n...\\ns5      0.204 +/- 0.050\\nbmi     0.176 +/- 0.048\\nbp      0.088 +/- 0.033\\nsex     0.056 +/- 0.023\\n---------new doc---------\\nNote that the importance values for the top features represent a large\\nfraction of the reference score of 0.356.\\nPermutation importances can be computed either on the training set or on a\\nheld-out testing or validation set. Using a held-out set makes it possible to\\nhighlight which features contribute the most to the generalization power of the\\ninspected model. Features that are important on the training set but not on the\\nheld-out set might cause the model to overfit.\\nThe permutation feature importance depends on the score function that is\\nspecified with the scoring argument. This argument accepts multiple scorers,\\nwhich is more computationally efficient than sequentially calling\\npermutation_importance several times with a different scorer, as it\\nreuses model predictions.\\n---------new doc---------\\nThe ranking of the features is approximately the same for different metrics even\\nif the scales of the importance values are very different. However, this is not\\nguaranteed and different metrics might lead to significantly different feature\\nimportances, in particular for models trained for imbalanced classification problems,\\nfor which the choice of the classification metric can be critical.\\n\\n\\n4.2.1. Outline of the permutation importance algorithm#\\n\\nInputs: fitted predictive model \\\\(m\\\\), tabular dataset (training or\\nvalidation) \\\\(D\\\\).\\nCompute the reference score \\\\(s\\\\) of the model \\\\(m\\\\) on data\\n\\\\(D\\\\) (for instance the accuracy for a classifier or the \\\\(R^2\\\\) for\\na regressor).\\nFor each feature \\\\(j\\\\) (column of \\\\(D\\\\)):\\n\\nFor each repetition \\\\(k\\\\) in \\\\({1, ..., K}\\\\):\\n\\nRandomly shuffle column \\\\(j\\\\) of dataset \\\\(D\\\\) to generate a\\ncorrupted version of the data named \\\\(\\\\tilde{D}_{k,j}\\\\).\\nCompute the score \\\\(s_{k,j}\\\\) of model \\\\(m\\\\) on corrupted data\\n\\\\(\\\\tilde{D}_{k,j}\\\\).\\n\\n\\nCompute importance \\\\(i_j\\\\) for feature \\\\(f_j\\\\) defined as:\\n\\n\\\\[i_j = s - \\\\frac{1}{K} \\\\sum_{k=1}^{K} s_{k,j}\\\\]\\n---------new doc---------\\nCompute importance \\\\(i_j\\\\) for feature \\\\(f_j\\\\) defined as:\\n\\n\\\\[i_j = s - \\\\frac{1}{K} \\\\sum_{k=1}^{K} s_{k,j}\\\\]\\n\\n\\n\\n\\n\\n\\n4.2.2. Relation to impurity-based importance in trees#\\nTree-based models provide an alternative measure of feature importances\\nbased on the mean decrease in impurity\\n(MDI). Impurity is quantified by the splitting criterion of the decision trees\\n(Gini, Log Loss or Mean Squared Error). However, this method can give high\\nimportance to features that may not be predictive on unseen data when the model\\nis overfitting. Permutation-based feature importance, on the other hand, avoids\\nthis issue, since it can be computed on unseen data.\\nFurthermore, impurity-based feature importance for trees are strongly\\nbiased and favor high cardinality features (typically numerical features)\\nover low cardinality features such as binary features or categorical variables\\nwith a small number of possible categories.\\nPermutation-based feature importances do not exhibit such a bias. Additionally,\\nthe permutation feature importance may be computed with any performance metric\\non the model predictions and can be used to analyze any model class (not just\\ntree-based models).\\nThe following example highlights the limitations of impurity-based feature\\nimportance in contrast to permutation-based feature importance:\\nPermutation Importance vs Random Forest Feature Importance (MDI).\\n---------new doc---------\\n4.2.3. Misleading values on strongly correlated features#\\nWhen two features are correlated and one of the features is permuted, the model\\nstill has access to the latter through its correlated feature. This results in a\\nlower reported importance value for both features, though they might actually\\nbe important.\\nThe figure below shows the permutation feature importance of a\\nRandomForestClassifier trained using the\\nBreast cancer wisconsin (diagnostic) dataset, which contains strongly correlated features. A\\nnaive interpretation would suggest that all features are unimportant:\\n\\n\\n\\n\\nOne way to handle the issue is to cluster features that are correlated and only\\nkeep one feature from each cluster.\\n\\n\\n\\n\\nFor more details on such strategy, see the example\\nPermutation Importance with Multicollinear or Correlated Features.\\nExamples\\n\\nPermutation Importance vs Random Forest Feature Importance (MDI)\\nPermutation Importance with Multicollinear or Correlated Features\\n\\nReferences\\n\\n\\n[1]\\nL. Breiman, “Random Forests”,\\nMachine Learning, 45(1), 5-32, 2001.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n4.1. Partial Dependence and Individual Conditional Expectation plots\\n\\n\\n\\n\\nnext\\n5. Visualizations\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.1. Pipelines and composite estimators\\n---------new doc---------\\nReferences\\n\\n\\n[1]\\nL. Breiman, “Random Forests”,\\nMachine Learning, 45(1), 5-32, 2001.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n4.1. Partial Dependence and Individual Conditional Expectation plots\\n\\n\\n\\n\\nnext\\n5. Visualizations\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.1. Pipelines and composite estimators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6.1. Pipelines and composite estimators#\\nTo build a composite estimator, transformers are usually combined with other\\ntransformers or with predictors (such as classifiers or regressors).\\nThe most common tool used for composing estimators is a Pipeline. Pipelines require all steps except the last to be a\\ntransformer. The last step can be anything, a transformer, a\\npredictor, or a clustering estimator which might have or not have a\\n.predict(...) method. A pipeline exposes all methods provided by the last\\nestimator: if the last step provides a transform method, then the pipeline\\nwould have a transform method and behave like a transformer. If the last step\\nprovides a predict method, then the pipeline would expose that method, and\\ngiven a data X, use all steps except the last to transform the data,\\nand then give that transformed data to the predict method of the last step of\\nthe pipeline. The class Pipeline is often used in combination with\\nColumnTransformer or\\nFeatureUnion which concatenate the output of transformers\\ninto a composite feature space.\\nTransformedTargetRegressor\\ndeals with transforming the target (i.e. log-transform y).\\n---------new doc---------\\n6.1.2. Transforming target in regression#\\nTransformedTargetRegressor transforms the\\ntargets y before fitting a regression model. The predictions are mapped\\nback to the original space via an inverse transform. It takes as an argument\\nthe regressor that will be used for prediction, and the transformer that will\\nbe applied to the target variable:\\n>>> import numpy as np\\n>>> from sklearn.datasets import fetch_california_housing\\n>>> from sklearn.compose import TransformedTargetRegressor\\n>>> from sklearn.preprocessing import QuantileTransformer\\n>>> from sklearn.linear_model import LinearRegression\\n>>> from sklearn.model_selection import train_test_split\\n>>> X, y = fetch_california_housing(return_X_y=True)\\n>>> X, y = X[:2000, :], y[:2000]  # select a subset of data\\n>>> transformer = QuantileTransformer(output_distribution='normal')\\n>>> regressor = LinearRegression()\\n>>> regr = TransformedTargetRegressor(regressor=regressor,\\n...                                   transformer=transformer)\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: 0.61\\n---------new doc---------\\n6.3.1. Standardization, or mean removal and variance scaling#\\nStandardization of datasets is a common requirement for many\\nmachine learning estimators implemented in scikit-learn; they might behave\\nbadly if the individual features do not more or less look like standard\\nnormally distributed data: Gaussian with zero mean and unit variance.\\nIn practice we often ignore the shape of the distribution and just\\ntransform the data to center it by removing the mean value of each\\nfeature, then scale it by dividing non-constant features by their\\nstandard deviation.\\nFor instance, many elements used in the objective function of\\na learning algorithm (such as the RBF kernel of Support Vector\\nMachines or the l1 and l2 regularizers of linear models) may assume that\\nall features are centered around zero or have variance in the same\\norder. If a feature has a variance that is orders of magnitude larger\\nthan others, it might dominate the objective function and make the\\nestimator unable to learn from other features correctly as expected.\\nThe preprocessing module provides the\\nStandardScaler utility class, which is a quick and\\neasy way to perform the following operation on an array-like\\ndataset:\\n>>> from sklearn import preprocessing\\n>>> import numpy as np\\n>>> X_train = np.array([[ 1., -1.,  2.],\\n...                     [ 2.,  0.,  0.],\\n...                     [ 0.,  1., -1.]])\\n>>> scaler = preprocessing.StandardScaler().fit(X_train)\\n>>> scaler\\nStandardScaler()\\n---------new doc---------\\n>>> X, y = make_classification(random_state=42)\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\n>>> pipe = make_pipeline(StandardScaler(), LogisticRegression())\\n>>> pipe.fit(X_train, y_train)  # apply scaling on training data\\nPipeline(steps=[('standardscaler', StandardScaler()),\\n                ('logisticregression', LogisticRegression())])\\n\\n>>> pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.\\n0.96\\n\\n\\nIt is possible to disable either centering or scaling by either\\npassing with_mean=False or with_std=False to the constructor\\nof StandardScaler.\\n---------new doc---------\\nNote\\nThis estimator is still experimental for now: default parameters or\\ndetails of behaviour might change without any deprecation cycle. Resolving\\nthe following issues would help stabilize IterativeImputer:\\nconvergence criteria (#14338) and default estimators\\n(#13286). To use it, you need to explicitly import\\nenable_iterative_imputer.\\n\\n>>> import numpy as np\\n>>> from sklearn.experimental import enable_iterative_imputer\\n>>> from sklearn.impute import IterativeImputer\\n>>> imp = IterativeImputer(max_iter=10, random_state=0)\\n>>> imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\\nIterativeImputer(random_state=0)\\n>>> X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\\n>>> # the model learns that the second feature is double the first\\n>>> print(np.round(imp.transform(X_test)))\\n[[ 1.  2.]\\n [ 6. 12.]\\n [ 3.  6.]]\\n\\n\\nBoth SimpleImputer and IterativeImputer can be used in a\\nPipeline as a way to build a composite estimator that supports imputation.\\nSee Imputing missing values before building an estimator.\\n---------new doc---------\\n6.4.3.2. Multiple vs. Single Imputation#\\nIn the statistics community, it is common practice to perform multiple\\nimputations, generating, for example, m separate imputations for a single\\nfeature matrix. Each of these m imputations is then put through the\\nsubsequent analysis pipeline (e.g. feature engineering, clustering, regression,\\nclassification). The m final analysis results (e.g. held-out validation\\nerrors) allow the data scientist to obtain understanding of how analytic\\nresults may differ as a consequence of the inherent uncertainty caused by the\\nmissing values. The above practice is called multiple imputation.\\nOur implementation of IterativeImputer was inspired by the R MICE\\npackage (Multivariate Imputation by Chained Equations) [1], but differs from\\nit by returning a single imputation instead of multiple imputations.  However,\\nIterativeImputer can also be used for multiple imputations by applying\\nit repeatedly to the same dataset with different random seeds when\\nsample_posterior=True. See [2], chapter 4 for more discussion on multiple\\nvs. single imputations.\\nIt is still an open problem as to how useful single vs. multiple imputation is\\nin the context of prediction and classification when the user is not\\ninterested in measuring uncertainty due to missing values.\\nNote that a call to the transform method of IterativeImputer is\\nnot allowed to change the number of samples. Therefore multiple imputations\\ncannot be achieved by a single call to transform.\\n\\n\\n6.4.3.3. References#\\n\\n\\n[1]\\nStef van Buuren, Karin Groothuis-Oudshoorn (2011). “mice: Multivariate\\nImputation by Chained Equations in R”. Journal of Statistical Software 45:\\n1-67.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"The coefficient estimates for Ordinary Least Squares rely on the\\nindependence of the features. When features are correlated and the\\ncolumns of the design matrix \\\\(X\\\\) have an approximately linear\\ndependence, the design matrix becomes close to singular\\nand as a result, the least-squares estimate becomes highly sensitive\\nto random errors in the observed target, producing a large\\nvariance. This situation of multicollinearity can arise, for\\nexample, when data are collected without an experimental design.\\nExamples\\n\\nOrdinary Least Squares Example\\n\\n\\n1.1.1.1. Non-Negative Least Squares#\\nIt is possible to constrain all the coefficients to be non-negative, which may\\nbe useful when they represent some physical or naturally non-negative\\nquantities (e.g., frequency counts or prices of goods).\\nLinearRegression accepts a boolean positive\\nparameter: when set to True Non-Negative Least Squares are then applied.\\nExamples\\n\\nNon-negative least squares\\n\\n\\n\\n1.1.1.2. Ordinary Least Squares Complexity#\\nThe least squares solution is computed using the singular value\\ndecomposition of X. If X is a matrix of shape (n_samples, n_features)\\nthis method has a cost of\\n\\\\(O(n_{\\\\text{samples}} n_{\\\\text{features}}^2)\\\\), assuming that\\n\\\\(n_{\\\\text{samples}} \\\\geq n_{\\\\text{features}}\\\\).\\n\\n\\n\\n1.1.2. Ridge regression and classification#\\n---------new doc---------\\n1.1.16. Robustness regression: outliers and modeling errors#\\nRobust regression aims to fit a regression model in the\\npresence of corrupt data: either outliers, or error in the model.\\n\\n\\n\\n\\n\\n1.1.16.1. Different scenario and useful concepts#\\nThere are different things to keep in mind when dealing with data\\ncorrupted by outliers:\\n\\nOutliers in X or in y?\\n\\n\\nOutliers in the y direction\\nOutliers in the X direction\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFraction of outliers versus amplitude of error\\nThe number of outlying points matters, but also how much they are\\noutliers.\\n\\n\\nSmall outliers\\nLarge outliers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAn important notion of robust fitting is that of breakdown point: the\\nfraction of data that can be outlying for the fit to start missing the\\ninlying data.\\nNote that in general, robust fitting in high-dimensional setting (large\\nn_features) is very hard. The robust models here will probably not work\\nin these settings.\\n\\nTrade-offs: which estimator ?\\nScikit-learn provides 3 robust regression estimators:\\nRANSAC,\\nTheil Sen and\\nHuberRegressor.\\n---------new doc---------\\nTrade-offs: which estimator ?\\nScikit-learn provides 3 robust regression estimators:\\nRANSAC,\\nTheil Sen and\\nHuberRegressor.\\n\\nHuberRegressor should be faster than\\nRANSAC and Theil Sen\\nunless the number of samples are very large, i.e. n_samples >> n_features.\\nThis is because RANSAC and Theil Sen\\nfit on smaller subsets of the data. However, both Theil Sen\\nand RANSAC are unlikely to be as robust as\\nHuberRegressor for the default parameters.\\nRANSAC is faster than Theil Sen\\nand scales much better with the number of samples.\\nRANSAC will deal better with large\\noutliers in the y direction (most common situation).\\nTheil Sen will cope better with\\nmedium-size outliers in the X direction, but this property will\\ndisappear in high-dimensional settings.\\n\\nWhen in doubt, use RANSAC.\\n---------new doc---------\\nWhen in doubt, use RANSAC.\\n\\n\\n\\n1.1.16.2. RANSAC: RANdom SAmple Consensus#\\nRANSAC (RANdom SAmple Consensus) fits a model from random subsets of\\ninliers from the complete data set.\\nRANSAC is a non-deterministic algorithm producing only a reasonable result with\\na certain probability, which is dependent on the number of iterations (see\\nmax_trials parameter). It is typically used for linear and non-linear\\nregression problems and is especially popular in the field of photogrammetric\\ncomputer vision.\\nThe algorithm splits the complete input sample data into a set of inliers,\\nwhich may be subject to noise, and outliers, which are e.g. caused by erroneous\\nmeasurements or invalid hypotheses about the data. The resulting model is then\\nestimated only from the determined inliers.\\n\\n\\n\\n\\nExamples\\n\\nRobust linear model estimation using RANSAC\\nRobust linear estimator fitting\\n\\n\\n\\nDetails of the algorithm#\\nEach iteration performs the following steps:\\n---------new doc---------\\nExamples\\n\\nRobust linear model estimation using RANSAC\\nRobust linear estimator fitting\\n\\n\\n\\nDetails of the algorithm#\\nEach iteration performs the following steps:\\n\\nSelect min_samples random samples from the original data and check\\nwhether the set of data is valid (see is_data_valid).\\nFit a model to the random subset (estimator.fit) and check\\nwhether the estimated model is valid (see is_model_valid).\\nClassify all data as inliers or outliers by calculating the residuals\\nto the estimated model (estimator.predict(X) - y) - all data\\nsamples with absolute residuals smaller than or equal to the\\nresidual_threshold are considered as inliers.\\nSave fitted model as best model if number of inlier samples is\\nmaximal. In case the current estimated model has the same number of\\ninliers, it is only considered as the best model if it has better score.\\n\\nThese steps are performed either a maximum number of times (max_trials) or\\nuntil one of the special stop criteria are met (see stop_n_inliers and\\nstop_score). The final model is estimated using all inlier samples (consensus\\nset) of the previously determined best model.\\nThe is_data_valid and is_model_valid functions allow to identify and reject\\ndegenerate combinations of random sub-samples. If the estimated model is not\\nneeded for identifying degenerate cases, is_data_valid should be used as it\\nis called prior to fitting the model and thus leading to better computational\\nperformance.\\n\\n\\n\\nReferences#\\n---------new doc---------\\n1.4.1. Classification#\\nSVC, NuSVC and LinearSVC are classes\\ncapable of performing binary and multi-class classification on a dataset.\\n\\n\\n\\n\\nSVC and NuSVC are similar methods, but accept slightly\\ndifferent sets of parameters and have different mathematical formulations (see\\nsection Mathematical formulation). On the other hand,\\nLinearSVC is another (faster) implementation of Support Vector\\nClassification for the case of a linear kernel. It also\\nlacks some of the attributes of SVC and NuSVC, like\\nsupport_. LinearSVC uses squared_hinge loss and due to its\\nimplementation in liblinear it also regularizes the intercept, if considered.\\nThis effect can however be reduced by carefully fine tuning its\\nintercept_scaling parameter, which allows the intercept term to have a\\ndifferent regularization behavior compared to the other features. The\\nclassification results and score can therefore differ from the other two\\nclassifiers.\\nAs other classifiers, SVC, NuSVC and\\nLinearSVC take as input two arrays: an array X of shape\\n(n_samples, n_features) holding the training samples, and an array y of\\nclass labels (strings or integers), of shape (n_samples):\\n>>> from sklearn import svm\\n>>> X = [[0, 0], [1, 1]]\\n>>> y = [0, 1]\\n>>> clf = svm.SVC()\\n>>> clf.fit(X, y)\\nSVC()\\n\\n\\nAfter being fitted, the model can then be used to predict new values:\\n>>> clf.predict([[2., 2.]])\\narray([1])\\n---------new doc---------\\nAfter being fitted, the model can then be used to predict new values:\\n>>> clf.predict([[2., 2.]])\\narray([1])\\n\\n\\nSVMs decision function (detailed in the Mathematical formulation)\\ndepends on some subset of the training data, called the support vectors. Some\\nproperties of these support vectors can be found in attributes\\nsupport_vectors_, support_ and n_support_:\\n>>> # get support vectors\\n>>> clf.support_vectors_\\narray([[0., 0.],\\n       [1., 1.]])\\n>>> # get indices of support vectors\\n>>> clf.support_\\narray([0, 1]...)\\n>>> # get number of support vectors for each class\\n>>> clf.n_support_\\narray([1, 1]...)\\n\\n\\nExamples\\n\\nSVM: Maximum margin separating hyperplane\\nSVM-Anova: SVM with univariate feature selection\\nPlot classification probability\\n---------new doc---------\\nExamples\\n\\nSVM: Maximum margin separating hyperplane\\nSVM-Anova: SVM with univariate feature selection\\nPlot classification probability\\n\\n\\n1.4.1.1. Multi-class classification#\\nSVC and NuSVC implement the “one-versus-one”\\napproach for multi-class classification. In total,\\nn_classes * (n_classes - 1) / 2\\nclassifiers are constructed and each one trains data from two classes.\\nTo provide a consistent interface with other classifiers, the\\ndecision_function_shape option allows to monotonically transform the\\nresults of the “one-versus-one” classifiers to a “one-vs-rest” decision\\nfunction of shape (n_samples, n_classes), which is the default setting\\nof the parameter (default=’ovr’).\\n>>> X = [[0], [1], [2], [3]]\\n>>> Y = [0, 1, 2, 3]\\n>>> clf = svm.SVC(decision_function_shape='ovo')\\n>>> clf.fit(X, Y)\\nSVC(decision_function_shape='ovo')\\n>>> dec = clf.decision_function([[1]])\\n>>> dec.shape[1] # 6 classes: 4*3/2 = 6\\n6\\n>>> clf.decision_function_shape = \\\"ovr\\\"\\n>>> dec = clf.decision_function([[1]])\\n>>> dec.shape[1] # 4 classes\\n4\\n---------new doc---------\\nOn the other hand, LinearSVC implements “one-vs-the-rest”\\nmulti-class strategy, thus training n_classes models.\\n>>> lin_clf = svm.LinearSVC()\\n>>> lin_clf.fit(X, Y)\\nLinearSVC()\\n>>> dec = lin_clf.decision_function([[1]])\\n>>> dec.shape[1]\\n4\\n\\n\\nSee Mathematical formulation for a complete description of\\nthe decision function.\\n---------new doc---------\\nDetails on multi-class strategies#\\nNote that the LinearSVC also implements an alternative multi-class\\nstrategy, the so-called multi-class SVM formulated by Crammer and Singer\\n[16], by using the option multi_class='crammer_singer'. In practice,\\none-vs-rest classification is usually preferred, since the results are mostly\\nsimilar, but the runtime is significantly less.\\nFor “one-vs-rest” LinearSVC the attributes coef_ and intercept_\\nhave the shape (n_classes, n_features) and (n_classes,) respectively.\\nEach row of the coefficients corresponds to one of the n_classes\\n“one-vs-rest” classifiers and similar for the intercepts, in the\\norder of the “one” class.\\nIn the case of “one-vs-one” SVC and NuSVC, the layout of\\nthe attributes is a little more involved. In the case of a linear\\nkernel, the attributes coef_ and intercept_ have the shape\\n(n_classes * (n_classes - 1) / 2, n_features) and (n_classes *\\n(n_classes - 1) / 2) respectively. This is similar to the layout for\\nLinearSVC described above, with each row now corresponding\\nto a binary classifier. The order for classes\\n0 to n is “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . .\\n. “n-1 vs n”.\\nThe shape of dual_coef_ is (n_classes-1, n_SV) with\\n---------new doc---------\\n\\\\(\\\\alpha^{0}_{0,1}\\\\)\\n\\\\(\\\\alpha^{1}_{0,1}\\\\)\\n\\\\(\\\\alpha^{2}_{0,1}\\\\)\\n\\\\(\\\\alpha^{0}_{1,0}\\\\)\\n\\\\(\\\\alpha^{1}_{1,0}\\\\)\\n\\\\(\\\\alpha^{0}_{2,0}\\\\)\\n\\\\(\\\\alpha^{1}_{2,0}\\\\)\\n\\n\\\\(\\\\alpha^{0}_{0,2}\\\\)\\n\\\\(\\\\alpha^{1}_{0,2}\\\\)\\n\\\\(\\\\alpha^{2}_{0,2}\\\\)\\n\\\\(\\\\alpha^{0}_{1,2}\\\\)\\n\\\\(\\\\alpha^{1}_{1,2}\\\\)\\n\\\\(\\\\alpha^{0}_{2,1}\\\\)\\n\\\\(\\\\alpha^{1}_{2,1}\\\\)\\n\\nCoefficients\\nfor SVs of class 0\\nCoefficients\\nfor SVs of class 1\\nCoefficients\\nfor SVs of class 2\\n\\n\\n\\n\\n\\nExamples\\n\\nPlot different SVM classifiers in the iris dataset\\n\\n\\n\\n1.4.1.2. Scores and probabilities#\\nThe decision_function method of SVC and NuSVC gives\\nper-class scores for each sample (or a single score per sample in the binary\\ncase). When the constructor option probability is set to True,\\nclass membership probability estimates (from the methods predict_proba and\\npredict_log_proba) are enabled. In the binary case, the probabilities are\\ncalibrated using Platt scaling [9]: logistic regression on the SVM’s scores,\\nfit by an additional cross-validation on the training data.\\nIn the multiclass case, this is extended as per [10].\\n---------new doc---------\\nNote\\nThe same probability calibration procedure is available for all estimators\\nvia the CalibratedClassifierCV (see\\nProbability calibration). In the case of SVC and NuSVC, this\\nprocedure is builtin in libsvm which is used under the hood, so it does\\nnot rely on scikit-learn’s\\nCalibratedClassifierCV.\\n\\nThe cross-validation involved in Platt scaling\\nis an expensive operation for large datasets.\\nIn addition, the probability estimates may be inconsistent with the scores:\\n\\nthe “argmax” of the scores may not be the argmax of the probabilities\\nin binary classification, a sample may be labeled by predict as\\nbelonging to the positive class even if the output of predict_proba is\\nless than 0.5; and similarly, it could be labeled as negative even if the\\noutput of predict_proba is more than 0.5.\\n\\nPlatt’s method is also known to have theoretical issues.\\nIf confidence scores are required, but these do not have to be probabilities,\\nthen it is advisable to set probability=False\\nand use decision_function instead of predict_proba.\\nPlease note that when decision_function_shape='ovr' and n_classes > 2,\\nunlike decision_function, the predict method does not try to break ties\\nby default. You can set break_ties=True for the output of predict to be\\nthe same as np.argmax(clf.decision_function(...), axis=1), otherwise the\\nfirst class among the tied classes will always be returned; but have in mind\\nthat it comes with a computational cost. See\\nSVM Tie Breaking Example for an example on\\ntie breaking.\\n---------new doc---------\\n1.4.1.3. Unbalanced problems#\\nIn problems where it is desired to give more importance to certain\\nclasses or certain individual samples, the parameters class_weight and\\nsample_weight can be used.\\nSVC (but not NuSVC) implements the parameter\\nclass_weight in the fit method. It’s a dictionary of the form\\n{class_label : value}, where value is a floating point number > 0\\nthat sets the parameter C of class class_label to C * value.\\nThe figure below illustrates the decision boundary of an unbalanced problem,\\nwith and without weight correction.\\n\\n\\n\\n\\nSVC, NuSVC, SVR, NuSVR, LinearSVC,\\nLinearSVR and OneClassSVM implement also weights for\\nindividual samples in the fit method through the sample_weight parameter.\\nSimilar to class_weight, this sets the parameter C for the i-th\\nexample to C * sample_weight[i], which will encourage the classifier to\\nget these samples right. The figure below illustrates the effect of sample\\nweighting on the decision boundary. The size of the circles is proportional\\nto the sample weights:\\n\\n\\n\\n\\nExamples\\n\\nSVM: Separating hyperplane for unbalanced classes\\nSVM: Weighted samples\\n---------new doc---------\\nSee section Preprocessing data for more details on scaling and\\nnormalization.\\n\\n\\n\\nRegarding the shrinking parameter, quoting [12]: We found that if the\\nnumber of iterations is large, then shrinking can shorten the training\\ntime. However, if we loosely solve the optimization problem (e.g., by\\nusing a large stopping tolerance), the code without using shrinking may\\nbe much faster\\nParameter nu in NuSVC/OneClassSVM/NuSVR\\napproximates the fraction of training errors and support vectors.\\nIn SVC, if the data is unbalanced (e.g. many\\npositive and few negative), set class_weight='balanced' and/or try\\ndifferent penalty parameters C.\\nRandomness of the underlying implementations: The underlying\\nimplementations of SVC and NuSVC use a random number\\ngenerator only to shuffle the data for probability estimation (when\\nprobability is set to True). This randomness can be controlled\\nwith the random_state parameter. If probability is set to False\\nthese estimators are not random and random_state has no effect on the\\nresults. The underlying OneClassSVM implementation is similar to\\nthe ones of SVC and NuSVC. As no probability estimation\\nis provided for OneClassSVM, it is not random.\\nThe underlying LinearSVC implementation uses a random number\\ngenerator to select features when fitting the model with a dual coordinate\\ndescent (i.e. when dual is set to True). It is thus not uncommon\\nto have slightly different results for the same input data. If that\\nhappens, try with a smaller tol parameter. This randomness can also be\\ncontrolled with the random_state parameter. When dual is\\nset to False the underlying implementation of LinearSVC is\\nnot random and random_state has no effect on the results.\\n---------new doc---------\\nIn both cases, the criterion is evaluated once by epoch, and the algorithm stops\\nwhen the criterion does not improve n_iter_no_change times in a row. The\\nimprovement is evaluated with absolute tolerance tol, and the algorithm\\nstops in any case after a maximum number of iteration max_iter.\\nSee Early stopping of Stochastic Gradient Descent for an\\nexample of the effects of early stopping.\\n\\n\\n1.5.7. Tips on Practical Use#\\n\\nStochastic Gradient Descent is sensitive to feature scaling, so it\\nis highly recommended to scale your data. For example, scale each\\nattribute on the input vector X to [0,1] or [-1,+1], or standardize\\nit to have mean 0 and variance 1. Note that the same scaling must be\\napplied to the test vector to obtain meaningful results. This can be easily\\ndone using StandardScaler:\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nscaler.fit(X_train)  # Don't cheat - fit only on training data\\nX_train = scaler.transform(X_train)\\nX_test = scaler.transform(X_test)  # apply same transformation to test data\\n\\n# Or better yet: use a pipeline!\\nfrom sklearn.pipeline import make_pipeline\\nest = make_pipeline(StandardScaler(), SGDClassifier())\\nest.fit(X_train)\\nest.predict(X_test)\\n\\n\\nIf your attributes have an intrinsic scale (e.g. word frequencies or\\nindicator features) scaling is not needed.\\n---------new doc---------\\nProbability calculation#\\nThe probability of category \\\\(t\\\\) in feature \\\\(i\\\\) given class\\n\\\\(c\\\\) is estimated as:\\n\\n\\\\[P(x_i = t \\\\mid y = c \\\\: ;\\\\, \\\\alpha) = \\\\frac{ N_{tic} + \\\\alpha}{N_{c} +\\n                                       \\\\alpha n_i},\\\\]\\nwhere \\\\(N_{tic} = |\\\\{j \\\\in J \\\\mid x_{ij} = t, y_j = c\\\\}|\\\\) is the number\\nof times category \\\\(t\\\\) appears in the samples \\\\(x_{i}\\\\), which belong\\nto class \\\\(c\\\\), \\\\(N_{c} = |\\\\{ j \\\\in J\\\\mid y_j = c\\\\}|\\\\) is the number\\nof samples with class c, \\\\(\\\\alpha\\\\) is a smoothing parameter and\\n\\\\(n_i\\\\) is the number of available categories of feature \\\\(i\\\\).\\n\\nCategoricalNB assumes that the sample matrix \\\\(X\\\\) is encoded (for\\ninstance with the help of OrdinalEncoder) such\\nthat all categories for each feature \\\\(i\\\\) are represented with numbers\\n\\\\(0, ..., n_i - 1\\\\) where \\\\(n_i\\\\) is the number of available categories\\nof feature \\\\(i\\\\).\\n---------new doc---------\\nSome advantages of decision trees are:\\n\\nSimple to understand and to interpret. Trees can be visualized.\\nRequires little data preparation. Other techniques often require data\\nnormalization, dummy variables need to be created and blank values to\\nbe removed. Some tree and algorithm combinations support\\nmissing values.\\nThe cost of using the tree (i.e., predicting data) is logarithmic in the\\nnumber of data points used to train the tree.\\nAble to handle both numerical and categorical data. However, the scikit-learn\\nimplementation does not support categorical variables for now. Other\\ntechniques are usually specialized in analyzing datasets that have only one type\\nof variable. See algorithms for more\\ninformation.\\nAble to handle multi-output problems.\\nUses a white box model. If a given situation is observable in a model,\\nthe explanation for the condition is easily explained by boolean logic.\\nBy contrast, in a black box model (e.g., in an artificial neural\\nnetwork), results may be more difficult to interpret.\\nPossible to validate a model using statistical tests. That makes it\\npossible to account for the reliability of the model.\\nPerforms well even if its assumptions are somewhat violated by\\nthe true model from which the data were generated.\\n\\nThe disadvantages of decision trees include:\\n---------new doc---------\\nThe disadvantages of decision trees include:\\n\\nDecision-tree learners can create over-complex trees that do not\\ngeneralize the data well. This is called overfitting. Mechanisms\\nsuch as pruning, setting the minimum number of samples required\\nat a leaf node or setting the maximum depth of the tree are\\nnecessary to avoid this problem.\\nDecision trees can be unstable because small variations in the\\ndata might result in a completely different tree being generated.\\nThis problem is mitigated by using decision trees within an\\nensemble.\\nPredictions of decision trees are neither smooth nor continuous, but\\npiecewise constant approximations as seen in the above figure. Therefore,\\nthey are not good at extrapolation.\\nThe problem of learning an optimal decision tree is known to be\\nNP-complete under several aspects of optimality and even for simple\\nconcepts. Consequently, practical decision-tree learning algorithms\\nare based on heuristic algorithms such as the greedy algorithm where\\nlocally optimal decisions are made at each node. Such algorithms\\ncannot guarantee to return the globally optimal decision tree.  This\\ncan be mitigated by training multiple trees in an ensemble learner,\\nwhere the features and samples are randomly sampled with replacement.\\nThere are concepts that are hard to learn because decision trees\\ndo not express them easily, such as XOR, parity or multiplexer problems.\\nDecision tree learners create biased trees if some classes dominate.\\nIt is therefore recommended to balance the dataset prior to fitting\\nwith the decision tree.\\n---------new doc---------\\n1.10.1. Classification#\\nDecisionTreeClassifier is a class capable of performing multi-class\\nclassification on a dataset.\\nAs with other classifiers, DecisionTreeClassifier takes as input two arrays:\\nan array X, sparse or dense, of shape (n_samples, n_features) holding the\\ntraining samples, and an array Y of integer values, shape (n_samples,),\\nholding the class labels for the training samples:\\n>>> from sklearn import tree\\n>>> X = [[0, 0], [1, 1]]\\n>>> Y = [0, 1]\\n>>> clf = tree.DecisionTreeClassifier()\\n>>> clf = clf.fit(X, Y)\\n\\n\\nAfter being fitted, the model can then be used to predict the class of samples:\\n>>> clf.predict([[2., 2.]])\\narray([1])\\n\\n\\nIn case that there are multiple classes with the same and highest\\nprobability, the classifier will predict the class with the lowest index\\namongst those classes.\\nAs an alternative to outputting a specific class, the probability of each class\\ncan be predicted, which is the fraction of training samples of the class in a\\nleaf:\\n>>> clf.predict_proba([[2., 2.]])\\narray([[0., 1.]])\\n---------new doc---------\\nIn case that there are multiple classes with the same and highest\\nprobability, the classifier will predict the class with the lowest index\\namongst those classes.\\nAs an alternative to outputting a specific class, the probability of each class\\ncan be predicted, which is the fraction of training samples of the class in a\\nleaf:\\n>>> clf.predict_proba([[2., 2.]])\\narray([[0., 1.]])\\n\\n\\nDecisionTreeClassifier is capable of both binary (where the\\nlabels are [-1, 1]) classification and multiclass (where the labels are\\n[0, …, K-1]) classification.\\nUsing the Iris dataset, we can construct a tree as follows:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn import tree\\n>>> iris = load_iris()\\n>>> X, y = iris.data, iris.target\\n>>> clf = tree.DecisionTreeClassifier()\\n>>> clf = clf.fit(X, y)\\n\\n\\nOnce trained, you can plot the tree with the plot_tree function:\\n>>> tree.plot_tree(clf)\\n[...]\\n---------new doc---------\\nAlternatively, the tree can also be exported in textual format with the\\nfunction export_text. This method doesn’t require the installation\\nof external libraries and is more compact:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> from sklearn.tree import export_text\\n>>> iris = load_iris()\\n>>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\\n>>> decision_tree = decision_tree.fit(iris.data, iris.target)\\n>>> r = export_text(decision_tree, feature_names=iris['feature_names'])\\n>>> print(r)\\n|--- petal width (cm) <= 0.80\\n|   |--- class: 0\\n|--- petal width (cm) >  0.80\\n|   |--- petal width (cm) <= 1.75\\n|   |   |--- class: 1\\n|   |--- petal width (cm) >  1.75\\n|   |   |--- class: 2\\n\\n\\n\\nExamples\\n\\nPlot the decision surface of decision trees trained on the iris dataset\\nUnderstanding the decision tree structure\\n\\n\\n\\n1.10.2. Regression#\\n---------new doc---------\\nExamples\\n\\nPlot the decision surface of decision trees trained on the iris dataset\\nUnderstanding the decision tree structure\\n\\n\\n\\n1.10.2. Regression#\\n\\n\\n\\n\\nDecision trees can also be applied to regression problems, using the\\nDecisionTreeRegressor class.\\nAs in the classification setting, the fit method will take as argument arrays X\\nand y, only that in this case y is expected to have floating point values\\ninstead of integer values:\\n>>> from sklearn import tree\\n>>> X = [[0, 0], [2, 2]]\\n>>> y = [0.5, 2.5]\\n>>> clf = tree.DecisionTreeRegressor()\\n>>> clf = clf.fit(X, y)\\n>>> clf.predict([[1, 1]])\\narray([0.5])\\n\\n\\nExamples\\n\\nDecision Tree Regression\\n---------new doc---------\\nExamples\\n\\nDecision Tree Regression\\n\\n\\n\\n1.10.3. Multi-output problems#\\nA multi-output problem is a supervised learning problem with several outputs\\nto predict, that is when Y is a 2d array of shape (n_samples, n_outputs).\\nWhen there is no correlation between the outputs, a very simple way to solve\\nthis kind of problem is to build n independent models, i.e. one for each\\noutput, and then to use those models to independently predict each one of the n\\noutputs. However, because it is likely that the output values related to the\\nsame input are themselves correlated, an often better way is to build a single\\nmodel capable of predicting simultaneously all n outputs. First, it requires\\nlower training time since only a single estimator is built. Second, the\\ngeneralization accuracy of the resulting estimator may often be increased.\\nWith regard to decision trees, this strategy can readily be used to support\\nmulti-output problems. This requires the following changes:\\n\\nStore n output values in leaves, instead of 1;\\nUse splitting criteria that compute the average reduction across all\\nn outputs.\\n\\nThis module offers support for multi-output problems by implementing this\\nstrategy in both DecisionTreeClassifier and\\nDecisionTreeRegressor. If a decision tree is fit on an output array Y\\nof shape (n_samples, n_outputs) then the resulting estimator will:\\n\\nOutput n_output values upon predict;\\nOutput a list of n_output arrays of class probabilities upon\\npredict_proba.\\n\\nThe use of multi-output trees for regression is demonstrated in\\nDecision Tree Regression. In this example, the input\\nX is a single real value and the outputs Y are the sine and cosine of X.\\n---------new doc---------\\nOutput n_output values upon predict;\\nOutput a list of n_output arrays of class probabilities upon\\npredict_proba.\\n\\nThe use of multi-output trees for regression is demonstrated in\\nDecision Tree Regression. In this example, the input\\nX is a single real value and the outputs Y are the sine and cosine of X.\\n\\n\\n\\n\\nThe use of multi-output trees for classification is demonstrated in\\nFace completion with a multi-output estimators. In this example, the inputs\\nX are the pixels of the upper half of faces and the outputs Y are the pixels of\\nthe lower half of those faces.\\n\\n\\n\\n\\nExamples\\n\\nFace completion with a multi-output estimators\\n\\nReferences\\n\\nM. Dumont et al,  Fast multi-class image annotation with random subwindows\\nand multiple output randomized trees,\\nInternational Conference on Computer Vision Theory and Applications 2009\\n---------new doc---------\\n1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART#\\nWhat are all the various decision tree algorithms and how do they differ\\nfrom each other? Which one is implemented in scikit-learn?\\n\\n\\nVarious decision tree algorithms#\\nID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan.\\nThe algorithm creates a multiway tree, finding for each node (i.e. in\\na greedy manner) the categorical feature that will yield the largest\\ninformation gain for categorical targets. Trees are grown to their\\nmaximum size and then a pruning step is usually applied to improve the\\nability of the tree to generalize to unseen data.\\nC4.5 is the successor to ID3 and removed the restriction that features\\nmust be categorical by dynamically defining a discrete attribute (based\\non numerical variables) that partitions the continuous attribute value\\ninto a discrete set of intervals. C4.5 converts the trained trees\\n(i.e. the output of the ID3 algorithm) into sets of if-then rules.\\nThe accuracy of each rule is then evaluated to determine the order\\nin which they should be applied. Pruning is done by removing a rule’s\\nprecondition if the accuracy of the rule improves without it.\\nC5.0 is Quinlan’s latest version release under a proprietary license.\\nIt uses less memory and builds smaller rulesets than C4.5 while being\\nmore accurate.\\nCART (Classification and Regression Trees) is very similar to C4.5, but\\nit differs in that it supports numerical target variables (regression) and\\ndoes not compute rule sets. CART constructs binary trees using the feature\\nand threshold that yield the largest information gain at each node.\\n---------new doc---------\\nscikit-learn uses an optimized version of the CART algorithm; however, the\\nscikit-learn implementation does not support categorical variables for now.\\n\\n\\n1.10.7. Mathematical formulation#\\nGiven training vectors \\\\(x_i \\\\in R^n\\\\), i=1,…, l and a label vector\\n\\\\(y \\\\in R^l\\\\), a decision tree recursively partitions the feature space\\nsuch that the samples with the same labels or similar target values are grouped\\ntogether.\\nLet the data at node \\\\(m\\\\) be represented by \\\\(Q_m\\\\) with \\\\(n_m\\\\)\\nsamples. For each candidate split \\\\(\\\\theta = (j, t_m)\\\\) consisting of a\\nfeature \\\\(j\\\\) and threshold \\\\(t_m\\\\), partition the data into\\n\\\\(Q_m^{left}(\\\\theta)\\\\) and \\\\(Q_m^{right}(\\\\theta)\\\\) subsets\\n\\n\\\\[ \\\\begin{align}\\\\begin{aligned}Q_m^{left}(\\\\theta) = \\\\{(x, y) | x_j \\\\leq t_m\\\\}\\\\\\\\Q_m^{right}(\\\\theta) = Q_m \\\\setminus Q_m^{left}(\\\\theta)\\\\end{aligned}\\\\end{align} \\\\]\\nThe quality of a candidate split of node \\\\(m\\\\) is then computed using an\\nimpurity function or loss function \\\\(H()\\\\), the choice of which depends on\\nthe task being solved (classification or regression)\\n---------new doc---------\\n1.10.8. Missing Values Support#\\nDecisionTreeClassifier, DecisionTreeRegressor\\nhave built-in support for missing values using splitter='best', where\\nthe splits are determined in a greedy fashion.\\nExtraTreeClassifier, and ExtraTreeRegressor have built-in\\nsupport for missing values for splitter='random', where the splits\\nare determined randomly. For more details on how the splitter differs on\\nnon-missing values, see the Forest section.\\nThe criterion supported when there are missing-values are\\n'gini', 'entropy’, or 'log_loss', for classification or\\n'squared_error', 'friedman_mse', or 'poisson' for regression.\\nFirst we will describe how DecisionTreeClassifier, DecisionTreeRegressor\\nhandle missing-values in the data.\\nFor each potential threshold on the non-missing data, the splitter will evaluate\\nthe split with all the missing values going to the left node or the right node.\\nDecisions are made as follows:\\n\\nBy default when predicting, the samples with missing values are classified\\nwith the class used in the split found during training:\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n>>> tree.predict(X)\\narray([0, 0, 1, 1])\\n---------new doc---------\\n>>> X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n>>> tree.predict(X)\\narray([0, 0, 1, 1])\\n\\n\\n\\nIf the criterion evaluation is the same for both nodes,\\nthen the tie for missing value at predict time is broken by going to the\\nright node. The splitter also checks the split where all the missing\\nvalues go to one child and non-missing values go to the other:\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n\\n>>> X_test = np.array([np.nan]).reshape(-1, 1)\\n>>> tree.predict(X_test)\\narray([1])\\n\\n\\n\\nIf no missing values are seen during training for a given feature, then during\\nprediction missing values are mapped to the child with the most samples:\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([0, 1, 2, 3]).reshape(-1, 1)\\n>>> y = [0, 1, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n---------new doc---------\\n>>> X = np.array([0, 1, 2, 3]).reshape(-1, 1)\\n>>> y = [0, 1, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n\\n>>> X_test = np.array([np.nan]).reshape(-1, 1)\\n>>> tree.predict(X_test)\\narray([1])\\n\\n\\n\\n\\nExtraTreeClassifier, and ExtraTreeRegressor handle missing values\\nin a slightly different way. When splitting a node, a random threshold will be chosen\\nto split the non-missing values on. Then the non-missing values will be sent to the\\nleft and right child based on the randomly selected threshold, while the missing\\nvalues will also be randomly sent to the left or right child. This is repeated for\\nevery feature considered at each split. The best split among these is chosen.\\nDuring prediction, the treatment of missing-values is the same as that of the\\ndecision tree:\\n\\nBy default when predicting, the samples with missing values are classified\\nwith the class used in the split found during training.\\nIf no missing values are seen during training for a given feature, then during\\nprediction missing values are mapped to the child with the most samples.\\n\\n\\n\\n1.10.9. Minimal Cost-Complexity Pruning#\\nMinimal cost-complexity pruning is an algorithm used to prune a tree to avoid\\nover-fitting, described in Chapter 3 of [BRE]. This algorithm is parameterized\\nby \\\\(\\\\alpha\\\\ge0\\\\) known as the complexity parameter. The complexity\\nparameter is used to define the cost-complexity measure, \\\\(R_\\\\alpha(T)\\\\) of\\na given tree \\\\(T\\\\):\\n---------new doc---------\\n1.11.1. Gradient-boosted trees#\\nGradient Tree Boosting\\nor Gradient Boosted Decision Trees (GBDT) is a generalization\\nof boosting to arbitrary differentiable loss functions, see the seminal work of\\n[Friedman2001]. GBDT is an excellent model for both regression and\\nclassification, in particular for tabular data.\\n\\nGradientBoostingClassifier vs HistGradientBoostingClassifier\\nScikit-learn provides two implementations of gradient-boosted trees:\\nHistGradientBoostingClassifier vs\\nGradientBoostingClassifier for classification, and the\\ncorresponding classes for regression. The former can be orders of\\nmagnitude faster than the latter when the number of samples is\\nlarger than tens of thousands of samples.\\nMissing values and categorical data are natively supported by the\\nHist… version, removing the need for additional preprocessing such as\\nimputation.\\nGradientBoostingClassifier and\\nGradientBoostingRegressor, might be preferred for small sample\\nsizes since binning may lead to split points that are too approximate\\nin this setting.\\n---------new doc---------\\n1.11.1.1. Histogram-Based Gradient Boosting#\\nScikit-learn 0.21 introduced two new implementations of\\ngradient boosted trees, namely HistGradientBoostingClassifier\\nand HistGradientBoostingRegressor, inspired by\\nLightGBM (See [LightGBM]).\\nThese histogram-based estimators can be orders of magnitude faster\\nthan GradientBoostingClassifier and\\nGradientBoostingRegressor when the number of samples is larger\\nthan tens of thousands of samples.\\nThey also have built-in support for missing values, which avoids the need\\nfor an imputer.\\nThese fast estimators first bin the input samples X into\\ninteger-valued bins (typically 256 bins) which tremendously reduces the\\nnumber of splitting points to consider, and allows the algorithm to\\nleverage integer-based data structures (histograms) instead of relying on\\nsorted continuous values when building the trees. The API of these\\nestimators is slightly different, and some of the features from\\nGradientBoostingClassifier and GradientBoostingRegressor\\nare not yet supported, for instance some loss functions.\\nExamples\\n\\nPartial Dependence and Individual Conditional Expectation Plots\\nComparing Random Forests and Histogram Gradient Boosting models\\n\\n\\n1.11.1.1.1. Usage#\\nMost of the parameters are unchanged from\\nGradientBoostingClassifier and GradientBoostingRegressor.\\nOne exception is the max_iter parameter that replaces n_estimators, and\\ncontrols the number of iterations of the boosting process:\\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\\n>>> from sklearn.datasets import make_hastie_10_2\\n---------new doc---------\\nNote that early-stopping is enabled by default if the number of samples is\\nlarger than 10,000. The early-stopping behaviour is controlled via the\\nearly_stopping, scoring, validation_fraction,\\nn_iter_no_change, and tol parameters. It is possible to early-stop\\nusing an arbitrary scorer, or just the training or validation loss.\\nNote that for technical reasons, using a callable as a scorer is significantly slower\\nthan using the loss. By default, early-stopping is performed if there are at least\\n10,000 samples in the training set, using the validation loss.\\n\\n\\n1.11.1.1.2. Missing values support#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor have built-in support for missing\\nvalues (NaNs).\\nDuring training, the tree grower learns at each split point whether samples\\nwith missing values should go to the left or right child, based on the\\npotential gain. When predicting, samples with missing values are assigned to\\nthe left or right child consequently:\\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\\n>>> gbdt.predict(X)\\narray([0, 0, 1, 1])\\n---------new doc---------\\n>>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\\n>>> gbdt.predict(X)\\narray([0, 0, 1, 1])\\n\\n\\nWhen the missingness pattern is predictive, the splits can be performed on\\nwhether the feature value is missing or not:\\n>>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)\\n>>> y = [0, 1, 0, 0, 1]\\n>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,\\n...                                       max_depth=2,\\n...                                       learning_rate=1,\\n...                                       max_iter=1).fit(X, y)\\n>>> gbdt.predict(X)\\narray([0, 1, 0, 0, 1])\\n\\n\\nIf no missing values were encountered for a given feature during training,\\nthen samples with missing values are mapped to whichever child has the most\\nsamples.\\nExamples\\n\\nFeatures in Histogram Gradient Boosting Trees\\n---------new doc---------\\nIf no missing values were encountered for a given feature during training,\\nthen samples with missing values are mapped to whichever child has the most\\nsamples.\\nExamples\\n\\nFeatures in Histogram Gradient Boosting Trees\\n\\n\\n\\n1.11.1.1.3. Sample weight support#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor support sample weights during\\nfit.\\nThe following toy example demonstrates that samples with a sample weight of zero are ignored:\\n>>> X = [[1, 0],\\n...      [1, 0],\\n...      [1, 0],\\n...      [0, 1]]\\n>>> y = [0, 0, 1, 0]\\n>>> # ignore the first 2 training samples by setting their weight to 0\\n>>> sample_weight = [0, 0, 1, 1]\\n>>> gb = HistGradientBoostingClassifier(min_samples_leaf=1)\\n>>> gb.fit(X, y, sample_weight=sample_weight)\\nHistGradientBoostingClassifier(...)\\n>>> gb.predict([[1, 0]])\\narray([1])\\n>>> gb.predict_proba([[1, 0]])[0, 1]\\n0.99...\\n\\n\\nAs you can see, the [1, 0] is comfortably classified as 1 since the first\\ntwo samples are ignored due to their sample weights.\\nImplementation detail: taking sample weights into account amounts to\\nmultiplying the gradients (and the hessians) by the sample weights. Note that\\nthe binning stage (specifically the quantiles computation) does not take the\\nweights into account.\\n---------new doc---------\\nSplit finding with categorical features#\\nThe canonical way of considering categorical splits in a tree is to consider\\nall of the \\\\(2^{K - 1} - 1\\\\) partitions, where \\\\(K\\\\) is the number of\\ncategories. This can quickly become prohibitive when \\\\(K\\\\) is large.\\nFortunately, since gradient boosting trees are always regression trees (even\\nfor classification problems), there exist a faster strategy that can yield\\nequivalent splits. First, the categories of a feature are sorted according to\\nthe variance of the target, for each category k. Once the categories are\\nsorted, one can consider continuous partitions, i.e. treat the categories\\nas if they were ordered continuous values (see Fisher [Fisher1958] for a\\nformal proof). As a result, only \\\\(K - 1\\\\) splits need to be considered\\ninstead of \\\\(2^{K - 1} - 1\\\\). The initial sorting is a\\n\\\\(\\\\mathcal{O}(K \\\\log(K))\\\\) operation, leading to a total complexity of\\n\\\\(\\\\mathcal{O}(K \\\\log(K) + K)\\\\), instead of \\\\(\\\\mathcal{O}(2^K)\\\\).\\n\\nExamples\\n\\nCategorical Feature Support in Gradient Boosting\\n\\n\\n\\n1.11.1.1.5. Monotonic Constraints#\\nDepending on the problem at hand, you may have prior knowledge indicating\\nthat a given feature should in general have a positive (or negative) effect\\non the target value. For example, all else being equal, a higher credit\\nscore should increase the probability of getting approved for a loan.\\nMonotonic constraints allow you to incorporate such prior knowledge into the\\nmodel.\\nFor a predictor \\\\(F\\\\) with two features:\\n\\na monotonic increase constraint is a constraint of the form:\\n---------new doc---------\\n1.11.1.1.8. Why it’s faster#\\nThe bottleneck of a gradient boosting procedure is building the decision\\ntrees. Building a traditional decision tree (as in the other GBDTs\\nGradientBoostingClassifier and GradientBoostingRegressor)\\nrequires sorting the samples at each node (for\\neach feature). Sorting is needed so that the potential gain of a split point\\ncan be computed efficiently. Splitting a single node has thus a complexity\\nof \\\\(\\\\mathcal{O}(n_\\\\text{features} \\\\times n \\\\log(n))\\\\) where \\\\(n\\\\)\\nis the number of samples at the node.\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor, in contrast, do not require sorting the\\nfeature values and instead use a data-structure called a histogram, where the\\nsamples are implicitly ordered. Building a histogram has a\\n\\\\(\\\\mathcal{O}(n)\\\\) complexity, so the node splitting procedure has a\\n\\\\(\\\\mathcal{O}(n_\\\\text{features} \\\\times n)\\\\) complexity, much smaller\\nthan the previous one. In addition, instead of considering \\\\(n\\\\) split\\npoints, we consider only max_bins split points, which might be much\\nsmaller.\\nIn order to build histograms, the input data X needs to be binned into\\ninteger-valued bins. This binning procedure does require sorting the feature\\nvalues, but it only happens once at the very beginning of the boosting process\\n(not at each node, like in GradientBoostingClassifier and\\nGradientBoostingRegressor).\\nFinally, many parts of the implementation of\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor are parallelized.\\nReferences\\n---------new doc---------\\n[XGBoost]\\n(1,2,3)\\nTianqi Chen, Carlos Guestrin, “XGBoost: A Scalable Tree\\nBoosting System”\\n\\n\\n[LightGBM]\\nKe et. al. “LightGBM: A Highly Efficient Gradient\\nBoostingDecision Tree”\\n\\n\\n[Fisher1958]\\nFisher, W.D. (1958). “On Grouping for Maximum Homogeneity”\\nJournal of the American Statistical Association, 53, 789-798.\\n\\n\\n\\n\\n\\n1.11.1.2. GradientBoostingClassifier and GradientBoostingRegressor#\\nThe usage and the parameters of GradientBoostingClassifier and\\nGradientBoostingRegressor are described below. The 2 most important\\nparameters of these estimators are n_estimators and learning_rate.\\n\\n\\nClassification#\\nGradientBoostingClassifier supports both binary and multi-class\\nclassification.\\nThe following example shows how to fit a gradient boosting classifier\\nwith 100 decision stumps as weak learners:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> X_train, X_test = X[:2000], X[2000:]\\n>>> y_train, y_test = y[:2000], y[2000:]\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> X_train, X_test = X[:2000], X[2000:]\\n>>> y_train, y_test = y[:2000], y[2000:]\\n\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.913...\\n\\n\\nThe number of weak learners (i.e. regression trees) is controlled by the\\nparameter n_estimators; The size of each tree can be controlled either by setting the tree\\ndepth via max_depth or by setting the number of leaf nodes via\\nmax_leaf_nodes. The learning_rate is a hyper-parameter in the range\\n(0.0, 1.0] that controls overfitting via shrinkage .\\n\\nNote\\nClassification with more than 2 classes requires the induction\\nof n_classes regression trees at each iteration,\\nthus, the total number of induced trees equals\\nn_classes * n_estimators. For datasets with a large number\\nof classes we strongly recommend to use\\nHistGradientBoostingClassifier as an alternative to\\nGradientBoostingClassifier .\\n---------new doc---------\\nNote\\nClassification with more than 2 classes requires the induction\\nof n_classes regression trees at each iteration,\\nthus, the total number of induced trees equals\\nn_classes * n_estimators. For datasets with a large number\\nof classes we strongly recommend to use\\nHistGradientBoostingClassifier as an alternative to\\nGradientBoostingClassifier .\\n\\n\\n\\n\\nRegression#\\nGradientBoostingRegressor supports a number of\\ndifferent loss functions\\nfor regression which can be specified via the argument\\nloss; the default loss function for regression is squared error\\n('squared_error').\\n>>> import numpy as np\\n>>> from sklearn.metrics import mean_squared_error\\n>>> from sklearn.datasets import make_friedman1\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n\\n>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\\n>>> X_train, X_test = X[:200], X[200:]\\n>>> y_train, y_test = y[:200], y[200:]\\n>>> est = GradientBoostingRegressor(\\n...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\\n...     loss='squared_error'\\n... ).fit(X_train, y_train)\\n>>> mean_squared_error(y_test, est.predict(X_test))\\n5.00...\\n---------new doc---------\\nThe figure below shows the results of applying GradientBoostingRegressor\\nwith least squares loss and 500 base learners to the diabetes dataset\\n(sklearn.datasets.load_diabetes).\\nThe plot shows the train and test error at each iteration.\\nThe train error at each iteration is stored in the\\ntrain_score_ attribute of the gradient boosting model.\\nThe test error at each iterations can be obtained\\nvia the staged_predict method which returns a\\ngenerator that yields the predictions at each stage. Plots like these can be used\\nto determine the optimal number of trees (i.e. n_estimators) by early stopping.\\n\\n\\n\\n\\n\\nExamples\\n\\nGradient Boosting regression\\nGradient Boosting Out-of-Bag estimates\\n\\n\\n1.11.1.2.1. Fitting additional weak-learners#\\nBoth GradientBoostingRegressor and GradientBoostingClassifier\\nsupport warm_start=True which allows you to add more estimators to an already\\nfitted model.\\n>>> import numpy as np\\n>>> from sklearn.metrics import mean_squared_error\\n>>> from sklearn.datasets import make_friedman1\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n---------new doc---------\\n>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\\n>>> X_train, X_test = X[:200], X[200:]\\n>>> y_train, y_test = y[:200], y[200:]\\n>>> est = GradientBoostingRegressor(\\n...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\\n...     loss='squared_error'\\n... )\\n>>> est = est.fit(X_train, y_train)  # fit with 100 trees\\n>>> mean_squared_error(y_test, est.predict(X_test))\\n5.00...\\n>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and increase num of trees\\n>>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est\\n>>> mean_squared_error(y_test, est.predict(X_test))\\n3.84...\\n---------new doc---------\\nGradient Boosting regularization\\nGradient Boosting Out-of-Bag estimates\\nOOB Errors for Random Forests\\n\\n\\n\\n1.11.1.2.7. Interpretation with feature importance#\\nIndividual decision trees can be interpreted easily by simply\\nvisualizing the tree structure. Gradient boosting models, however,\\ncomprise hundreds of regression trees thus they cannot be easily\\ninterpreted by visual inspection of the individual trees. Fortunately,\\na number of techniques have been proposed to summarize and interpret\\ngradient boosting models.\\nOften features do not contribute equally to predict the target\\nresponse; in many situations the majority of the features are in fact\\nirrelevant.\\nWhen interpreting a model, the first question usually is: what are\\nthose important features and how do they contributing in predicting\\nthe target response?\\nIndividual decision trees intrinsically perform feature selection by selecting\\nappropriate split points. This information can be used to measure the\\nimportance of each feature; the basic idea is: the more often a\\nfeature is used in the split points of a tree the more important that\\nfeature is. This notion of importance can be extended to decision tree\\nensembles by simply averaging the impurity-based feature importance of each tree (see\\nFeature importance evaluation for more details).\\nThe feature importance scores of a fit gradient boosting model can be\\naccessed via the feature_importances_ property:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\n>>> clf.feature_importances_\\narray([0.10..., 0.10..., 0.11..., ...\\n\\n\\nNote that this computation of feature importance is based on entropy, and it\\nis distinct from sklearn.inspection.permutation_importance which is\\nbased on permutation of the features.\\nExamples\\n\\nGradient Boosting regression\\n\\nReferences\\n\\n\\n[Friedman2001]\\n(1,2,3,4)\\nFriedman, J.H. (2001). Greedy function approximation: A gradient\\nboosting machine.\\nAnnals of Statistics, 29, 1189-1232.\\n\\n\\n[Friedman2002]\\nFriedman, J.H. (2002). Stochastic gradient boosting..\\nComputational Statistics & Data Analysis, 38, 367-378.\\n\\n\\n[R2007]\\nG. Ridgeway (2006). Generalized Boosted Models: A guide to the gbm\\npackage\\n---------new doc---------\\nLike decision trees, forests of trees also extend to\\nmulti-output problems  (if Y is an array\\nof shape (n_samples, n_outputs)).\\n\\n1.11.2.1. Random Forests#\\nIn random forests (see RandomForestClassifier and\\nRandomForestRegressor classes), each tree in the ensemble is built\\nfrom a sample drawn with replacement (i.e., a bootstrap sample) from the\\ntraining set.\\nFurthermore, when splitting each node during the construction of a tree, the\\nbest split is found through an exhaustive search of the features values of\\neither all input features or a random subset of size max_features.\\n(See the parameter tuning guidelines for more details.)\\nThe purpose of these two sources of randomness is to decrease the variance of\\nthe forest estimator. Indeed, individual decision trees typically exhibit high\\nvariance and tend to overfit. The injected randomness in forests yield decision\\ntrees with somewhat decoupled prediction errors. By taking an average of those\\npredictions, some errors can cancel out. Random forests achieve a reduced\\nvariance by combining diverse trees, sometimes at the cost of a slight increase\\nin bias. In practice the variance reduction is often significant hence yielding\\nan overall better model.\\nIn contrast to the original publication [B2001], the scikit-learn\\nimplementation combines classifiers by averaging their probabilistic\\nprediction, instead of letting each classifier vote for a single class.\\nA competitive alternative to random forests are\\nHistogram-Based Gradient Boosting (HGBT) models:\\n---------new doc---------\\n>>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\\n...     random_state=0)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean()\\n0.98...\\n\\n>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,\\n...     min_samples_split=2, random_state=0)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean()\\n0.999...\\n\\n>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\\n...     min_samples_split=2, random_state=0)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean() > 0.999\\nTrue\\n---------new doc---------\\n1.11.2.3. Parameters#\\nThe main parameters to adjust when using these methods is n_estimators and\\nmax_features. The former is the number of trees in the forest. The larger\\nthe better, but also the longer it will take to compute. In addition, note that\\nresults will stop getting significantly better beyond a critical number of\\ntrees. The latter is the size of the random subsets of features to consider\\nwhen splitting a node. The lower the greater the reduction of variance, but\\nalso the greater the increase in bias. Empirical good default values are\\nmax_features=1.0 or equivalently max_features=None (always considering\\nall features instead of a random subset) for regression problems, and\\nmax_features=\\\"sqrt\\\" (using a random subset of size sqrt(n_features))\\nfor classification tasks (where n_features is the number of features in\\nthe data). The default value of max_features=1.0 is equivalent to bagged\\ntrees and more randomness can be achieved by setting smaller values (e.g. 0.3\\nis a typical default in the literature). Good results are often achieved when\\nsetting max_depth=None in combination with min_samples_split=2 (i.e.,\\nwhen fully developing the trees). Bear in mind though that these values are\\nusually not optimal, and might result in models that consume a lot of RAM.\\nThe best parameter values should always be cross-validated. In addition, note\\nthat in random forests, bootstrap samples are used by default\\n(bootstrap=True) while the default strategy for extra-trees is to use the\\nwhole dataset (bootstrap=False). When using bootstrap sampling the\\ngeneralization error can be estimated on the left out or out-of-bag samples.\\n---------new doc---------\\nThe best parameter values should always be cross-validated. In addition, note\\nthat in random forests, bootstrap samples are used by default\\n(bootstrap=True) while the default strategy for extra-trees is to use the\\nwhole dataset (bootstrap=False). When using bootstrap sampling the\\ngeneralization error can be estimated on the left out or out-of-bag samples.\\nThis can be enabled by setting oob_score=True.\\n---------new doc---------\\nBreiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.\\n\\n\\n\\n[B1998]\\n\\nBreiman, “Arcing Classifiers”, Annals of Statistics 1998.\\n\\n\\n\\n\\nP. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized\\ntrees”, Machine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n1.11.2.5. Feature importance evaluation#\\nThe relative rank (i.e. depth) of a feature used as a decision node in a\\ntree can be used to assess the relative importance of that feature with\\nrespect to the predictability of the target variable. Features used at\\nthe top of the tree contribute to the final prediction decision of a\\nlarger fraction of the input samples. The expected fraction of the\\nsamples they contribute to can thus be used as an estimate of the\\nrelative importance of the features. In scikit-learn, the fraction of\\nsamples a feature contributes to is combined with the decrease in impurity\\nfrom splitting them to create a normalized estimate of the predictive power\\nof that feature.\\nBy averaging the estimates of predictive ability over several randomized\\ntrees one can reduce the variance of such an estimate and use it\\nfor feature selection. This is known as the mean decrease in impurity, or MDI.\\nRefer to [L2014] for more information on MDI and feature importance\\nevaluation with Random Forests.\\n---------new doc---------\\nWarning\\nThe impurity-based feature importances computed on tree-based models suffer\\nfrom two flaws that can lead to misleading conclusions. First they are\\ncomputed on statistics derived from the training dataset and therefore do\\nnot necessarily inform us on which features are most important to make good\\npredictions on held-out dataset. Secondly, they favor high cardinality\\nfeatures, that is features with many unique values.\\nPermutation feature importance is an alternative to impurity-based feature\\nimportance that does not suffer from these flaws. These two methods of\\nobtaining feature importance are explored in:\\nPermutation Importance vs Random Forest Feature Importance (MDI).\\n\\nIn practice those estimates are stored as an attribute named\\nfeature_importances_ on the fitted model. This is an array with shape\\n(n_features,) whose values are positive and sum to 1.0. The higher\\nthe value, the more important is the contribution of the matching feature\\nto the prediction function.\\nExamples\\n\\nFeature importances with a forest of trees\\n\\nReferences\\n\\n\\n[L2014]\\nG. Louppe, “Understanding Random Forests: From Theory to\\nPractice”,\\nPhD Thesis, U. of Liege, 2014.\\n---------new doc---------\\nIn scikit-learn, bagging methods are offered as a unified\\nBaggingClassifier meta-estimator  (resp. BaggingRegressor),\\ntaking as input a user-specified estimator along with parameters\\nspecifying the strategy to draw random subsets. In particular, max_samples\\nand max_features control the size of the subsets (in terms of samples and\\nfeatures), while bootstrap and bootstrap_features control whether\\nsamples and features are drawn with or without replacement. When using a subset\\nof the available samples the generalization accuracy can be estimated with the\\nout-of-bag samples by setting oob_score=True. As an example, the\\nsnippet below illustrates how to instantiate a bagging ensemble of\\nKNeighborsClassifier estimators, each built on random\\nsubsets of 50% of the samples and 50% of the features.\\n>>> from sklearn.ensemble import BaggingClassifier\\n>>> from sklearn.neighbors import KNeighborsClassifier\\n>>> bagging = BaggingClassifier(KNeighborsClassifier(),\\n...                             max_samples=0.5, max_features=0.5)\\n\\n\\nExamples\\n\\nSingle estimator versus bagging: bias-variance decomposition\\n\\nReferences\\n\\n\\n[B1999]\\nL. Breiman, “Pasting small votes for classification in large\\ndatabases and on-line”, Machine Learning, 36(1), 85-103, 1999.\\n\\n\\n[B1996]\\nL. Breiman, “Bagging predictors”, Machine Learning, 24(2),\\n123-140, 1996.\\n---------new doc---------\\n1.11.4.1. Majority Class Labels (Majority/Hard Voting)#\\nIn majority voting, the predicted class label for a particular sample is\\nthe class label that represents the majority (mode) of the class labels\\npredicted by each individual classifier.\\nE.g., if the prediction for a given sample is\\n\\nclassifier 1 -> class 1\\nclassifier 2 -> class 1\\nclassifier 3 -> class 2\\n\\nthe VotingClassifier (with voting='hard') would classify the sample\\nas “class 1” based on the majority class label.\\nIn the cases of a tie, the VotingClassifier will select the class\\nbased on the ascending sort order. E.g., in the following scenario\\n\\nclassifier 1 -> class 2\\nclassifier 2 -> class 1\\n\\nthe class label 1 will be assigned to the sample.\\n\\n\\n1.11.4.2. Usage#\\nThe following example shows how to fit the majority rule classifier:\\n>>> from sklearn import datasets\\n>>> from sklearn.model_selection import cross_val_score\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.naive_bayes import GaussianNB\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.ensemble import VotingClassifier\\n\\n>>> iris = datasets.load_iris()\\n>>> X, y = iris.data[:, 1:3], iris.target\\n\\n>>> clf1 = LogisticRegression(random_state=1)\\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\\n>>> clf3 = GaussianNB()\\n---------new doc---------\\n>>> iris = datasets.load_iris()\\n>>> X, y = iris.data[:, 1:3], iris.target\\n\\n>>> clf1 = LogisticRegression(random_state=1)\\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\\n>>> clf3 = GaussianNB()\\n\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='hard')\\n\\n>>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\\n...     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\\n...     print(\\\"Accuracy: %0.2f (+/- %0.2f) [%s]\\\" % (scores.mean(), scores.std(), label))\\nAccuracy: 0.95 (+/- 0.04) [Logistic Regression]\\nAccuracy: 0.94 (+/- 0.04) [Random Forest]\\nAccuracy: 0.91 (+/- 0.04) [naive Bayes]\\nAccuracy: 0.95 (+/- 0.04) [Ensemble]\\n---------new doc---------\\nclassifier 1\\nw1 * 0.2\\nw1 * 0.5\\nw1 * 0.3\\n\\nclassifier 2\\nw2 * 0.6\\nw2 * 0.3\\nw2 * 0.1\\n\\nclassifier 3\\nw3 * 0.3\\nw3 * 0.4\\nw3 * 0.3\\n\\nweighted average\\n0.37\\n0.4\\n0.23\\n\\n\\n\\n\\nHere, the predicted class label is 2, since it has the\\nhighest average probability.\\nThe following example illustrates how the decision regions may change\\nwhen a soft VotingClassifier is used based on a linear Support\\nVector Machine, a Decision Tree, and a K-nearest neighbor classifier:\\n>>> from sklearn import datasets\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> from sklearn.neighbors import KNeighborsClassifier\\n>>> from sklearn.svm import SVC\\n>>> from itertools import product\\n>>> from sklearn.ensemble import VotingClassifier\\n\\n>>> # Loading some example data\\n>>> iris = datasets.load_iris()\\n>>> X = iris.data[:, [0, 2]]\\n>>> y = iris.target\\n---------new doc---------\\n>>> # Loading some example data\\n>>> iris = datasets.load_iris()\\n>>> X = iris.data[:, [0, 2]]\\n>>> y = iris.target\\n\\n>>> # Training classifiers\\n>>> clf1 = DecisionTreeClassifier(max_depth=4)\\n>>> clf2 = KNeighborsClassifier(n_neighbors=7)\\n>>> clf3 = SVC(kernel='rbf', probability=True)\\n>>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\\n...                         voting='soft', weights=[2, 1, 2])\\n\\n>>> clf1 = clf1.fit(X, y)\\n>>> clf2 = clf2.fit(X, y)\\n>>> clf3 = clf3.fit(X, y)\\n>>> eclf = eclf.fit(X, y)\\n\\n\\n\\n\\n\\n\\n\\n\\n1.11.4.4. Usage#\\nIn order to predict the class labels based on the predicted\\nclass-probabilities (scikit-learn estimators in the VotingClassifier\\nmust support predict_proba method):\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='soft'\\n... )\\n---------new doc---------\\nOptionally, weights can be provided for the individual classifiers:\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='soft', weights=[2,5,1]\\n... )\\n\\n\\n\\n\\nUsing the VotingClassifier with GridSearchCV#\\nThe VotingClassifier can also be used together with\\nGridSearchCV in order to tune the\\nhyperparameters of the individual estimators:\\n>>> from sklearn.model_selection import GridSearchCV\\n>>> clf1 = LogisticRegression(random_state=1)\\n>>> clf2 = RandomForestClassifier(random_state=1)\\n>>> clf3 = GaussianNB()\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='soft'\\n... )\\n\\n>>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\\n\\n>>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\\n>>> grid = grid.fit(iris.data, iris.target)\\n\\n\\n\\n\\n\\n\\n1.11.5. Voting Regressor#\\nThe idea behind the VotingRegressor is to combine conceptually\\ndifferent machine learning regressors and return the average predicted values.\\nSuch a regressor can be useful for a set of equally well performing models\\nin order to balance out their individual weaknesses.\\n---------new doc---------\\n1.11.5. Voting Regressor#\\nThe idea behind the VotingRegressor is to combine conceptually\\ndifferent machine learning regressors and return the average predicted values.\\nSuch a regressor can be useful for a set of equally well performing models\\nin order to balance out their individual weaknesses.\\n\\n1.11.5.1. Usage#\\nThe following example shows how to fit the VotingRegressor:\\n>>> from sklearn.datasets import load_diabetes\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>> from sklearn.ensemble import RandomForestRegressor\\n>>> from sklearn.linear_model import LinearRegression\\n>>> from sklearn.ensemble import VotingRegressor\\n\\n>>> # Loading some example data\\n>>> X, y = load_diabetes(return_X_y=True)\\n\\n>>> # Training classifiers\\n>>> reg1 = GradientBoostingRegressor(random_state=1)\\n>>> reg2 = RandomForestRegressor(random_state=1)\\n>>> reg3 = LinearRegression()\\n>>> ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\\n>>> ereg = ereg.fit(X, y)\\n\\n\\n\\n\\n\\n\\nExamples\\n\\nPlot individual and voting regression predictions\\n---------new doc---------\\nThe final_estimator will use the predictions of the estimators as input. It\\nneeds to be a classifier or a regressor when using StackingClassifier\\nor StackingRegressor, respectively:\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>> from sklearn.ensemble import StackingRegressor\\n>>> final_estimator = GradientBoostingRegressor(\\n...     n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,\\n...     random_state=42)\\n>>> reg = StackingRegressor(\\n...     estimators=estimators,\\n...     final_estimator=final_estimator)\\n\\n\\nTo train the estimators and final_estimator, the fit method needs\\nto be called on the training data:\\n>>> from sklearn.datasets import load_diabetes\\n>>> X, y = load_diabetes(return_X_y=True)\\n>>> from sklearn.model_selection import train_test_split\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\\n...                                                     random_state=42)\\n>>> reg.fit(X_train, y_train)\\nStackingRegressor(...)\\n---------new doc---------\\nDuring training, the estimators are fitted on the whole training data\\nX_train. They will be used when calling predict or predict_proba. To\\ngeneralize and avoid over-fitting, the final_estimator is trained on\\nout-samples using sklearn.model_selection.cross_val_predict internally.\\nFor StackingClassifier, note that the output of the estimators is\\ncontrolled by the parameter stack_method and it is called by each estimator.\\nThis parameter is either a string, being estimator method names, or 'auto'\\nwhich will automatically identify an available method depending on the\\navailability, tested in the order of preference: predict_proba,\\ndecision_function and predict.\\nA StackingRegressor and StackingClassifier can be used as\\nany other regressor or classifier, exposing a predict, predict_proba, or\\ndecision_function method, e.g.:\\n>>> y_pred = reg.predict(X_test)\\n>>> from sklearn.metrics import r2_score\\n>>> print('R2 score: {:.2f}'.format(r2_score(y_test, y_pred)))\\nR2 score: 0.53\\n\\n\\nNote that it is also possible to get the output of the stacked\\nestimators using the transform method:\\n>>> reg.transform(X_test[:5])\\narray([[142..., 138..., 146...],\\n       [179..., 182..., 151...],\\n       [139..., 132..., 158...],\\n       [286..., 292..., 225...],\\n       [126..., 124..., 164...]])\\n---------new doc---------\\nIn practice, a stacking predictor predicts as good as the best predictor of the\\nbase layer and even sometimes outperforms it by combining the different\\nstrengths of the these predictors. However, training a stacking predictor is\\ncomputationally expensive.\\n\\nNote\\nFor StackingClassifier, when using stack_method_='predict_proba',\\nthe first column is dropped when the problem is a binary classification\\nproblem. Indeed, both probability columns predicted by each estimator are\\nperfectly collinear.\\n---------new doc---------\\nNote\\nMultiple stacking layers can be achieved by assigning final_estimator to\\na StackingClassifier or StackingRegressor:\\n>>> final_layer_rfr = RandomForestRegressor(\\n...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\\n>>> final_layer_gbr = GradientBoostingRegressor(\\n...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\\n>>> final_layer = StackingRegressor(\\n...     estimators=[('rf', final_layer_rfr),\\n...                 ('gbrt', final_layer_gbr)],\\n...     final_estimator=RidgeCV()\\n...     )\\n>>> multi_layer_regressor = StackingRegressor(\\n...     estimators=[('ridge', RidgeCV()),\\n...                 ('lasso', LassoCV(random_state=42)),\\n...                 ('knr', KNeighborsRegressor(n_neighbors=20,\\n...                                             metric='euclidean'))],\\n...     final_estimator=final_layer\\n... )\\n---------new doc---------\\nAdaBoost can be used both for classification and regression problems:\\n\\nFor multi-class classification, AdaBoostClassifier implements\\nAdaBoost.SAMME [ZZRH2009].\\nFor regression, AdaBoostRegressor implements AdaBoost.R2 [D1997].\\n\\n\\n1.11.7.1. Usage#\\nThe following example shows how to fit an AdaBoost classifier with 100 weak\\nlearners:\\n>>> from sklearn.model_selection import cross_val_score\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.ensemble import AdaBoostClassifier\\n\\n>>> X, y = load_iris(return_X_y=True)\\n>>> clf = AdaBoostClassifier(n_estimators=100)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean()\\n0.9...\\n\\n\\nThe number of weak learners is controlled by the parameter n_estimators. The\\nlearning_rate parameter controls the contribution of the weak learners in\\nthe final combination. By default, weak learners are decision stumps. Different\\nweak learners can be specified through the estimator parameter.\\nThe main parameters to tune to obtain good results are n_estimators and\\nthe complexity of the base estimators (e.g., its depth max_depth or\\nminimum required number of samples to consider a split min_samples_split).\\nExamples\\n\\nMulti-class AdaBoosted Decision Trees shows the performance\\nof AdaBoost on a multi-class problem.\\nTwo-class AdaBoost shows the decision boundary\\nand decision function values for a non-linearly separable two-class problem\\nusing AdaBoost-SAMME.\\nDecision Tree Regression with AdaBoost demonstrates regression\\nwith the AdaBoost.R2 algorithm.\\n\\nReferences\\n---------new doc---------\\nMulticlass as One-Vs-One:\\n\\nsvm.NuSVC\\nsvm.SVC.\\ngaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_one”)\\n\\n\\nMulticlass as One-Vs-The-Rest:\\n\\nensemble.GradientBoostingClassifier\\ngaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_rest”)\\nsvm.LinearSVC (setting multi_class=”ovr”)\\nlinear_model.LogisticRegression (most solvers)\\nlinear_model.LogisticRegressionCV (most solvers)\\nlinear_model.SGDClassifier\\nlinear_model.Perceptron\\nlinear_model.PassiveAggressiveClassifier\\n\\n\\nSupport multilabel:\\n\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nneighbors.KNeighborsClassifier\\nneural_network.MLPClassifier\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\nlinear_model.RidgeClassifier\\nlinear_model.RidgeClassifierCV\\n\\n\\nSupport multiclass-multioutput:\\n\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nneighbors.KNeighborsClassifier\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\n\\n\\n\\n\\n1.12.1. Multiclass classification#\\n---------new doc---------\\nSupport multiclass-multioutput:\\n\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nneighbors.KNeighborsClassifier\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\n\\n\\n\\n\\n1.12.1. Multiclass classification#\\n\\nWarning\\nAll classifiers in scikit-learn do multiclass classification\\nout-of-the-box. You don’t need to use the sklearn.multiclass module\\nunless you want to experiment with different multiclass strategies.\\n\\nMulticlass classification is a classification task with more than two\\nclasses. Each sample can only be labeled as one class.\\nFor example, classification using features extracted from a set of images of\\nfruit, where each image may either be of an orange, an apple, or a pear.\\nEach image is one sample and is labeled as one of the 3 possible classes.\\nMulticlass classification makes the assumption that each sample is assigned\\nto one and only one label - one sample cannot, for example, be both a pear\\nand an apple.\\nWhile all scikit-learn classifiers are capable of multiclass classification,\\nthe meta-estimators offered by sklearn.multiclass\\npermit changing the way they handle more than two classes\\nbecause this may have an effect on classifier performance\\n(either in terms of generalization error or required computational resources).\\n\\n1.12.1.1. Target format#\\nValid multiclass representations for\\ntype_of_target (y) are:\\n---------new doc---------\\n1.12.1.2. OneVsRestClassifier#\\nThe one-vs-rest strategy, also known as one-vs-all, is implemented in\\nOneVsRestClassifier.  The strategy consists in\\nfitting one classifier per class. For each classifier, the class is fitted\\nagainst all the other classes. In addition to its computational efficiency\\n(only n_classes classifiers are needed), one advantage of this approach is\\nits interpretability. Since each class is represented by one and only one\\nclassifier, it is possible to gain knowledge about the class by inspecting its\\ncorresponding classifier. This is the most commonly used strategy and is a fair\\ndefault choice.\\nBelow is an example of multiclass learning using OvR:\\n>>> from sklearn import datasets\\n>>> from sklearn.multiclass import OneVsRestClassifier\\n>>> from sklearn.svm import LinearSVC\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n---------new doc---------\\n1.12.1.3. OneVsOneClassifier#\\nOneVsOneClassifier constructs one classifier per\\npair of classes. At prediction time, the class which received the most votes\\nis selected. In the event of a tie (among two classes with an equal number of\\nvotes), it selects the class with the highest aggregate classification\\nconfidence by summing over the pair-wise classification confidence levels\\ncomputed by the underlying binary classifiers.\\nSince it requires to fit n_classes * (n_classes - 1) / 2 classifiers,\\nthis method is usually slower than one-vs-the-rest, due to its\\nO(n_classes^2) complexity. However, this method may be advantageous for\\nalgorithms such as kernel algorithms which don’t scale well with\\nn_samples. This is because each individual learning problem only involves\\na small subset of the data whereas, with one-vs-the-rest, the complete\\ndataset is used n_classes times. The decision function is the result\\nof a monotonic transformation of the one-versus-one classification.\\nBelow is an example of multiclass learning using OvO:\\n>>> from sklearn import datasets\\n>>> from sklearn.multiclass import OneVsOneClassifier\\n>>> from sklearn.svm import LinearSVC\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n---------new doc---------\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n       1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\n---------new doc---------\\none-vs-the-rest. In theory, log2(n_classes) / n_classes is sufficient to\\nrepresent each class unambiguously. However, in practice, it may not lead to\\ngood accuracy since log2(n_classes) is much smaller than n_classes.\\nA number greater than 1 will require more classifiers than\\none-vs-the-rest. In this case, some classifiers will in theory correct for\\nthe mistakes made by other classifiers, hence the name “error-correcting”.\\nIn practice, however, this may not happen as classifier mistakes will\\ntypically be correlated. The error-correcting output codes have a similar\\neffect to bagging.\\nBelow is an example of multiclass learning using Output-Codes:\\n>>> from sklearn import datasets\\n>>> from sklearn.multiclass import OutputCodeClassifier\\n>>> from sklearn.svm import LinearSVC\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> clf = OutputCodeClassifier(LinearSVC(random_state=0), code_size=2, random_state=0)\\n>>> clf.fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n---------new doc---------\\nReferences\\n\\n“Solving multiclass learning problems via error-correcting output codes”,\\nDietterich T., Bakiri G., Journal of Artificial Intelligence Research 2, 1995.\\n\\n\\n\\n[3]\\n“The error coding method and PICTs”, James G., Hastie T.,\\nJournal of Computational and Graphical statistics 7, 1998.\\n\\n\\n\\n“The Elements of Statistical Learning”,\\nHastie T., Tibshirani R., Friedman J., page 606 (second-edition), 2008.\\n\\n\\n\\n\\n1.12.2. Multilabel classification#\\nMultilabel classification (closely related to multioutput\\nclassification) is a classification task labeling each sample with m\\nlabels from n_classes possible classes, where m can be 0 to\\nn_classes inclusive. This can be thought of as predicting properties of a\\nsample that are not mutually exclusive. Formally, a binary output is assigned\\nto each class, for every sample. Positive classes are indicated with 1 and\\nnegative classes with 0 or -1. It is thus comparable to running n_classes\\nbinary classification tasks, for example with\\nMultiOutputClassifier. This approach treats\\neach label independently whereas multilabel classifiers may treat the\\nmultiple classes simultaneously, accounting for correlated behavior among\\nthem.\\nFor example, prediction of the topics relevant to a text document or video.\\nThe document or video may be about one of ‘religion’, ‘politics’, ‘finance’\\nor ‘education’, several of the topic classes or all of the topic classes.\\n---------new doc---------\\n1.12.2.2. MultiOutputClassifier#\\nMultilabel classification support can be added to any classifier with\\nMultiOutputClassifier. This strategy consists of\\nfitting one classifier per target.  This allows multiple target variable\\nclassifications. The purpose of this class is to extend estimators\\nto be able to estimate a series of target functions (f1,f2,f3…,fn)\\nthat are trained on a single X predictor matrix to predict a series\\nof responses (y1,y2,y3…,yn).\\nYou can find a usage example for\\nMultiOutputClassifier\\nas part of the section on Multiclass-multioutput classification\\nsince it is a generalization of multilabel classification to\\nmulticlass outputs instead of binary outputs.\\n---------new doc---------\\n1.12.2.3. ClassifierChain#\\nClassifier chains (see ClassifierChain) are a way\\nof combining a number of binary classifiers into a single multi-label model\\nthat is capable of exploiting correlations among targets.\\nFor a multi-label classification problem with N classes, N binary\\nclassifiers are assigned an integer between 0 and N-1. These integers\\ndefine the order of models in the chain. Each classifier is then fit on the\\navailable training data plus the true labels of the classes whose\\nmodels were assigned a lower number.\\nWhen predicting, the true labels will not be available. Instead the\\npredictions of each model are passed on to the subsequent models in the\\nchain to be used as features.\\nClearly the order of the chain is important. The first model in the chain\\nhas no information about the other labels while the last model in the chain\\nhas features indicating the presence of all of the other labels. In general\\none does not know the optimal ordering of the models in the chain so\\ntypically many randomly ordered chains are fit and their predictions are\\naveraged together.\\nReferences\\n\\nJesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,\\n“Classifier Chains for Multi-label Classification”, 2009.\\n---------new doc---------\\n1.12.3. Multiclass-multioutput classification#\\nMulticlass-multioutput classification\\n(also known as multitask classification) is a\\nclassification task which labels each sample with a set of non-binary\\nproperties. Both the number of properties and the number of\\nclasses per property is greater than 2. A single estimator thus\\nhandles several joint classification tasks. This is both a generalization of\\nthe multilabel classification task, which only considers binary\\nattributes, as well as a generalization of the multiclass classification\\ntask, where only one property is considered.\\nFor example, classification of the properties “type of fruit” and “colour”\\nfor a set of images of fruit. The property “type of fruit” has the possible\\nclasses: “apple”, “pear” and “orange”. The property “colour” has the\\npossible classes: “green”, “red”, “yellow” and “orange”. Each sample is an\\nimage of a fruit, a label is output for both properties and each label is\\none of the possible classes of the corresponding property.\\nNote that all classifiers handling multiclass-multioutput (also known as\\nmultitask classification) tasks, support the multilabel classification task\\nas a special case. Multitask classification is similar to the multioutput\\nclassification task with different model formulations. For more information,\\nsee the relevant estimator documentation.\\nBelow is an example of multiclass-multioutput classification:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.multioutput import MultiOutputClassifier\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.utils import shuffle\\n>>> import numpy as np\\n---------new doc---------\\nclassification task with different model formulations. For more information,\\nsee the relevant estimator documentation.\\nBelow is an example of multiclass-multioutput classification:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.multioutput import MultiOutputClassifier\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.utils import shuffle\\n>>> import numpy as np\\n>>> X, y1 = make_classification(n_samples=10, n_features=100,\\n...                             n_informative=30, n_classes=3,\\n...                             random_state=1)\\n>>> y2 = shuffle(y1, random_state=1)\\n>>> y3 = shuffle(y1, random_state=2)\\n>>> Y = np.vstack((y1, y2, y3)).T\\n>>> n_samples, n_features = X.shape # 10,100\\n>>> n_outputs = Y.shape[1] # 3\\n>>> n_classes = 3\\n>>> forest = RandomForestClassifier(random_state=1)\\n>>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)\\n>>> multi_target_forest.fit(X, Y).predict(X)\\narray([[2, 2, 0],\\n       [1, 2, 1],\\n       [2, 1, 0],\\n       [0, 0, 2],\\n---------new doc---------\\n>>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)\\n>>> multi_target_forest.fit(X, Y).predict(X)\\narray([[2, 2, 0],\\n       [1, 2, 1],\\n       [2, 1, 0],\\n       [0, 0, 2],\\n       [0, 2, 1],\\n       [0, 0, 2],\\n       [1, 1, 0],\\n       [1, 1, 1],\\n       [0, 0, 2],\\n       [2, 0, 0]])\\n---------new doc---------\\nWarning\\nAt present, no metric in sklearn.metrics\\nsupports the multiclass-multioutput classification task.\\n\\n\\n1.12.3.1. Target format#\\nA valid representation of multioutput y is a dense matrix of shape\\n(n_samples, n_classes) of class labels. A column wise concatenation of 1d\\nmulticlass variables. An example of y for 3 samples:\\n>>> y = np.array([['apple', 'green'], ['orange', 'orange'], ['pear', 'green']])\\n>>> print(y)\\n[['apple' 'green']\\n ['orange' 'orange']\\n ['pear' 'green']]\\n\\n\\n\\n\\n\\n1.12.4. Multioutput regression#\\nMultioutput regression predicts multiple numerical properties for each\\nsample. Each property is a numerical variable and the number of properties\\nto be predicted for each sample is greater than or equal to 2. Some estimators\\nthat support multioutput regression are faster than just running n_output\\nestimators.\\nFor example, prediction of both wind speed and wind direction, in degrees,\\nusing data obtained at a certain location. Each sample would be data\\nobtained at one location and both wind speed and direction would be\\noutput for each sample.\\nThe following regressors natively support multioutput regression:\\n---------new doc---------\\n1.12.4.2. MultiOutputRegressor#\\nMultioutput regression support can be added to any regressor with\\nMultiOutputRegressor.  This strategy consists of\\nfitting one regressor per target. Since each target is represented by exactly\\none regressor it is possible to gain knowledge about the target by\\ninspecting its corresponding regressor. As\\nMultiOutputRegressor fits one regressor per\\ntarget it can not take advantage of correlations between targets.\\nBelow is an example of multioutput regression:\\n>>> from sklearn.datasets import make_regression\\n>>> from sklearn.multioutput import MultiOutputRegressor\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>> X, y = make_regression(n_samples=10, n_targets=3, random_state=1)\\n>>> MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X)\\narray([[-154.75474165, -147.03498585,  -50.03812219],\\n       [   7.12165031,    5.12914884,  -81.46081961],\\n       [-187.8948621 , -100.44373091,   13.88978285],\\n       [-141.62745778,   95.02891072, -191.48204257],\\n       [  97.03260883,  165.34867495,  139.52003279],\\n       [ 123.92529176,   21.25719016,   -7.84253   ],\\n---------new doc---------\\n1.12.4.3. RegressorChain#\\nRegressor chains (see RegressorChain) is\\nanalogous to ClassifierChain as a way of\\ncombining a number of regressions into a single multi-target model that is\\ncapable of exploiting correlations among targets.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking\\n\\n\\n\\n\\nnext\\n1.13. Feature selection\\n\\n\\n --- \\n\\n\\n1. Supervised learning\\n1.13. Feature selection\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1.13. Feature selection#\\nThe classes in the sklearn.feature_selection module can be used\\nfor feature selection/dimensionality reduction on sample sets, either to\\nimprove estimators’ accuracy scores or to boost their performance on very\\nhigh-dimensional datasets.\\n\\n1.13.1. Removing features with low variance#\\nVarianceThreshold is a simple baseline approach to feature selection.\\nIt removes all features whose variance doesn’t meet some threshold.\\nBy default, it removes all zero-variance features,\\ni.e. features that have the same value in all samples.\\nAs an example, suppose that we have a dataset with boolean features,\\nand we want to remove all features that are either one or zero (on or off)\\nin more than 80% of the samples.\\nBoolean features are Bernoulli random variables,\\nand the variance of such variables is given by\\n---------new doc---------\\n\\\\[\\\\mathrm{Var}[X] = p(1 - p)\\\\]\\nso we can select using the threshold .8 * (1 - .8):\\n>>> from sklearn.feature_selection import VarianceThreshold\\n>>> X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\\n>>> sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\\n>>> sel.fit_transform(X)\\narray([[0, 1],\\n       [1, 0],\\n       [0, 0],\\n       [1, 1],\\n       [1, 0],\\n       [1, 1]])\\n\\n\\nAs expected, VarianceThreshold has removed the first column,\\nwhich has a probability \\\\(p = 5/6 > .8\\\\) of containing a zero.\\n\\n\\n1.13.2. Univariate feature selection#\\nUnivariate feature selection works by selecting the best features based on\\nunivariate statistical tests. It can be seen as a preprocessing step\\nto an estimator. Scikit-learn exposes feature selection routines\\nas objects that implement the transform method:\\n---------new doc---------\\nAs expected, VarianceThreshold has removed the first column,\\nwhich has a probability \\\\(p = 5/6 > .8\\\\) of containing a zero.\\n\\n\\n1.13.2. Univariate feature selection#\\nUnivariate feature selection works by selecting the best features based on\\nunivariate statistical tests. It can be seen as a preprocessing step\\nto an estimator. Scikit-learn exposes feature selection routines\\nas objects that implement the transform method:\\n\\nSelectKBest removes all but the \\\\(k\\\\) highest scoring features\\nSelectPercentile removes all but a user-specified highest scoring\\npercentage of features\\nusing common univariate statistical tests for each feature:\\nfalse positive rate SelectFpr, false discovery rate\\nSelectFdr, or family wise error SelectFwe.\\nGenericUnivariateSelect allows to perform univariate feature\\nselection with a configurable strategy. This allows to select the best\\nunivariate selection strategy with hyper-parameter search estimator.\\n\\nFor instance, we can use a F-test to retrieve the two\\nbest features for a dataset as follows:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.feature_selection import SelectKBest\\n>>> from sklearn.feature_selection import f_classif\\n>>> X, y = load_iris(return_X_y=True)\\n>>> X.shape\\n(150, 4)\\n>>> X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\\n>>> X_new.shape\\n(150, 2)\\n\\n\\nThese objects take as input a scoring function that returns univariate scores\\nand p-values (or only scores for SelectKBest and\\nSelectPercentile):\\n---------new doc---------\\nWarning\\nBeware not to use a regression scoring function with a classification\\nproblem, you will get useless results.\\n\\n\\nNote\\nThe SelectPercentile and SelectKBest support unsupervised\\nfeature selection as well. One needs to provide a score_func where y=None.\\nThe score_func should use internally X to compute the scores.\\n\\nExamples\\n\\nUnivariate Feature Selection\\nComparison of F-test and mutual information\\n\\n\\n\\n1.13.3. Recursive feature elimination#\\nGiven an external estimator that assigns weights to features (e.g., the\\ncoefficients of a linear model), the goal of recursive feature elimination (RFE)\\nis to select features by recursively considering smaller and smaller sets of\\nfeatures. First, the estimator is trained on the initial set of features and\\nthe importance of each feature is obtained either through any specific attribute\\n(such as coef_, feature_importances_) or callable. Then, the least important\\nfeatures are pruned from current set of features. That procedure is recursively\\nrepeated on the pruned set until the desired number of features to select is\\neventually reached.\\nRFECV performs RFE in a cross-validation loop to find the optimal\\nnumber of features. In more details, the number of features selected is tuned\\nautomatically by fitting an RFE selector on the different\\ncross-validation splits (provided by the cv parameter). The performance\\nof the RFE selector are evaluated using scorer for different number\\nof selected features and aggregated together. Finally, the scores are averaged\\nacross folds and the number of features selected is set to the number of\\nfeatures that maximize the cross-validation score.\\nExamples\\n---------new doc---------\\nRecursive feature elimination: A recursive feature elimination example\\nshowing the relevance of pixels in a digit classification task.\\nRecursive feature elimination with cross-validation: A recursive feature\\nelimination example with automatic tuning of the number of features\\nselected with cross-validation.\\n\\n\\n\\n1.13.4. Feature selection using SelectFromModel#\\nSelectFromModel is a meta-transformer that can be used alongside any\\nestimator that assigns importance to each feature through a specific attribute (such as\\ncoef_, feature_importances_) or via an importance_getter callable after fitting.\\nThe features are considered unimportant and removed if the corresponding\\nimportance of the feature values are below the provided\\nthreshold parameter. Apart from specifying the threshold numerically,\\nthere are built-in heuristics for finding a threshold using a string argument.\\nAvailable heuristics are “mean”, “median” and float multiples of these like\\n“0.1*mean”. In combination with the threshold criteria, one can use the\\nmax_features parameter to set a limit on the number of features to select.\\nFor examples on how it is to be used refer to the sections below.\\nExamples\\n\\nModel-based and sequential feature selection\\n---------new doc---------\\n1.13.4.2. Tree-based feature selection#\\nTree-based estimators (see the sklearn.tree module and forest\\nof trees in the sklearn.ensemble module) can be used to compute\\nimpurity-based feature importances, which in turn can be used to discard irrelevant\\nfeatures (when coupled with the SelectFromModel\\nmeta-transformer):\\n>>> from sklearn.ensemble import ExtraTreesClassifier\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.feature_selection import SelectFromModel\\n>>> X, y = load_iris(return_X_y=True)\\n>>> X.shape\\n(150, 4)\\n>>> clf = ExtraTreesClassifier(n_estimators=50)\\n>>> clf = clf.fit(X, y)\\n>>> clf.feature_importances_  \\narray([ 0.04...,  0.05...,  0.4...,  0.4...])\\n>>> model = SelectFromModel(clf, prefit=True)\\n>>> X_new = model.transform(X)\\n>>> X_new.shape               \\n(150, 2)\\n\\n\\nExamples\\n\\nFeature importances with a forest of trees: example on\\nsynthetic data showing the recovery of the actually meaningful features.\\nPermutation Importance vs Random Forest Feature Importance (MDI): example\\ndiscussing the caveats of using impurity-based feature importances as a proxy for\\nfeature relevance.\\n---------new doc---------\\nExamples\\n\\nFeature importances with a forest of trees: example on\\nsynthetic data showing the recovery of the actually meaningful features.\\nPermutation Importance vs Random Forest Feature Importance (MDI): example\\ndiscussing the caveats of using impurity-based feature importances as a proxy for\\nfeature relevance.\\n\\n\\n\\n\\n1.13.5. Sequential Feature Selection#\\nSequential Feature Selection [sfs] (SFS) is available in the\\nSequentialFeatureSelector transformer.\\nSFS can be either forward or backward:\\nForward-SFS is a greedy procedure that iteratively finds the best new feature\\nto add to the set of selected features. Concretely, we initially start with\\nzero features and find the one feature that maximizes a cross-validated score\\nwhen an estimator is trained on this single feature. Once that first feature\\nis selected, we repeat the procedure by adding a new feature to the set of\\nselected features. The procedure stops when the desired number of selected\\nfeatures is reached, as determined by the n_features_to_select parameter.\\nBackward-SFS follows the same idea but works in the opposite direction:\\ninstead of starting with no features and greedily adding features, we start\\nwith all the features and greedily remove features from the set. The\\ndirection parameter controls whether forward or backward SFS is used.\\n---------new doc---------\\nDetails on Sequential Feature Selection#\\nIn general, forward and backward selection do not yield equivalent results.\\nAlso, one may be much faster than the other depending on the requested number\\nof selected features: if we have 10 features and ask for 7 selected features,\\nforward selection would need to perform 7 iterations while backward selection\\nwould only need to perform 3.\\nSFS differs from RFE and\\nSelectFromModel in that it does not\\nrequire the underlying model to expose a coef_ or feature_importances_\\nattribute. It may however be slower considering that more models need to be\\nevaluated, compared to the other approaches. For example in backward\\nselection, the iteration going from m features to m - 1 features using k-fold\\ncross-validation requires fitting m * k models, while\\nRFE would require only a single fit, and\\nSelectFromModel always just does a single\\nfit and requires no iterations.\\nReferences\\n\\n\\n[sfs]\\nFerri et al, Comparative study of techniques for\\nlarge-scale feature selection.\\n\\n\\n\\nExamples\\n\\nModel-based and sequential feature selection\\n\\n\\n\\n1.13.6. Feature selection as part of a pipeline#\\nFeature selection is usually used as a pre-processing step before doing\\nthe actual learning. The recommended way to do this in scikit-learn is\\nto use a Pipeline:\\nclf = Pipeline([\\n  ('feature_selection', SelectFromModel(LinearSVC(penalty=\\\"l1\\\"))),\\n  ('classification', RandomForestClassifier())\\n])\\nclf.fit(X, y)\\n---------new doc---------\\nIn this snippet we make use of a LinearSVC\\ncoupled with SelectFromModel\\nto evaluate feature importances and select the most relevant features.\\nThen, a RandomForestClassifier is trained on the\\ntransformed output, i.e. using only relevant features. You can perform\\nsimilar operations with the other feature selection methods and also\\nclassifiers that provide a way to evaluate feature importances of course.\\nSee the Pipeline examples for more details.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n1.12. Multiclass and multioutput algorithms\\n\\n\\n\\n\\nnext\\n1.14. Semi-supervised learning\\n\\n\\n --- \\n\\n\\n1. Supervised learning\\n1.14. Semi-supervised learning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1.14. Semi-supervised learning#\\nSemi-supervised learning is a situation\\nin which in your training data some of the samples are not labeled. The\\nsemi-supervised estimators in sklearn.semi_supervised are able to\\nmake use of this additional unlabeled data to better capture the shape of\\nthe underlying data distribution and generalize better to new samples.\\nThese algorithms can perform well when we have a very small amount of\\nlabeled points and a large amount of unlabeled points.\\n\\nUnlabeled entries in y\\nIt is important to assign an identifier to unlabeled points along with the\\nlabeled data when training the model with the fit method. The\\nidentifier that this implementation uses is the integer value \\\\(-1\\\\).\\nNote that for string labels, the dtype of y should be object so that it\\ncan contain both strings and integers.\\n\\n\\nNote\\nSemi-supervised algorithms need to make assumptions about the distribution\\nof the dataset in order to achieve performance gains. See here\\nfor more details.\\n---------new doc---------\\nNote\\nSemi-supervised algorithms need to make assumptions about the distribution\\nof the dataset in order to achieve performance gains. See here\\nfor more details.\\n\\n\\n1.14.1. Self Training#\\nThis self-training implementation is based on Yarowsky’s [1] algorithm. Using\\nthis algorithm, a given supervised classifier can function as a semi-supervised\\nclassifier, allowing it to learn from unlabeled data.\\nSelfTrainingClassifier can be called with any classifier that\\nimplements predict_proba, passed as the parameter base_classifier. In\\neach iteration, the base_classifier predicts labels for the unlabeled\\nsamples and adds a subset of these labels to the labeled dataset.\\nThe choice of this subset is determined by the selection criterion. This\\nselection can be done using a threshold on the prediction probabilities, or\\nby choosing the k_best samples according to the prediction probabilities.\\nThe labels used for the final fit as well as the iteration in which each sample\\nwas labeled are available as attributes. The optional max_iter parameter\\nspecifies how many times the loop is executed at most.\\nThe max_iter parameter may be set to None, causing the algorithm to iterate\\nuntil all samples have labels or no new samples are selected in that iteration.\\n\\nNote\\nWhen using the self-training classifier, the\\ncalibration of the classifier is important.\\n\\nExamples\\n\\nEffect of varying threshold for self-training\\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\\n\\nReferences\\n---------new doc---------\\nNote\\nWhen using the self-training classifier, the\\ncalibration of the classifier is important.\\n\\nExamples\\n\\nEffect of varying threshold for self-training\\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\\n\\nReferences\\n\\n\\n[1]\\n“Unsupervised word sense disambiguation rivaling supervised methods”\\nDavid Yarowsky, Proceedings of the 33rd annual meeting on Association for\\nComputational Linguistics (ACL ‘95). Association for Computational Linguistics,\\nStroudsburg, PA, USA, 189-196.\\n\\n\\n\\n\\n1.14.2. Label Propagation#\\nLabel propagation denotes a few variations of semi-supervised graph\\ninference algorithms.\\n\\nA few features available in this model:\\nUsed for classification tasks\\nKernel methods to project data into alternate dimensional spaces\\n\\n\\n\\nscikit-learn provides two label propagation models:\\nLabelPropagation and LabelSpreading. Both work by\\nconstructing a similarity graph over all items in the input dataset.\\n\\n\\n\\n\\nAn illustration of label-propagation: the structure of unlabeled\\nobservations is consistent with the class structure, and thus the\\nclass label can be propagated to the unlabeled observations of the\\ntraining set.#\\n---------new doc---------\\nExamples\\n\\nIsotonic Regression\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n1.14. Semi-supervised learning\\n\\n\\n\\n\\nnext\\n1.16. Probability calibration\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThis Page\\n\\nShow Source\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      © Copyright 2007 - 2025, scikit-learn developers (BSD License).\\n\\n\\n --- \\n\\n\\n1. Supervised learning\\n1.16. Probability calibration\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1.16. Probability calibration#\\nWhen performing classification you often want not only to predict the class\\nlabel, but also obtain a probability of the respective label. This probability\\ngives you some kind of confidence on the prediction. Some models can give you\\npoor estimates of the class probabilities and some even do not support\\nprobability prediction (e.g., some instances of\\nSGDClassifier).\\nThe calibration module allows you to better calibrate\\nthe probabilities of a given model, or to add support for probability\\nprediction.\\nWell calibrated classifiers are probabilistic classifiers for which the output\\nof the predict_proba method can be directly interpreted as a confidence\\nlevel.\\nFor instance, a well calibrated (binary) classifier should classify the samples such\\nthat among the samples to which it gave a predict_proba value close to, say,\\n0.8, approximately 80% actually belong to the positive class.\\nBefore we show how to re-calibrate a classifier, we first need a way to detect how\\ngood a classifier is calibrated.\\n---------new doc---------\\nLogisticRegression is more likely to return well calibrated predictions by itself as it has a\\ncanonical link function for its loss, i.e. the logit-link for the Log loss.\\nIn the unpenalized case, this leads to the so-called balance property, see [8] and Logistic regression.\\nIn the plot above, data is generated according to a linear mechanism, which is\\nconsistent with the LogisticRegression model (the model is ‘well specified’),\\nand the value of the regularization parameter C is tuned to be\\nappropriate (neither too strong nor too low). As a consequence, this model returns\\naccurate predictions from its predict_proba method.\\nIn contrast to that, the other shown models return biased probabilities; with\\ndifferent biases per model.\\nGaussianNB (Naive Bayes) tends to push probabilities to 0 or 1 (note the counts\\nin the histograms). This is mainly because it makes the assumption that\\nfeatures are conditionally independent given the class, which is not the\\ncase in this dataset which contains 2 redundant features.\\nRandomForestClassifier shows the opposite behavior: the histograms\\nshow peaks at probabilities approximately 0.2 and 0.9, while probabilities\\nclose to 0 or 1 are very rare. An explanation for this is given by\\nNiculescu-Mizil and Caruana [3]: “Methods such as bagging and random\\nforests that average predictions from a base set of models can have\\ndifficulty making predictions near 0 and 1 because variance in the\\nunderlying base models will bias predictions that should be near zero or one\\naway from these values. Because predictions are restricted to the interval\\n[0,1], errors caused by variance tend to be one-sided near zero and one. For\\n---------new doc---------\\nforests that average predictions from a base set of models can have\\ndifficulty making predictions near 0 and 1 because variance in the\\nunderlying base models will bias predictions that should be near zero or one\\naway from these values. Because predictions are restricted to the interval\\n[0,1], errors caused by variance tend to be one-sided near zero and one. For\\nexample, if a model should predict p = 0 for a case, the only way bagging\\ncan achieve this is if all bagged trees predict zero. If we add noise to the\\ntrees that bagging is averaging over, this noise will cause some trees to\\npredict values larger than 0 for this case, thus moving the average\\nprediction of the bagged ensemble away from 0. We observe this effect most\\nstrongly with random forests because the base-level trees trained with\\nrandom forests have relatively high variance due to feature subsetting.” As\\na result, the calibration curve shows a characteristic sigmoid shape, indicating that\\nthe classifier could trust its “intuition” more and return probabilities closer\\nto 0 or 1 typically.\\nLinearSVC (SVC) shows an even more sigmoid curve than the random forest, which\\nis typical for maximum-margin methods (compare Niculescu-Mizil and Caruana [3]), which\\nfocus on difficult to classify samples that are close to the decision boundary (the\\nsupport vectors).\\n---------new doc---------\\n1.16.2. Calibrating a classifier#\\nCalibrating a classifier consists of fitting a regressor (called a\\ncalibrator) that maps the output of the classifier (as given by\\ndecision_function or predict_proba) to a calibrated probability\\nin [0, 1]. Denoting the output of the classifier for a given sample by \\\\(f_i\\\\),\\nthe calibrator tries to predict the conditional event probability\\n\\\\(P(y_i = 1 | f_i)\\\\).\\nIdeally, the calibrator is fit on a dataset independent of the training data used to\\nfit the classifier in the first place.\\nThis is because performance of the classifier on its training data would be\\nbetter than for novel data. Using the classifier output of training data\\nto fit the calibrator would thus result in a biased calibrator that maps to\\nprobabilities closer to 0 and 1 than it should.\\n\\n\\n1.16.3. Usage#\\nThe CalibratedClassifierCV class is used to calibrate a classifier.\\nCalibratedClassifierCV uses a cross-validation approach to ensure\\nunbiased data is always used to fit the calibrator. The data is split into k\\n(train_set, test_set) couples (as determined by cv). When ensemble=True\\n(default), the following procedure is repeated independently for each\\ncross-validation split:\\n\\na clone of base_estimator is trained on the train subset\\nthe trained base_estimator makes predictions on the test subset\\nthe predictions are used to fit a calibrator (either a sigmoid or isotonic\\nregressor) (when the data is multiclass, a calibrator is fit for every class)\\n---------new doc---------\\nThis results in an\\nensemble of k (classifier, calibrator) couples where each calibrator maps\\nthe output of its corresponding classifier into [0, 1]. Each couple is exposed\\nin the calibrated_classifiers_ attribute, where each entry is a calibrated\\nclassifier with a predict_proba method that outputs calibrated\\nprobabilities. The output of predict_proba for the main\\nCalibratedClassifierCV instance corresponds to the average of the\\npredicted probabilities of the k estimators in the calibrated_classifiers_\\nlist. The output of predict is the class that has the highest\\nprobability.\\nIt is important to choose cv carefully when using ensemble=True.\\nAll classes should be present in both train and test subsets for every split.\\nWhen a class is absent in the train subset, the predicted probability for that\\nclass will default to 0 for the (classifier, calibrator) couple of that split.\\nThis skews the predict_proba as it averages across all couples.\\nWhen a class is absent in the test subset, the calibrator for that class\\n(within the (classifier, calibrator) couple of that split) is\\nfit on data with no positive class. This results in ineffective calibration.\\nWhen ensemble=False, cross-validation is used to obtain ‘unbiased’\\npredictions for all the data, via\\ncross_val_predict.\\nThese unbiased predictions are then used to train the calibrator. The attribute\\ncalibrated_classifiers_ consists of only one (classifier, calibrator)\\ncouple where the classifier is the base_estimator trained on all the data.\\nIn this case the output of predict_proba for\\nCalibratedClassifierCV is the predicted probabilities obtained\\nfrom the single (classifier, calibrator) couple.\\n---------new doc---------\\ncross_val_predict.\\nThese unbiased predictions are then used to train the calibrator. The attribute\\ncalibrated_classifiers_ consists of only one (classifier, calibrator)\\ncouple where the classifier is the base_estimator trained on all the data.\\nIn this case the output of predict_proba for\\nCalibratedClassifierCV is the predicted probabilities obtained\\nfrom the single (classifier, calibrator) couple.\\nThe main advantage of ensemble=True is to benefit from the traditional\\nensembling effect (similar to Bagging meta-estimator). The resulting ensemble should\\nboth be well calibrated and slightly more accurate than with ensemble=False.\\nThe main advantage of using ensemble=False is computational: it reduces the\\noverall fit time by training only a single base classifier and calibrator\\npair, decreases the final model size and increases prediction speed.\\nAlternatively an already fitted classifier can be calibrated by using a\\nFrozenEstimator as\\nCalibratedClassifierCV(estimator=FrozenEstimator(estimator)).\\nIt is up to the user to make sure that the data used for fitting the classifier\\nis disjoint from the data used for fitting the regressor.\\ndata used for fitting the regressor.\\nCalibratedClassifierCV supports the use of two regression techniques\\nfor calibration via the method parameter: \\\"sigmoid\\\" and \\\"isotonic\\\".\\n---------new doc---------\\nThe disadvantages of Multi-layer Perceptron (MLP) include:\\n\\nMLP with hidden layers have a non-convex loss function where there exists\\nmore than one local minimum. Therefore different random weight\\ninitializations can lead to different validation accuracy.\\nMLP requires tuning a number of hyperparameters such as the number of\\nhidden neurons, layers, and iterations.\\nMLP is sensitive to feature scaling.\\n\\nPlease see Tips on Practical Use section that addresses\\nsome of these disadvantages.\\n\\n\\n\\n1.17.2. Classification#\\nClass MLPClassifier implements a multi-layer perceptron (MLP) algorithm\\nthat trains using Backpropagation.\\nMLP trains on two arrays: array X of size (n_samples, n_features), which holds\\nthe training samples represented as floating point feature vectors; and array\\ny of size (n_samples,), which holds the target values (class labels) for the\\ntraining samples:\\n>>> from sklearn.neural_network import MLPClassifier\\n>>> X = [[0., 0.], [1., 1.]]\\n>>> y = [0, 1]\\n>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\\n...                     hidden_layer_sizes=(5, 2), random_state=1)\\n...\\n>>> clf.fit(X, y)\\nMLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\\n              solver='lbfgs')\\n---------new doc---------\\nMLPClassifier supports multi-class classification by\\napplying Softmax\\nas the output function.\\nFurther, the model supports multi-label classification\\nin which a sample can belong to more than one class. For each class, the raw\\noutput passes through the logistic function. Values larger or equal to 0.5\\nare rounded to 1, otherwise to 0. For a predicted output of a sample, the\\nindices where the value is 1 represents the assigned classes of that sample:\\n>>> X = [[0., 0.], [1., 1.]]\\n>>> y = [[0, 1], [1, 1]]\\n>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\\n...                     hidden_layer_sizes=(15,), random_state=1)\\n...\\n>>> clf.fit(X, y)\\nMLPClassifier(alpha=1e-05, hidden_layer_sizes=(15,), random_state=1,\\n              solver='lbfgs')\\n>>> clf.predict([[1., 2.]])\\narray([[1, 1]])\\n>>> clf.predict([[0., 0.]])\\narray([[0, 1]])\\n\\n\\nSee the examples below and the docstring of\\nMLPClassifier.fit for further information.\\nExamples\\n\\nCompare Stochastic learning strategies for MLPClassifier\\nSee Visualization of MLP weights on MNIST for\\nvisualized representation of trained weights.\\n---------new doc---------\\nThe first row of output array indicates that there are three samples whose\\ntrue cluster is “a”. Of them, two are in predicted cluster 0, one is in 1,\\nand none is in 2. And the second row indicates that there are three samples\\nwhose true cluster is “b”. Of them, none is in predicted cluster 0, one is in\\n1 and two are in 2.\\nA confusion matrix for classification is a square\\ncontingency matrix where the order of rows and columns correspond to a list\\nof classes.\\n\\nAdvantages:\\n\\nAllows to examine the spread of each true cluster across predicted clusters\\nand vice versa.\\nThe contingency table calculated is typically utilized in the calculation of\\na similarity statistic (like the others listed in this document) between the\\ntwo clusterings.\\n\\n\\n\\nDrawbacks:\\n\\nContingency matrix is easy to interpret for a small number of clusters, but\\nbecomes very hard to interpret for a large number of clusters.\\nIt doesn’t give a single metric to use as an objective for clustering\\noptimisation.\\n\\n\\n\\n\\nReferences#\\n\\nWikipedia entry for contingency matrix\\n\\n\\n\\n\\n2.3.11.9. Pair Confusion Matrix#\\nThe pair confusion matrix\\n(sklearn.metrics.cluster.pair_confusion_matrix) is a 2x2\\nsimilarity matrix\\n---------new doc---------\\n\\\\[\\\\begin{split}C = \\\\left[\\\\begin{matrix}\\nC_{00} & C_{01} \\\\\\\\\\nC_{10} & C_{11}\\n\\\\end{matrix}\\\\right]\\\\end{split}\\\\]\\nbetween two clusterings computed by considering all pairs of samples and\\ncounting pairs that are assigned into the same or into different clusters\\nunder the true and predicted clusterings.\\nIt has the following entries:\\n\\\\(C_{00}\\\\) : number of pairs with both clusterings having the samples\\nnot clustered together\\n\\\\(C_{10}\\\\) : number of pairs with the true label clustering having the\\nsamples clustered together but the other clustering not having the samples\\nclustered together\\n\\\\(C_{01}\\\\) : number of pairs with the true label clustering not having\\nthe samples clustered together but the other clustering having the samples\\nclustered together\\n\\\\(C_{11}\\\\) : number of pairs with both clusterings having the samples\\nclustered together\\nConsidering a pair of samples that is clustered together a positive pair,\\nthen as in binary classification the count of true negatives is\\n\\\\(C_{00}\\\\), false negatives is \\\\(C_{10}\\\\), true positives is\\n\\\\(C_{11}\\\\) and false positives is \\\\(C_{01}\\\\).\\nPerfectly matching labelings have all non-zero entries on the\\ndiagonal regardless of actual label values:\\n>>> from sklearn.metrics.cluster import pair_confusion_matrix\\n>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])\\narray([[8, 0],\\n       [0, 4]])\\n---------new doc---------\\n2.7. Novelty and Outlier Detection#\\nMany applications require being able to decide whether a new observation\\nbelongs to the same distribution as existing observations (it is an\\ninlier), or should be considered as different (it is an outlier).\\nOften, this ability is used to clean real data sets. Two important\\ndistinctions must be made:\\n\\noutlier detection:\\nThe training data contains outliers which are defined as observations that\\nare far from the others. Outlier detection estimators thus try to fit the\\nregions where the training data is the most concentrated, ignoring the\\ndeviant observations.\\n\\nnovelty detection:\\nThe training data is not polluted by outliers and we are interested in\\ndetecting whether a new observation is an outlier. In this context an\\noutlier is also called a novelty.\\n\\n\\nOutlier detection and novelty detection are both used for anomaly\\ndetection, where one is interested in detecting abnormal or unusual\\nobservations. Outlier detection is then also known as unsupervised anomaly\\ndetection and novelty detection as semi-supervised anomaly detection. In the\\ncontext of outlier detection, the outliers/anomalies cannot form a\\ndense cluster as available estimators assume that the outliers/anomalies are\\nlocated in low density regions. On the contrary, in the context of novelty\\ndetection, novelties/anomalies can form a dense cluster as long as they are in\\na low density region of the training data, considered as normal in this\\ncontext.\\nThe scikit-learn project provides a set of machine learning tools that\\ncan be used both for novelty or outlier detection. This strategy is\\nimplemented with objects learning in an unsupervised way from the data:\\nestimator.fit(X_train)\\n---------new doc---------\\nnew observations can then be sorted as inliers or outliers with a\\npredict method:\\nestimator.predict(X_test)\\n\\n\\nInliers are labeled 1, while outliers are labeled -1. The predict method\\nmakes use of a threshold on the raw scoring function computed by the\\nestimator. This scoring function is accessible through the score_samples\\nmethod, while the threshold can be controlled by the contamination\\nparameter.\\nThe decision_function method is also defined from the scoring function,\\nin such a way that negative values are outliers and non-negative ones are\\ninliers:\\nestimator.decision_function(X_test)\\n\\n\\nNote that neighbors.LocalOutlierFactor does not support\\npredict, decision_function and score_samples methods by default\\nbut only a fit_predict method, as this estimator was originally meant to\\nbe applied for outlier detection. The scores of abnormality of the training\\nsamples are accessible through the negative_outlier_factor_ attribute.\\nIf you really want to use neighbors.LocalOutlierFactor for novelty\\ndetection, i.e. predict labels or compute the score of abnormality of new\\nunseen data, you can instantiate the estimator with the novelty parameter\\nset to True before fitting the estimator. In this case, fit_predict is\\nnot available.\\n---------new doc---------\\nWarning\\nNovelty detection with Local Outlier Factor\\nWhen novelty is set to True be aware that you must only use\\npredict, decision_function and score_samples on new unseen data\\nand not on the training samples as this would lead to wrong results.\\nI.e., the result of predict will not be the same as fit_predict.\\nThe scores of abnormality of the training samples are always accessible\\nthrough the negative_outlier_factor_ attribute.\\n\\nThe behavior of neighbors.LocalOutlierFactor is summarized in the\\nfollowing table.\\n\\n\\nMethod\\nOutlier detection\\nNovelty detection\\n\\n\\n\\nfit_predict\\nOK\\nNot available\\n\\npredict\\nNot available\\nUse only on new data\\n\\ndecision_function\\nNot available\\nUse only on new data\\n\\nscore_samples\\nUse negative_outlier_factor_\\nUse only on new data\\n\\nnegative_outlier_factor_\\nOK\\nOK\\n\\n\\n\\n\\n\\n2.7.1. Overview of outlier detection methods#\\nA comparison of the outlier detection algorithms in scikit-learn. Local\\nOutlier Factor (LOF) does not show a decision boundary in black as it\\nhas no predict method to be applied on new data when it is used for outlier\\ndetection.\\n---------new doc---------\\nnegative_outlier_factor_\\nOK\\nOK\\n\\n\\n\\n\\n\\n2.7.1. Overview of outlier detection methods#\\nA comparison of the outlier detection algorithms in scikit-learn. Local\\nOutlier Factor (LOF) does not show a decision boundary in black as it\\nhas no predict method to be applied on new data when it is used for outlier\\ndetection.\\n\\n\\n\\n\\nensemble.IsolationForest and neighbors.LocalOutlierFactor\\nperform reasonably well on the data sets considered here.\\nThe svm.OneClassSVM is known to be sensitive to outliers and thus\\ndoes not perform very well for outlier detection. That being said, outlier\\ndetection in high-dimension, or without any assumptions on the distribution\\nof the inlying data is very challenging. svm.OneClassSVM may still\\nbe used with outlier detection but requires fine-tuning of its hyperparameter\\nnu to handle outliers and prevent overfitting.\\nlinear_model.SGDOneClassSVM provides an implementation of a\\nlinear One-Class SVM with a linear complexity in the number of samples. This\\nimplementation is here used with a kernel approximation technique to obtain\\nresults similar to svm.OneClassSVM which uses a Gaussian kernel\\nby default. Finally, covariance.EllipticEnvelope assumes the data is\\nGaussian and learns an ellipse. For more details on the different estimators\\nrefer to the example\\nComparing anomaly detection algorithms for outlier detection on toy datasets and the\\nsections hereunder.\\nExamples\\n---------new doc---------\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison of the svm.OneClassSVM, the\\nensemble.IsolationForest, the\\nneighbors.LocalOutlierFactor and\\ncovariance.EllipticEnvelope.\\nSee Evaluation of outlier detection estimators\\nfor an example showing how to evaluate outlier detection estimators,\\nthe neighbors.LocalOutlierFactor and the\\nensemble.IsolationForest, using ROC curves from\\nmetrics.RocCurveDisplay.\\n---------new doc---------\\n2.7.2. Novelty Detection#\\nConsider a data set of \\\\(n\\\\) observations from the same\\ndistribution described by \\\\(p\\\\) features.  Consider now that we\\nadd one more observation to that data set. Is the new observation so\\ndifferent from the others that we can doubt it is regular? (i.e. does\\nit come from the same distribution?) Or on the contrary, is it so\\nsimilar to the other that we cannot distinguish it from the original\\nobservations? This is the question addressed by the novelty detection\\ntools and methods.\\nIn general, it is about to learn a rough, close frontier delimiting\\nthe contour of the initial observations distribution, plotted in\\nembedding \\\\(p\\\\)-dimensional space. Then, if further observations\\nlay within the frontier-delimited subspace, they are considered as\\ncoming from the same population than the initial\\nobservations. Otherwise, if they lay outside the frontier, we can say\\nthat they are abnormal with a given confidence in our assessment.\\nThe One-Class SVM has been introduced by Schölkopf et al. for that purpose\\nand implemented in the Support Vector Machines module in the\\nsvm.OneClassSVM object. It requires the choice of a\\nkernel and a scalar parameter to define a frontier.  The RBF kernel is\\nusually chosen although there exists no exact formula or algorithm to\\nset its bandwidth parameter. This is the default in the scikit-learn\\nimplementation. The nu parameter, also known as the margin of\\nthe One-Class SVM, corresponds to the probability of finding a new,\\nbut regular, observation outside the frontier.\\nReferences\\n\\nEstimating the support of a high-dimensional distribution\\nSchölkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471.\\n---------new doc---------\\n2.7.3. Outlier Detection#\\nOutlier detection is similar to novelty detection in the sense that\\nthe goal is to separate a core of regular observations from some\\npolluting ones, called outliers. Yet, in the case of outlier\\ndetection, we don’t have a clean data set representing the population\\nof regular observations that can be used to train any tool.\\n\\n2.7.3.1. Fitting an elliptic envelope#\\nOne common way of performing outlier detection is to assume that the\\nregular data come from a known distribution (e.g. data are Gaussian\\ndistributed). From this assumption, we generally try to define the\\n“shape” of the data, and can define outlying observations as\\nobservations which stand far enough from the fit shape.\\nThe scikit-learn provides an object\\ncovariance.EllipticEnvelope that fits a robust covariance\\nestimate to the data, and thus fits an ellipse to the central data\\npoints, ignoring points outside the central mode.\\nFor instance, assuming that the inlier data are Gaussian distributed, it\\nwill estimate the inlier location and covariance in a robust way (i.e.\\nwithout being influenced by outliers). The Mahalanobis distances\\nobtained from this estimate is used to derive a measure of outlyingness.\\nThis strategy is illustrated below.\\n\\n\\n\\n\\nExamples\\n---------new doc---------\\nThe ensemble.IsolationForest supports warm_start=True which\\nallows you to add more trees to an already fitted model:\\n>>> from sklearn.ensemble import IsolationForest\\n>>> import numpy as np\\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\\n>>> clf = IsolationForest(n_estimators=10, warm_start=True)\\n>>> clf.fit(X)  # fit 10 trees  \\n>>> clf.set_params(n_estimators=20)  # add 10 more trees  \\n>>> clf.fit(X)  # fit the added trees  \\n\\n\\nExamples\\n\\nSee IsolationForest example for\\nan illustration of the use of IsolationForest.\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison of ensemble.IsolationForest with\\nneighbors.LocalOutlierFactor,\\nsvm.OneClassSVM (tuned to perform like an outlier detection\\nmethod), linear_model.SGDOneClassSVM, and a covariance-based\\noutlier detection with covariance.EllipticEnvelope.\\n\\nReferences\\n\\nLiu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”\\nData Mining, 2008. ICDM’08. Eighth IEEE International Conference on.\\n---------new doc---------\\nWhen the proportion of outliers is high (i.e. greater than 10 %, as in the\\nexample below), n_neighbors should be greater (n_neighbors=35 in the example\\nbelow).\\nThe strength of the LOF algorithm is that it takes both local and global\\nproperties of datasets into consideration: it can perform well even in datasets\\nwhere abnormal samples have different underlying densities.\\nThe question is not, how isolated the sample is, but how isolated it is\\nwith respect to the surrounding neighborhood.\\nWhen applying LOF for outlier detection, there are no predict,\\ndecision_function and score_samples methods but only a fit_predict\\nmethod. The scores of abnormality of the training samples are accessible\\nthrough the negative_outlier_factor_ attribute.\\nNote that predict, decision_function and score_samples can be used\\non new unseen data when LOF is applied for novelty detection, i.e. when the\\nnovelty parameter is set to True, but the result of predict may\\ndiffer from that of fit_predict. See Novelty detection with Local Outlier Factor.\\nThis strategy is illustrated below.\\n---------new doc---------\\nExamples\\n\\nSee Outlier detection with Local Outlier Factor (LOF)\\nfor an illustration of the use of neighbors.LocalOutlierFactor.\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison with other anomaly detection methods.\\n\\nReferences\\n\\nBreunig, Kriegel, Ng, and Sander (2000)\\nLOF: identifying density-based local outliers.\\nProc. ACM SIGMOD\\n\\n\\n\\n\\n2.7.4. Novelty detection with Local Outlier Factor#\\nTo use neighbors.LocalOutlierFactor for novelty detection, i.e.\\npredict labels or compute the score of abnormality of new unseen data, you\\nneed to instantiate the estimator with the novelty parameter\\nset to True before fitting the estimator:\\nlof = LocalOutlierFactor(novelty=True)\\nlof.fit(X_train)\\n\\n\\nNote that fit_predict is not available in this case to avoid inconsistencies.\\n\\nWarning\\nNovelty detection with Local Outlier Factor`\\nWhen novelty is set to True be aware that you must only use\\npredict, decision_function and score_samples on new unseen data\\nand not on the training samples as this would lead to wrong results.\\nI.e., the result of predict will not be the same as fit_predict.\\nThe scores of abnormality of the training samples are always accessible\\nthrough the negative_outlier_factor_ attribute.\\n\\nNovelty detection with Local Outlier Factor is illustrated below.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n2.6. Covariance estimation\\n\\n\\n\\n\\nnext\\n2.8. Density Estimation\\n\\n\\n --- \\n\\n\\n2. Unsupervised learning\\n2.8. Density Estimation\\n---------new doc---------\\nIn scikit-learn a random split into training and test sets\\ncan be quickly computed with the train_test_split helper function.\\nLet’s load the iris data set to fit a linear support vector machine on it:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn import datasets\\n>>> from sklearn import svm\\n\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> X.shape, y.shape\\n((150, 4), (150,))\\n\\n\\nWe can now quickly sample a training set while holding out 40% of the\\ndata for testing (evaluating) our classifier:\\n>>> X_train, X_test, y_train, y_test = train_test_split(\\n...     X, y, test_size=0.4, random_state=0)\\n\\n>>> X_train.shape, y_train.shape\\n((90, 4), (90,))\\n>>> X_test.shape, y_test.shape\\n((60, 4), (60,))\\n\\n>>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.96...\\n---------new doc---------\\n>>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.96...\\n\\n\\nWhen evaluating different settings (“hyperparameters”) for estimators,\\nsuch as the C setting that must be manually set for an SVM,\\nthere is still a risk of overfitting on the test set\\nbecause the parameters can be tweaked until the estimator performs optimally.\\nThis way, knowledge about the test set can “leak” into the model\\nand evaluation metrics no longer report on generalization performance.\\nTo solve this problem, yet another part of the dataset can be held out\\nas a so-called “validation set”: training proceeds on the training set,\\nafter which evaluation is done on the validation set,\\nand when the experiment seems to be successful,\\nfinal evaluation can be done on the test set.\\nHowever, by partitioning the available data into three sets,\\nwe drastically reduce the number of samples\\nwhich can be used for learning the model,\\nand the results can depend on a particular random choice for the pair of\\n(train, validation) sets.\\nA solution to this problem is a procedure called\\ncross-validation\\n(CV for short).\\nA test set should still be held out for final evaluation,\\nbut the validation set is no longer needed when doing CV.\\nIn the basic approach, called k-fold CV,\\nthe training set is split into k smaller sets\\n(other approaches are described below,\\nbut generally follow the same principles).\\nThe following procedure is followed for each of the k “folds”:\\n---------new doc---------\\nA model is trained using \\\\(k-1\\\\) of the folds as training data;\\nthe resulting model is validated on the remaining part of the data\\n(i.e., it is used as a test set to compute a performance measure\\nsuch as accuracy).\\n\\nThe performance measure reported by k-fold cross-validation\\nis then the average of the values computed in the loop.\\nThis approach can be computationally expensive,\\nbut does not waste too much data\\n(as is the case when fixing an arbitrary validation set),\\nwhich is a major advantage in problems such as inverse inference\\nwhere the number of samples is very small.\\n\\n\\n\\n3.1.1. Computing cross-validated metrics#\\nThe simplest way to use cross-validation is to call the\\ncross_val_score helper function on the estimator and the dataset.\\nThe following example demonstrates how to estimate the accuracy of a linear\\nkernel support vector machine on the iris dataset by splitting the data, fitting\\na model and computing the score 5 consecutive times (with different splits each\\ntime):\\n>>> from sklearn.model_selection import cross_val_score\\n>>> clf = svm.SVC(kernel='linear', C=1, random_state=42)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores\\narray([0.96..., 1. , 0.96..., 0.96..., 1. ])\\n\\n\\nThe mean score and the standard deviation are hence given by:\\n>>> print(\\\"%0.2f accuracy with a standard deviation of %0.2f\\\" % (scores.mean(), scores.std()))\\n0.98 accuracy with a standard deviation of 0.02\\n---------new doc---------\\nSee The scoring parameter: defining model evaluation rules for details.\\nIn the case of the Iris dataset, the samples are balanced across target\\nclasses hence the accuracy and the F1-score are almost equal.\\nWhen the cv argument is an integer, cross_val_score uses the\\nKFold or StratifiedKFold strategies by default, the latter\\nbeing used if the estimator derives from ClassifierMixin.\\nIt is also possible to use other cross validation strategies by passing a cross\\nvalidation iterator instead, for instance:\\n>>> from sklearn.model_selection import ShuffleSplit\\n>>> n_samples = X.shape[0]\\n>>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\\n>>> cross_val_score(clf, X, y, cv=cv)\\narray([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])\\n---------new doc---------\\nData transformation with held-out data#\\nJust as it is important to test a predictor on data held-out from\\ntraining, preprocessing (such as standardization, feature selection, etc.)\\nand similar data transformations similarly should\\nbe learnt from a training set and applied to held-out data for prediction:\\n>>> from sklearn import preprocessing\\n>>> X_train, X_test, y_train, y_test = train_test_split(\\n...     X, y, test_size=0.4, random_state=0)\\n>>> scaler = preprocessing.StandardScaler().fit(X_train)\\n>>> X_train_transformed = scaler.transform(X_train)\\n>>> clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\\n>>> X_test_transformed = scaler.transform(X_test)\\n>>> clf.score(X_test_transformed, y_test)\\n0.9333...\\n\\n\\nA Pipeline makes it easier to compose\\nestimators, providing this behavior under cross-validation:\\n>>> from sklearn.pipeline import make_pipeline\\n>>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\\n>>> cross_val_score(clf, X, y, cv=cv)\\narray([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])\\n\\n\\nSee Pipelines and composite estimators.\\n\\n\\n3.1.1.1. The cross_validate function and multiple metric evaluation#\\nThe cross_validate function differs from cross_val_score in\\ntwo ways:\\n---------new doc---------\\n3.1.1.2. Obtaining predictions by cross-validation#\\nThe function cross_val_predict has a similar interface to\\ncross_val_score, but returns, for each element in the input, the\\nprediction that was obtained for that element when it was in the test set. Only\\ncross-validation strategies that assign all elements to a test set exactly once\\ncan be used (otherwise, an exception is raised).\\n\\nWarning\\nNote on inappropriate usage of cross_val_predict\\nThe result of cross_val_predict may be different from those\\nobtained using cross_val_score as the elements are grouped in\\ndifferent ways. The function cross_val_score takes an average\\nover cross-validation folds, whereas cross_val_predict simply\\nreturns the labels (or probabilities) from several distinct models\\nundistinguished. Thus, cross_val_predict is not an appropriate\\nmeasure of generalization error.\\n\\n\\nThe function cross_val_predict is appropriate for:\\nVisualization of predictions obtained from different models.\\nModel blending: When predictions of one supervised estimator are used to\\ntrain another estimator in ensemble methods.\\n\\n\\n\\nThe available cross validation iterators are introduced in the following\\nsection.\\nExamples\\n\\nReceiver Operating Characteristic (ROC) with cross validation,\\nRecursive feature elimination with cross-validation,\\nCustom refit strategy of a grid search with cross-validation,\\nSample pipeline for text feature extraction and evaluation,\\nPlotting Cross-Validated Predictions,\\nNested versus non-nested cross-validation.\\n\\n\\n\\n\\n3.1.2. Cross validation iterators#\\nThe following sections list utilities to generate indices\\nthat can be used to generate dataset splits according to different cross\\nvalidation strategies.\\n---------new doc---------\\n3.1.2. Cross validation iterators#\\nThe following sections list utilities to generate indices\\nthat can be used to generate dataset splits according to different cross\\nvalidation strategies.\\n\\n3.1.2.1. Cross-validation iterators for i.i.d. data#\\nAssuming that some data is Independent and Identically Distributed (i.i.d.) is\\nmaking the assumption that all samples stem from the same generative process\\nand that the generative process is assumed to have no memory of past generated\\nsamples.\\nThe following cross-validators can be used in such cases.\\n\\nNote\\nWhile i.i.d. data is a common assumption in machine learning theory, it rarely\\nholds in practice. If one knows that the samples have been generated using a\\ntime-dependent process, it is safer to\\nuse a time-series aware cross-validation scheme.\\nSimilarly, if we know that the generative process has a group structure\\n(samples collected from different subjects, experiments, measurement\\ndevices), it is safer to use group-wise cross-validation.\\n\\n\\n3.1.2.1.1. K-fold#\\nKFold divides all the samples in \\\\(k\\\\) groups of samples,\\ncalled folds (if \\\\(k = n\\\\), this is equivalent to the Leave One\\nOut strategy), of equal sizes (if possible). The prediction function is\\nlearned using \\\\(k - 1\\\\) folds, and the fold left out is used for test.\\nExample of 2-fold cross-validation on a dataset with 4 samples:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import KFold\\n---------new doc---------\\n>>> X = [\\\"a\\\", \\\"b\\\", \\\"c\\\", \\\"d\\\"]\\n>>> kf = KFold(n_splits=2)\\n>>> for train, test in kf.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[2 3] [0 1]\\n[0 1] [2 3]\\n\\n\\nHere is a visualization of the cross-validation behavior. Note that\\nKFold is not affected by classes or groups.\\n\\n\\n\\n\\nEach fold is constituted by two arrays: the first one is related to the\\ntraining set, and the second one to the test set.\\nThus, one can create the training/test sets using numpy indexing:\\n>>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\\n>>> y = np.array([0, 1, 0, 1])\\n>>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\\n---------new doc---------\\n3.1.2.1.2. Repeated K-Fold#\\nRepeatedKFold repeats K-Fold n times. It can be used when one\\nrequires to run KFold n times, producing different splits in\\neach repetition.\\nExample of 2-fold K-Fold repeated 2 times:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import RepeatedKFold\\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\\n>>> random_state = 12883823\\n>>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\\n>>> for train, test in rkf.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n...\\n[2 3] [0 1]\\n[0 1] [2 3]\\n[0 2] [1 3]\\n[1 3] [0 2]\\n\\n\\nSimilarly, RepeatedStratifiedKFold repeats Stratified K-Fold n times\\nwith different randomization in each repetition.\\n\\n\\n3.1.2.1.3. Leave One Out (LOO)#\\nLeaveOneOut (or LOO) is a simple cross-validation. Each learning\\nset is created by taking all the samples except one, the test set being\\nthe sample left out. Thus, for \\\\(n\\\\) samples, we have \\\\(n\\\\) different\\ntraining sets and \\\\(n\\\\) different tests set. This cross-validation\\nprocedure does not waste much data as only one sample is removed from the\\ntraining set:\\n>>> from sklearn.model_selection import LeaveOneOut\\n---------new doc---------\\n>>> X = [1, 2, 3, 4]\\n>>> loo = LeaveOneOut()\\n>>> for train, test in loo.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[1 2 3] [0]\\n[0 2 3] [1]\\n[0 1 3] [2]\\n[0 1 2] [3]\\n\\n\\nPotential users of LOO for model selection should weigh a few known caveats.\\nWhen compared with \\\\(k\\\\)-fold cross validation, one builds \\\\(n\\\\) models\\nfrom \\\\(n\\\\) samples instead of \\\\(k\\\\) models, where \\\\(n > k\\\\).\\nMoreover, each is trained on \\\\(n - 1\\\\) samples rather than\\n\\\\((k-1) n / k\\\\). In both ways, assuming \\\\(k\\\\) is not too large\\nand \\\\(k < n\\\\), LOO is more computationally expensive than \\\\(k\\\\)-fold\\ncross validation.\\nIn terms of accuracy, LOO often results in high variance as an estimator for the\\ntest error. Intuitively, since \\\\(n - 1\\\\) of\\nthe \\\\(n\\\\) samples are used to build each model, models constructed from\\nfolds are virtually identical to each other and to the model built from the\\nentire training set.\\nHowever, if the learning curve is steep for the training size in question,\\nthen 5- or 10- fold cross validation can overestimate the generalization error.\\nAs a general rule, most authors, and empirical evidence, suggest that 5- or 10-\\nfold cross validation should be preferred to LOO.\\n\\n\\nReferences#\\n---------new doc---------\\nReferences#\\n\\nhttp://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html;\\nT. Hastie, R. Tibshirani, J. Friedman,  The Elements of Statistical Learning, Springer 2009\\nL. Breiman, P. Spector Submodel selection and evaluation in regression: The X-random case, International Statistical Review 1992;\\nR. Kohavi, A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, Intl. Jnt. Conf. AI\\nR. Bharat Rao, G. Fung, R. Rosales, On the Dangers of Cross-Validation. An Experimental Evaluation, SIAM 2008;\\nG. James, D. Witten, T. Hastie, R Tibshirani, An Introduction to\\nStatistical Learning, Springer 2013.\\n\\n\\n\\n\\n3.1.2.1.4. Leave P Out (LPO)#\\nLeavePOut is very similar to LeaveOneOut as it creates all\\nthe possible training/test sets by removing \\\\(p\\\\) samples from the complete\\nset. For \\\\(n\\\\) samples, this produces \\\\({n \\\\choose p}\\\\) train-test\\npairs. Unlike LeaveOneOut and KFold, the test sets will\\noverlap for \\\\(p > 1\\\\).\\nExample of Leave-2-Out on a dataset with 4 samples:\\n>>> from sklearn.model_selection import LeavePOut\\n---------new doc---------\\n>>> X = np.ones(4)\\n>>> lpo = LeavePOut(p=2)\\n>>> for train, test in lpo.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[2 3] [0 1]\\n[1 3] [0 2]\\n[1 2] [0 3]\\n[0 3] [1 2]\\n[0 2] [1 3]\\n[0 1] [2 3]\\n\\n\\n\\n\\n3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split#\\nThe ShuffleSplit iterator will generate a user defined number of\\nindependent train / test dataset splits. Samples are first shuffled and\\nthen split into a pair of train and test sets.\\nIt is possible to control the randomness for reproducibility of the\\nresults by explicitly seeding the random_state pseudo random number\\ngenerator.\\nHere is a usage example:\\n>>> from sklearn.model_selection import ShuffleSplit\\n>>> X = np.arange(10)\\n>>> ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\\n>>> for train_index, test_index in ss.split(X):\\n...     print(\\\"%s %s\\\" % (train_index, test_index))\\n[9 1 6 7 3 0 5] [2 8 4]\\n[2 9 8 0 6 7 4] [3 5 1]\\n[4 5 1 0 6 9 7] [2 3 8]\\n[2 7 5 8 0 3 4] [6 1 9]\\n[4 1 0 6 8 9 3] [5 2 7]\\n---------new doc---------\\nHere is a visualization of the cross-validation behavior. Note that\\nShuffleSplit is not affected by classes or groups.\\n\\n\\n\\n\\nShuffleSplit is thus a good alternative to KFold cross\\nvalidation that allows a finer control on the number of iterations and\\nthe proportion of samples on each side of the train / test split.\\n\\n\\n\\n3.1.2.2. Cross-validation iterators with stratification based on class labels#\\nSome classification problems can exhibit a large imbalance in the distribution\\nof the target classes: for instance there could be several times more negative\\nsamples than positive samples. In such cases it is recommended to use\\nstratified sampling as implemented in StratifiedKFold and\\nStratifiedShuffleSplit to ensure that relative class frequencies is\\napproximately preserved in each train and validation fold.\\n---------new doc---------\\n3.1.2.2.1. Stratified k-fold#\\nStratifiedKFold is a variation of k-fold which returns stratified\\nfolds: each set contains approximately the same percentage of samples of each\\ntarget class as the complete set.\\nHere is an example of stratified 3-fold cross-validation on a dataset with 50 samples from\\ntwo unbalanced classes.  We show the number of samples in each class and compare with\\nKFold.\\n>>> from sklearn.model_selection import StratifiedKFold, KFold\\n>>> import numpy as np\\n>>> X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\\n>>> skf = StratifiedKFold(n_splits=3)\\n>>> for train, test in skf.split(X, y):\\n...     print('train -  {}   |   test -  {}'.format(\\n...         np.bincount(y[train]), np.bincount(y[test])))\\ntrain -  [30  3]   |   test -  [15  2]\\ntrain -  [30  3]   |   test -  [15  2]\\ntrain -  [30  4]   |   test -  [15  1]\\n>>> kf = KFold(n_splits=3)\\n>>> for train, test in kf.split(X, y):\\n...     print('train -  {}   |   test -  {}'.format(\\n---------new doc---------\\nWe can see that StratifiedKFold preserves the class ratios\\n(approximately 1 / 10) in both train and test dataset.\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nRepeatedStratifiedKFold can be used to repeat Stratified K-Fold n times\\nwith different randomization in each repetition.\\n\\n\\n3.1.2.2.2. Stratified Shuffle Split#\\nStratifiedShuffleSplit is a variation of ShuffleSplit, which returns\\nstratified splits, i.e which creates splits by preserving the same\\npercentage for each target class as in the complete set.\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\n\\n\\n\\n3.1.2.3. Predefined fold-splits / Validation-sets#\\nFor some datasets, a pre-defined split of the data into training- and\\nvalidation fold or into several cross-validation folds already\\nexists. Using PredefinedSplit it is possible to use these folds\\ne.g. when searching for hyperparameters.\\nFor example, when using a validation set, set the test_fold to 0 for all\\nsamples that are part of the validation set, and to -1 for all other samples.\\n---------new doc---------\\n3.1.2.4. Cross-validation iterators for grouped data#\\nThe i.i.d. assumption is broken if the underlying generative process yields\\ngroups of dependent samples.\\nSuch a grouping of data is domain specific. An example would be when there is\\nmedical data collected from multiple patients, with multiple samples taken from\\neach patient. And such data is likely to be dependent on the individual group.\\nIn our example, the patient id for each sample will be its group identifier.\\nIn this case we would like to know if a model trained on a particular set of\\ngroups generalizes well to the unseen groups. To measure this, we need to\\nensure that all the samples in the validation fold come from groups that are\\nnot represented at all in the paired training fold.\\nThe following cross-validation splitters can be used to do that.\\nThe grouping identifier for the samples is specified via the groups\\nparameter.\\n\\n3.1.2.4.1. Group k-fold#\\nGroupKFold is a variation of k-fold which ensures that the same group is\\nnot represented in both testing and training sets. For example if the data is\\nobtained from different subjects with several samples per-subject and if the\\nmodel is flexible enough to learn from highly person specific features it\\ncould fail to generalize to new subjects. GroupKFold makes it possible\\nto detect this kind of overfitting situations.\\nImagine you have three subjects, each with an associated number from 1 to 3:\\n>>> from sklearn.model_selection import GroupKFold\\n---------new doc---------\\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\\n>>> y = [\\\"a\\\", \\\"b\\\", \\\"b\\\", \\\"b\\\", \\\"c\\\", \\\"c\\\", \\\"c\\\", \\\"d\\\", \\\"d\\\", \\\"d\\\"]\\n>>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\\n\\n>>> gkf = GroupKFold(n_splits=3)\\n>>> for train, test in gkf.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[0 1 2 3 4 5] [6 7 8 9]\\n[0 1 2 6 7 8 9] [3 4 5]\\n[3 4 5 6 7 8 9] [0 1 2]\\n\\n\\nEach subject is in a different testing fold, and the same subject is never in\\nboth testing and training. Notice that the folds do not have exactly the same\\nsize due to the imbalance in the data. If class proportions must be balanced\\nacross folds, StratifiedGroupKFold is a better option.\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nSimilar to KFold, the test sets from GroupKFold will form a\\ncomplete partition of all the data.\\nWhile GroupKFold attempts to place the same number of samples in each\\nfold when shuffle=False, when shuffle=True it attempts to place equal\\nnumber of distinct groups in each fold (but doesn not account for group sizes).\\n---------new doc---------\\n3.1.2.4.2. StratifiedGroupKFold#\\nStratifiedGroupKFold is a cross-validation scheme that combines both\\nStratifiedKFold and GroupKFold. The idea is to try to\\npreserve the distribution of classes in each split while keeping each group\\nwithin a single split. That might be useful when you have an unbalanced\\ndataset so that using just GroupKFold might produce skewed splits.\\nExample:\\n>>> from sklearn.model_selection import StratifiedGroupKFold\\n>>> X = list(range(18))\\n>>> y = [1] * 6 + [0] * 12\\n>>> groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]\\n>>> sgkf = StratifiedGroupKFold(n_splits=3)\\n>>> for train, test in sgkf.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]\\n[ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]\\n[ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]\\n\\n\\n\\n\\nImplementation notes#\\n\\nWith the current implementation full shuffle is not possible in most\\nscenarios. When shuffle=True, the following happens:\\n---------new doc---------\\nImplementation notes#\\n\\nWith the current implementation full shuffle is not possible in most\\nscenarios. When shuffle=True, the following happens:\\n\\nAll groups are shuffled.\\nGroups are sorted by standard deviation of classes using stable sort.\\nSorted groups are iterated over and assigned to folds.\\n\\nThat means that only groups with the same standard deviation of class\\ndistribution will be shuffled, which might be useful when each group has only\\na single class.\\n\\nThe algorithm greedily assigns each group to one of n_splits test sets,\\nchoosing the test set that minimises the variance in class distribution\\nacross test sets. Group assignment proceeds from groups with highest to\\nlowest variance in class frequency, i.e. large groups peaked on one or few\\nclasses are assigned first.\\nThis split is suboptimal in a sense that it might produce imbalanced splits\\neven if perfect stratification is possible. If you have relatively close\\ndistribution of classes in each group, using GroupKFold is better.\\n\\n\\nHere is a visualization of cross-validation behavior for uneven groups:\\n---------new doc---------\\nHere is a visualization of cross-validation behavior for uneven groups:\\n\\n\\n\\n\\n\\n\\n3.1.2.4.3. Leave One Group Out#\\nLeaveOneGroupOut is a cross-validation scheme where each split holds\\nout samples belonging to one specific group. Group information is\\nprovided via an array that encodes the group of each sample.\\nEach training set is thus constituted by all the samples except the ones\\nrelated to a specific group. This is the same as LeavePGroupsOut with\\nn_groups=1 and the same as GroupKFold with n_splits equal to the\\nnumber of unique labels passed to the groups parameter.\\nFor example, in the cases of multiple experiments, LeaveOneGroupOut\\ncan be used to create a cross-validation based on the different experiments:\\nwe create a training set using the samples of all the experiments except one:\\n>>> from sklearn.model_selection import LeaveOneGroupOut\\n\\n>>> X = [1, 5, 10, 50, 60, 70, 80]\\n>>> y = [0, 1, 1, 2, 2, 2, 2]\\n>>> groups = [1, 1, 2, 2, 3, 3, 3]\\n>>> logo = LeaveOneGroupOut()\\n>>> for train, test in logo.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[2 3 4 5 6] [0 1]\\n[0 1 4 5 6] [2 3]\\n[0 1 2 3] [4 5 6]\\n\\n\\nAnother common application is to use time information: for instance the\\ngroups could be the year of collection of the samples and thus allow\\nfor cross-validation against time-based splits.\\n---------new doc---------\\nAnother common application is to use time information: for instance the\\ngroups could be the year of collection of the samples and thus allow\\nfor cross-validation against time-based splits.\\n\\n\\n3.1.2.4.4. Leave P Groups Out#\\nLeavePGroupsOut is similar as LeaveOneGroupOut, but removes\\nsamples related to \\\\(P\\\\) groups for each training/test set. All possible\\ncombinations of \\\\(P\\\\) groups are left out, meaning test sets will overlap\\nfor \\\\(P>1\\\\).\\nExample of Leave-2-Group Out:\\n>>> from sklearn.model_selection import LeavePGroupsOut\\n\\n>>> X = np.arange(6)\\n>>> y = [1, 1, 1, 2, 2, 2]\\n>>> groups = [1, 1, 2, 2, 3, 3]\\n>>> lpgo = LeavePGroupsOut(n_groups=2)\\n>>> for train, test in lpgo.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[4 5] [0 1 2 3]\\n[2 3] [0 1 4 5]\\n[0 1] [2 3 4 5]\\n\\n\\n\\n\\n3.1.2.4.5. Group Shuffle Split#\\nThe GroupShuffleSplit iterator behaves as a combination of\\nShuffleSplit and LeavePGroupsOut, and generates a\\nsequence of randomized partitions in which a subset of groups are held\\nout for each split. Each train/test split is performed independently meaning\\nthere is no guaranteed relationship between successive test sets.\\nHere is a usage example:\\n>>> from sklearn.model_selection import GroupShuffleSplit\\n---------new doc---------\\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\\n>>> y = [\\\"a\\\", \\\"b\\\", \\\"b\\\", \\\"b\\\", \\\"c\\\", \\\"c\\\", \\\"c\\\", \\\"a\\\"]\\n>>> groups = [1, 1, 2, 2, 3, 3, 4, 4]\\n>>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\\n>>> for train, test in gss.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n...\\n[0 1 2 3] [4 5 6 7]\\n[2 3 6 7] [0 1 4 5]\\n[2 3 4 5] [0 1 6 7]\\n[4 5 6 7] [0 1 2 3]\\n\\n\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nThis class is useful when the behavior of LeavePGroupsOut is\\ndesired, but the number of groups is large enough that generating all\\npossible partitions with \\\\(P\\\\) groups withheld would be prohibitively\\nexpensive. In such a scenario, GroupShuffleSplit provides\\na random sample (with replacement) of the train / test splits\\ngenerated by LeavePGroupsOut.\\n---------new doc---------\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nThis class is useful when the behavior of LeavePGroupsOut is\\ndesired, but the number of groups is large enough that generating all\\npossible partitions with \\\\(P\\\\) groups withheld would be prohibitively\\nexpensive. In such a scenario, GroupShuffleSplit provides\\na random sample (with replacement) of the train / test splits\\ngenerated by LeavePGroupsOut.\\n\\n\\n\\n3.1.2.5. Using cross-validation iterators to split train and test#\\nThe above group cross-validation functions may also be useful for splitting a\\ndataset into training and testing subsets. Note that the convenience\\nfunction train_test_split is a wrapper around ShuffleSplit\\nand thus only allows for stratified splitting (using the class labels)\\nand cannot account for groups.\\nTo perform the train and test split, use the indices for the train and test\\nsubsets yielded by the generator output by the split() method of the\\ncross-validation splitter. For example:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import GroupShuffleSplit\\n---------new doc---------\\n3.1.2.6. Cross validation of time series data#\\nTime series data is characterized by the correlation between observations\\nthat are near in time (autocorrelation). However, classical\\ncross-validation techniques such as KFold and\\nShuffleSplit assume the samples are independent and\\nidentically distributed, and would result in unreasonable correlation\\nbetween training and testing instances (yielding poor estimates of\\ngeneralization error) on time series data. Therefore, it is very important\\nto evaluate our model for time series data on the “future” observations\\nleast like those that are used to train the model. To achieve this, one\\nsolution is provided by TimeSeriesSplit.\\n\\n3.1.2.6.1. Time Series Split#\\nTimeSeriesSplit is a variation of k-fold which\\nreturns first \\\\(k\\\\) folds as train set and the \\\\((k+1)\\\\) th\\nfold as test set. Note that unlike standard cross-validation methods,\\nsuccessive training sets are supersets of those that come before them.\\nAlso, it adds all surplus data to the first training partition, which\\nis always used to train the model.\\nThis class can be used to cross-validate time series data samples\\nthat are observed at fixed time intervals.\\nExample of 3-split time series cross-validation on a dataset with 6 samples:\\n>>> from sklearn.model_selection import TimeSeriesSplit\\n---------new doc---------\\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\\n>>> y = np.array([1, 2, 3, 4, 5, 6])\\n>>> tscv = TimeSeriesSplit(n_splits=3)\\n>>> print(tscv)\\nTimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)\\n>>> for train, test in tscv.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[0 1 2] [3]\\n[0 1 2 3] [4]\\n[0 1 2 3 4] [5]\\n\\n\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\n\\n\\n\\n\\n3.1.3. A note on shuffling#\\nIf the data ordering is not arbitrary (e.g. samples with the same class label\\nare contiguous), shuffling it first may be essential to get a meaningful cross-\\nvalidation result. However, the opposite may be true if the samples are not\\nindependently and identically distributed. For example, if samples correspond\\nto news articles, and are ordered by their time of publication, then shuffling\\nthe data will likely lead to a model that is overfit and an inflated validation\\nscore: it will be tested on samples that are artificially similar (close in\\ntime) to training samples.\\nSome cross validation iterators, such as KFold, have an inbuilt option\\nto shuffle the data indices before splitting them. Note that:\\n---------new doc---------\\nThis consumes less memory than shuffling the data directly.\\nBy default no shuffling occurs, including for the (stratified) K fold cross-\\nvalidation performed by specifying cv=some_integer to\\ncross_val_score, grid search, etc. Keep in mind that\\ntrain_test_split still returns a random split.\\nThe random_state parameter defaults to None, meaning that the\\nshuffling will be different every time KFold(..., shuffle=True) is\\niterated. However, GridSearchCV will use the same shuffling for each set\\nof parameters validated by a single call to its fit method.\\nTo get identical results for each split, set random_state to an integer.\\n\\nFor more details on how to control the randomness of cv splitters and avoid\\ncommon pitfalls, see Controlling randomness.\\n\\n\\n3.1.4. Cross validation and model selection#\\nCross validation iterators can also be used to directly perform model\\nselection using Grid Search for the optimal hyperparameters of the\\nmodel. This is the topic of the next section: Tuning the hyper-parameters of an estimator.\\n---------new doc---------\\nthe data. In the latter case, using a more appropriate classifier that\\nis able to utilize the structure in the data, would result in a lower\\np-value.\\nCross-validation provides information about how well a classifier generalizes,\\nspecifically the range of expected errors of the classifier. However, a\\nclassifier trained on a high dimensional dataset with no structure may still\\nperform better than expected on cross-validation, just by chance.\\nThis can typically happen with small datasets with less than a few hundred\\nsamples.\\npermutation_test_score provides information\\non whether the classifier has found a real class structure and can help in\\nevaluating the performance of the classifier.\\nIt is important to note that this test has been shown to produce low\\np-values even if there is only weak structure in the data because in the\\ncorresponding permutated datasets there is absolutely no structure. This\\ntest is therefore only able to show when the model reliably outperforms\\nrandom guessing.\\nFinally, permutation_test_score is computed\\nusing brute force and internally fits (n_permutations + 1) * n_cv models.\\nIt is therefore only tractable with small datasets for which fitting an\\nindividual model is very fast.\\nExamples\\n---------new doc---------\\nTest with permutations the significance of a classification score\\n\\n\\n\\nReferences#\\n\\nOjala and Garriga. Permutation Tests for Studying Classifier Performance.\\nJ. Mach. Learn. Res. 2010.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3. Model selection and evaluation\\n\\n\\n\\n\\nnext\\n3.2. Tuning the hyper-parameters of an estimator\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.2. Tuning the hyper-parameters of an estimator\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3.2. Tuning the hyper-parameters of an estimator#\\nHyper-parameters are parameters that are not directly learnt within estimators.\\nIn scikit-learn they are passed as arguments to the constructor of the\\nestimator classes. Typical examples include C, kernel and gamma\\nfor Support Vector Classifier, alpha for Lasso, etc.\\nIt is possible and recommended to search the hyper-parameter space for the\\nbest cross validation score.\\nAny parameter provided when constructing an estimator may be optimized in this\\nmanner. Specifically, to find the names and current values for all parameters\\nfor a given estimator, use:\\nestimator.get_params()\\n\\n\\nA search consists of:\\n\\nan estimator (regressor or classifier such as sklearn.svm.SVC());\\na parameter space;\\na method for searching or sampling candidates;\\na cross-validation scheme; and\\na score function.\\n---------new doc---------\\nAdvanced examples#\\n\\nSee Nested versus non-nested cross-validation\\nfor an example of Grid Search within a cross validation loop on the iris\\ndataset. This is the best practice for evaluating the performance of a\\nmodel with grid search.\\nSee Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV\\nfor an example of GridSearchCV being used to evaluate multiple\\nmetrics simultaneously.\\nSee Balance model complexity and cross-validated score\\nfor an example of using refit=callable interface in\\nGridSearchCV. The example shows how this interface adds certain\\namount of flexibility in identifying the “best” estimator. This interface\\ncan also be used in multiple metrics evaluation.\\nSee Statistical comparison of models using grid search\\nfor an example of how to do a statistical comparison on the outputs of\\nGridSearchCV.\\n\\n\\n\\n\\n3.2.2. Randomized Parameter Optimization#\\nWhile using a grid of parameter settings is currently the most widely used\\nmethod for parameter optimization, other search methods have more\\nfavorable properties.\\nRandomizedSearchCV implements a randomized search over parameters,\\nwhere each setting is sampled from a distribution over possible parameter values.\\nThis has two main benefits over an exhaustive search:\\n\\nA budget can be chosen independent of the number of parameters and possible values.\\nAdding parameters that do not influence the performance does not decrease efficiency.\\n---------new doc---------\\nA budget can be chosen independent of the number of parameters and possible values.\\nAdding parameters that do not influence the performance does not decrease efficiency.\\n\\nSpecifying how parameters should be sampled is done using a dictionary, very\\nsimilar to specifying parameters for GridSearchCV. Additionally,\\na computation budget, being the number of sampled candidates or sampling\\niterations, is specified using the n_iter parameter.\\nFor each parameter, either a distribution over possible values or a list of\\ndiscrete choices (which will be sampled uniformly) can be specified:\\n{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\\n  'kernel': ['rbf'], 'class_weight':['balanced', None]}\\n\\n\\nThis example uses the scipy.stats module, which contains many useful\\ndistributions for sampling parameters, such as expon, gamma,\\nuniform, loguniform or randint.\\nIn principle, any function can be passed that provides a rvs (random\\nvariate sample) method to sample a value. A call to the rvs function should\\nprovide independent random samples from possible parameter values on\\nconsecutive calls.\\n\\nWarning\\nThe distributions in scipy.stats prior to version scipy 0.16\\ndo not allow specifying a random state. Instead, they use the global\\nnumpy random state, that can be seeded via np.random.seed or set\\nusing np.random.set_state. However, beginning scikit-learn 0.18,\\nthe sklearn.model_selection module sets the random state provided\\nby the user if scipy >= 0.16 is also available.\\n---------new doc---------\\nChoosing a resource#\\nBy default, the resource is defined in terms of number of samples. That is,\\neach iteration will use an increasing amount of samples to train on. You can\\nhowever manually specify a parameter to use as the resource with the\\nresource parameter. Here is an example where the resource is defined in\\nterms of the number of estimators of a random forest:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\\n>>> from sklearn.model_selection import HalvingGridSearchCV\\n>>> import pandas as pd\\n>>> param_grid = {'max_depth': [3, 5, 10],\\n...               'min_samples_split': [2, 5, 10]}\\n>>> base_estimator = RandomForestClassifier(random_state=0)\\n>>> X, y = make_classification(n_samples=1000, random_state=0)\\n>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\\n...                          factor=2, resource='n_estimators',\\n...                          max_resources=30).fit(X, y)\\n>>> sh.best_estimator_\\nRandomForestClassifier(max_depth=5, n_estimators=24, random_state=0)\\n---------new doc---------\\n3.2.3.1. Aggressive elimination of candidates#\\nUsing the aggressive_elimination parameter, you can force the search\\nprocess to end up with less than factor candidates at the last\\niteration.\\n\\n\\nCode example of aggressive elimination#\\nIdeally, we want the last iteration to evaluate factor candidates. We\\nthen just have to pick the best one. When the number of available resources is\\nsmall with respect to the number of candidates, the last iteration may have to\\nevaluate more than factor candidates:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.svm import SVC\\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\\n>>> from sklearn.model_selection import HalvingGridSearchCV\\n>>> import pandas as pd\\n>>> param_grid = {'kernel': ('linear', 'rbf'),\\n...               'C': [1, 10, 100]}\\n>>> base_estimator = SVC(gamma='scale')\\n>>> X, y = make_classification(n_samples=1000)\\n>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\\n...                          factor=2, max_resources=40,\\n...                          aggressive_elimination=False).fit(X, y)\\n>>> sh.n_resources_\\n[20, 40]\\n>>> sh.n_candidates_\\n[6, 3]\\n---------new doc---------\\nNotice that we end with 2 candidates at the last iteration since we have\\neliminated enough candidates during the first iterations, using n_resources =\\nmin_resources = 20.\\n\\n\\n\\n3.2.3.2. Analyzing results with the cv_results_ attribute#\\nThe cv_results_ attribute contains useful information for analyzing the\\nresults of a search. It can be converted to a pandas dataframe with df =\\npd.DataFrame(est.cv_results_). The cv_results_ attribute of\\nHalvingGridSearchCV and HalvingRandomSearchCV is similar\\nto that of GridSearchCV and RandomizedSearchCV, with\\nadditional information related to the successive halving process.\\n\\n\\nExample of a (truncated) output dataframe:#\\n\\n\\n\\niter\\nn_resources\\nmean_test_score\\nparams\\n\\n\\n\\n0\\n0\\n125\\n0.983667\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 5}\\n\\n1\\n0\\n125\\n0.983667\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 8, ‘min_samples_split’: 7}\\n\\n2\\n0\\n125\\n0.983667\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\\n---------new doc---------\\nHere, <estimator> is the parameter name of the nested estimator,\\nin this case estimator.\\nIf the meta-estimator is constructed as a collection of estimators as in\\npipeline.Pipeline, then <estimator> refers to the name of the estimator,\\nsee Access to nested parameters. In practice, there can be several\\nlevels of nesting:\\n>>> from sklearn.pipeline import Pipeline\\n>>> from sklearn.feature_selection import SelectKBest\\n>>> pipe = Pipeline([\\n...    ('select', SelectKBest()),\\n...    ('model', calibrated_forest)])\\n>>> param_grid = {\\n...    'select__k': [1, 2],\\n...    'model__estimator__max_depth': [2, 4, 6, 8]}\\n>>> search = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)\\n\\n\\nPlease refer to Pipeline: chaining estimators for performing parameter searches over\\npipelines.\\n\\n\\n3.2.4.4. Model selection: development and evaluation#\\nModel selection by evaluating various parameter settings can be seen as a way\\nto use the labeled data to “train” the parameters of the grid.\\nWhen evaluating the resulting model it is important to do it on\\nheld-out samples that were not seen during the grid search process:\\nit is recommended to split the data into a development set (to\\nbe fed to the GridSearchCV instance) and an evaluation set\\nto compute performance metrics.\\nThis can be done by using the train_test_split\\nutility function.\\n---------new doc---------\\nensemble.RandomForestClassifier([...])\\nA random forest classifier.\\n\\nensemble.RandomForestRegressor([...])\\nA random forest regressor.\\n\\nensemble.ExtraTreesClassifier([...])\\nAn extra-trees classifier.\\n\\nensemble.ExtraTreesRegressor([n_estimators, ...])\\nAn extra-trees regressor.\\n\\nensemble.GradientBoostingClassifier(*[, ...])\\nGradient Boosting for classification.\\n\\nensemble.GradientBoostingRegressor(*[, ...])\\nGradient Boosting for regression.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.1. Cross-validation: evaluating estimator performance\\n\\n\\n\\n\\nnext\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3.3. Tuning the decision threshold for class prediction#\\nClassification is best divided into two parts:\\n\\nthe statistical problem of learning a model to predict, ideally, class probabilities;\\nthe decision problem to take concrete action based on those probability predictions.\\n---------new doc---------\\nLet’s take a straightforward example related to weather forecasting: the first point is\\nrelated to answering “what is the chance that it will rain tomorrow?” while the second\\npoint is related to answering “should I take an umbrella tomorrow?”.\\nWhen it comes to the scikit-learn API, the first point is addressed providing scores\\nusing predict_proba or decision_function. The former returns conditional\\nprobability estimates \\\\(P(y|X)\\\\) for each class, while the latter returns a decision\\nscore for each class.\\nThe decision corresponding to the labels are obtained with predict. In binary\\nclassification, a decision rule or action is then defined by thresholding the scores,\\nleading to the prediction of a single class label for each sample. For binary\\nclassification in scikit-learn, class labels predictions are obtained by hard-coded\\ncut-off rules: a positive class is predicted when the conditional probability\\n\\\\(P(y|X)\\\\) is greater than 0.5 (obtained with predict_proba) or if the\\ndecision score is greater than 0 (obtained with decision_function).\\nHere, we show an example that illustrates the relation between conditional\\nprobability estimates \\\\(P(y|X)\\\\) and class labels:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> X, y = make_classification(random_state=0)\\n>>> classifier = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y)\\n>>> classifier.predict_proba(X[:4])\\narray([[0.94     , 0.06     ],\\n---------new doc---------\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> X, y = make_classification(random_state=0)\\n>>> classifier = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y)\\n>>> classifier.predict_proba(X[:4])\\narray([[0.94     , 0.06     ],\\n       [0.94     , 0.06     ],\\n       [0.0416..., 0.9583...],\\n       [0.0416..., 0.9583...]])\\n>>> classifier.predict(X[:4])\\narray([0, 0, 1, 1])\\n---------new doc---------\\n3.3.1. Post-tuning the decision threshold#\\nOne solution to address the problem stated in the introduction is to tune the decision\\nthreshold of the classifier once the model has been trained. The\\nTunedThresholdClassifierCV tunes this threshold using\\nan internal cross-validation. The optimum threshold is chosen to maximize a given\\nmetric.\\nThe following image illustrates the tuning of the decision threshold for a gradient\\nboosting classifier. While the vanilla and tuned classifiers provide the same\\npredict_proba outputs and thus the same Receiver Operating Characteristic (ROC)\\nand Precision-Recall curves, the class label predictions differ because of the tuned\\ndecision threshold. The vanilla classifier predicts the class of interest for a\\nconditional probability greater than 0.5 while the tuned classifier predicts the class\\nof interest for a very low probability (around 0.02). This decision threshold optimizes\\na utility metric defined by the business (in this case an insurance company).\\n\\n\\n\\n\\n\\n3.3.1.1. Options to tune the decision threshold#\\nThe decision threshold can be tuned through different strategies controlled by the\\nparameter scoring.\\nOne way to tune the threshold is by maximizing a pre-defined scikit-learn metric. These\\nmetrics can be found by calling the function get_scorer_names.\\nBy default, the balanced accuracy is the metric used but be aware that one should choose\\na meaningful metric for their use case.\\n---------new doc---------\\n3.3.1.2. Important notes regarding the internal cross-validation#\\nBy default TunedThresholdClassifierCV uses a 5-fold\\nstratified cross-validation to tune the decision threshold. The parameter cv allows to\\ncontrol the cross-validation strategy. It is possible to bypass cross-validation by\\nsetting cv=\\\"prefit\\\" and providing a fitted classifier. In this case, the decision\\nthreshold is tuned on the data provided to the fit method.\\nHowever, you should be extremely careful when using this option. You should never use\\nthe same data for training the classifier and tuning the decision threshold due to the\\nrisk of overfitting. Refer to the following example section for more details (cf.\\nConsideration regarding model refitting and cross-validation). If you have limited resources, consider using\\na float number for cv to limit to an internal single train-test split.\\nThe option cv=\\\"prefit\\\" should only be used when the provided classifier was already\\ntrained, and you just want to find the best decision threshold using a new validation\\nset.\\n\\n\\n3.3.1.3. Manually setting the decision threshold#\\nThe previous sections discussed strategies to find an optimal decision threshold. It is\\nalso possible to manually set the decision threshold using the class\\nFixedThresholdClassifier. In case that you don’t want\\nto refit the model when calling fit, wrap your sub-estimator with a\\nFrozenEstimator and do\\nFixedThresholdClassifier(FrozenEstimator(estimator), ...).\\n\\n\\n3.3.1.4. Examples#\\n---------new doc---------\\nclass 0       0.67      1.00      0.80         2\\n     class 1       0.00      0.00      0.00         1\\n     class 2       1.00      0.50      0.67         2\\n\\n    accuracy                           0.60         5\\n   macro avg       0.56      0.50      0.49         5\\nweighted avg       0.67      0.60      0.59         5\\n\\n\\nExamples\\n\\nSee Recognizing hand-written digits\\nfor an example of classification report usage for\\nhand-written digits.\\nSee Custom refit strategy of a grid search with cross-validation\\nfor an example of classification report usage for\\ngrid search with nested cross-validation.\\n---------new doc---------\\nExamples\\n\\nSee Multiclass Receiver Operating Characteristic (ROC) for an example of\\nusing ROC to evaluate the quality of the output of a classifier.\\nSee Receiver Operating Characteristic (ROC) with cross validation  for an\\nexample of using ROC to evaluate classifier output quality, using cross-validation.\\nSee Species distribution modeling\\nfor an example of using ROC to model species distribution.\\n\\nReferences\\n\\n\\n[HT2001]\\n(1,2)\\nHand, D.J. and Till, R.J., (2001). A simple generalisation\\nof the area under the ROC curve for multiple class classification problems.\\nMachine learning, 45(2), pp. 171-186.\\n\\n\\n[FC2009]\\nFerri, Cèsar & Hernandez-Orallo, Jose & Modroiu, R. (2009).\\nAn Experimental Comparison of Performance Measures for Classification.\\nPattern Recognition Letters. 30. 27-38.\\n\\n\\n[PD2000]\\nProvost, F., Domingos, P. (2000). Well-trained PETs: Improving\\nprobability estimation trees\\n(Section 6.2), CeDER Working Paper #IS-00-04, Stern School of Business,\\nNew York University.\\n\\n\\n[F2006]\\nFawcett, T., 2006. An introduction to ROC analysis.\\nPattern Recognition Letters, 27(8), pp. 861-874.\\n\\n\\n[F2001]\\nFawcett, T., 2001. Using rule sets to maximize\\nROC performance\\nIn Data Mining, 2001.\\nProceedings IEEE International Conference, pp. 131-138.\\n---------new doc---------\\nSee Effect of transforming the targets in regression model for\\nan example on how to use PredictionErrorDisplay\\nto visualize the prediction quality improvement of a regression model\\nobtained by transforming the target before learning.\\n\\n\\n\\n\\n3.4.7. Clustering metrics#\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions to measure clustering performance. For more information see the\\nClustering performance evaluation section for instance clustering, and\\nBiclustering evaluation for biclustering.\\n\\n\\n3.4.8. Dummy estimators#\\nWhen doing supervised learning, a simple sanity check consists of comparing\\none’s estimator against simple rules of thumb. DummyClassifier\\nimplements several such simple strategies for classification:\\n\\nstratified generates random predictions by respecting the training\\nset class distribution.\\nmost_frequent always predicts the most frequent label in the training set.\\nprior always predicts the class that maximizes the class prior\\n(like most_frequent) and predict_proba returns the class prior.\\nuniform generates predictions uniformly at random.\\n\\nconstant always predicts a constant label that is provided by the user.A major motivation of this method is F1-scoring, when the positive class\\nis in the minority.\\n---------new doc---------\\nconstant always predicts a constant label that is provided by the user.A major motivation of this method is F1-scoring, when the positive class\\nis in the minority.\\n\\n\\n\\n\\nNote that with all these strategies, the predict method completely ignores\\nthe input data!\\nTo illustrate DummyClassifier, first let’s create an imbalanced\\ndataset:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.model_selection import train_test_split\\n>>> X, y = load_iris(return_X_y=True)\\n>>> y[y != 1] = -1\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\n\\nNext, let’s compare the accuracy of SVC and most_frequent:\\n>>> from sklearn.dummy import DummyClassifier\\n>>> from sklearn.svm import SVC\\n>>> clf = SVC(kernel='linear', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.63...\\n>>> clf = DummyClassifier(strategy='most_frequent', random_state=0)\\n>>> clf.fit(X_train, y_train)\\nDummyClassifier(random_state=0, strategy='most_frequent')\\n>>> clf.score(X_test, y_test)\\n0.57...\\n---------new doc---------\\nWe see that SVC doesn’t do much better than a dummy classifier. Now, let’s\\nchange the kernel:\\n>>> clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.94...\\n\\n\\nWe see that the accuracy was boosted to almost 100%.  A cross validation\\nstrategy is recommended for a better estimate of the accuracy, if it\\nis not too CPU costly. For more information see the Cross-validation: evaluating estimator performance\\nsection. Moreover if you want to optimize over the parameter space, it is highly\\nrecommended to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator\\nsection for details.\\nMore generally, when the accuracy of a classifier is too close to random, it\\nprobably means that something went wrong: features are not helpful, a\\nhyperparameter is not correctly tuned, the classifier is suffering from class\\nimbalance, etc…\\nDummyRegressor also implements four simple rules of thumb for regression:\\n\\nmean always predicts the mean of the training targets.\\nmedian always predicts the median of the training targets.\\nquantile always predicts a user provided quantile of the training targets.\\nconstant always predicts a constant value that is provided by the user.\\n\\nIn all these strategies, the predict method completely ignores\\nthe input data.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n\\n\\nnext\\n3.5. Validation curves: plotting scores to evaluate models\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.5. Validation curves: plotting scores to evaluate models\\n---------new doc---------\\nIn all these strategies, the predict method completely ignores\\nthe input data.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n\\n\\nnext\\n3.5. Validation curves: plotting scores to evaluate models\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.5. Validation curves: plotting scores to evaluate models\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3.5. Validation curves: plotting scores to evaluate models#\\nEvery estimator has its advantages and drawbacks. Its generalization error\\ncan be decomposed in terms of bias, variance and noise. The bias of an\\nestimator is its average error for different training sets. The variance\\nof an estimator indicates how sensitive it is to varying training sets. Noise\\nis a property of the data.\\nIn the following plot, we see a function \\\\(f(x) = \\\\cos (\\\\frac{3}{2} \\\\pi x)\\\\)\\nand some noisy samples from that function. We use three different estimators\\nto fit the function: linear regression with polynomial features of degree 1,\\n4 and 15. We see that the first estimator can at best provide only a poor fit\\nto the samples and the true function because it is too simple (high bias),\\nthe second estimator approximates it almost perfectly and the last estimator\\napproximates the training data perfectly but does not fit the true function\\nvery well, i.e. it is very sensitive to varying training data (high variance).\\n---------new doc---------\\nUnderfitting vs. Overfitting\\nEffect of model regularization on training and test error\\nPlotting Learning Curves and Checking Models’ Scalability\\n\\n\\n3.5.1. Validation curve#\\nTo validate a model we need a scoring function (see Metrics and scoring: quantifying the quality of predictions),\\nfor example accuracy for classifiers. The proper way of choosing multiple\\nhyperparameters of an estimator is of course grid search or similar methods\\n(see Tuning the hyper-parameters of an estimator) that select the hyperparameter with the maximum score\\non a validation set or multiple validation sets. Note that if we optimize\\nthe hyperparameters based on a validation score the validation score is biased\\nand not a good estimate of the generalization any longer. To get a proper\\nestimate of the generalization we have to compute the score on another test\\nset.\\nHowever, it is sometimes helpful to plot the influence of a single\\nhyperparameter on the training score and the validation score to find out\\nwhether the estimator is overfitting or underfitting for some hyperparameter\\nvalues.\\nThe function validation_curve can help in this case:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import validation_curve\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.svm import SVC\\n\\n>>> np.random.seed(0)\\n>>> X, y = load_iris(return_X_y=True)\\n>>> indices = np.arange(y.shape[0])\\n>>> np.random.shuffle(indices)\\n>>> X, y = X[indices], y[indices]\\n---------new doc---------\\n>>> np.random.seed(0)\\n>>> X, y = load_iris(return_X_y=True)\\n>>> indices = np.arange(y.shape[0])\\n>>> np.random.shuffle(indices)\\n>>> X, y = X[indices], y[indices]\\n\\n>>> train_scores, valid_scores = validation_curve(\\n...     SVC(kernel=\\\"linear\\\"), X, y, param_name=\\\"C\\\", param_range=np.logspace(-7, 3, 3),\\n... )\\n>>> train_scores\\narray([[0.90..., 0.94..., 0.91..., 0.89..., 0.92...],\\n       [0.9... , 0.92..., 0.93..., 0.92..., 0.93...],\\n       [0.97..., 1...   , 0.98..., 0.97..., 0.99...]])\\n>>> valid_scores\\narray([[0.9..., 0.9... , 0.9... , 0.96..., 0.9... ],\\n       [0.9..., 0.83..., 0.96..., 0.96..., 0.93...],\\n       [1.... , 0.93..., 1....  , 1....  , 0.9... ]])\\n---------new doc---------\\nIf you intend to plot the validation curves only, the class\\nValidationCurveDisplay is more direct than\\nusing matplotlib manually on the results of a call to validation_curve.\\nYou can use the method\\nfrom_estimator similarly\\nto validation_curve to generate and plot the validation curve:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import ValidationCurveDisplay\\nfrom sklearn.svm import SVC\\nfrom sklearn.utils import shuffle\\nX, y = load_iris(return_X_y=True)\\nX, y = shuffle(X, y, random_state=0)\\nValidationCurveDisplay.from_estimator(\\n   SVC(kernel=\\\"linear\\\"), X, y, param_name=\\\"C\\\", param_range=np.logspace(-7, 3, 10)\\n)\\n\\n\\n\\n\\n\\nIf the training score and the validation score are both low, the estimator will\\nbe underfitting. If the training score is high and the validation score is low,\\nthe estimator is overfitting and otherwise it is working very well. A low\\ntraining score and a high validation score is usually not possible.\\n---------new doc---------\\nIf the training score and the validation score are both low, the estimator will\\nbe underfitting. If the training score is high and the validation score is low,\\nthe estimator is overfitting and otherwise it is working very well. A low\\ntraining score and a high validation score is usually not possible.\\n\\n\\n3.5.2. Learning curve#\\nA learning curve shows the validation and training score of an estimator\\nfor varying numbers of training samples. It is a tool to find out how much\\nwe benefit from adding more training data and whether the estimator suffers\\nmore from a variance error or a bias error. Consider the following example\\nwhere we plot the learning curve of a naive Bayes classifier and an SVM.\\nFor the naive Bayes, both the validation score and the training score\\nconverge to a value that is quite low with increasing size of the training\\nset. Thus, we will probably not benefit much from more training data.\\nIn contrast, for small amounts of data, the training score of the SVM is\\nmuch greater than the validation score. Adding more training samples will\\nmost likely increase generalization.\\n\\n\\n\\n\\nWe can use the function learning_curve to generate the values\\nthat are required to plot such a learning curve (number of samples\\nthat have been used, the average scores on the training sets and the\\naverage scores on the validation sets):\\n>>> from sklearn.model_selection import learning_curve\\n>>> from sklearn.svm import SVC\\n---------new doc---------\\nWe can use the function learning_curve to generate the values\\nthat are required to plot such a learning curve (number of samples\\nthat have been used, the average scores on the training sets and the\\naverage scores on the validation sets):\\n>>> from sklearn.model_selection import learning_curve\\n>>> from sklearn.svm import SVC\\n\\n>>> train_sizes, train_scores, valid_scores = learning_curve(\\n...     SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)\\n>>> train_sizes\\narray([ 50, 80, 110])\\n>>> train_scores\\narray([[0.98..., 0.98 , 0.98..., 0.98..., 0.98...],\\n       [0.98..., 1.   , 0.98..., 0.98..., 0.98...],\\n       [0.98..., 1.   , 0.98..., 0.98..., 0.99...]])\\n>>> valid_scores\\narray([[1. ,  0.93...,  1. ,  1. ,  0.96...],\\n       [1. ,  0.96...,  1. ,  1. ,  0.96...],\\n       [1. ,  0.96...,  1. ,  1. ,  0.96...]])\\n---------new doc---------\\nIf you intend to plot the learning curves only, the class\\nLearningCurveDisplay will be easier to use.\\nYou can use the method\\nfrom_estimator similarly\\nto learning_curve to generate and plot the learning curve:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import LearningCurveDisplay\\nfrom sklearn.svm import SVC\\nfrom sklearn.utils import shuffle\\nX, y = load_iris(return_X_y=True)\\nX, y = shuffle(X, y, random_state=0)\\nLearningCurveDisplay.from_estimator(\\n   SVC(kernel=\\\"linear\\\"), X, y, train_sizes=[50, 80, 110], cv=5)\\n\\n\\n\\n\\n\\nExamples\\n\\nSee Plotting Learning Curves and Checking Models’ Scalability for an\\nexample of using learning curves to check the scalability of a predictive model.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.4. Metrics and scoring: quantifying the quality of predictions\\n\\n\\n\\n\\nnext\\n4. Inspection\\n\\n\\n --- \\n\\n\\n4. Inspection\\n4.1. Partial Dependence and Individual Conditional Expectation plots\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4.1. Partial Dependence and Individual Conditional Expectation plots#\\nPartial dependence plots (PDP) and individual conditional expectation (ICE)\\nplots can be used to visualize and analyze interaction between the target\\nresponse [1] and a set of input features of interest.\\nBoth PDPs [H2009] and ICEs [G2015] assume that the input features of interest\\nare independent from the complement features, and this assumption is often\\nviolated in practice. Thus, in the case of correlated features, we will\\ncreate absurd data points to compute the PDP/ICE [M2019].\\n---------new doc---------\\nOne-way PDPs tell us about the interaction between the target response and an input\\nfeature of interest (e.g. linear, non-linear). The left plot in the above figure\\nshows the effect of the temperature on the number of bike rentals; we can clearly see\\nthat a higher temperature is related with a higher number of bike rentals. Similarly, we\\ncould analyze the effect of the humidity on the number of bike rentals (middle plot).\\nThus, these interpretations are marginal, considering a feature at a time.\\nPDPs with two input features of interest show the interactions among the two features.\\nFor example, the two-variable PDP in the above figure shows the dependence of the number\\nof bike rentals on joint values of temperature and humidity. We can clearly see an\\ninteraction between the two features: with a temperature higher than 20 degrees Celsius,\\nmainly the humidity has a strong impact on the number of bike rentals. For lower\\ntemperatures, both the temperature and the humidity have an impact on the number of bike\\nrentals.\\nThe sklearn.inspection module provides a convenience function\\nfrom_estimator to create one-way and two-way partial\\ndependence plots. In the below example we show how to create a grid of\\npartial dependence plots: two one-way PDPs for the features 0 and 1\\nand a two-way PDP between the two features:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n>>> from sklearn.inspection import PartialDependenceDisplay\\n---------new doc---------\\nThe same parameter target is used to specify the target in multi-output\\nregression settings.\\n\\nIf you need the raw values of the partial dependence function rather than\\nthe plots, you can use the\\nsklearn.inspection.partial_dependence function:\\n>>> from sklearn.inspection import partial_dependence\\n\\n>>> results = partial_dependence(clf, X, [0])\\n>>> results[\\\"average\\\"]\\narray([[ 2.466...,  2.466..., ...\\n>>> results[\\\"grid_values\\\"]\\n[array([-1.624..., -1.592..., ...\\n\\n\\nThe values at which the partial dependence should be evaluated are directly\\ngenerated from X. For 2-way partial dependence, a 2D-grid of values is\\ngenerated. The values field returned by\\nsklearn.inspection.partial_dependence gives the actual values\\nused in the grid for each input feature of interest. They also correspond to\\nthe axis of the plots.\\n\\n\\n4.1.2. Individual conditional expectation (ICE) plot#\\nSimilar to a PDP, an individual conditional expectation (ICE) plot\\nshows the dependence between the target function and an input feature of\\ninterest. However, unlike a PDP, which shows the average effect of the input\\nfeature, an ICE plot visualizes the dependence of the prediction on a\\nfeature for each sample separately with one line per sample.\\nDue to the limits of human perception, only one input feature of interest is\\nsupported for ICE plots.\\nThe figures below show two ICE plots for the bike sharing dataset,\\nwith a HistGradientBoostingRegressor:.\\nThe figures plot the corresponding PD line overlaid on ICE lines.\\n---------new doc---------\\nWhile the PDPs are good at showing the average effect of the target features,\\nthey can obscure a heterogeneous relationship created by interactions.\\nWhen interactions are present the ICE plot will provide many more insights.\\nFor example, we see that the ICE for the temperature feature gives us some\\nadditional information: Some of the ICE lines are flat while some others\\nshows a decrease of the dependence for temperature above 35 degrees Celsius.\\nWe observe a similar pattern for the humidity feature: some of the ICE\\nlines show a sharp decrease when the humidity is above 80%.\\nThe sklearn.inspection module’s PartialDependenceDisplay.from_estimator\\nconvenience function can be used to create ICE plots by setting\\nkind='individual'. In the example below, we show how to create a grid of\\nICE plots:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n>>> from sklearn.inspection import PartialDependenceDisplay\\n\\n\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\n>>> features = [0, 1]\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind='individual')\\n<...>\\n---------new doc---------\\nbeing weighted by the fraction of training samples that entered that branch.\\nFinally, the partial dependence is given by a weighted average of all the\\nvisited leaves values.\\nWith the ‘brute’ method, the parameter X is used both for generating the\\ngrid of values \\\\(x_S\\\\) and the complement feature values \\\\(x_C\\\\).\\nHowever with the ‘recursion’ method, X is only used for the grid values:\\nimplicitly, the \\\\(x_C\\\\) values are those of the training data.\\nBy default, the ‘recursion’ method is used for plotting PDPs on tree-based\\nestimators that support it, and ‘brute’ is used for the rest.\\n---------new doc---------\\nNote\\nWhile both methods should be close in general, they might differ in some\\nspecific settings. The ‘brute’ method assumes the existence of the\\ndata points \\\\((x_S, x_C^{(i)})\\\\). When the features are correlated,\\nsuch artificial samples may have a very low probability mass. The ‘brute’\\nand ‘recursion’ methods will likely disagree regarding the value of the\\npartial dependence, because they will treat these unlikely\\nsamples differently. Remember, however, that the primary assumption for\\ninterpreting PDPs is that the features should be independent.\\n\\nExamples\\n\\nPartial Dependence and Individual Conditional Expectation Plots\\n\\nFootnotes\\n\\n\\n[1]\\nFor classification, the target response may be the probability of a\\nclass (the positive class for binary classification), or the decision\\nfunction.\\n\\n\\nReferences\\n\\n\\n[H2009]\\nT. Hastie, R. Tibshirani and J. Friedman,\\nThe Elements of Statistical Learning,\\nSecond Edition, Section 10.13.2, Springer, 2009.\\n\\n\\n[M2019]\\nC. Molnar,\\nInterpretable Machine Learning,\\nSection 5.1, 2019.\\n\\n\\n[G2015]\\n(1,2)\\nA. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin,\\n“Peeking Inside the Black Box: Visualizing Statistical\\nLearning With Plots of Individual Conditional Expectation”\\nJournal of Computational and Graphical Statistics,\\n24(1): 44-65, Springer, 2015.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n4. Inspection\\n\\n\\n\\n\\nnext\\n4.2. Permutation feature importance\\n\\n\\n --- \\n\\n\\n4. Inspection\\n4.2. Permutation feature importance\\n---------new doc---------\\nprevious\\n4. Inspection\\n\\n\\n\\n\\nnext\\n4.2. Permutation feature importance\\n\\n\\n --- \\n\\n\\n4. Inspection\\n4.2. Permutation feature importance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4.2. Permutation feature importance#\\nPermutation feature importance is a model inspection technique that measures the\\ncontribution of each feature to a fitted model’s statistical performance\\non a given tabular dataset. This technique is particularly useful for non-linear\\nor opaque estimators, and involves randomly shuffling the values of a\\nsingle feature and observing the resulting degradation of the model’s score\\n[1]. By breaking the relationship between the feature and the target, we\\ndetermine how much the model relies on such particular feature.\\nIn the following figures, we observe the effect of permuting features on the correlation\\nbetween the feature and the target and consequently on the model statistical\\nperformance.\\n---------new doc---------\\nOn the top figure, we observe that permuting a predictive feature breaks the\\ncorrelation between the feature and the target, and consequently the model\\nstatistical performance decreases. On the bottom figure, we observe that permuting\\na non-predictive feature does not significantly degrade the model statistical performance.\\nOne key advantage of permutation feature importance is that it is\\nmodel-agnostic, i.e. it can be applied to any fitted estimator. Moreover, it can\\nbe calculated multiple times with different permutations of the feature, further\\nproviding a measure of the variance in the estimated feature importances for the\\nspecific trained model.\\nThe figure below shows the permutation feature importance of a\\nRandomForestClassifier trained on an augmented\\nversion of the titanic dataset that contains a random_cat and a random_num\\nfeatures, i.e. a categrical and a numerical feature that are not correlated in\\nany way with the target variable:\\n\\n\\n\\n\\n\\nWarning\\nFeatures that are deemed of low importance for a bad model (low\\ncross-validation score) could be very important for a good model.\\nTherefore it is always important to evaluate the predictive power of a model\\nusing a held-out set (or better with cross-validation) prior to computing\\nimportances. Permutation importance does not reflect to the intrinsic\\npredictive value of a feature by itself but how important this feature is\\nfor a particular model.\\n---------new doc---------\\nThe permutation_importance function calculates the feature importance\\nof estimators for a given dataset. The n_repeats parameter sets the\\nnumber of times a feature is randomly shuffled and returns a sample of feature\\nimportances.\\nLet’s consider the following trained regression model:\\n>>> from sklearn.datasets import load_diabetes\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn.linear_model import Ridge\\n>>> diabetes = load_diabetes()\\n>>> X_train, X_val, y_train, y_val = train_test_split(\\n...     diabetes.data, diabetes.target, random_state=0)\\n...\\n>>> model = Ridge(alpha=1e-2).fit(X_train, y_train)\\n>>> model.score(X_val, y_val)\\n0.356...\\n---------new doc---------\\nIts validation performance, measured via the \\\\(R^2\\\\) score, is\\nsignificantly larger than the chance level. This makes it possible to use the\\npermutation_importance function to probe which features are most\\npredictive:\\n>>> from sklearn.inspection import permutation_importance\\n>>> r = permutation_importance(model, X_val, y_val,\\n...                            n_repeats=30,\\n...                            random_state=0)\\n...\\n>>> for i in r.importances_mean.argsort()[::-1]:\\n...     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\\n...         print(f\\\"{diabetes.feature_names[i]:<8}\\\"\\n...               f\\\"{r.importances_mean[i]:.3f}\\\"\\n...               f\\\" +/- {r.importances_std[i]:.3f}\\\")\\n...\\ns5      0.204 +/- 0.050\\nbmi     0.176 +/- 0.048\\nbp      0.088 +/- 0.033\\nsex     0.056 +/- 0.023\\n---------new doc---------\\nNote that the importance values for the top features represent a large\\nfraction of the reference score of 0.356.\\nPermutation importances can be computed either on the training set or on a\\nheld-out testing or validation set. Using a held-out set makes it possible to\\nhighlight which features contribute the most to the generalization power of the\\ninspected model. Features that are important on the training set but not on the\\nheld-out set might cause the model to overfit.\\nThe permutation feature importance depends on the score function that is\\nspecified with the scoring argument. This argument accepts multiple scorers,\\nwhich is more computationally efficient than sequentially calling\\npermutation_importance several times with a different scorer, as it\\nreuses model predictions.\\n---------new doc---------\\nThe ranking of the features is approximately the same for different metrics even\\nif the scales of the importance values are very different. However, this is not\\nguaranteed and different metrics might lead to significantly different feature\\nimportances, in particular for models trained for imbalanced classification problems,\\nfor which the choice of the classification metric can be critical.\\n\\n\\n4.2.1. Outline of the permutation importance algorithm#\\n\\nInputs: fitted predictive model \\\\(m\\\\), tabular dataset (training or\\nvalidation) \\\\(D\\\\).\\nCompute the reference score \\\\(s\\\\) of the model \\\\(m\\\\) on data\\n\\\\(D\\\\) (for instance the accuracy for a classifier or the \\\\(R^2\\\\) for\\na regressor).\\nFor each feature \\\\(j\\\\) (column of \\\\(D\\\\)):\\n\\nFor each repetition \\\\(k\\\\) in \\\\({1, ..., K}\\\\):\\n\\nRandomly shuffle column \\\\(j\\\\) of dataset \\\\(D\\\\) to generate a\\ncorrupted version of the data named \\\\(\\\\tilde{D}_{k,j}\\\\).\\nCompute the score \\\\(s_{k,j}\\\\) of model \\\\(m\\\\) on corrupted data\\n\\\\(\\\\tilde{D}_{k,j}\\\\).\\n\\n\\nCompute importance \\\\(i_j\\\\) for feature \\\\(f_j\\\\) defined as:\\n\\n\\\\[i_j = s - \\\\frac{1}{K} \\\\sum_{k=1}^{K} s_{k,j}\\\\]\\n---------new doc---------\\nCompute importance \\\\(i_j\\\\) for feature \\\\(f_j\\\\) defined as:\\n\\n\\\\[i_j = s - \\\\frac{1}{K} \\\\sum_{k=1}^{K} s_{k,j}\\\\]\\n\\n\\n\\n\\n\\n\\n4.2.2. Relation to impurity-based importance in trees#\\nTree-based models provide an alternative measure of feature importances\\nbased on the mean decrease in impurity\\n(MDI). Impurity is quantified by the splitting criterion of the decision trees\\n(Gini, Log Loss or Mean Squared Error). However, this method can give high\\nimportance to features that may not be predictive on unseen data when the model\\nis overfitting. Permutation-based feature importance, on the other hand, avoids\\nthis issue, since it can be computed on unseen data.\\nFurthermore, impurity-based feature importance for trees are strongly\\nbiased and favor high cardinality features (typically numerical features)\\nover low cardinality features such as binary features or categorical variables\\nwith a small number of possible categories.\\nPermutation-based feature importances do not exhibit such a bias. Additionally,\\nthe permutation feature importance may be computed with any performance metric\\non the model predictions and can be used to analyze any model class (not just\\ntree-based models).\\nThe following example highlights the limitations of impurity-based feature\\nimportance in contrast to permutation-based feature importance:\\nPermutation Importance vs Random Forest Feature Importance (MDI).\\n---------new doc---------\\n4.2.3. Misleading values on strongly correlated features#\\nWhen two features are correlated and one of the features is permuted, the model\\nstill has access to the latter through its correlated feature. This results in a\\nlower reported importance value for both features, though they might actually\\nbe important.\\nThe figure below shows the permutation feature importance of a\\nRandomForestClassifier trained using the\\nBreast cancer wisconsin (diagnostic) dataset, which contains strongly correlated features. A\\nnaive interpretation would suggest that all features are unimportant:\\n\\n\\n\\n\\nOne way to handle the issue is to cluster features that are correlated and only\\nkeep one feature from each cluster.\\n\\n\\n\\n\\nFor more details on such strategy, see the example\\nPermutation Importance with Multicollinear or Correlated Features.\\nExamples\\n\\nPermutation Importance vs Random Forest Feature Importance (MDI)\\nPermutation Importance with Multicollinear or Correlated Features\\n\\nReferences\\n\\n\\n[1]\\nL. Breiman, “Random Forests”,\\nMachine Learning, 45(1), 5-32, 2001.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n4.1. Partial Dependence and Individual Conditional Expectation plots\\n\\n\\n\\n\\nnext\\n5. Visualizations\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.1. Pipelines and composite estimators\\n---------new doc---------\\nReferences\\n\\n\\n[1]\\nL. Breiman, “Random Forests”,\\nMachine Learning, 45(1), 5-32, 2001.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n4.1. Partial Dependence and Individual Conditional Expectation plots\\n\\n\\n\\n\\nnext\\n5. Visualizations\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.1. Pipelines and composite estimators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6.1. Pipelines and composite estimators#\\nTo build a composite estimator, transformers are usually combined with other\\ntransformers or with predictors (such as classifiers or regressors).\\nThe most common tool used for composing estimators is a Pipeline. Pipelines require all steps except the last to be a\\ntransformer. The last step can be anything, a transformer, a\\npredictor, or a clustering estimator which might have or not have a\\n.predict(...) method. A pipeline exposes all methods provided by the last\\nestimator: if the last step provides a transform method, then the pipeline\\nwould have a transform method and behave like a transformer. If the last step\\nprovides a predict method, then the pipeline would expose that method, and\\ngiven a data X, use all steps except the last to transform the data,\\nand then give that transformed data to the predict method of the last step of\\nthe pipeline. The class Pipeline is often used in combination with\\nColumnTransformer or\\nFeatureUnion which concatenate the output of transformers\\ninto a composite feature space.\\nTransformedTargetRegressor\\ndeals with transforming the target (i.e. log-transform y).\\n---------new doc---------\\n6.1.2. Transforming target in regression#\\nTransformedTargetRegressor transforms the\\ntargets y before fitting a regression model. The predictions are mapped\\nback to the original space via an inverse transform. It takes as an argument\\nthe regressor that will be used for prediction, and the transformer that will\\nbe applied to the target variable:\\n>>> import numpy as np\\n>>> from sklearn.datasets import fetch_california_housing\\n>>> from sklearn.compose import TransformedTargetRegressor\\n>>> from sklearn.preprocessing import QuantileTransformer\\n>>> from sklearn.linear_model import LinearRegression\\n>>> from sklearn.model_selection import train_test_split\\n>>> X, y = fetch_california_housing(return_X_y=True)\\n>>> X, y = X[:2000, :], y[:2000]  # select a subset of data\\n>>> transformer = QuantileTransformer(output_distribution='normal')\\n>>> regressor = LinearRegression()\\n>>> regr = TransformedTargetRegressor(regressor=regressor,\\n...                                   transformer=transformer)\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: 0.61\\n---------new doc---------\\n6.3.1. Standardization, or mean removal and variance scaling#\\nStandardization of datasets is a common requirement for many\\nmachine learning estimators implemented in scikit-learn; they might behave\\nbadly if the individual features do not more or less look like standard\\nnormally distributed data: Gaussian with zero mean and unit variance.\\nIn practice we often ignore the shape of the distribution and just\\ntransform the data to center it by removing the mean value of each\\nfeature, then scale it by dividing non-constant features by their\\nstandard deviation.\\nFor instance, many elements used in the objective function of\\na learning algorithm (such as the RBF kernel of Support Vector\\nMachines or the l1 and l2 regularizers of linear models) may assume that\\nall features are centered around zero or have variance in the same\\norder. If a feature has a variance that is orders of magnitude larger\\nthan others, it might dominate the objective function and make the\\nestimator unable to learn from other features correctly as expected.\\nThe preprocessing module provides the\\nStandardScaler utility class, which is a quick and\\neasy way to perform the following operation on an array-like\\ndataset:\\n>>> from sklearn import preprocessing\\n>>> import numpy as np\\n>>> X_train = np.array([[ 1., -1.,  2.],\\n...                     [ 2.,  0.,  0.],\\n...                     [ 0.,  1., -1.]])\\n>>> scaler = preprocessing.StandardScaler().fit(X_train)\\n>>> scaler\\nStandardScaler()\\n---------new doc---------\\n>>> X, y = make_classification(random_state=42)\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\n>>> pipe = make_pipeline(StandardScaler(), LogisticRegression())\\n>>> pipe.fit(X_train, y_train)  # apply scaling on training data\\nPipeline(steps=[('standardscaler', StandardScaler()),\\n                ('logisticregression', LogisticRegression())])\\n\\n>>> pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.\\n0.96\\n\\n\\nIt is possible to disable either centering or scaling by either\\npassing with_mean=False or with_std=False to the constructor\\nof StandardScaler.\\n---------new doc---------\\nNote\\nThis estimator is still experimental for now: default parameters or\\ndetails of behaviour might change without any deprecation cycle. Resolving\\nthe following issues would help stabilize IterativeImputer:\\nconvergence criteria (#14338) and default estimators\\n(#13286). To use it, you need to explicitly import\\nenable_iterative_imputer.\\n\\n>>> import numpy as np\\n>>> from sklearn.experimental import enable_iterative_imputer\\n>>> from sklearn.impute import IterativeImputer\\n>>> imp = IterativeImputer(max_iter=10, random_state=0)\\n>>> imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\\nIterativeImputer(random_state=0)\\n>>> X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\\n>>> # the model learns that the second feature is double the first\\n>>> print(np.round(imp.transform(X_test)))\\n[[ 1.  2.]\\n [ 6. 12.]\\n [ 3.  6.]]\\n\\n\\nBoth SimpleImputer and IterativeImputer can be used in a\\nPipeline as a way to build a composite estimator that supports imputation.\\nSee Imputing missing values before building an estimator.\\n---------new doc---------\\n6.4.3.2. Multiple vs. Single Imputation#\\nIn the statistics community, it is common practice to perform multiple\\nimputations, generating, for example, m separate imputations for a single\\nfeature matrix. Each of these m imputations is then put through the\\nsubsequent analysis pipeline (e.g. feature engineering, clustering, regression,\\nclassification). The m final analysis results (e.g. held-out validation\\nerrors) allow the data scientist to obtain understanding of how analytic\\nresults may differ as a consequence of the inherent uncertainty caused by the\\nmissing values. The above practice is called multiple imputation.\\nOur implementation of IterativeImputer was inspired by the R MICE\\npackage (Multivariate Imputation by Chained Equations) [1], but differs from\\nit by returning a single imputation instead of multiple imputations.  However,\\nIterativeImputer can also be used for multiple imputations by applying\\nit repeatedly to the same dataset with different random seeds when\\nsample_posterior=True. See [2], chapter 4 for more discussion on multiple\\nvs. single imputations.\\nIt is still an open problem as to how useful single vs. multiple imputation is\\nin the context of prediction and classification when the user is not\\ninterested in measuring uncertainty due to missing values.\\nNote that a call to the transform method of IterativeImputer is\\nnot allowed to change the number of samples. Therefore multiple imputations\\ncannot be achieved by a single call to transform.\\n\\n\\n6.4.3.3. References#\\n\\n\\n[1]\\nStef van Buuren, Karin Groothuis-Oudshoorn (2011). “mice: Multivariate\\nImputation by Chained Equations in R”. Journal of Statistical Software 45:\\n1-67.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Your job is to give a detailed summary of the following documents:\\n\\nDocuments:\\nThe coefficient estimates for Ordinary Least Squares rely on the\\nindependence of the features. When features are correlated and the\\ncolumns of the design matrix \\\\(X\\\\) have an approximately linear\\ndependence, the design matrix becomes close to singular\\nand as a result, the least-squares estimate becomes highly sensitive\\nto random errors in the observed target, producing a large\\nvariance. This situation of multicollinearity can arise, for\\nexample, when data are collected without an experimental design.\\nExamples\\n\\nOrdinary Least Squares Example\\n\\n\\n1.1.1.1. Non-Negative Least Squares#\\nIt is possible to constrain all the coefficients to be non-negative, which may\\nbe useful when they represent some physical or naturally non-negative\\nquantities (e.g., frequency counts or prices of goods).\\nLinearRegression accepts a boolean positive\\nparameter: when set to True Non-Negative Least Squares are then applied.\\nExamples\\n\\nNon-negative least squares\\n\\n\\n\\n1.1.1.2. Ordinary Least Squares Complexity#\\nThe least squares solution is computed using the singular value\\ndecomposition of X. If X is a matrix of shape (n_samples, n_features)\\nthis method has a cost of\\n\\\\(O(n_{\\\\text{samples}} n_{\\\\text{features}}^2)\\\\), assuming that\\n\\\\(n_{\\\\text{samples}} \\\\geq n_{\\\\text{features}}\\\\).\\n\\n\\n\\n1.1.2. Ridge regression and classification#\\n---------new doc---------\\n1.1.16. Robustness regression: outliers and modeling errors#\\nRobust regression aims to fit a regression model in the\\npresence of corrupt data: either outliers, or error in the model.\\n\\n\\n\\n\\n\\n1.1.16.1. Different scenario and useful concepts#\\nThere are different things to keep in mind when dealing with data\\ncorrupted by outliers:\\n\\nOutliers in X or in y?\\n\\n\\nOutliers in the y direction\\nOutliers in the X direction\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFraction of outliers versus amplitude of error\\nThe number of outlying points matters, but also how much they are\\noutliers.\\n\\n\\nSmall outliers\\nLarge outliers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAn important notion of robust fitting is that of breakdown point: the\\nfraction of data that can be outlying for the fit to start missing the\\ninlying data.\\nNote that in general, robust fitting in high-dimensional setting (large\\nn_features) is very hard. The robust models here will probably not work\\nin these settings.\\n\\nTrade-offs: which estimator ?\\nScikit-learn provides 3 robust regression estimators:\\nRANSAC,\\nTheil Sen and\\nHuberRegressor.\\n---------new doc---------\\nTrade-offs: which estimator ?\\nScikit-learn provides 3 robust regression estimators:\\nRANSAC,\\nTheil Sen and\\nHuberRegressor.\\n\\nHuberRegressor should be faster than\\nRANSAC and Theil Sen\\nunless the number of samples are very large, i.e. n_samples >> n_features.\\nThis is because RANSAC and Theil Sen\\nfit on smaller subsets of the data. However, both Theil Sen\\nand RANSAC are unlikely to be as robust as\\nHuberRegressor for the default parameters.\\nRANSAC is faster than Theil Sen\\nand scales much better with the number of samples.\\nRANSAC will deal better with large\\noutliers in the y direction (most common situation).\\nTheil Sen will cope better with\\nmedium-size outliers in the X direction, but this property will\\ndisappear in high-dimensional settings.\\n\\nWhen in doubt, use RANSAC.\\n---------new doc---------\\nWhen in doubt, use RANSAC.\\n\\n\\n\\n1.1.16.2. RANSAC: RANdom SAmple Consensus#\\nRANSAC (RANdom SAmple Consensus) fits a model from random subsets of\\ninliers from the complete data set.\\nRANSAC is a non-deterministic algorithm producing only a reasonable result with\\na certain probability, which is dependent on the number of iterations (see\\nmax_trials parameter). It is typically used for linear and non-linear\\nregression problems and is especially popular in the field of photogrammetric\\ncomputer vision.\\nThe algorithm splits the complete input sample data into a set of inliers,\\nwhich may be subject to noise, and outliers, which are e.g. caused by erroneous\\nmeasurements or invalid hypotheses about the data. The resulting model is then\\nestimated only from the determined inliers.\\n\\n\\n\\n\\nExamples\\n\\nRobust linear model estimation using RANSAC\\nRobust linear estimator fitting\\n\\n\\n\\nDetails of the algorithm#\\nEach iteration performs the following steps:\\n---------new doc---------\\nExamples\\n\\nRobust linear model estimation using RANSAC\\nRobust linear estimator fitting\\n\\n\\n\\nDetails of the algorithm#\\nEach iteration performs the following steps:\\n\\nSelect min_samples random samples from the original data and check\\nwhether the set of data is valid (see is_data_valid).\\nFit a model to the random subset (estimator.fit) and check\\nwhether the estimated model is valid (see is_model_valid).\\nClassify all data as inliers or outliers by calculating the residuals\\nto the estimated model (estimator.predict(X) - y) - all data\\nsamples with absolute residuals smaller than or equal to the\\nresidual_threshold are considered as inliers.\\nSave fitted model as best model if number of inlier samples is\\nmaximal. In case the current estimated model has the same number of\\ninliers, it is only considered as the best model if it has better score.\\n\\nThese steps are performed either a maximum number of times (max_trials) or\\nuntil one of the special stop criteria are met (see stop_n_inliers and\\nstop_score). The final model is estimated using all inlier samples (consensus\\nset) of the previously determined best model.\\nThe is_data_valid and is_model_valid functions allow to identify and reject\\ndegenerate combinations of random sub-samples. If the estimated model is not\\nneeded for identifying degenerate cases, is_data_valid should be used as it\\nis called prior to fitting the model and thus leading to better computational\\nperformance.\\n\\n\\n\\nReferences#\\n---------new doc---------\\n1.4.1. Classification#\\nSVC, NuSVC and LinearSVC are classes\\ncapable of performing binary and multi-class classification on a dataset.\\n\\n\\n\\n\\nSVC and NuSVC are similar methods, but accept slightly\\ndifferent sets of parameters and have different mathematical formulations (see\\nsection Mathematical formulation). On the other hand,\\nLinearSVC is another (faster) implementation of Support Vector\\nClassification for the case of a linear kernel. It also\\nlacks some of the attributes of SVC and NuSVC, like\\nsupport_. LinearSVC uses squared_hinge loss and due to its\\nimplementation in liblinear it also regularizes the intercept, if considered.\\nThis effect can however be reduced by carefully fine tuning its\\nintercept_scaling parameter, which allows the intercept term to have a\\ndifferent regularization behavior compared to the other features. The\\nclassification results and score can therefore differ from the other two\\nclassifiers.\\nAs other classifiers, SVC, NuSVC and\\nLinearSVC take as input two arrays: an array X of shape\\n(n_samples, n_features) holding the training samples, and an array y of\\nclass labels (strings or integers), of shape (n_samples):\\n>>> from sklearn import svm\\n>>> X = [[0, 0], [1, 1]]\\n>>> y = [0, 1]\\n>>> clf = svm.SVC()\\n>>> clf.fit(X, y)\\nSVC()\\n\\n\\nAfter being fitted, the model can then be used to predict new values:\\n>>> clf.predict([[2., 2.]])\\narray([1])\\n---------new doc---------\\nAfter being fitted, the model can then be used to predict new values:\\n>>> clf.predict([[2., 2.]])\\narray([1])\\n\\n\\nSVMs decision function (detailed in the Mathematical formulation)\\ndepends on some subset of the training data, called the support vectors. Some\\nproperties of these support vectors can be found in attributes\\nsupport_vectors_, support_ and n_support_:\\n>>> # get support vectors\\n>>> clf.support_vectors_\\narray([[0., 0.],\\n       [1., 1.]])\\n>>> # get indices of support vectors\\n>>> clf.support_\\narray([0, 1]...)\\n>>> # get number of support vectors for each class\\n>>> clf.n_support_\\narray([1, 1]...)\\n\\n\\nExamples\\n\\nSVM: Maximum margin separating hyperplane\\nSVM-Anova: SVM with univariate feature selection\\nPlot classification probability\\n---------new doc---------\\nExamples\\n\\nSVM: Maximum margin separating hyperplane\\nSVM-Anova: SVM with univariate feature selection\\nPlot classification probability\\n\\n\\n1.4.1.1. Multi-class classification#\\nSVC and NuSVC implement the “one-versus-one”\\napproach for multi-class classification. In total,\\nn_classes * (n_classes - 1) / 2\\nclassifiers are constructed and each one trains data from two classes.\\nTo provide a consistent interface with other classifiers, the\\ndecision_function_shape option allows to monotonically transform the\\nresults of the “one-versus-one” classifiers to a “one-vs-rest” decision\\nfunction of shape (n_samples, n_classes), which is the default setting\\nof the parameter (default=’ovr’).\\n>>> X = [[0], [1], [2], [3]]\\n>>> Y = [0, 1, 2, 3]\\n>>> clf = svm.SVC(decision_function_shape='ovo')\\n>>> clf.fit(X, Y)\\nSVC(decision_function_shape='ovo')\\n>>> dec = clf.decision_function([[1]])\\n>>> dec.shape[1] # 6 classes: 4*3/2 = 6\\n6\\n>>> clf.decision_function_shape = \\\"ovr\\\"\\n>>> dec = clf.decision_function([[1]])\\n>>> dec.shape[1] # 4 classes\\n4\\n---------new doc---------\\nOn the other hand, LinearSVC implements “one-vs-the-rest”\\nmulti-class strategy, thus training n_classes models.\\n>>> lin_clf = svm.LinearSVC()\\n>>> lin_clf.fit(X, Y)\\nLinearSVC()\\n>>> dec = lin_clf.decision_function([[1]])\\n>>> dec.shape[1]\\n4\\n\\n\\nSee Mathematical formulation for a complete description of\\nthe decision function.\\n---------new doc---------\\nDetails on multi-class strategies#\\nNote that the LinearSVC also implements an alternative multi-class\\nstrategy, the so-called multi-class SVM formulated by Crammer and Singer\\n[16], by using the option multi_class='crammer_singer'. In practice,\\none-vs-rest classification is usually preferred, since the results are mostly\\nsimilar, but the runtime is significantly less.\\nFor “one-vs-rest” LinearSVC the attributes coef_ and intercept_\\nhave the shape (n_classes, n_features) and (n_classes,) respectively.\\nEach row of the coefficients corresponds to one of the n_classes\\n“one-vs-rest” classifiers and similar for the intercepts, in the\\norder of the “one” class.\\nIn the case of “one-vs-one” SVC and NuSVC, the layout of\\nthe attributes is a little more involved. In the case of a linear\\nkernel, the attributes coef_ and intercept_ have the shape\\n(n_classes * (n_classes - 1) / 2, n_features) and (n_classes *\\n(n_classes - 1) / 2) respectively. This is similar to the layout for\\nLinearSVC described above, with each row now corresponding\\nto a binary classifier. The order for classes\\n0 to n is “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . .\\n. “n-1 vs n”.\\nThe shape of dual_coef_ is (n_classes-1, n_SV) with\\n---------new doc---------\\n\\\\(\\\\alpha^{0}_{0,1}\\\\)\\n\\\\(\\\\alpha^{1}_{0,1}\\\\)\\n\\\\(\\\\alpha^{2}_{0,1}\\\\)\\n\\\\(\\\\alpha^{0}_{1,0}\\\\)\\n\\\\(\\\\alpha^{1}_{1,0}\\\\)\\n\\\\(\\\\alpha^{0}_{2,0}\\\\)\\n\\\\(\\\\alpha^{1}_{2,0}\\\\)\\n\\n\\\\(\\\\alpha^{0}_{0,2}\\\\)\\n\\\\(\\\\alpha^{1}_{0,2}\\\\)\\n\\\\(\\\\alpha^{2}_{0,2}\\\\)\\n\\\\(\\\\alpha^{0}_{1,2}\\\\)\\n\\\\(\\\\alpha^{1}_{1,2}\\\\)\\n\\\\(\\\\alpha^{0}_{2,1}\\\\)\\n\\\\(\\\\alpha^{1}_{2,1}\\\\)\\n\\nCoefficients\\nfor SVs of class 0\\nCoefficients\\nfor SVs of class 1\\nCoefficients\\nfor SVs of class 2\\n\\n\\n\\n\\n\\nExamples\\n\\nPlot different SVM classifiers in the iris dataset\\n\\n\\n\\n1.4.1.2. Scores and probabilities#\\nThe decision_function method of SVC and NuSVC gives\\nper-class scores for each sample (or a single score per sample in the binary\\ncase). When the constructor option probability is set to True,\\nclass membership probability estimates (from the methods predict_proba and\\npredict_log_proba) are enabled. In the binary case, the probabilities are\\ncalibrated using Platt scaling [9]: logistic regression on the SVM’s scores,\\nfit by an additional cross-validation on the training data.\\nIn the multiclass case, this is extended as per [10].\\n---------new doc---------\\nNote\\nThe same probability calibration procedure is available for all estimators\\nvia the CalibratedClassifierCV (see\\nProbability calibration). In the case of SVC and NuSVC, this\\nprocedure is builtin in libsvm which is used under the hood, so it does\\nnot rely on scikit-learn’s\\nCalibratedClassifierCV.\\n\\nThe cross-validation involved in Platt scaling\\nis an expensive operation for large datasets.\\nIn addition, the probability estimates may be inconsistent with the scores:\\n\\nthe “argmax” of the scores may not be the argmax of the probabilities\\nin binary classification, a sample may be labeled by predict as\\nbelonging to the positive class even if the output of predict_proba is\\nless than 0.5; and similarly, it could be labeled as negative even if the\\noutput of predict_proba is more than 0.5.\\n\\nPlatt’s method is also known to have theoretical issues.\\nIf confidence scores are required, but these do not have to be probabilities,\\nthen it is advisable to set probability=False\\nand use decision_function instead of predict_proba.\\nPlease note that when decision_function_shape='ovr' and n_classes > 2,\\nunlike decision_function, the predict method does not try to break ties\\nby default. You can set break_ties=True for the output of predict to be\\nthe same as np.argmax(clf.decision_function(...), axis=1), otherwise the\\nfirst class among the tied classes will always be returned; but have in mind\\nthat it comes with a computational cost. See\\nSVM Tie Breaking Example for an example on\\ntie breaking.\\n---------new doc---------\\n1.4.1.3. Unbalanced problems#\\nIn problems where it is desired to give more importance to certain\\nclasses or certain individual samples, the parameters class_weight and\\nsample_weight can be used.\\nSVC (but not NuSVC) implements the parameter\\nclass_weight in the fit method. It’s a dictionary of the form\\n{class_label : value}, where value is a floating point number > 0\\nthat sets the parameter C of class class_label to C * value.\\nThe figure below illustrates the decision boundary of an unbalanced problem,\\nwith and without weight correction.\\n\\n\\n\\n\\nSVC, NuSVC, SVR, NuSVR, LinearSVC,\\nLinearSVR and OneClassSVM implement also weights for\\nindividual samples in the fit method through the sample_weight parameter.\\nSimilar to class_weight, this sets the parameter C for the i-th\\nexample to C * sample_weight[i], which will encourage the classifier to\\nget these samples right. The figure below illustrates the effect of sample\\nweighting on the decision boundary. The size of the circles is proportional\\nto the sample weights:\\n\\n\\n\\n\\nExamples\\n\\nSVM: Separating hyperplane for unbalanced classes\\nSVM: Weighted samples\\n---------new doc---------\\nSee section Preprocessing data for more details on scaling and\\nnormalization.\\n\\n\\n\\nRegarding the shrinking parameter, quoting [12]: We found that if the\\nnumber of iterations is large, then shrinking can shorten the training\\ntime. However, if we loosely solve the optimization problem (e.g., by\\nusing a large stopping tolerance), the code without using shrinking may\\nbe much faster\\nParameter nu in NuSVC/OneClassSVM/NuSVR\\napproximates the fraction of training errors and support vectors.\\nIn SVC, if the data is unbalanced (e.g. many\\npositive and few negative), set class_weight='balanced' and/or try\\ndifferent penalty parameters C.\\nRandomness of the underlying implementations: The underlying\\nimplementations of SVC and NuSVC use a random number\\ngenerator only to shuffle the data for probability estimation (when\\nprobability is set to True). This randomness can be controlled\\nwith the random_state parameter. If probability is set to False\\nthese estimators are not random and random_state has no effect on the\\nresults. The underlying OneClassSVM implementation is similar to\\nthe ones of SVC and NuSVC. As no probability estimation\\nis provided for OneClassSVM, it is not random.\\nThe underlying LinearSVC implementation uses a random number\\ngenerator to select features when fitting the model with a dual coordinate\\ndescent (i.e. when dual is set to True). It is thus not uncommon\\nto have slightly different results for the same input data. If that\\nhappens, try with a smaller tol parameter. This randomness can also be\\ncontrolled with the random_state parameter. When dual is\\nset to False the underlying implementation of LinearSVC is\\nnot random and random_state has no effect on the results.\\n---------new doc---------\\nIn both cases, the criterion is evaluated once by epoch, and the algorithm stops\\nwhen the criterion does not improve n_iter_no_change times in a row. The\\nimprovement is evaluated with absolute tolerance tol, and the algorithm\\nstops in any case after a maximum number of iteration max_iter.\\nSee Early stopping of Stochastic Gradient Descent for an\\nexample of the effects of early stopping.\\n\\n\\n1.5.7. Tips on Practical Use#\\n\\nStochastic Gradient Descent is sensitive to feature scaling, so it\\nis highly recommended to scale your data. For example, scale each\\nattribute on the input vector X to [0,1] or [-1,+1], or standardize\\nit to have mean 0 and variance 1. Note that the same scaling must be\\napplied to the test vector to obtain meaningful results. This can be easily\\ndone using StandardScaler:\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nscaler.fit(X_train)  # Don't cheat - fit only on training data\\nX_train = scaler.transform(X_train)\\nX_test = scaler.transform(X_test)  # apply same transformation to test data\\n\\n# Or better yet: use a pipeline!\\nfrom sklearn.pipeline import make_pipeline\\nest = make_pipeline(StandardScaler(), SGDClassifier())\\nest.fit(X_train)\\nest.predict(X_test)\\n\\n\\nIf your attributes have an intrinsic scale (e.g. word frequencies or\\nindicator features) scaling is not needed.\\n---------new doc---------\\nProbability calculation#\\nThe probability of category \\\\(t\\\\) in feature \\\\(i\\\\) given class\\n\\\\(c\\\\) is estimated as:\\n\\n\\\\[P(x_i = t \\\\mid y = c \\\\: ;\\\\, \\\\alpha) = \\\\frac{ N_{tic} + \\\\alpha}{N_{c} +\\n                                       \\\\alpha n_i},\\\\]\\nwhere \\\\(N_{tic} = |\\\\{j \\\\in J \\\\mid x_{ij} = t, y_j = c\\\\}|\\\\) is the number\\nof times category \\\\(t\\\\) appears in the samples \\\\(x_{i}\\\\), which belong\\nto class \\\\(c\\\\), \\\\(N_{c} = |\\\\{ j \\\\in J\\\\mid y_j = c\\\\}|\\\\) is the number\\nof samples with class c, \\\\(\\\\alpha\\\\) is a smoothing parameter and\\n\\\\(n_i\\\\) is the number of available categories of feature \\\\(i\\\\).\\n\\nCategoricalNB assumes that the sample matrix \\\\(X\\\\) is encoded (for\\ninstance with the help of OrdinalEncoder) such\\nthat all categories for each feature \\\\(i\\\\) are represented with numbers\\n\\\\(0, ..., n_i - 1\\\\) where \\\\(n_i\\\\) is the number of available categories\\nof feature \\\\(i\\\\).\\n---------new doc---------\\nSome advantages of decision trees are:\\n\\nSimple to understand and to interpret. Trees can be visualized.\\nRequires little data preparation. Other techniques often require data\\nnormalization, dummy variables need to be created and blank values to\\nbe removed. Some tree and algorithm combinations support\\nmissing values.\\nThe cost of using the tree (i.e., predicting data) is logarithmic in the\\nnumber of data points used to train the tree.\\nAble to handle both numerical and categorical data. However, the scikit-learn\\nimplementation does not support categorical variables for now. Other\\ntechniques are usually specialized in analyzing datasets that have only one type\\nof variable. See algorithms for more\\ninformation.\\nAble to handle multi-output problems.\\nUses a white box model. If a given situation is observable in a model,\\nthe explanation for the condition is easily explained by boolean logic.\\nBy contrast, in a black box model (e.g., in an artificial neural\\nnetwork), results may be more difficult to interpret.\\nPossible to validate a model using statistical tests. That makes it\\npossible to account for the reliability of the model.\\nPerforms well even if its assumptions are somewhat violated by\\nthe true model from which the data were generated.\\n\\nThe disadvantages of decision trees include:\\n---------new doc---------\\nThe disadvantages of decision trees include:\\n\\nDecision-tree learners can create over-complex trees that do not\\ngeneralize the data well. This is called overfitting. Mechanisms\\nsuch as pruning, setting the minimum number of samples required\\nat a leaf node or setting the maximum depth of the tree are\\nnecessary to avoid this problem.\\nDecision trees can be unstable because small variations in the\\ndata might result in a completely different tree being generated.\\nThis problem is mitigated by using decision trees within an\\nensemble.\\nPredictions of decision trees are neither smooth nor continuous, but\\npiecewise constant approximations as seen in the above figure. Therefore,\\nthey are not good at extrapolation.\\nThe problem of learning an optimal decision tree is known to be\\nNP-complete under several aspects of optimality and even for simple\\nconcepts. Consequently, practical decision-tree learning algorithms\\nare based on heuristic algorithms such as the greedy algorithm where\\nlocally optimal decisions are made at each node. Such algorithms\\ncannot guarantee to return the globally optimal decision tree.  This\\ncan be mitigated by training multiple trees in an ensemble learner,\\nwhere the features and samples are randomly sampled with replacement.\\nThere are concepts that are hard to learn because decision trees\\ndo not express them easily, such as XOR, parity or multiplexer problems.\\nDecision tree learners create biased trees if some classes dominate.\\nIt is therefore recommended to balance the dataset prior to fitting\\nwith the decision tree.\\n---------new doc---------\\n1.10.1. Classification#\\nDecisionTreeClassifier is a class capable of performing multi-class\\nclassification on a dataset.\\nAs with other classifiers, DecisionTreeClassifier takes as input two arrays:\\nan array X, sparse or dense, of shape (n_samples, n_features) holding the\\ntraining samples, and an array Y of integer values, shape (n_samples,),\\nholding the class labels for the training samples:\\n>>> from sklearn import tree\\n>>> X = [[0, 0], [1, 1]]\\n>>> Y = [0, 1]\\n>>> clf = tree.DecisionTreeClassifier()\\n>>> clf = clf.fit(X, Y)\\n\\n\\nAfter being fitted, the model can then be used to predict the class of samples:\\n>>> clf.predict([[2., 2.]])\\narray([1])\\n\\n\\nIn case that there are multiple classes with the same and highest\\nprobability, the classifier will predict the class with the lowest index\\namongst those classes.\\nAs an alternative to outputting a specific class, the probability of each class\\ncan be predicted, which is the fraction of training samples of the class in a\\nleaf:\\n>>> clf.predict_proba([[2., 2.]])\\narray([[0., 1.]])\\n---------new doc---------\\nIn case that there are multiple classes with the same and highest\\nprobability, the classifier will predict the class with the lowest index\\namongst those classes.\\nAs an alternative to outputting a specific class, the probability of each class\\ncan be predicted, which is the fraction of training samples of the class in a\\nleaf:\\n>>> clf.predict_proba([[2., 2.]])\\narray([[0., 1.]])\\n\\n\\nDecisionTreeClassifier is capable of both binary (where the\\nlabels are [-1, 1]) classification and multiclass (where the labels are\\n[0, …, K-1]) classification.\\nUsing the Iris dataset, we can construct a tree as follows:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn import tree\\n>>> iris = load_iris()\\n>>> X, y = iris.data, iris.target\\n>>> clf = tree.DecisionTreeClassifier()\\n>>> clf = clf.fit(X, y)\\n\\n\\nOnce trained, you can plot the tree with the plot_tree function:\\n>>> tree.plot_tree(clf)\\n[...]\\n---------new doc---------\\nAlternatively, the tree can also be exported in textual format with the\\nfunction export_text. This method doesn’t require the installation\\nof external libraries and is more compact:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> from sklearn.tree import export_text\\n>>> iris = load_iris()\\n>>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\\n>>> decision_tree = decision_tree.fit(iris.data, iris.target)\\n>>> r = export_text(decision_tree, feature_names=iris['feature_names'])\\n>>> print(r)\\n|--- petal width (cm) <= 0.80\\n|   |--- class: 0\\n|--- petal width (cm) >  0.80\\n|   |--- petal width (cm) <= 1.75\\n|   |   |--- class: 1\\n|   |--- petal width (cm) >  1.75\\n|   |   |--- class: 2\\n\\n\\n\\nExamples\\n\\nPlot the decision surface of decision trees trained on the iris dataset\\nUnderstanding the decision tree structure\\n\\n\\n\\n1.10.2. Regression#\\n---------new doc---------\\nExamples\\n\\nPlot the decision surface of decision trees trained on the iris dataset\\nUnderstanding the decision tree structure\\n\\n\\n\\n1.10.2. Regression#\\n\\n\\n\\n\\nDecision trees can also be applied to regression problems, using the\\nDecisionTreeRegressor class.\\nAs in the classification setting, the fit method will take as argument arrays X\\nand y, only that in this case y is expected to have floating point values\\ninstead of integer values:\\n>>> from sklearn import tree\\n>>> X = [[0, 0], [2, 2]]\\n>>> y = [0.5, 2.5]\\n>>> clf = tree.DecisionTreeRegressor()\\n>>> clf = clf.fit(X, y)\\n>>> clf.predict([[1, 1]])\\narray([0.5])\\n\\n\\nExamples\\n\\nDecision Tree Regression\\n---------new doc---------\\nExamples\\n\\nDecision Tree Regression\\n\\n\\n\\n1.10.3. Multi-output problems#\\nA multi-output problem is a supervised learning problem with several outputs\\nto predict, that is when Y is a 2d array of shape (n_samples, n_outputs).\\nWhen there is no correlation between the outputs, a very simple way to solve\\nthis kind of problem is to build n independent models, i.e. one for each\\noutput, and then to use those models to independently predict each one of the n\\noutputs. However, because it is likely that the output values related to the\\nsame input are themselves correlated, an often better way is to build a single\\nmodel capable of predicting simultaneously all n outputs. First, it requires\\nlower training time since only a single estimator is built. Second, the\\ngeneralization accuracy of the resulting estimator may often be increased.\\nWith regard to decision trees, this strategy can readily be used to support\\nmulti-output problems. This requires the following changes:\\n\\nStore n output values in leaves, instead of 1;\\nUse splitting criteria that compute the average reduction across all\\nn outputs.\\n\\nThis module offers support for multi-output problems by implementing this\\nstrategy in both DecisionTreeClassifier and\\nDecisionTreeRegressor. If a decision tree is fit on an output array Y\\nof shape (n_samples, n_outputs) then the resulting estimator will:\\n\\nOutput n_output values upon predict;\\nOutput a list of n_output arrays of class probabilities upon\\npredict_proba.\\n\\nThe use of multi-output trees for regression is demonstrated in\\nDecision Tree Regression. In this example, the input\\nX is a single real value and the outputs Y are the sine and cosine of X.\\n---------new doc---------\\nOutput n_output values upon predict;\\nOutput a list of n_output arrays of class probabilities upon\\npredict_proba.\\n\\nThe use of multi-output trees for regression is demonstrated in\\nDecision Tree Regression. In this example, the input\\nX is a single real value and the outputs Y are the sine and cosine of X.\\n\\n\\n\\n\\nThe use of multi-output trees for classification is demonstrated in\\nFace completion with a multi-output estimators. In this example, the inputs\\nX are the pixels of the upper half of faces and the outputs Y are the pixels of\\nthe lower half of those faces.\\n\\n\\n\\n\\nExamples\\n\\nFace completion with a multi-output estimators\\n\\nReferences\\n\\nM. Dumont et al,  Fast multi-class image annotation with random subwindows\\nand multiple output randomized trees,\\nInternational Conference on Computer Vision Theory and Applications 2009\\n---------new doc---------\\n1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART#\\nWhat are all the various decision tree algorithms and how do they differ\\nfrom each other? Which one is implemented in scikit-learn?\\n\\n\\nVarious decision tree algorithms#\\nID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan.\\nThe algorithm creates a multiway tree, finding for each node (i.e. in\\na greedy manner) the categorical feature that will yield the largest\\ninformation gain for categorical targets. Trees are grown to their\\nmaximum size and then a pruning step is usually applied to improve the\\nability of the tree to generalize to unseen data.\\nC4.5 is the successor to ID3 and removed the restriction that features\\nmust be categorical by dynamically defining a discrete attribute (based\\non numerical variables) that partitions the continuous attribute value\\ninto a discrete set of intervals. C4.5 converts the trained trees\\n(i.e. the output of the ID3 algorithm) into sets of if-then rules.\\nThe accuracy of each rule is then evaluated to determine the order\\nin which they should be applied. Pruning is done by removing a rule’s\\nprecondition if the accuracy of the rule improves without it.\\nC5.0 is Quinlan’s latest version release under a proprietary license.\\nIt uses less memory and builds smaller rulesets than C4.5 while being\\nmore accurate.\\nCART (Classification and Regression Trees) is very similar to C4.5, but\\nit differs in that it supports numerical target variables (regression) and\\ndoes not compute rule sets. CART constructs binary trees using the feature\\nand threshold that yield the largest information gain at each node.\\n---------new doc---------\\nscikit-learn uses an optimized version of the CART algorithm; however, the\\nscikit-learn implementation does not support categorical variables for now.\\n\\n\\n1.10.7. Mathematical formulation#\\nGiven training vectors \\\\(x_i \\\\in R^n\\\\), i=1,…, l and a label vector\\n\\\\(y \\\\in R^l\\\\), a decision tree recursively partitions the feature space\\nsuch that the samples with the same labels or similar target values are grouped\\ntogether.\\nLet the data at node \\\\(m\\\\) be represented by \\\\(Q_m\\\\) with \\\\(n_m\\\\)\\nsamples. For each candidate split \\\\(\\\\theta = (j, t_m)\\\\) consisting of a\\nfeature \\\\(j\\\\) and threshold \\\\(t_m\\\\), partition the data into\\n\\\\(Q_m^{left}(\\\\theta)\\\\) and \\\\(Q_m^{right}(\\\\theta)\\\\) subsets\\n\\n\\\\[ \\\\begin{align}\\\\begin{aligned}Q_m^{left}(\\\\theta) = \\\\{(x, y) | x_j \\\\leq t_m\\\\}\\\\\\\\Q_m^{right}(\\\\theta) = Q_m \\\\setminus Q_m^{left}(\\\\theta)\\\\end{aligned}\\\\end{align} \\\\]\\nThe quality of a candidate split of node \\\\(m\\\\) is then computed using an\\nimpurity function or loss function \\\\(H()\\\\), the choice of which depends on\\nthe task being solved (classification or regression)\\n---------new doc---------\\n1.10.8. Missing Values Support#\\nDecisionTreeClassifier, DecisionTreeRegressor\\nhave built-in support for missing values using splitter='best', where\\nthe splits are determined in a greedy fashion.\\nExtraTreeClassifier, and ExtraTreeRegressor have built-in\\nsupport for missing values for splitter='random', where the splits\\nare determined randomly. For more details on how the splitter differs on\\nnon-missing values, see the Forest section.\\nThe criterion supported when there are missing-values are\\n'gini', 'entropy’, or 'log_loss', for classification or\\n'squared_error', 'friedman_mse', or 'poisson' for regression.\\nFirst we will describe how DecisionTreeClassifier, DecisionTreeRegressor\\nhandle missing-values in the data.\\nFor each potential threshold on the non-missing data, the splitter will evaluate\\nthe split with all the missing values going to the left node or the right node.\\nDecisions are made as follows:\\n\\nBy default when predicting, the samples with missing values are classified\\nwith the class used in the split found during training:\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n>>> tree.predict(X)\\narray([0, 0, 1, 1])\\n---------new doc---------\\n>>> X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n>>> tree.predict(X)\\narray([0, 0, 1, 1])\\n\\n\\n\\nIf the criterion evaluation is the same for both nodes,\\nthen the tie for missing value at predict time is broken by going to the\\nright node. The splitter also checks the split where all the missing\\nvalues go to one child and non-missing values go to the other:\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n\\n>>> X_test = np.array([np.nan]).reshape(-1, 1)\\n>>> tree.predict(X_test)\\narray([1])\\n\\n\\n\\nIf no missing values are seen during training for a given feature, then during\\nprediction missing values are mapped to the child with the most samples:\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([0, 1, 2, 3]).reshape(-1, 1)\\n>>> y = [0, 1, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n---------new doc---------\\n>>> X = np.array([0, 1, 2, 3]).reshape(-1, 1)\\n>>> y = [0, 1, 1, 1]\\n\\n>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\\n\\n>>> X_test = np.array([np.nan]).reshape(-1, 1)\\n>>> tree.predict(X_test)\\narray([1])\\n\\n\\n\\n\\nExtraTreeClassifier, and ExtraTreeRegressor handle missing values\\nin a slightly different way. When splitting a node, a random threshold will be chosen\\nto split the non-missing values on. Then the non-missing values will be sent to the\\nleft and right child based on the randomly selected threshold, while the missing\\nvalues will also be randomly sent to the left or right child. This is repeated for\\nevery feature considered at each split. The best split among these is chosen.\\nDuring prediction, the treatment of missing-values is the same as that of the\\ndecision tree:\\n\\nBy default when predicting, the samples with missing values are classified\\nwith the class used in the split found during training.\\nIf no missing values are seen during training for a given feature, then during\\nprediction missing values are mapped to the child with the most samples.\\n\\n\\n\\n1.10.9. Minimal Cost-Complexity Pruning#\\nMinimal cost-complexity pruning is an algorithm used to prune a tree to avoid\\nover-fitting, described in Chapter 3 of [BRE]. This algorithm is parameterized\\nby \\\\(\\\\alpha\\\\ge0\\\\) known as the complexity parameter. The complexity\\nparameter is used to define the cost-complexity measure, \\\\(R_\\\\alpha(T)\\\\) of\\na given tree \\\\(T\\\\):\\n---------new doc---------\\n1.11.1. Gradient-boosted trees#\\nGradient Tree Boosting\\nor Gradient Boosted Decision Trees (GBDT) is a generalization\\nof boosting to arbitrary differentiable loss functions, see the seminal work of\\n[Friedman2001]. GBDT is an excellent model for both regression and\\nclassification, in particular for tabular data.\\n\\nGradientBoostingClassifier vs HistGradientBoostingClassifier\\nScikit-learn provides two implementations of gradient-boosted trees:\\nHistGradientBoostingClassifier vs\\nGradientBoostingClassifier for classification, and the\\ncorresponding classes for regression. The former can be orders of\\nmagnitude faster than the latter when the number of samples is\\nlarger than tens of thousands of samples.\\nMissing values and categorical data are natively supported by the\\nHist… version, removing the need for additional preprocessing such as\\nimputation.\\nGradientBoostingClassifier and\\nGradientBoostingRegressor, might be preferred for small sample\\nsizes since binning may lead to split points that are too approximate\\nin this setting.\\n---------new doc---------\\n1.11.1.1. Histogram-Based Gradient Boosting#\\nScikit-learn 0.21 introduced two new implementations of\\ngradient boosted trees, namely HistGradientBoostingClassifier\\nand HistGradientBoostingRegressor, inspired by\\nLightGBM (See [LightGBM]).\\nThese histogram-based estimators can be orders of magnitude faster\\nthan GradientBoostingClassifier and\\nGradientBoostingRegressor when the number of samples is larger\\nthan tens of thousands of samples.\\nThey also have built-in support for missing values, which avoids the need\\nfor an imputer.\\nThese fast estimators first bin the input samples X into\\ninteger-valued bins (typically 256 bins) which tremendously reduces the\\nnumber of splitting points to consider, and allows the algorithm to\\nleverage integer-based data structures (histograms) instead of relying on\\nsorted continuous values when building the trees. The API of these\\nestimators is slightly different, and some of the features from\\nGradientBoostingClassifier and GradientBoostingRegressor\\nare not yet supported, for instance some loss functions.\\nExamples\\n\\nPartial Dependence and Individual Conditional Expectation Plots\\nComparing Random Forests and Histogram Gradient Boosting models\\n\\n\\n1.11.1.1.1. Usage#\\nMost of the parameters are unchanged from\\nGradientBoostingClassifier and GradientBoostingRegressor.\\nOne exception is the max_iter parameter that replaces n_estimators, and\\ncontrols the number of iterations of the boosting process:\\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\\n>>> from sklearn.datasets import make_hastie_10_2\\n---------new doc---------\\nNote that early-stopping is enabled by default if the number of samples is\\nlarger than 10,000. The early-stopping behaviour is controlled via the\\nearly_stopping, scoring, validation_fraction,\\nn_iter_no_change, and tol parameters. It is possible to early-stop\\nusing an arbitrary scorer, or just the training or validation loss.\\nNote that for technical reasons, using a callable as a scorer is significantly slower\\nthan using the loss. By default, early-stopping is performed if there are at least\\n10,000 samples in the training set, using the validation loss.\\n\\n\\n1.11.1.1.2. Missing values support#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor have built-in support for missing\\nvalues (NaNs).\\nDuring training, the tree grower learns at each split point whether samples\\nwith missing values should go to the left or right child, based on the\\npotential gain. When predicting, samples with missing values are assigned to\\nthe left or right child consequently:\\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\\n>>> import numpy as np\\n\\n>>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\\n>>> gbdt.predict(X)\\narray([0, 0, 1, 1])\\n---------new doc---------\\n>>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\\n>>> y = [0, 0, 1, 1]\\n\\n>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\\n>>> gbdt.predict(X)\\narray([0, 0, 1, 1])\\n\\n\\nWhen the missingness pattern is predictive, the splits can be performed on\\nwhether the feature value is missing or not:\\n>>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)\\n>>> y = [0, 1, 0, 0, 1]\\n>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,\\n...                                       max_depth=2,\\n...                                       learning_rate=1,\\n...                                       max_iter=1).fit(X, y)\\n>>> gbdt.predict(X)\\narray([0, 1, 0, 0, 1])\\n\\n\\nIf no missing values were encountered for a given feature during training,\\nthen samples with missing values are mapped to whichever child has the most\\nsamples.\\nExamples\\n\\nFeatures in Histogram Gradient Boosting Trees\\n---------new doc---------\\nIf no missing values were encountered for a given feature during training,\\nthen samples with missing values are mapped to whichever child has the most\\nsamples.\\nExamples\\n\\nFeatures in Histogram Gradient Boosting Trees\\n\\n\\n\\n1.11.1.1.3. Sample weight support#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor support sample weights during\\nfit.\\nThe following toy example demonstrates that samples with a sample weight of zero are ignored:\\n>>> X = [[1, 0],\\n...      [1, 0],\\n...      [1, 0],\\n...      [0, 1]]\\n>>> y = [0, 0, 1, 0]\\n>>> # ignore the first 2 training samples by setting their weight to 0\\n>>> sample_weight = [0, 0, 1, 1]\\n>>> gb = HistGradientBoostingClassifier(min_samples_leaf=1)\\n>>> gb.fit(X, y, sample_weight=sample_weight)\\nHistGradientBoostingClassifier(...)\\n>>> gb.predict([[1, 0]])\\narray([1])\\n>>> gb.predict_proba([[1, 0]])[0, 1]\\n0.99...\\n\\n\\nAs you can see, the [1, 0] is comfortably classified as 1 since the first\\ntwo samples are ignored due to their sample weights.\\nImplementation detail: taking sample weights into account amounts to\\nmultiplying the gradients (and the hessians) by the sample weights. Note that\\nthe binning stage (specifically the quantiles computation) does not take the\\nweights into account.\\n---------new doc---------\\nSplit finding with categorical features#\\nThe canonical way of considering categorical splits in a tree is to consider\\nall of the \\\\(2^{K - 1} - 1\\\\) partitions, where \\\\(K\\\\) is the number of\\ncategories. This can quickly become prohibitive when \\\\(K\\\\) is large.\\nFortunately, since gradient boosting trees are always regression trees (even\\nfor classification problems), there exist a faster strategy that can yield\\nequivalent splits. First, the categories of a feature are sorted according to\\nthe variance of the target, for each category k. Once the categories are\\nsorted, one can consider continuous partitions, i.e. treat the categories\\nas if they were ordered continuous values (see Fisher [Fisher1958] for a\\nformal proof). As a result, only \\\\(K - 1\\\\) splits need to be considered\\ninstead of \\\\(2^{K - 1} - 1\\\\). The initial sorting is a\\n\\\\(\\\\mathcal{O}(K \\\\log(K))\\\\) operation, leading to a total complexity of\\n\\\\(\\\\mathcal{O}(K \\\\log(K) + K)\\\\), instead of \\\\(\\\\mathcal{O}(2^K)\\\\).\\n\\nExamples\\n\\nCategorical Feature Support in Gradient Boosting\\n\\n\\n\\n1.11.1.1.5. Monotonic Constraints#\\nDepending on the problem at hand, you may have prior knowledge indicating\\nthat a given feature should in general have a positive (or negative) effect\\non the target value. For example, all else being equal, a higher credit\\nscore should increase the probability of getting approved for a loan.\\nMonotonic constraints allow you to incorporate such prior knowledge into the\\nmodel.\\nFor a predictor \\\\(F\\\\) with two features:\\n\\na monotonic increase constraint is a constraint of the form:\\n---------new doc---------\\n1.11.1.1.8. Why it’s faster#\\nThe bottleneck of a gradient boosting procedure is building the decision\\ntrees. Building a traditional decision tree (as in the other GBDTs\\nGradientBoostingClassifier and GradientBoostingRegressor)\\nrequires sorting the samples at each node (for\\neach feature). Sorting is needed so that the potential gain of a split point\\ncan be computed efficiently. Splitting a single node has thus a complexity\\nof \\\\(\\\\mathcal{O}(n_\\\\text{features} \\\\times n \\\\log(n))\\\\) where \\\\(n\\\\)\\nis the number of samples at the node.\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor, in contrast, do not require sorting the\\nfeature values and instead use a data-structure called a histogram, where the\\nsamples are implicitly ordered. Building a histogram has a\\n\\\\(\\\\mathcal{O}(n)\\\\) complexity, so the node splitting procedure has a\\n\\\\(\\\\mathcal{O}(n_\\\\text{features} \\\\times n)\\\\) complexity, much smaller\\nthan the previous one. In addition, instead of considering \\\\(n\\\\) split\\npoints, we consider only max_bins split points, which might be much\\nsmaller.\\nIn order to build histograms, the input data X needs to be binned into\\ninteger-valued bins. This binning procedure does require sorting the feature\\nvalues, but it only happens once at the very beginning of the boosting process\\n(not at each node, like in GradientBoostingClassifier and\\nGradientBoostingRegressor).\\nFinally, many parts of the implementation of\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor are parallelized.\\nReferences\\n---------new doc---------\\n[XGBoost]\\n(1,2,3)\\nTianqi Chen, Carlos Guestrin, “XGBoost: A Scalable Tree\\nBoosting System”\\n\\n\\n[LightGBM]\\nKe et. al. “LightGBM: A Highly Efficient Gradient\\nBoostingDecision Tree”\\n\\n\\n[Fisher1958]\\nFisher, W.D. (1958). “On Grouping for Maximum Homogeneity”\\nJournal of the American Statistical Association, 53, 789-798.\\n\\n\\n\\n\\n\\n1.11.1.2. GradientBoostingClassifier and GradientBoostingRegressor#\\nThe usage and the parameters of GradientBoostingClassifier and\\nGradientBoostingRegressor are described below. The 2 most important\\nparameters of these estimators are n_estimators and learning_rate.\\n\\n\\nClassification#\\nGradientBoostingClassifier supports both binary and multi-class\\nclassification.\\nThe following example shows how to fit a gradient boosting classifier\\nwith 100 decision stumps as weak learners:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> X_train, X_test = X[:2000], X[2000:]\\n>>> y_train, y_test = y[:2000], y[2000:]\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> X_train, X_test = X[:2000], X[2000:]\\n>>> y_train, y_test = y[:2000], y[2000:]\\n\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.913...\\n\\n\\nThe number of weak learners (i.e. regression trees) is controlled by the\\nparameter n_estimators; The size of each tree can be controlled either by setting the tree\\ndepth via max_depth or by setting the number of leaf nodes via\\nmax_leaf_nodes. The learning_rate is a hyper-parameter in the range\\n(0.0, 1.0] that controls overfitting via shrinkage .\\n\\nNote\\nClassification with more than 2 classes requires the induction\\nof n_classes regression trees at each iteration,\\nthus, the total number of induced trees equals\\nn_classes * n_estimators. For datasets with a large number\\nof classes we strongly recommend to use\\nHistGradientBoostingClassifier as an alternative to\\nGradientBoostingClassifier .\\n---------new doc---------\\nNote\\nClassification with more than 2 classes requires the induction\\nof n_classes regression trees at each iteration,\\nthus, the total number of induced trees equals\\nn_classes * n_estimators. For datasets with a large number\\nof classes we strongly recommend to use\\nHistGradientBoostingClassifier as an alternative to\\nGradientBoostingClassifier .\\n\\n\\n\\n\\nRegression#\\nGradientBoostingRegressor supports a number of\\ndifferent loss functions\\nfor regression which can be specified via the argument\\nloss; the default loss function for regression is squared error\\n('squared_error').\\n>>> import numpy as np\\n>>> from sklearn.metrics import mean_squared_error\\n>>> from sklearn.datasets import make_friedman1\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n\\n>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\\n>>> X_train, X_test = X[:200], X[200:]\\n>>> y_train, y_test = y[:200], y[200:]\\n>>> est = GradientBoostingRegressor(\\n...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\\n...     loss='squared_error'\\n... ).fit(X_train, y_train)\\n>>> mean_squared_error(y_test, est.predict(X_test))\\n5.00...\\n---------new doc---------\\nThe figure below shows the results of applying GradientBoostingRegressor\\nwith least squares loss and 500 base learners to the diabetes dataset\\n(sklearn.datasets.load_diabetes).\\nThe plot shows the train and test error at each iteration.\\nThe train error at each iteration is stored in the\\ntrain_score_ attribute of the gradient boosting model.\\nThe test error at each iterations can be obtained\\nvia the staged_predict method which returns a\\ngenerator that yields the predictions at each stage. Plots like these can be used\\nto determine the optimal number of trees (i.e. n_estimators) by early stopping.\\n\\n\\n\\n\\n\\nExamples\\n\\nGradient Boosting regression\\nGradient Boosting Out-of-Bag estimates\\n\\n\\n1.11.1.2.1. Fitting additional weak-learners#\\nBoth GradientBoostingRegressor and GradientBoostingClassifier\\nsupport warm_start=True which allows you to add more estimators to an already\\nfitted model.\\n>>> import numpy as np\\n>>> from sklearn.metrics import mean_squared_error\\n>>> from sklearn.datasets import make_friedman1\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n---------new doc---------\\n>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\\n>>> X_train, X_test = X[:200], X[200:]\\n>>> y_train, y_test = y[:200], y[200:]\\n>>> est = GradientBoostingRegressor(\\n...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\\n...     loss='squared_error'\\n... )\\n>>> est = est.fit(X_train, y_train)  # fit with 100 trees\\n>>> mean_squared_error(y_test, est.predict(X_test))\\n5.00...\\n>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and increase num of trees\\n>>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est\\n>>> mean_squared_error(y_test, est.predict(X_test))\\n3.84...\\n---------new doc---------\\nGradient Boosting regularization\\nGradient Boosting Out-of-Bag estimates\\nOOB Errors for Random Forests\\n\\n\\n\\n1.11.1.2.7. Interpretation with feature importance#\\nIndividual decision trees can be interpreted easily by simply\\nvisualizing the tree structure. Gradient boosting models, however,\\ncomprise hundreds of regression trees thus they cannot be easily\\ninterpreted by visual inspection of the individual trees. Fortunately,\\na number of techniques have been proposed to summarize and interpret\\ngradient boosting models.\\nOften features do not contribute equally to predict the target\\nresponse; in many situations the majority of the features are in fact\\nirrelevant.\\nWhen interpreting a model, the first question usually is: what are\\nthose important features and how do they contributing in predicting\\nthe target response?\\nIndividual decision trees intrinsically perform feature selection by selecting\\nappropriate split points. This information can be used to measure the\\nimportance of each feature; the basic idea is: the more often a\\nfeature is used in the split points of a tree the more important that\\nfeature is. This notion of importance can be extended to decision tree\\nensembles by simply averaging the impurity-based feature importance of each tree (see\\nFeature importance evaluation for more details).\\nThe feature importance scores of a fit gradient boosting model can be\\naccessed via the feature_importances_ property:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\n>>> clf.feature_importances_\\narray([0.10..., 0.10..., 0.11..., ...\\n\\n\\nNote that this computation of feature importance is based on entropy, and it\\nis distinct from sklearn.inspection.permutation_importance which is\\nbased on permutation of the features.\\nExamples\\n\\nGradient Boosting regression\\n\\nReferences\\n\\n\\n[Friedman2001]\\n(1,2,3,4)\\nFriedman, J.H. (2001). Greedy function approximation: A gradient\\nboosting machine.\\nAnnals of Statistics, 29, 1189-1232.\\n\\n\\n[Friedman2002]\\nFriedman, J.H. (2002). Stochastic gradient boosting..\\nComputational Statistics & Data Analysis, 38, 367-378.\\n\\n\\n[R2007]\\nG. Ridgeway (2006). Generalized Boosted Models: A guide to the gbm\\npackage\\n---------new doc---------\\nLike decision trees, forests of trees also extend to\\nmulti-output problems  (if Y is an array\\nof shape (n_samples, n_outputs)).\\n\\n1.11.2.1. Random Forests#\\nIn random forests (see RandomForestClassifier and\\nRandomForestRegressor classes), each tree in the ensemble is built\\nfrom a sample drawn with replacement (i.e., a bootstrap sample) from the\\ntraining set.\\nFurthermore, when splitting each node during the construction of a tree, the\\nbest split is found through an exhaustive search of the features values of\\neither all input features or a random subset of size max_features.\\n(See the parameter tuning guidelines for more details.)\\nThe purpose of these two sources of randomness is to decrease the variance of\\nthe forest estimator. Indeed, individual decision trees typically exhibit high\\nvariance and tend to overfit. The injected randomness in forests yield decision\\ntrees with somewhat decoupled prediction errors. By taking an average of those\\npredictions, some errors can cancel out. Random forests achieve a reduced\\nvariance by combining diverse trees, sometimes at the cost of a slight increase\\nin bias. In practice the variance reduction is often significant hence yielding\\nan overall better model.\\nIn contrast to the original publication [B2001], the scikit-learn\\nimplementation combines classifiers by averaging their probabilistic\\nprediction, instead of letting each classifier vote for a single class.\\nA competitive alternative to random forests are\\nHistogram-Based Gradient Boosting (HGBT) models:\\n---------new doc---------\\n>>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\\n...     random_state=0)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean()\\n0.98...\\n\\n>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,\\n...     min_samples_split=2, random_state=0)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean()\\n0.999...\\n\\n>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\\n...     min_samples_split=2, random_state=0)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean() > 0.999\\nTrue\\n---------new doc---------\\n1.11.2.3. Parameters#\\nThe main parameters to adjust when using these methods is n_estimators and\\nmax_features. The former is the number of trees in the forest. The larger\\nthe better, but also the longer it will take to compute. In addition, note that\\nresults will stop getting significantly better beyond a critical number of\\ntrees. The latter is the size of the random subsets of features to consider\\nwhen splitting a node. The lower the greater the reduction of variance, but\\nalso the greater the increase in bias. Empirical good default values are\\nmax_features=1.0 or equivalently max_features=None (always considering\\nall features instead of a random subset) for regression problems, and\\nmax_features=\\\"sqrt\\\" (using a random subset of size sqrt(n_features))\\nfor classification tasks (where n_features is the number of features in\\nthe data). The default value of max_features=1.0 is equivalent to bagged\\ntrees and more randomness can be achieved by setting smaller values (e.g. 0.3\\nis a typical default in the literature). Good results are often achieved when\\nsetting max_depth=None in combination with min_samples_split=2 (i.e.,\\nwhen fully developing the trees). Bear in mind though that these values are\\nusually not optimal, and might result in models that consume a lot of RAM.\\nThe best parameter values should always be cross-validated. In addition, note\\nthat in random forests, bootstrap samples are used by default\\n(bootstrap=True) while the default strategy for extra-trees is to use the\\nwhole dataset (bootstrap=False). When using bootstrap sampling the\\ngeneralization error can be estimated on the left out or out-of-bag samples.\\n---------new doc---------\\nThe best parameter values should always be cross-validated. In addition, note\\nthat in random forests, bootstrap samples are used by default\\n(bootstrap=True) while the default strategy for extra-trees is to use the\\nwhole dataset (bootstrap=False). When using bootstrap sampling the\\ngeneralization error can be estimated on the left out or out-of-bag samples.\\nThis can be enabled by setting oob_score=True.\\n---------new doc---------\\nBreiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.\\n\\n\\n\\n[B1998]\\n\\nBreiman, “Arcing Classifiers”, Annals of Statistics 1998.\\n\\n\\n\\n\\nP. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized\\ntrees”, Machine Learning, 63(1), 3-42, 2006.\\n\\n\\n\\n1.11.2.5. Feature importance evaluation#\\nThe relative rank (i.e. depth) of a feature used as a decision node in a\\ntree can be used to assess the relative importance of that feature with\\nrespect to the predictability of the target variable. Features used at\\nthe top of the tree contribute to the final prediction decision of a\\nlarger fraction of the input samples. The expected fraction of the\\nsamples they contribute to can thus be used as an estimate of the\\nrelative importance of the features. In scikit-learn, the fraction of\\nsamples a feature contributes to is combined with the decrease in impurity\\nfrom splitting them to create a normalized estimate of the predictive power\\nof that feature.\\nBy averaging the estimates of predictive ability over several randomized\\ntrees one can reduce the variance of such an estimate and use it\\nfor feature selection. This is known as the mean decrease in impurity, or MDI.\\nRefer to [L2014] for more information on MDI and feature importance\\nevaluation with Random Forests.\\n---------new doc---------\\nWarning\\nThe impurity-based feature importances computed on tree-based models suffer\\nfrom two flaws that can lead to misleading conclusions. First they are\\ncomputed on statistics derived from the training dataset and therefore do\\nnot necessarily inform us on which features are most important to make good\\npredictions on held-out dataset. Secondly, they favor high cardinality\\nfeatures, that is features with many unique values.\\nPermutation feature importance is an alternative to impurity-based feature\\nimportance that does not suffer from these flaws. These two methods of\\nobtaining feature importance are explored in:\\nPermutation Importance vs Random Forest Feature Importance (MDI).\\n\\nIn practice those estimates are stored as an attribute named\\nfeature_importances_ on the fitted model. This is an array with shape\\n(n_features,) whose values are positive and sum to 1.0. The higher\\nthe value, the more important is the contribution of the matching feature\\nto the prediction function.\\nExamples\\n\\nFeature importances with a forest of trees\\n\\nReferences\\n\\n\\n[L2014]\\nG. Louppe, “Understanding Random Forests: From Theory to\\nPractice”,\\nPhD Thesis, U. of Liege, 2014.\\n---------new doc---------\\nIn scikit-learn, bagging methods are offered as a unified\\nBaggingClassifier meta-estimator  (resp. BaggingRegressor),\\ntaking as input a user-specified estimator along with parameters\\nspecifying the strategy to draw random subsets. In particular, max_samples\\nand max_features control the size of the subsets (in terms of samples and\\nfeatures), while bootstrap and bootstrap_features control whether\\nsamples and features are drawn with or without replacement. When using a subset\\nof the available samples the generalization accuracy can be estimated with the\\nout-of-bag samples by setting oob_score=True. As an example, the\\nsnippet below illustrates how to instantiate a bagging ensemble of\\nKNeighborsClassifier estimators, each built on random\\nsubsets of 50% of the samples and 50% of the features.\\n>>> from sklearn.ensemble import BaggingClassifier\\n>>> from sklearn.neighbors import KNeighborsClassifier\\n>>> bagging = BaggingClassifier(KNeighborsClassifier(),\\n...                             max_samples=0.5, max_features=0.5)\\n\\n\\nExamples\\n\\nSingle estimator versus bagging: bias-variance decomposition\\n\\nReferences\\n\\n\\n[B1999]\\nL. Breiman, “Pasting small votes for classification in large\\ndatabases and on-line”, Machine Learning, 36(1), 85-103, 1999.\\n\\n\\n[B1996]\\nL. Breiman, “Bagging predictors”, Machine Learning, 24(2),\\n123-140, 1996.\\n---------new doc---------\\n1.11.4.1. Majority Class Labels (Majority/Hard Voting)#\\nIn majority voting, the predicted class label for a particular sample is\\nthe class label that represents the majority (mode) of the class labels\\npredicted by each individual classifier.\\nE.g., if the prediction for a given sample is\\n\\nclassifier 1 -> class 1\\nclassifier 2 -> class 1\\nclassifier 3 -> class 2\\n\\nthe VotingClassifier (with voting='hard') would classify the sample\\nas “class 1” based on the majority class label.\\nIn the cases of a tie, the VotingClassifier will select the class\\nbased on the ascending sort order. E.g., in the following scenario\\n\\nclassifier 1 -> class 2\\nclassifier 2 -> class 1\\n\\nthe class label 1 will be assigned to the sample.\\n\\n\\n1.11.4.2. Usage#\\nThe following example shows how to fit the majority rule classifier:\\n>>> from sklearn import datasets\\n>>> from sklearn.model_selection import cross_val_score\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.naive_bayes import GaussianNB\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.ensemble import VotingClassifier\\n\\n>>> iris = datasets.load_iris()\\n>>> X, y = iris.data[:, 1:3], iris.target\\n\\n>>> clf1 = LogisticRegression(random_state=1)\\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\\n>>> clf3 = GaussianNB()\\n---------new doc---------\\n>>> iris = datasets.load_iris()\\n>>> X, y = iris.data[:, 1:3], iris.target\\n\\n>>> clf1 = LogisticRegression(random_state=1)\\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\\n>>> clf3 = GaussianNB()\\n\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='hard')\\n\\n>>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\\n...     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\\n...     print(\\\"Accuracy: %0.2f (+/- %0.2f) [%s]\\\" % (scores.mean(), scores.std(), label))\\nAccuracy: 0.95 (+/- 0.04) [Logistic Regression]\\nAccuracy: 0.94 (+/- 0.04) [Random Forest]\\nAccuracy: 0.91 (+/- 0.04) [naive Bayes]\\nAccuracy: 0.95 (+/- 0.04) [Ensemble]\\n---------new doc---------\\nclassifier 1\\nw1 * 0.2\\nw1 * 0.5\\nw1 * 0.3\\n\\nclassifier 2\\nw2 * 0.6\\nw2 * 0.3\\nw2 * 0.1\\n\\nclassifier 3\\nw3 * 0.3\\nw3 * 0.4\\nw3 * 0.3\\n\\nweighted average\\n0.37\\n0.4\\n0.23\\n\\n\\n\\n\\nHere, the predicted class label is 2, since it has the\\nhighest average probability.\\nThe following example illustrates how the decision regions may change\\nwhen a soft VotingClassifier is used based on a linear Support\\nVector Machine, a Decision Tree, and a K-nearest neighbor classifier:\\n>>> from sklearn import datasets\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> from sklearn.neighbors import KNeighborsClassifier\\n>>> from sklearn.svm import SVC\\n>>> from itertools import product\\n>>> from sklearn.ensemble import VotingClassifier\\n\\n>>> # Loading some example data\\n>>> iris = datasets.load_iris()\\n>>> X = iris.data[:, [0, 2]]\\n>>> y = iris.target\\n---------new doc---------\\n>>> # Loading some example data\\n>>> iris = datasets.load_iris()\\n>>> X = iris.data[:, [0, 2]]\\n>>> y = iris.target\\n\\n>>> # Training classifiers\\n>>> clf1 = DecisionTreeClassifier(max_depth=4)\\n>>> clf2 = KNeighborsClassifier(n_neighbors=7)\\n>>> clf3 = SVC(kernel='rbf', probability=True)\\n>>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\\n...                         voting='soft', weights=[2, 1, 2])\\n\\n>>> clf1 = clf1.fit(X, y)\\n>>> clf2 = clf2.fit(X, y)\\n>>> clf3 = clf3.fit(X, y)\\n>>> eclf = eclf.fit(X, y)\\n\\n\\n\\n\\n\\n\\n\\n\\n1.11.4.4. Usage#\\nIn order to predict the class labels based on the predicted\\nclass-probabilities (scikit-learn estimators in the VotingClassifier\\nmust support predict_proba method):\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='soft'\\n... )\\n---------new doc---------\\nOptionally, weights can be provided for the individual classifiers:\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='soft', weights=[2,5,1]\\n... )\\n\\n\\n\\n\\nUsing the VotingClassifier with GridSearchCV#\\nThe VotingClassifier can also be used together with\\nGridSearchCV in order to tune the\\nhyperparameters of the individual estimators:\\n>>> from sklearn.model_selection import GridSearchCV\\n>>> clf1 = LogisticRegression(random_state=1)\\n>>> clf2 = RandomForestClassifier(random_state=1)\\n>>> clf3 = GaussianNB()\\n>>> eclf = VotingClassifier(\\n...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\\n...     voting='soft'\\n... )\\n\\n>>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\\n\\n>>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\\n>>> grid = grid.fit(iris.data, iris.target)\\n\\n\\n\\n\\n\\n\\n1.11.5. Voting Regressor#\\nThe idea behind the VotingRegressor is to combine conceptually\\ndifferent machine learning regressors and return the average predicted values.\\nSuch a regressor can be useful for a set of equally well performing models\\nin order to balance out their individual weaknesses.\\n---------new doc---------\\n1.11.5. Voting Regressor#\\nThe idea behind the VotingRegressor is to combine conceptually\\ndifferent machine learning regressors and return the average predicted values.\\nSuch a regressor can be useful for a set of equally well performing models\\nin order to balance out their individual weaknesses.\\n\\n1.11.5.1. Usage#\\nThe following example shows how to fit the VotingRegressor:\\n>>> from sklearn.datasets import load_diabetes\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>> from sklearn.ensemble import RandomForestRegressor\\n>>> from sklearn.linear_model import LinearRegression\\n>>> from sklearn.ensemble import VotingRegressor\\n\\n>>> # Loading some example data\\n>>> X, y = load_diabetes(return_X_y=True)\\n\\n>>> # Training classifiers\\n>>> reg1 = GradientBoostingRegressor(random_state=1)\\n>>> reg2 = RandomForestRegressor(random_state=1)\\n>>> reg3 = LinearRegression()\\n>>> ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\\n>>> ereg = ereg.fit(X, y)\\n\\n\\n\\n\\n\\n\\nExamples\\n\\nPlot individual and voting regression predictions\\n---------new doc---------\\nThe final_estimator will use the predictions of the estimators as input. It\\nneeds to be a classifier or a regressor when using StackingClassifier\\nor StackingRegressor, respectively:\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>> from sklearn.ensemble import StackingRegressor\\n>>> final_estimator = GradientBoostingRegressor(\\n...     n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,\\n...     random_state=42)\\n>>> reg = StackingRegressor(\\n...     estimators=estimators,\\n...     final_estimator=final_estimator)\\n\\n\\nTo train the estimators and final_estimator, the fit method needs\\nto be called on the training data:\\n>>> from sklearn.datasets import load_diabetes\\n>>> X, y = load_diabetes(return_X_y=True)\\n>>> from sklearn.model_selection import train_test_split\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\\n...                                                     random_state=42)\\n>>> reg.fit(X_train, y_train)\\nStackingRegressor(...)\\n---------new doc---------\\nDuring training, the estimators are fitted on the whole training data\\nX_train. They will be used when calling predict or predict_proba. To\\ngeneralize and avoid over-fitting, the final_estimator is trained on\\nout-samples using sklearn.model_selection.cross_val_predict internally.\\nFor StackingClassifier, note that the output of the estimators is\\ncontrolled by the parameter stack_method and it is called by each estimator.\\nThis parameter is either a string, being estimator method names, or 'auto'\\nwhich will automatically identify an available method depending on the\\navailability, tested in the order of preference: predict_proba,\\ndecision_function and predict.\\nA StackingRegressor and StackingClassifier can be used as\\nany other regressor or classifier, exposing a predict, predict_proba, or\\ndecision_function method, e.g.:\\n>>> y_pred = reg.predict(X_test)\\n>>> from sklearn.metrics import r2_score\\n>>> print('R2 score: {:.2f}'.format(r2_score(y_test, y_pred)))\\nR2 score: 0.53\\n\\n\\nNote that it is also possible to get the output of the stacked\\nestimators using the transform method:\\n>>> reg.transform(X_test[:5])\\narray([[142..., 138..., 146...],\\n       [179..., 182..., 151...],\\n       [139..., 132..., 158...],\\n       [286..., 292..., 225...],\\n       [126..., 124..., 164...]])\\n---------new doc---------\\nIn practice, a stacking predictor predicts as good as the best predictor of the\\nbase layer and even sometimes outperforms it by combining the different\\nstrengths of the these predictors. However, training a stacking predictor is\\ncomputationally expensive.\\n\\nNote\\nFor StackingClassifier, when using stack_method_='predict_proba',\\nthe first column is dropped when the problem is a binary classification\\nproblem. Indeed, both probability columns predicted by each estimator are\\nperfectly collinear.\\n---------new doc---------\\nNote\\nMultiple stacking layers can be achieved by assigning final_estimator to\\na StackingClassifier or StackingRegressor:\\n>>> final_layer_rfr = RandomForestRegressor(\\n...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\\n>>> final_layer_gbr = GradientBoostingRegressor(\\n...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\\n>>> final_layer = StackingRegressor(\\n...     estimators=[('rf', final_layer_rfr),\\n...                 ('gbrt', final_layer_gbr)],\\n...     final_estimator=RidgeCV()\\n...     )\\n>>> multi_layer_regressor = StackingRegressor(\\n...     estimators=[('ridge', RidgeCV()),\\n...                 ('lasso', LassoCV(random_state=42)),\\n...                 ('knr', KNeighborsRegressor(n_neighbors=20,\\n...                                             metric='euclidean'))],\\n...     final_estimator=final_layer\\n... )\\n---------new doc---------\\nAdaBoost can be used both for classification and regression problems:\\n\\nFor multi-class classification, AdaBoostClassifier implements\\nAdaBoost.SAMME [ZZRH2009].\\nFor regression, AdaBoostRegressor implements AdaBoost.R2 [D1997].\\n\\n\\n1.11.7.1. Usage#\\nThe following example shows how to fit an AdaBoost classifier with 100 weak\\nlearners:\\n>>> from sklearn.model_selection import cross_val_score\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.ensemble import AdaBoostClassifier\\n\\n>>> X, y = load_iris(return_X_y=True)\\n>>> clf = AdaBoostClassifier(n_estimators=100)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores.mean()\\n0.9...\\n\\n\\nThe number of weak learners is controlled by the parameter n_estimators. The\\nlearning_rate parameter controls the contribution of the weak learners in\\nthe final combination. By default, weak learners are decision stumps. Different\\nweak learners can be specified through the estimator parameter.\\nThe main parameters to tune to obtain good results are n_estimators and\\nthe complexity of the base estimators (e.g., its depth max_depth or\\nminimum required number of samples to consider a split min_samples_split).\\nExamples\\n\\nMulti-class AdaBoosted Decision Trees shows the performance\\nof AdaBoost on a multi-class problem.\\nTwo-class AdaBoost shows the decision boundary\\nand decision function values for a non-linearly separable two-class problem\\nusing AdaBoost-SAMME.\\nDecision Tree Regression with AdaBoost demonstrates regression\\nwith the AdaBoost.R2 algorithm.\\n\\nReferences\\n---------new doc---------\\nMulticlass as One-Vs-One:\\n\\nsvm.NuSVC\\nsvm.SVC.\\ngaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_one”)\\n\\n\\nMulticlass as One-Vs-The-Rest:\\n\\nensemble.GradientBoostingClassifier\\ngaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_rest”)\\nsvm.LinearSVC (setting multi_class=”ovr”)\\nlinear_model.LogisticRegression (most solvers)\\nlinear_model.LogisticRegressionCV (most solvers)\\nlinear_model.SGDClassifier\\nlinear_model.Perceptron\\nlinear_model.PassiveAggressiveClassifier\\n\\n\\nSupport multilabel:\\n\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nneighbors.KNeighborsClassifier\\nneural_network.MLPClassifier\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\nlinear_model.RidgeClassifier\\nlinear_model.RidgeClassifierCV\\n\\n\\nSupport multiclass-multioutput:\\n\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nneighbors.KNeighborsClassifier\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\n\\n\\n\\n\\n1.12.1. Multiclass classification#\\n---------new doc---------\\nSupport multiclass-multioutput:\\n\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nneighbors.KNeighborsClassifier\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\n\\n\\n\\n\\n1.12.1. Multiclass classification#\\n\\nWarning\\nAll classifiers in scikit-learn do multiclass classification\\nout-of-the-box. You don’t need to use the sklearn.multiclass module\\nunless you want to experiment with different multiclass strategies.\\n\\nMulticlass classification is a classification task with more than two\\nclasses. Each sample can only be labeled as one class.\\nFor example, classification using features extracted from a set of images of\\nfruit, where each image may either be of an orange, an apple, or a pear.\\nEach image is one sample and is labeled as one of the 3 possible classes.\\nMulticlass classification makes the assumption that each sample is assigned\\nto one and only one label - one sample cannot, for example, be both a pear\\nand an apple.\\nWhile all scikit-learn classifiers are capable of multiclass classification,\\nthe meta-estimators offered by sklearn.multiclass\\npermit changing the way they handle more than two classes\\nbecause this may have an effect on classifier performance\\n(either in terms of generalization error or required computational resources).\\n\\n1.12.1.1. Target format#\\nValid multiclass representations for\\ntype_of_target (y) are:\\n---------new doc---------\\n1.12.1.2. OneVsRestClassifier#\\nThe one-vs-rest strategy, also known as one-vs-all, is implemented in\\nOneVsRestClassifier.  The strategy consists in\\nfitting one classifier per class. For each classifier, the class is fitted\\nagainst all the other classes. In addition to its computational efficiency\\n(only n_classes classifiers are needed), one advantage of this approach is\\nits interpretability. Since each class is represented by one and only one\\nclassifier, it is possible to gain knowledge about the class by inspecting its\\ncorresponding classifier. This is the most commonly used strategy and is a fair\\ndefault choice.\\nBelow is an example of multiclass learning using OvR:\\n>>> from sklearn import datasets\\n>>> from sklearn.multiclass import OneVsRestClassifier\\n>>> from sklearn.svm import LinearSVC\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n---------new doc---------\\n1.12.1.3. OneVsOneClassifier#\\nOneVsOneClassifier constructs one classifier per\\npair of classes. At prediction time, the class which received the most votes\\nis selected. In the event of a tie (among two classes with an equal number of\\nvotes), it selects the class with the highest aggregate classification\\nconfidence by summing over the pair-wise classification confidence levels\\ncomputed by the underlying binary classifiers.\\nSince it requires to fit n_classes * (n_classes - 1) / 2 classifiers,\\nthis method is usually slower than one-vs-the-rest, due to its\\nO(n_classes^2) complexity. However, this method may be advantageous for\\nalgorithms such as kernel algorithms which don’t scale well with\\nn_samples. This is because each individual learning problem only involves\\na small subset of the data whereas, with one-vs-the-rest, the complete\\ndataset is used n_classes times. The decision function is the result\\nof a monotonic transformation of the one-versus-one classification.\\nBelow is an example of multiclass learning using OvO:\\n>>> from sklearn import datasets\\n>>> from sklearn.multiclass import OneVsOneClassifier\\n>>> from sklearn.svm import LinearSVC\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n---------new doc---------\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n       1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\n---------new doc---------\\none-vs-the-rest. In theory, log2(n_classes) / n_classes is sufficient to\\nrepresent each class unambiguously. However, in practice, it may not lead to\\ngood accuracy since log2(n_classes) is much smaller than n_classes.\\nA number greater than 1 will require more classifiers than\\none-vs-the-rest. In this case, some classifiers will in theory correct for\\nthe mistakes made by other classifiers, hence the name “error-correcting”.\\nIn practice, however, this may not happen as classifier mistakes will\\ntypically be correlated. The error-correcting output codes have a similar\\neffect to bagging.\\nBelow is an example of multiclass learning using Output-Codes:\\n>>> from sklearn import datasets\\n>>> from sklearn.multiclass import OutputCodeClassifier\\n>>> from sklearn.svm import LinearSVC\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> clf = OutputCodeClassifier(LinearSVC(random_state=0), code_size=2, random_state=0)\\n>>> clf.fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n---------new doc---------\\nReferences\\n\\n“Solving multiclass learning problems via error-correcting output codes”,\\nDietterich T., Bakiri G., Journal of Artificial Intelligence Research 2, 1995.\\n\\n\\n\\n[3]\\n“The error coding method and PICTs”, James G., Hastie T.,\\nJournal of Computational and Graphical statistics 7, 1998.\\n\\n\\n\\n“The Elements of Statistical Learning”,\\nHastie T., Tibshirani R., Friedman J., page 606 (second-edition), 2008.\\n\\n\\n\\n\\n1.12.2. Multilabel classification#\\nMultilabel classification (closely related to multioutput\\nclassification) is a classification task labeling each sample with m\\nlabels from n_classes possible classes, where m can be 0 to\\nn_classes inclusive. This can be thought of as predicting properties of a\\nsample that are not mutually exclusive. Formally, a binary output is assigned\\nto each class, for every sample. Positive classes are indicated with 1 and\\nnegative classes with 0 or -1. It is thus comparable to running n_classes\\nbinary classification tasks, for example with\\nMultiOutputClassifier. This approach treats\\neach label independently whereas multilabel classifiers may treat the\\nmultiple classes simultaneously, accounting for correlated behavior among\\nthem.\\nFor example, prediction of the topics relevant to a text document or video.\\nThe document or video may be about one of ‘religion’, ‘politics’, ‘finance’\\nor ‘education’, several of the topic classes or all of the topic classes.\\n---------new doc---------\\n1.12.2.2. MultiOutputClassifier#\\nMultilabel classification support can be added to any classifier with\\nMultiOutputClassifier. This strategy consists of\\nfitting one classifier per target.  This allows multiple target variable\\nclassifications. The purpose of this class is to extend estimators\\nto be able to estimate a series of target functions (f1,f2,f3…,fn)\\nthat are trained on a single X predictor matrix to predict a series\\nof responses (y1,y2,y3…,yn).\\nYou can find a usage example for\\nMultiOutputClassifier\\nas part of the section on Multiclass-multioutput classification\\nsince it is a generalization of multilabel classification to\\nmulticlass outputs instead of binary outputs.\\n---------new doc---------\\n1.12.2.3. ClassifierChain#\\nClassifier chains (see ClassifierChain) are a way\\nof combining a number of binary classifiers into a single multi-label model\\nthat is capable of exploiting correlations among targets.\\nFor a multi-label classification problem with N classes, N binary\\nclassifiers are assigned an integer between 0 and N-1. These integers\\ndefine the order of models in the chain. Each classifier is then fit on the\\navailable training data plus the true labels of the classes whose\\nmodels were assigned a lower number.\\nWhen predicting, the true labels will not be available. Instead the\\npredictions of each model are passed on to the subsequent models in the\\nchain to be used as features.\\nClearly the order of the chain is important. The first model in the chain\\nhas no information about the other labels while the last model in the chain\\nhas features indicating the presence of all of the other labels. In general\\none does not know the optimal ordering of the models in the chain so\\ntypically many randomly ordered chains are fit and their predictions are\\naveraged together.\\nReferences\\n\\nJesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,\\n“Classifier Chains for Multi-label Classification”, 2009.\\n---------new doc---------\\n1.12.3. Multiclass-multioutput classification#\\nMulticlass-multioutput classification\\n(also known as multitask classification) is a\\nclassification task which labels each sample with a set of non-binary\\nproperties. Both the number of properties and the number of\\nclasses per property is greater than 2. A single estimator thus\\nhandles several joint classification tasks. This is both a generalization of\\nthe multilabel classification task, which only considers binary\\nattributes, as well as a generalization of the multiclass classification\\ntask, where only one property is considered.\\nFor example, classification of the properties “type of fruit” and “colour”\\nfor a set of images of fruit. The property “type of fruit” has the possible\\nclasses: “apple”, “pear” and “orange”. The property “colour” has the\\npossible classes: “green”, “red”, “yellow” and “orange”. Each sample is an\\nimage of a fruit, a label is output for both properties and each label is\\none of the possible classes of the corresponding property.\\nNote that all classifiers handling multiclass-multioutput (also known as\\nmultitask classification) tasks, support the multilabel classification task\\nas a special case. Multitask classification is similar to the multioutput\\nclassification task with different model formulations. For more information,\\nsee the relevant estimator documentation.\\nBelow is an example of multiclass-multioutput classification:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.multioutput import MultiOutputClassifier\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.utils import shuffle\\n>>> import numpy as np\\n---------new doc---------\\nclassification task with different model formulations. For more information,\\nsee the relevant estimator documentation.\\nBelow is an example of multiclass-multioutput classification:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.multioutput import MultiOutputClassifier\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.utils import shuffle\\n>>> import numpy as np\\n>>> X, y1 = make_classification(n_samples=10, n_features=100,\\n...                             n_informative=30, n_classes=3,\\n...                             random_state=1)\\n>>> y2 = shuffle(y1, random_state=1)\\n>>> y3 = shuffle(y1, random_state=2)\\n>>> Y = np.vstack((y1, y2, y3)).T\\n>>> n_samples, n_features = X.shape # 10,100\\n>>> n_outputs = Y.shape[1] # 3\\n>>> n_classes = 3\\n>>> forest = RandomForestClassifier(random_state=1)\\n>>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)\\n>>> multi_target_forest.fit(X, Y).predict(X)\\narray([[2, 2, 0],\\n       [1, 2, 1],\\n       [2, 1, 0],\\n       [0, 0, 2],\\n---------new doc---------\\n>>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)\\n>>> multi_target_forest.fit(X, Y).predict(X)\\narray([[2, 2, 0],\\n       [1, 2, 1],\\n       [2, 1, 0],\\n       [0, 0, 2],\\n       [0, 2, 1],\\n       [0, 0, 2],\\n       [1, 1, 0],\\n       [1, 1, 1],\\n       [0, 0, 2],\\n       [2, 0, 0]])\\n---------new doc---------\\nWarning\\nAt present, no metric in sklearn.metrics\\nsupports the multiclass-multioutput classification task.\\n\\n\\n1.12.3.1. Target format#\\nA valid representation of multioutput y is a dense matrix of shape\\n(n_samples, n_classes) of class labels. A column wise concatenation of 1d\\nmulticlass variables. An example of y for 3 samples:\\n>>> y = np.array([['apple', 'green'], ['orange', 'orange'], ['pear', 'green']])\\n>>> print(y)\\n[['apple' 'green']\\n ['orange' 'orange']\\n ['pear' 'green']]\\n\\n\\n\\n\\n\\n1.12.4. Multioutput regression#\\nMultioutput regression predicts multiple numerical properties for each\\nsample. Each property is a numerical variable and the number of properties\\nto be predicted for each sample is greater than or equal to 2. Some estimators\\nthat support multioutput regression are faster than just running n_output\\nestimators.\\nFor example, prediction of both wind speed and wind direction, in degrees,\\nusing data obtained at a certain location. Each sample would be data\\nobtained at one location and both wind speed and direction would be\\noutput for each sample.\\nThe following regressors natively support multioutput regression:\\n---------new doc---------\\n1.12.4.2. MultiOutputRegressor#\\nMultioutput regression support can be added to any regressor with\\nMultiOutputRegressor.  This strategy consists of\\nfitting one regressor per target. Since each target is represented by exactly\\none regressor it is possible to gain knowledge about the target by\\ninspecting its corresponding regressor. As\\nMultiOutputRegressor fits one regressor per\\ntarget it can not take advantage of correlations between targets.\\nBelow is an example of multioutput regression:\\n>>> from sklearn.datasets import make_regression\\n>>> from sklearn.multioutput import MultiOutputRegressor\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>> X, y = make_regression(n_samples=10, n_targets=3, random_state=1)\\n>>> MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X)\\narray([[-154.75474165, -147.03498585,  -50.03812219],\\n       [   7.12165031,    5.12914884,  -81.46081961],\\n       [-187.8948621 , -100.44373091,   13.88978285],\\n       [-141.62745778,   95.02891072, -191.48204257],\\n       [  97.03260883,  165.34867495,  139.52003279],\\n       [ 123.92529176,   21.25719016,   -7.84253   ],\\n---------new doc---------\\n1.12.4.3. RegressorChain#\\nRegressor chains (see RegressorChain) is\\nanalogous to ClassifierChain as a way of\\ncombining a number of regressions into a single multi-target model that is\\ncapable of exploiting correlations among targets.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking\\n\\n\\n\\n\\nnext\\n1.13. Feature selection\\n\\n\\n --- \\n\\n\\n1. Supervised learning\\n1.13. Feature selection\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1.13. Feature selection#\\nThe classes in the sklearn.feature_selection module can be used\\nfor feature selection/dimensionality reduction on sample sets, either to\\nimprove estimators’ accuracy scores or to boost their performance on very\\nhigh-dimensional datasets.\\n\\n1.13.1. Removing features with low variance#\\nVarianceThreshold is a simple baseline approach to feature selection.\\nIt removes all features whose variance doesn’t meet some threshold.\\nBy default, it removes all zero-variance features,\\ni.e. features that have the same value in all samples.\\nAs an example, suppose that we have a dataset with boolean features,\\nand we want to remove all features that are either one or zero (on or off)\\nin more than 80% of the samples.\\nBoolean features are Bernoulli random variables,\\nand the variance of such variables is given by\\n---------new doc---------\\n\\\\[\\\\mathrm{Var}[X] = p(1 - p)\\\\]\\nso we can select using the threshold .8 * (1 - .8):\\n>>> from sklearn.feature_selection import VarianceThreshold\\n>>> X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\\n>>> sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\\n>>> sel.fit_transform(X)\\narray([[0, 1],\\n       [1, 0],\\n       [0, 0],\\n       [1, 1],\\n       [1, 0],\\n       [1, 1]])\\n\\n\\nAs expected, VarianceThreshold has removed the first column,\\nwhich has a probability \\\\(p = 5/6 > .8\\\\) of containing a zero.\\n\\n\\n1.13.2. Univariate feature selection#\\nUnivariate feature selection works by selecting the best features based on\\nunivariate statistical tests. It can be seen as a preprocessing step\\nto an estimator. Scikit-learn exposes feature selection routines\\nas objects that implement the transform method:\\n---------new doc---------\\nAs expected, VarianceThreshold has removed the first column,\\nwhich has a probability \\\\(p = 5/6 > .8\\\\) of containing a zero.\\n\\n\\n1.13.2. Univariate feature selection#\\nUnivariate feature selection works by selecting the best features based on\\nunivariate statistical tests. It can be seen as a preprocessing step\\nto an estimator. Scikit-learn exposes feature selection routines\\nas objects that implement the transform method:\\n\\nSelectKBest removes all but the \\\\(k\\\\) highest scoring features\\nSelectPercentile removes all but a user-specified highest scoring\\npercentage of features\\nusing common univariate statistical tests for each feature:\\nfalse positive rate SelectFpr, false discovery rate\\nSelectFdr, or family wise error SelectFwe.\\nGenericUnivariateSelect allows to perform univariate feature\\nselection with a configurable strategy. This allows to select the best\\nunivariate selection strategy with hyper-parameter search estimator.\\n\\nFor instance, we can use a F-test to retrieve the two\\nbest features for a dataset as follows:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.feature_selection import SelectKBest\\n>>> from sklearn.feature_selection import f_classif\\n>>> X, y = load_iris(return_X_y=True)\\n>>> X.shape\\n(150, 4)\\n>>> X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\\n>>> X_new.shape\\n(150, 2)\\n\\n\\nThese objects take as input a scoring function that returns univariate scores\\nand p-values (or only scores for SelectKBest and\\nSelectPercentile):\\n---------new doc---------\\nWarning\\nBeware not to use a regression scoring function with a classification\\nproblem, you will get useless results.\\n\\n\\nNote\\nThe SelectPercentile and SelectKBest support unsupervised\\nfeature selection as well. One needs to provide a score_func where y=None.\\nThe score_func should use internally X to compute the scores.\\n\\nExamples\\n\\nUnivariate Feature Selection\\nComparison of F-test and mutual information\\n\\n\\n\\n1.13.3. Recursive feature elimination#\\nGiven an external estimator that assigns weights to features (e.g., the\\ncoefficients of a linear model), the goal of recursive feature elimination (RFE)\\nis to select features by recursively considering smaller and smaller sets of\\nfeatures. First, the estimator is trained on the initial set of features and\\nthe importance of each feature is obtained either through any specific attribute\\n(such as coef_, feature_importances_) or callable. Then, the least important\\nfeatures are pruned from current set of features. That procedure is recursively\\nrepeated on the pruned set until the desired number of features to select is\\neventually reached.\\nRFECV performs RFE in a cross-validation loop to find the optimal\\nnumber of features. In more details, the number of features selected is tuned\\nautomatically by fitting an RFE selector on the different\\ncross-validation splits (provided by the cv parameter). The performance\\nof the RFE selector are evaluated using scorer for different number\\nof selected features and aggregated together. Finally, the scores are averaged\\nacross folds and the number of features selected is set to the number of\\nfeatures that maximize the cross-validation score.\\nExamples\\n---------new doc---------\\nRecursive feature elimination: A recursive feature elimination example\\nshowing the relevance of pixels in a digit classification task.\\nRecursive feature elimination with cross-validation: A recursive feature\\nelimination example with automatic tuning of the number of features\\nselected with cross-validation.\\n\\n\\n\\n1.13.4. Feature selection using SelectFromModel#\\nSelectFromModel is a meta-transformer that can be used alongside any\\nestimator that assigns importance to each feature through a specific attribute (such as\\ncoef_, feature_importances_) or via an importance_getter callable after fitting.\\nThe features are considered unimportant and removed if the corresponding\\nimportance of the feature values are below the provided\\nthreshold parameter. Apart from specifying the threshold numerically,\\nthere are built-in heuristics for finding a threshold using a string argument.\\nAvailable heuristics are “mean”, “median” and float multiples of these like\\n“0.1*mean”. In combination with the threshold criteria, one can use the\\nmax_features parameter to set a limit on the number of features to select.\\nFor examples on how it is to be used refer to the sections below.\\nExamples\\n\\nModel-based and sequential feature selection\\n---------new doc---------\\n1.13.4.2. Tree-based feature selection#\\nTree-based estimators (see the sklearn.tree module and forest\\nof trees in the sklearn.ensemble module) can be used to compute\\nimpurity-based feature importances, which in turn can be used to discard irrelevant\\nfeatures (when coupled with the SelectFromModel\\nmeta-transformer):\\n>>> from sklearn.ensemble import ExtraTreesClassifier\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.feature_selection import SelectFromModel\\n>>> X, y = load_iris(return_X_y=True)\\n>>> X.shape\\n(150, 4)\\n>>> clf = ExtraTreesClassifier(n_estimators=50)\\n>>> clf = clf.fit(X, y)\\n>>> clf.feature_importances_  \\narray([ 0.04...,  0.05...,  0.4...,  0.4...])\\n>>> model = SelectFromModel(clf, prefit=True)\\n>>> X_new = model.transform(X)\\n>>> X_new.shape               \\n(150, 2)\\n\\n\\nExamples\\n\\nFeature importances with a forest of trees: example on\\nsynthetic data showing the recovery of the actually meaningful features.\\nPermutation Importance vs Random Forest Feature Importance (MDI): example\\ndiscussing the caveats of using impurity-based feature importances as a proxy for\\nfeature relevance.\\n---------new doc---------\\nExamples\\n\\nFeature importances with a forest of trees: example on\\nsynthetic data showing the recovery of the actually meaningful features.\\nPermutation Importance vs Random Forest Feature Importance (MDI): example\\ndiscussing the caveats of using impurity-based feature importances as a proxy for\\nfeature relevance.\\n\\n\\n\\n\\n1.13.5. Sequential Feature Selection#\\nSequential Feature Selection [sfs] (SFS) is available in the\\nSequentialFeatureSelector transformer.\\nSFS can be either forward or backward:\\nForward-SFS is a greedy procedure that iteratively finds the best new feature\\nto add to the set of selected features. Concretely, we initially start with\\nzero features and find the one feature that maximizes a cross-validated score\\nwhen an estimator is trained on this single feature. Once that first feature\\nis selected, we repeat the procedure by adding a new feature to the set of\\nselected features. The procedure stops when the desired number of selected\\nfeatures is reached, as determined by the n_features_to_select parameter.\\nBackward-SFS follows the same idea but works in the opposite direction:\\ninstead of starting with no features and greedily adding features, we start\\nwith all the features and greedily remove features from the set. The\\ndirection parameter controls whether forward or backward SFS is used.\\n---------new doc---------\\nDetails on Sequential Feature Selection#\\nIn general, forward and backward selection do not yield equivalent results.\\nAlso, one may be much faster than the other depending on the requested number\\nof selected features: if we have 10 features and ask for 7 selected features,\\nforward selection would need to perform 7 iterations while backward selection\\nwould only need to perform 3.\\nSFS differs from RFE and\\nSelectFromModel in that it does not\\nrequire the underlying model to expose a coef_ or feature_importances_\\nattribute. It may however be slower considering that more models need to be\\nevaluated, compared to the other approaches. For example in backward\\nselection, the iteration going from m features to m - 1 features using k-fold\\ncross-validation requires fitting m * k models, while\\nRFE would require only a single fit, and\\nSelectFromModel always just does a single\\nfit and requires no iterations.\\nReferences\\n\\n\\n[sfs]\\nFerri et al, Comparative study of techniques for\\nlarge-scale feature selection.\\n\\n\\n\\nExamples\\n\\nModel-based and sequential feature selection\\n\\n\\n\\n1.13.6. Feature selection as part of a pipeline#\\nFeature selection is usually used as a pre-processing step before doing\\nthe actual learning. The recommended way to do this in scikit-learn is\\nto use a Pipeline:\\nclf = Pipeline([\\n  ('feature_selection', SelectFromModel(LinearSVC(penalty=\\\"l1\\\"))),\\n  ('classification', RandomForestClassifier())\\n])\\nclf.fit(X, y)\\n---------new doc---------\\nIn this snippet we make use of a LinearSVC\\ncoupled with SelectFromModel\\nto evaluate feature importances and select the most relevant features.\\nThen, a RandomForestClassifier is trained on the\\ntransformed output, i.e. using only relevant features. You can perform\\nsimilar operations with the other feature selection methods and also\\nclassifiers that provide a way to evaluate feature importances of course.\\nSee the Pipeline examples for more details.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n1.12. Multiclass and multioutput algorithms\\n\\n\\n\\n\\nnext\\n1.14. Semi-supervised learning\\n\\n\\n --- \\n\\n\\n1. Supervised learning\\n1.14. Semi-supervised learning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1.14. Semi-supervised learning#\\nSemi-supervised learning is a situation\\nin which in your training data some of the samples are not labeled. The\\nsemi-supervised estimators in sklearn.semi_supervised are able to\\nmake use of this additional unlabeled data to better capture the shape of\\nthe underlying data distribution and generalize better to new samples.\\nThese algorithms can perform well when we have a very small amount of\\nlabeled points and a large amount of unlabeled points.\\n\\nUnlabeled entries in y\\nIt is important to assign an identifier to unlabeled points along with the\\nlabeled data when training the model with the fit method. The\\nidentifier that this implementation uses is the integer value \\\\(-1\\\\).\\nNote that for string labels, the dtype of y should be object so that it\\ncan contain both strings and integers.\\n\\n\\nNote\\nSemi-supervised algorithms need to make assumptions about the distribution\\nof the dataset in order to achieve performance gains. See here\\nfor more details.\\n---------new doc---------\\nNote\\nSemi-supervised algorithms need to make assumptions about the distribution\\nof the dataset in order to achieve performance gains. See here\\nfor more details.\\n\\n\\n1.14.1. Self Training#\\nThis self-training implementation is based on Yarowsky’s [1] algorithm. Using\\nthis algorithm, a given supervised classifier can function as a semi-supervised\\nclassifier, allowing it to learn from unlabeled data.\\nSelfTrainingClassifier can be called with any classifier that\\nimplements predict_proba, passed as the parameter base_classifier. In\\neach iteration, the base_classifier predicts labels for the unlabeled\\nsamples and adds a subset of these labels to the labeled dataset.\\nThe choice of this subset is determined by the selection criterion. This\\nselection can be done using a threshold on the prediction probabilities, or\\nby choosing the k_best samples according to the prediction probabilities.\\nThe labels used for the final fit as well as the iteration in which each sample\\nwas labeled are available as attributes. The optional max_iter parameter\\nspecifies how many times the loop is executed at most.\\nThe max_iter parameter may be set to None, causing the algorithm to iterate\\nuntil all samples have labels or no new samples are selected in that iteration.\\n\\nNote\\nWhen using the self-training classifier, the\\ncalibration of the classifier is important.\\n\\nExamples\\n\\nEffect of varying threshold for self-training\\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\\n\\nReferences\\n---------new doc---------\\nNote\\nWhen using the self-training classifier, the\\ncalibration of the classifier is important.\\n\\nExamples\\n\\nEffect of varying threshold for self-training\\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\\n\\nReferences\\n\\n\\n[1]\\n“Unsupervised word sense disambiguation rivaling supervised methods”\\nDavid Yarowsky, Proceedings of the 33rd annual meeting on Association for\\nComputational Linguistics (ACL ‘95). Association for Computational Linguistics,\\nStroudsburg, PA, USA, 189-196.\\n\\n\\n\\n\\n1.14.2. Label Propagation#\\nLabel propagation denotes a few variations of semi-supervised graph\\ninference algorithms.\\n\\nA few features available in this model:\\nUsed for classification tasks\\nKernel methods to project data into alternate dimensional spaces\\n\\n\\n\\nscikit-learn provides two label propagation models:\\nLabelPropagation and LabelSpreading. Both work by\\nconstructing a similarity graph over all items in the input dataset.\\n\\n\\n\\n\\nAn illustration of label-propagation: the structure of unlabeled\\nobservations is consistent with the class structure, and thus the\\nclass label can be propagated to the unlabeled observations of the\\ntraining set.#\\n---------new doc---------\\nExamples\\n\\nIsotonic Regression\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n1.14. Semi-supervised learning\\n\\n\\n\\n\\nnext\\n1.16. Probability calibration\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThis Page\\n\\nShow Source\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      © Copyright 2007 - 2025, scikit-learn developers (BSD License).\\n\\n\\n --- \\n\\n\\n1. Supervised learning\\n1.16. Probability calibration\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1.16. Probability calibration#\\nWhen performing classification you often want not only to predict the class\\nlabel, but also obtain a probability of the respective label. This probability\\ngives you some kind of confidence on the prediction. Some models can give you\\npoor estimates of the class probabilities and some even do not support\\nprobability prediction (e.g., some instances of\\nSGDClassifier).\\nThe calibration module allows you to better calibrate\\nthe probabilities of a given model, or to add support for probability\\nprediction.\\nWell calibrated classifiers are probabilistic classifiers for which the output\\nof the predict_proba method can be directly interpreted as a confidence\\nlevel.\\nFor instance, a well calibrated (binary) classifier should classify the samples such\\nthat among the samples to which it gave a predict_proba value close to, say,\\n0.8, approximately 80% actually belong to the positive class.\\nBefore we show how to re-calibrate a classifier, we first need a way to detect how\\ngood a classifier is calibrated.\\n---------new doc---------\\nLogisticRegression is more likely to return well calibrated predictions by itself as it has a\\ncanonical link function for its loss, i.e. the logit-link for the Log loss.\\nIn the unpenalized case, this leads to the so-called balance property, see [8] and Logistic regression.\\nIn the plot above, data is generated according to a linear mechanism, which is\\nconsistent with the LogisticRegression model (the model is ‘well specified’),\\nand the value of the regularization parameter C is tuned to be\\nappropriate (neither too strong nor too low). As a consequence, this model returns\\naccurate predictions from its predict_proba method.\\nIn contrast to that, the other shown models return biased probabilities; with\\ndifferent biases per model.\\nGaussianNB (Naive Bayes) tends to push probabilities to 0 or 1 (note the counts\\nin the histograms). This is mainly because it makes the assumption that\\nfeatures are conditionally independent given the class, which is not the\\ncase in this dataset which contains 2 redundant features.\\nRandomForestClassifier shows the opposite behavior: the histograms\\nshow peaks at probabilities approximately 0.2 and 0.9, while probabilities\\nclose to 0 or 1 are very rare. An explanation for this is given by\\nNiculescu-Mizil and Caruana [3]: “Methods such as bagging and random\\nforests that average predictions from a base set of models can have\\ndifficulty making predictions near 0 and 1 because variance in the\\nunderlying base models will bias predictions that should be near zero or one\\naway from these values. Because predictions are restricted to the interval\\n[0,1], errors caused by variance tend to be one-sided near zero and one. For\\n---------new doc---------\\nforests that average predictions from a base set of models can have\\ndifficulty making predictions near 0 and 1 because variance in the\\nunderlying base models will bias predictions that should be near zero or one\\naway from these values. Because predictions are restricted to the interval\\n[0,1], errors caused by variance tend to be one-sided near zero and one. For\\nexample, if a model should predict p = 0 for a case, the only way bagging\\ncan achieve this is if all bagged trees predict zero. If we add noise to the\\ntrees that bagging is averaging over, this noise will cause some trees to\\npredict values larger than 0 for this case, thus moving the average\\nprediction of the bagged ensemble away from 0. We observe this effect most\\nstrongly with random forests because the base-level trees trained with\\nrandom forests have relatively high variance due to feature subsetting.” As\\na result, the calibration curve shows a characteristic sigmoid shape, indicating that\\nthe classifier could trust its “intuition” more and return probabilities closer\\nto 0 or 1 typically.\\nLinearSVC (SVC) shows an even more sigmoid curve than the random forest, which\\nis typical for maximum-margin methods (compare Niculescu-Mizil and Caruana [3]), which\\nfocus on difficult to classify samples that are close to the decision boundary (the\\nsupport vectors).\\n---------new doc---------\\n1.16.2. Calibrating a classifier#\\nCalibrating a classifier consists of fitting a regressor (called a\\ncalibrator) that maps the output of the classifier (as given by\\ndecision_function or predict_proba) to a calibrated probability\\nin [0, 1]. Denoting the output of the classifier for a given sample by \\\\(f_i\\\\),\\nthe calibrator tries to predict the conditional event probability\\n\\\\(P(y_i = 1 | f_i)\\\\).\\nIdeally, the calibrator is fit on a dataset independent of the training data used to\\nfit the classifier in the first place.\\nThis is because performance of the classifier on its training data would be\\nbetter than for novel data. Using the classifier output of training data\\nto fit the calibrator would thus result in a biased calibrator that maps to\\nprobabilities closer to 0 and 1 than it should.\\n\\n\\n1.16.3. Usage#\\nThe CalibratedClassifierCV class is used to calibrate a classifier.\\nCalibratedClassifierCV uses a cross-validation approach to ensure\\nunbiased data is always used to fit the calibrator. The data is split into k\\n(train_set, test_set) couples (as determined by cv). When ensemble=True\\n(default), the following procedure is repeated independently for each\\ncross-validation split:\\n\\na clone of base_estimator is trained on the train subset\\nthe trained base_estimator makes predictions on the test subset\\nthe predictions are used to fit a calibrator (either a sigmoid or isotonic\\nregressor) (when the data is multiclass, a calibrator is fit for every class)\\n---------new doc---------\\nThis results in an\\nensemble of k (classifier, calibrator) couples where each calibrator maps\\nthe output of its corresponding classifier into [0, 1]. Each couple is exposed\\nin the calibrated_classifiers_ attribute, where each entry is a calibrated\\nclassifier with a predict_proba method that outputs calibrated\\nprobabilities. The output of predict_proba for the main\\nCalibratedClassifierCV instance corresponds to the average of the\\npredicted probabilities of the k estimators in the calibrated_classifiers_\\nlist. The output of predict is the class that has the highest\\nprobability.\\nIt is important to choose cv carefully when using ensemble=True.\\nAll classes should be present in both train and test subsets for every split.\\nWhen a class is absent in the train subset, the predicted probability for that\\nclass will default to 0 for the (classifier, calibrator) couple of that split.\\nThis skews the predict_proba as it averages across all couples.\\nWhen a class is absent in the test subset, the calibrator for that class\\n(within the (classifier, calibrator) couple of that split) is\\nfit on data with no positive class. This results in ineffective calibration.\\nWhen ensemble=False, cross-validation is used to obtain ‘unbiased’\\npredictions for all the data, via\\ncross_val_predict.\\nThese unbiased predictions are then used to train the calibrator. The attribute\\ncalibrated_classifiers_ consists of only one (classifier, calibrator)\\ncouple where the classifier is the base_estimator trained on all the data.\\nIn this case the output of predict_proba for\\nCalibratedClassifierCV is the predicted probabilities obtained\\nfrom the single (classifier, calibrator) couple.\\n---------new doc---------\\ncross_val_predict.\\nThese unbiased predictions are then used to train the calibrator. The attribute\\ncalibrated_classifiers_ consists of only one (classifier, calibrator)\\ncouple where the classifier is the base_estimator trained on all the data.\\nIn this case the output of predict_proba for\\nCalibratedClassifierCV is the predicted probabilities obtained\\nfrom the single (classifier, calibrator) couple.\\nThe main advantage of ensemble=True is to benefit from the traditional\\nensembling effect (similar to Bagging meta-estimator). The resulting ensemble should\\nboth be well calibrated and slightly more accurate than with ensemble=False.\\nThe main advantage of using ensemble=False is computational: it reduces the\\noverall fit time by training only a single base classifier and calibrator\\npair, decreases the final model size and increases prediction speed.\\nAlternatively an already fitted classifier can be calibrated by using a\\nFrozenEstimator as\\nCalibratedClassifierCV(estimator=FrozenEstimator(estimator)).\\nIt is up to the user to make sure that the data used for fitting the classifier\\nis disjoint from the data used for fitting the regressor.\\ndata used for fitting the regressor.\\nCalibratedClassifierCV supports the use of two regression techniques\\nfor calibration via the method parameter: \\\"sigmoid\\\" and \\\"isotonic\\\".\\n---------new doc---------\\nThe disadvantages of Multi-layer Perceptron (MLP) include:\\n\\nMLP with hidden layers have a non-convex loss function where there exists\\nmore than one local minimum. Therefore different random weight\\ninitializations can lead to different validation accuracy.\\nMLP requires tuning a number of hyperparameters such as the number of\\nhidden neurons, layers, and iterations.\\nMLP is sensitive to feature scaling.\\n\\nPlease see Tips on Practical Use section that addresses\\nsome of these disadvantages.\\n\\n\\n\\n1.17.2. Classification#\\nClass MLPClassifier implements a multi-layer perceptron (MLP) algorithm\\nthat trains using Backpropagation.\\nMLP trains on two arrays: array X of size (n_samples, n_features), which holds\\nthe training samples represented as floating point feature vectors; and array\\ny of size (n_samples,), which holds the target values (class labels) for the\\ntraining samples:\\n>>> from sklearn.neural_network import MLPClassifier\\n>>> X = [[0., 0.], [1., 1.]]\\n>>> y = [0, 1]\\n>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\\n...                     hidden_layer_sizes=(5, 2), random_state=1)\\n...\\n>>> clf.fit(X, y)\\nMLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\\n              solver='lbfgs')\\n---------new doc---------\\nMLPClassifier supports multi-class classification by\\napplying Softmax\\nas the output function.\\nFurther, the model supports multi-label classification\\nin which a sample can belong to more than one class. For each class, the raw\\noutput passes through the logistic function. Values larger or equal to 0.5\\nare rounded to 1, otherwise to 0. For a predicted output of a sample, the\\nindices where the value is 1 represents the assigned classes of that sample:\\n>>> X = [[0., 0.], [1., 1.]]\\n>>> y = [[0, 1], [1, 1]]\\n>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\\n...                     hidden_layer_sizes=(15,), random_state=1)\\n...\\n>>> clf.fit(X, y)\\nMLPClassifier(alpha=1e-05, hidden_layer_sizes=(15,), random_state=1,\\n              solver='lbfgs')\\n>>> clf.predict([[1., 2.]])\\narray([[1, 1]])\\n>>> clf.predict([[0., 0.]])\\narray([[0, 1]])\\n\\n\\nSee the examples below and the docstring of\\nMLPClassifier.fit for further information.\\nExamples\\n\\nCompare Stochastic learning strategies for MLPClassifier\\nSee Visualization of MLP weights on MNIST for\\nvisualized representation of trained weights.\\n---------new doc---------\\nThe first row of output array indicates that there are three samples whose\\ntrue cluster is “a”. Of them, two are in predicted cluster 0, one is in 1,\\nand none is in 2. And the second row indicates that there are three samples\\nwhose true cluster is “b”. Of them, none is in predicted cluster 0, one is in\\n1 and two are in 2.\\nA confusion matrix for classification is a square\\ncontingency matrix where the order of rows and columns correspond to a list\\nof classes.\\n\\nAdvantages:\\n\\nAllows to examine the spread of each true cluster across predicted clusters\\nand vice versa.\\nThe contingency table calculated is typically utilized in the calculation of\\na similarity statistic (like the others listed in this document) between the\\ntwo clusterings.\\n\\n\\n\\nDrawbacks:\\n\\nContingency matrix is easy to interpret for a small number of clusters, but\\nbecomes very hard to interpret for a large number of clusters.\\nIt doesn’t give a single metric to use as an objective for clustering\\noptimisation.\\n\\n\\n\\n\\nReferences#\\n\\nWikipedia entry for contingency matrix\\n\\n\\n\\n\\n2.3.11.9. Pair Confusion Matrix#\\nThe pair confusion matrix\\n(sklearn.metrics.cluster.pair_confusion_matrix) is a 2x2\\nsimilarity matrix\\n---------new doc---------\\n\\\\[\\\\begin{split}C = \\\\left[\\\\begin{matrix}\\nC_{00} & C_{01} \\\\\\\\\\nC_{10} & C_{11}\\n\\\\end{matrix}\\\\right]\\\\end{split}\\\\]\\nbetween two clusterings computed by considering all pairs of samples and\\ncounting pairs that are assigned into the same or into different clusters\\nunder the true and predicted clusterings.\\nIt has the following entries:\\n\\\\(C_{00}\\\\) : number of pairs with both clusterings having the samples\\nnot clustered together\\n\\\\(C_{10}\\\\) : number of pairs with the true label clustering having the\\nsamples clustered together but the other clustering not having the samples\\nclustered together\\n\\\\(C_{01}\\\\) : number of pairs with the true label clustering not having\\nthe samples clustered together but the other clustering having the samples\\nclustered together\\n\\\\(C_{11}\\\\) : number of pairs with both clusterings having the samples\\nclustered together\\nConsidering a pair of samples that is clustered together a positive pair,\\nthen as in binary classification the count of true negatives is\\n\\\\(C_{00}\\\\), false negatives is \\\\(C_{10}\\\\), true positives is\\n\\\\(C_{11}\\\\) and false positives is \\\\(C_{01}\\\\).\\nPerfectly matching labelings have all non-zero entries on the\\ndiagonal regardless of actual label values:\\n>>> from sklearn.metrics.cluster import pair_confusion_matrix\\n>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])\\narray([[8, 0],\\n       [0, 4]])\\n---------new doc---------\\n2.7. Novelty and Outlier Detection#\\nMany applications require being able to decide whether a new observation\\nbelongs to the same distribution as existing observations (it is an\\ninlier), or should be considered as different (it is an outlier).\\nOften, this ability is used to clean real data sets. Two important\\ndistinctions must be made:\\n\\noutlier detection:\\nThe training data contains outliers which are defined as observations that\\nare far from the others. Outlier detection estimators thus try to fit the\\nregions where the training data is the most concentrated, ignoring the\\ndeviant observations.\\n\\nnovelty detection:\\nThe training data is not polluted by outliers and we are interested in\\ndetecting whether a new observation is an outlier. In this context an\\noutlier is also called a novelty.\\n\\n\\nOutlier detection and novelty detection are both used for anomaly\\ndetection, where one is interested in detecting abnormal or unusual\\nobservations. Outlier detection is then also known as unsupervised anomaly\\ndetection and novelty detection as semi-supervised anomaly detection. In the\\ncontext of outlier detection, the outliers/anomalies cannot form a\\ndense cluster as available estimators assume that the outliers/anomalies are\\nlocated in low density regions. On the contrary, in the context of novelty\\ndetection, novelties/anomalies can form a dense cluster as long as they are in\\na low density region of the training data, considered as normal in this\\ncontext.\\nThe scikit-learn project provides a set of machine learning tools that\\ncan be used both for novelty or outlier detection. This strategy is\\nimplemented with objects learning in an unsupervised way from the data:\\nestimator.fit(X_train)\\n---------new doc---------\\nnew observations can then be sorted as inliers or outliers with a\\npredict method:\\nestimator.predict(X_test)\\n\\n\\nInliers are labeled 1, while outliers are labeled -1. The predict method\\nmakes use of a threshold on the raw scoring function computed by the\\nestimator. This scoring function is accessible through the score_samples\\nmethod, while the threshold can be controlled by the contamination\\nparameter.\\nThe decision_function method is also defined from the scoring function,\\nin such a way that negative values are outliers and non-negative ones are\\ninliers:\\nestimator.decision_function(X_test)\\n\\n\\nNote that neighbors.LocalOutlierFactor does not support\\npredict, decision_function and score_samples methods by default\\nbut only a fit_predict method, as this estimator was originally meant to\\nbe applied for outlier detection. The scores of abnormality of the training\\nsamples are accessible through the negative_outlier_factor_ attribute.\\nIf you really want to use neighbors.LocalOutlierFactor for novelty\\ndetection, i.e. predict labels or compute the score of abnormality of new\\nunseen data, you can instantiate the estimator with the novelty parameter\\nset to True before fitting the estimator. In this case, fit_predict is\\nnot available.\\n---------new doc---------\\nWarning\\nNovelty detection with Local Outlier Factor\\nWhen novelty is set to True be aware that you must only use\\npredict, decision_function and score_samples on new unseen data\\nand not on the training samples as this would lead to wrong results.\\nI.e., the result of predict will not be the same as fit_predict.\\nThe scores of abnormality of the training samples are always accessible\\nthrough the negative_outlier_factor_ attribute.\\n\\nThe behavior of neighbors.LocalOutlierFactor is summarized in the\\nfollowing table.\\n\\n\\nMethod\\nOutlier detection\\nNovelty detection\\n\\n\\n\\nfit_predict\\nOK\\nNot available\\n\\npredict\\nNot available\\nUse only on new data\\n\\ndecision_function\\nNot available\\nUse only on new data\\n\\nscore_samples\\nUse negative_outlier_factor_\\nUse only on new data\\n\\nnegative_outlier_factor_\\nOK\\nOK\\n\\n\\n\\n\\n\\n2.7.1. Overview of outlier detection methods#\\nA comparison of the outlier detection algorithms in scikit-learn. Local\\nOutlier Factor (LOF) does not show a decision boundary in black as it\\nhas no predict method to be applied on new data when it is used for outlier\\ndetection.\\n---------new doc---------\\nnegative_outlier_factor_\\nOK\\nOK\\n\\n\\n\\n\\n\\n2.7.1. Overview of outlier detection methods#\\nA comparison of the outlier detection algorithms in scikit-learn. Local\\nOutlier Factor (LOF) does not show a decision boundary in black as it\\nhas no predict method to be applied on new data when it is used for outlier\\ndetection.\\n\\n\\n\\n\\nensemble.IsolationForest and neighbors.LocalOutlierFactor\\nperform reasonably well on the data sets considered here.\\nThe svm.OneClassSVM is known to be sensitive to outliers and thus\\ndoes not perform very well for outlier detection. That being said, outlier\\ndetection in high-dimension, or without any assumptions on the distribution\\nof the inlying data is very challenging. svm.OneClassSVM may still\\nbe used with outlier detection but requires fine-tuning of its hyperparameter\\nnu to handle outliers and prevent overfitting.\\nlinear_model.SGDOneClassSVM provides an implementation of a\\nlinear One-Class SVM with a linear complexity in the number of samples. This\\nimplementation is here used with a kernel approximation technique to obtain\\nresults similar to svm.OneClassSVM which uses a Gaussian kernel\\nby default. Finally, covariance.EllipticEnvelope assumes the data is\\nGaussian and learns an ellipse. For more details on the different estimators\\nrefer to the example\\nComparing anomaly detection algorithms for outlier detection on toy datasets and the\\nsections hereunder.\\nExamples\\n---------new doc---------\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison of the svm.OneClassSVM, the\\nensemble.IsolationForest, the\\nneighbors.LocalOutlierFactor and\\ncovariance.EllipticEnvelope.\\nSee Evaluation of outlier detection estimators\\nfor an example showing how to evaluate outlier detection estimators,\\nthe neighbors.LocalOutlierFactor and the\\nensemble.IsolationForest, using ROC curves from\\nmetrics.RocCurveDisplay.\\n---------new doc---------\\n2.7.2. Novelty Detection#\\nConsider a data set of \\\\(n\\\\) observations from the same\\ndistribution described by \\\\(p\\\\) features.  Consider now that we\\nadd one more observation to that data set. Is the new observation so\\ndifferent from the others that we can doubt it is regular? (i.e. does\\nit come from the same distribution?) Or on the contrary, is it so\\nsimilar to the other that we cannot distinguish it from the original\\nobservations? This is the question addressed by the novelty detection\\ntools and methods.\\nIn general, it is about to learn a rough, close frontier delimiting\\nthe contour of the initial observations distribution, plotted in\\nembedding \\\\(p\\\\)-dimensional space. Then, if further observations\\nlay within the frontier-delimited subspace, they are considered as\\ncoming from the same population than the initial\\nobservations. Otherwise, if they lay outside the frontier, we can say\\nthat they are abnormal with a given confidence in our assessment.\\nThe One-Class SVM has been introduced by Schölkopf et al. for that purpose\\nand implemented in the Support Vector Machines module in the\\nsvm.OneClassSVM object. It requires the choice of a\\nkernel and a scalar parameter to define a frontier.  The RBF kernel is\\nusually chosen although there exists no exact formula or algorithm to\\nset its bandwidth parameter. This is the default in the scikit-learn\\nimplementation. The nu parameter, also known as the margin of\\nthe One-Class SVM, corresponds to the probability of finding a new,\\nbut regular, observation outside the frontier.\\nReferences\\n\\nEstimating the support of a high-dimensional distribution\\nSchölkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471.\\n---------new doc---------\\n2.7.3. Outlier Detection#\\nOutlier detection is similar to novelty detection in the sense that\\nthe goal is to separate a core of regular observations from some\\npolluting ones, called outliers. Yet, in the case of outlier\\ndetection, we don’t have a clean data set representing the population\\nof regular observations that can be used to train any tool.\\n\\n2.7.3.1. Fitting an elliptic envelope#\\nOne common way of performing outlier detection is to assume that the\\nregular data come from a known distribution (e.g. data are Gaussian\\ndistributed). From this assumption, we generally try to define the\\n“shape” of the data, and can define outlying observations as\\nobservations which stand far enough from the fit shape.\\nThe scikit-learn provides an object\\ncovariance.EllipticEnvelope that fits a robust covariance\\nestimate to the data, and thus fits an ellipse to the central data\\npoints, ignoring points outside the central mode.\\nFor instance, assuming that the inlier data are Gaussian distributed, it\\nwill estimate the inlier location and covariance in a robust way (i.e.\\nwithout being influenced by outliers). The Mahalanobis distances\\nobtained from this estimate is used to derive a measure of outlyingness.\\nThis strategy is illustrated below.\\n\\n\\n\\n\\nExamples\\n---------new doc---------\\nThe ensemble.IsolationForest supports warm_start=True which\\nallows you to add more trees to an already fitted model:\\n>>> from sklearn.ensemble import IsolationForest\\n>>> import numpy as np\\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\\n>>> clf = IsolationForest(n_estimators=10, warm_start=True)\\n>>> clf.fit(X)  # fit 10 trees  \\n>>> clf.set_params(n_estimators=20)  # add 10 more trees  \\n>>> clf.fit(X)  # fit the added trees  \\n\\n\\nExamples\\n\\nSee IsolationForest example for\\nan illustration of the use of IsolationForest.\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison of ensemble.IsolationForest with\\nneighbors.LocalOutlierFactor,\\nsvm.OneClassSVM (tuned to perform like an outlier detection\\nmethod), linear_model.SGDOneClassSVM, and a covariance-based\\noutlier detection with covariance.EllipticEnvelope.\\n\\nReferences\\n\\nLiu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”\\nData Mining, 2008. ICDM’08. Eighth IEEE International Conference on.\\n---------new doc---------\\nWhen the proportion of outliers is high (i.e. greater than 10 %, as in the\\nexample below), n_neighbors should be greater (n_neighbors=35 in the example\\nbelow).\\nThe strength of the LOF algorithm is that it takes both local and global\\nproperties of datasets into consideration: it can perform well even in datasets\\nwhere abnormal samples have different underlying densities.\\nThe question is not, how isolated the sample is, but how isolated it is\\nwith respect to the surrounding neighborhood.\\nWhen applying LOF for outlier detection, there are no predict,\\ndecision_function and score_samples methods but only a fit_predict\\nmethod. The scores of abnormality of the training samples are accessible\\nthrough the negative_outlier_factor_ attribute.\\nNote that predict, decision_function and score_samples can be used\\non new unseen data when LOF is applied for novelty detection, i.e. when the\\nnovelty parameter is set to True, but the result of predict may\\ndiffer from that of fit_predict. See Novelty detection with Local Outlier Factor.\\nThis strategy is illustrated below.\\n---------new doc---------\\nExamples\\n\\nSee Outlier detection with Local Outlier Factor (LOF)\\nfor an illustration of the use of neighbors.LocalOutlierFactor.\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison with other anomaly detection methods.\\n\\nReferences\\n\\nBreunig, Kriegel, Ng, and Sander (2000)\\nLOF: identifying density-based local outliers.\\nProc. ACM SIGMOD\\n\\n\\n\\n\\n2.7.4. Novelty detection with Local Outlier Factor#\\nTo use neighbors.LocalOutlierFactor for novelty detection, i.e.\\npredict labels or compute the score of abnormality of new unseen data, you\\nneed to instantiate the estimator with the novelty parameter\\nset to True before fitting the estimator:\\nlof = LocalOutlierFactor(novelty=True)\\nlof.fit(X_train)\\n\\n\\nNote that fit_predict is not available in this case to avoid inconsistencies.\\n\\nWarning\\nNovelty detection with Local Outlier Factor`\\nWhen novelty is set to True be aware that you must only use\\npredict, decision_function and score_samples on new unseen data\\nand not on the training samples as this would lead to wrong results.\\nI.e., the result of predict will not be the same as fit_predict.\\nThe scores of abnormality of the training samples are always accessible\\nthrough the negative_outlier_factor_ attribute.\\n\\nNovelty detection with Local Outlier Factor is illustrated below.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n2.6. Covariance estimation\\n\\n\\n\\n\\nnext\\n2.8. Density Estimation\\n\\n\\n --- \\n\\n\\n2. Unsupervised learning\\n2.8. Density Estimation\\n---------new doc---------\\nIn scikit-learn a random split into training and test sets\\ncan be quickly computed with the train_test_split helper function.\\nLet’s load the iris data set to fit a linear support vector machine on it:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn import datasets\\n>>> from sklearn import svm\\n\\n>>> X, y = datasets.load_iris(return_X_y=True)\\n>>> X.shape, y.shape\\n((150, 4), (150,))\\n\\n\\nWe can now quickly sample a training set while holding out 40% of the\\ndata for testing (evaluating) our classifier:\\n>>> X_train, X_test, y_train, y_test = train_test_split(\\n...     X, y, test_size=0.4, random_state=0)\\n\\n>>> X_train.shape, y_train.shape\\n((90, 4), (90,))\\n>>> X_test.shape, y_test.shape\\n((60, 4), (60,))\\n\\n>>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.96...\\n---------new doc---------\\n>>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.96...\\n\\n\\nWhen evaluating different settings (“hyperparameters”) for estimators,\\nsuch as the C setting that must be manually set for an SVM,\\nthere is still a risk of overfitting on the test set\\nbecause the parameters can be tweaked until the estimator performs optimally.\\nThis way, knowledge about the test set can “leak” into the model\\nand evaluation metrics no longer report on generalization performance.\\nTo solve this problem, yet another part of the dataset can be held out\\nas a so-called “validation set”: training proceeds on the training set,\\nafter which evaluation is done on the validation set,\\nand when the experiment seems to be successful,\\nfinal evaluation can be done on the test set.\\nHowever, by partitioning the available data into three sets,\\nwe drastically reduce the number of samples\\nwhich can be used for learning the model,\\nand the results can depend on a particular random choice for the pair of\\n(train, validation) sets.\\nA solution to this problem is a procedure called\\ncross-validation\\n(CV for short).\\nA test set should still be held out for final evaluation,\\nbut the validation set is no longer needed when doing CV.\\nIn the basic approach, called k-fold CV,\\nthe training set is split into k smaller sets\\n(other approaches are described below,\\nbut generally follow the same principles).\\nThe following procedure is followed for each of the k “folds”:\\n---------new doc---------\\nA model is trained using \\\\(k-1\\\\) of the folds as training data;\\nthe resulting model is validated on the remaining part of the data\\n(i.e., it is used as a test set to compute a performance measure\\nsuch as accuracy).\\n\\nThe performance measure reported by k-fold cross-validation\\nis then the average of the values computed in the loop.\\nThis approach can be computationally expensive,\\nbut does not waste too much data\\n(as is the case when fixing an arbitrary validation set),\\nwhich is a major advantage in problems such as inverse inference\\nwhere the number of samples is very small.\\n\\n\\n\\n3.1.1. Computing cross-validated metrics#\\nThe simplest way to use cross-validation is to call the\\ncross_val_score helper function on the estimator and the dataset.\\nThe following example demonstrates how to estimate the accuracy of a linear\\nkernel support vector machine on the iris dataset by splitting the data, fitting\\na model and computing the score 5 consecutive times (with different splits each\\ntime):\\n>>> from sklearn.model_selection import cross_val_score\\n>>> clf = svm.SVC(kernel='linear', C=1, random_state=42)\\n>>> scores = cross_val_score(clf, X, y, cv=5)\\n>>> scores\\narray([0.96..., 1. , 0.96..., 0.96..., 1. ])\\n\\n\\nThe mean score and the standard deviation are hence given by:\\n>>> print(\\\"%0.2f accuracy with a standard deviation of %0.2f\\\" % (scores.mean(), scores.std()))\\n0.98 accuracy with a standard deviation of 0.02\\n---------new doc---------\\nSee The scoring parameter: defining model evaluation rules for details.\\nIn the case of the Iris dataset, the samples are balanced across target\\nclasses hence the accuracy and the F1-score are almost equal.\\nWhen the cv argument is an integer, cross_val_score uses the\\nKFold or StratifiedKFold strategies by default, the latter\\nbeing used if the estimator derives from ClassifierMixin.\\nIt is also possible to use other cross validation strategies by passing a cross\\nvalidation iterator instead, for instance:\\n>>> from sklearn.model_selection import ShuffleSplit\\n>>> n_samples = X.shape[0]\\n>>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\\n>>> cross_val_score(clf, X, y, cv=cv)\\narray([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])\\n---------new doc---------\\nData transformation with held-out data#\\nJust as it is important to test a predictor on data held-out from\\ntraining, preprocessing (such as standardization, feature selection, etc.)\\nand similar data transformations similarly should\\nbe learnt from a training set and applied to held-out data for prediction:\\n>>> from sklearn import preprocessing\\n>>> X_train, X_test, y_train, y_test = train_test_split(\\n...     X, y, test_size=0.4, random_state=0)\\n>>> scaler = preprocessing.StandardScaler().fit(X_train)\\n>>> X_train_transformed = scaler.transform(X_train)\\n>>> clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\\n>>> X_test_transformed = scaler.transform(X_test)\\n>>> clf.score(X_test_transformed, y_test)\\n0.9333...\\n\\n\\nA Pipeline makes it easier to compose\\nestimators, providing this behavior under cross-validation:\\n>>> from sklearn.pipeline import make_pipeline\\n>>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\\n>>> cross_val_score(clf, X, y, cv=cv)\\narray([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])\\n\\n\\nSee Pipelines and composite estimators.\\n\\n\\n3.1.1.1. The cross_validate function and multiple metric evaluation#\\nThe cross_validate function differs from cross_val_score in\\ntwo ways:\\n---------new doc---------\\n3.1.1.2. Obtaining predictions by cross-validation#\\nThe function cross_val_predict has a similar interface to\\ncross_val_score, but returns, for each element in the input, the\\nprediction that was obtained for that element when it was in the test set. Only\\ncross-validation strategies that assign all elements to a test set exactly once\\ncan be used (otherwise, an exception is raised).\\n\\nWarning\\nNote on inappropriate usage of cross_val_predict\\nThe result of cross_val_predict may be different from those\\nobtained using cross_val_score as the elements are grouped in\\ndifferent ways. The function cross_val_score takes an average\\nover cross-validation folds, whereas cross_val_predict simply\\nreturns the labels (or probabilities) from several distinct models\\nundistinguished. Thus, cross_val_predict is not an appropriate\\nmeasure of generalization error.\\n\\n\\nThe function cross_val_predict is appropriate for:\\nVisualization of predictions obtained from different models.\\nModel blending: When predictions of one supervised estimator are used to\\ntrain another estimator in ensemble methods.\\n\\n\\n\\nThe available cross validation iterators are introduced in the following\\nsection.\\nExamples\\n\\nReceiver Operating Characteristic (ROC) with cross validation,\\nRecursive feature elimination with cross-validation,\\nCustom refit strategy of a grid search with cross-validation,\\nSample pipeline for text feature extraction and evaluation,\\nPlotting Cross-Validated Predictions,\\nNested versus non-nested cross-validation.\\n\\n\\n\\n\\n3.1.2. Cross validation iterators#\\nThe following sections list utilities to generate indices\\nthat can be used to generate dataset splits according to different cross\\nvalidation strategies.\\n---------new doc---------\\n3.1.2. Cross validation iterators#\\nThe following sections list utilities to generate indices\\nthat can be used to generate dataset splits according to different cross\\nvalidation strategies.\\n\\n3.1.2.1. Cross-validation iterators for i.i.d. data#\\nAssuming that some data is Independent and Identically Distributed (i.i.d.) is\\nmaking the assumption that all samples stem from the same generative process\\nand that the generative process is assumed to have no memory of past generated\\nsamples.\\nThe following cross-validators can be used in such cases.\\n\\nNote\\nWhile i.i.d. data is a common assumption in machine learning theory, it rarely\\nholds in practice. If one knows that the samples have been generated using a\\ntime-dependent process, it is safer to\\nuse a time-series aware cross-validation scheme.\\nSimilarly, if we know that the generative process has a group structure\\n(samples collected from different subjects, experiments, measurement\\ndevices), it is safer to use group-wise cross-validation.\\n\\n\\n3.1.2.1.1. K-fold#\\nKFold divides all the samples in \\\\(k\\\\) groups of samples,\\ncalled folds (if \\\\(k = n\\\\), this is equivalent to the Leave One\\nOut strategy), of equal sizes (if possible). The prediction function is\\nlearned using \\\\(k - 1\\\\) folds, and the fold left out is used for test.\\nExample of 2-fold cross-validation on a dataset with 4 samples:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import KFold\\n---------new doc---------\\n>>> X = [\\\"a\\\", \\\"b\\\", \\\"c\\\", \\\"d\\\"]\\n>>> kf = KFold(n_splits=2)\\n>>> for train, test in kf.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[2 3] [0 1]\\n[0 1] [2 3]\\n\\n\\nHere is a visualization of the cross-validation behavior. Note that\\nKFold is not affected by classes or groups.\\n\\n\\n\\n\\nEach fold is constituted by two arrays: the first one is related to the\\ntraining set, and the second one to the test set.\\nThus, one can create the training/test sets using numpy indexing:\\n>>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\\n>>> y = np.array([0, 1, 0, 1])\\n>>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\\n---------new doc---------\\n3.1.2.1.2. Repeated K-Fold#\\nRepeatedKFold repeats K-Fold n times. It can be used when one\\nrequires to run KFold n times, producing different splits in\\neach repetition.\\nExample of 2-fold K-Fold repeated 2 times:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import RepeatedKFold\\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\\n>>> random_state = 12883823\\n>>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\\n>>> for train, test in rkf.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n...\\n[2 3] [0 1]\\n[0 1] [2 3]\\n[0 2] [1 3]\\n[1 3] [0 2]\\n\\n\\nSimilarly, RepeatedStratifiedKFold repeats Stratified K-Fold n times\\nwith different randomization in each repetition.\\n\\n\\n3.1.2.1.3. Leave One Out (LOO)#\\nLeaveOneOut (or LOO) is a simple cross-validation. Each learning\\nset is created by taking all the samples except one, the test set being\\nthe sample left out. Thus, for \\\\(n\\\\) samples, we have \\\\(n\\\\) different\\ntraining sets and \\\\(n\\\\) different tests set. This cross-validation\\nprocedure does not waste much data as only one sample is removed from the\\ntraining set:\\n>>> from sklearn.model_selection import LeaveOneOut\\n---------new doc---------\\n>>> X = [1, 2, 3, 4]\\n>>> loo = LeaveOneOut()\\n>>> for train, test in loo.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[1 2 3] [0]\\n[0 2 3] [1]\\n[0 1 3] [2]\\n[0 1 2] [3]\\n\\n\\nPotential users of LOO for model selection should weigh a few known caveats.\\nWhen compared with \\\\(k\\\\)-fold cross validation, one builds \\\\(n\\\\) models\\nfrom \\\\(n\\\\) samples instead of \\\\(k\\\\) models, where \\\\(n > k\\\\).\\nMoreover, each is trained on \\\\(n - 1\\\\) samples rather than\\n\\\\((k-1) n / k\\\\). In both ways, assuming \\\\(k\\\\) is not too large\\nand \\\\(k < n\\\\), LOO is more computationally expensive than \\\\(k\\\\)-fold\\ncross validation.\\nIn terms of accuracy, LOO often results in high variance as an estimator for the\\ntest error. Intuitively, since \\\\(n - 1\\\\) of\\nthe \\\\(n\\\\) samples are used to build each model, models constructed from\\nfolds are virtually identical to each other and to the model built from the\\nentire training set.\\nHowever, if the learning curve is steep for the training size in question,\\nthen 5- or 10- fold cross validation can overestimate the generalization error.\\nAs a general rule, most authors, and empirical evidence, suggest that 5- or 10-\\nfold cross validation should be preferred to LOO.\\n\\n\\nReferences#\\n---------new doc---------\\nReferences#\\n\\nhttp://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html;\\nT. Hastie, R. Tibshirani, J. Friedman,  The Elements of Statistical Learning, Springer 2009\\nL. Breiman, P. Spector Submodel selection and evaluation in regression: The X-random case, International Statistical Review 1992;\\nR. Kohavi, A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, Intl. Jnt. Conf. AI\\nR. Bharat Rao, G. Fung, R. Rosales, On the Dangers of Cross-Validation. An Experimental Evaluation, SIAM 2008;\\nG. James, D. Witten, T. Hastie, R Tibshirani, An Introduction to\\nStatistical Learning, Springer 2013.\\n\\n\\n\\n\\n3.1.2.1.4. Leave P Out (LPO)#\\nLeavePOut is very similar to LeaveOneOut as it creates all\\nthe possible training/test sets by removing \\\\(p\\\\) samples from the complete\\nset. For \\\\(n\\\\) samples, this produces \\\\({n \\\\choose p}\\\\) train-test\\npairs. Unlike LeaveOneOut and KFold, the test sets will\\noverlap for \\\\(p > 1\\\\).\\nExample of Leave-2-Out on a dataset with 4 samples:\\n>>> from sklearn.model_selection import LeavePOut\\n---------new doc---------\\n>>> X = np.ones(4)\\n>>> lpo = LeavePOut(p=2)\\n>>> for train, test in lpo.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[2 3] [0 1]\\n[1 3] [0 2]\\n[1 2] [0 3]\\n[0 3] [1 2]\\n[0 2] [1 3]\\n[0 1] [2 3]\\n\\n\\n\\n\\n3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split#\\nThe ShuffleSplit iterator will generate a user defined number of\\nindependent train / test dataset splits. Samples are first shuffled and\\nthen split into a pair of train and test sets.\\nIt is possible to control the randomness for reproducibility of the\\nresults by explicitly seeding the random_state pseudo random number\\ngenerator.\\nHere is a usage example:\\n>>> from sklearn.model_selection import ShuffleSplit\\n>>> X = np.arange(10)\\n>>> ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\\n>>> for train_index, test_index in ss.split(X):\\n...     print(\\\"%s %s\\\" % (train_index, test_index))\\n[9 1 6 7 3 0 5] [2 8 4]\\n[2 9 8 0 6 7 4] [3 5 1]\\n[4 5 1 0 6 9 7] [2 3 8]\\n[2 7 5 8 0 3 4] [6 1 9]\\n[4 1 0 6 8 9 3] [5 2 7]\\n---------new doc---------\\nHere is a visualization of the cross-validation behavior. Note that\\nShuffleSplit is not affected by classes or groups.\\n\\n\\n\\n\\nShuffleSplit is thus a good alternative to KFold cross\\nvalidation that allows a finer control on the number of iterations and\\nthe proportion of samples on each side of the train / test split.\\n\\n\\n\\n3.1.2.2. Cross-validation iterators with stratification based on class labels#\\nSome classification problems can exhibit a large imbalance in the distribution\\nof the target classes: for instance there could be several times more negative\\nsamples than positive samples. In such cases it is recommended to use\\nstratified sampling as implemented in StratifiedKFold and\\nStratifiedShuffleSplit to ensure that relative class frequencies is\\napproximately preserved in each train and validation fold.\\n---------new doc---------\\n3.1.2.2.1. Stratified k-fold#\\nStratifiedKFold is a variation of k-fold which returns stratified\\nfolds: each set contains approximately the same percentage of samples of each\\ntarget class as the complete set.\\nHere is an example of stratified 3-fold cross-validation on a dataset with 50 samples from\\ntwo unbalanced classes.  We show the number of samples in each class and compare with\\nKFold.\\n>>> from sklearn.model_selection import StratifiedKFold, KFold\\n>>> import numpy as np\\n>>> X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\\n>>> skf = StratifiedKFold(n_splits=3)\\n>>> for train, test in skf.split(X, y):\\n...     print('train -  {}   |   test -  {}'.format(\\n...         np.bincount(y[train]), np.bincount(y[test])))\\ntrain -  [30  3]   |   test -  [15  2]\\ntrain -  [30  3]   |   test -  [15  2]\\ntrain -  [30  4]   |   test -  [15  1]\\n>>> kf = KFold(n_splits=3)\\n>>> for train, test in kf.split(X, y):\\n...     print('train -  {}   |   test -  {}'.format(\\n---------new doc---------\\nWe can see that StratifiedKFold preserves the class ratios\\n(approximately 1 / 10) in both train and test dataset.\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nRepeatedStratifiedKFold can be used to repeat Stratified K-Fold n times\\nwith different randomization in each repetition.\\n\\n\\n3.1.2.2.2. Stratified Shuffle Split#\\nStratifiedShuffleSplit is a variation of ShuffleSplit, which returns\\nstratified splits, i.e which creates splits by preserving the same\\npercentage for each target class as in the complete set.\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\n\\n\\n\\n3.1.2.3. Predefined fold-splits / Validation-sets#\\nFor some datasets, a pre-defined split of the data into training- and\\nvalidation fold or into several cross-validation folds already\\nexists. Using PredefinedSplit it is possible to use these folds\\ne.g. when searching for hyperparameters.\\nFor example, when using a validation set, set the test_fold to 0 for all\\nsamples that are part of the validation set, and to -1 for all other samples.\\n---------new doc---------\\n3.1.2.4. Cross-validation iterators for grouped data#\\nThe i.i.d. assumption is broken if the underlying generative process yields\\ngroups of dependent samples.\\nSuch a grouping of data is domain specific. An example would be when there is\\nmedical data collected from multiple patients, with multiple samples taken from\\neach patient. And such data is likely to be dependent on the individual group.\\nIn our example, the patient id for each sample will be its group identifier.\\nIn this case we would like to know if a model trained on a particular set of\\ngroups generalizes well to the unseen groups. To measure this, we need to\\nensure that all the samples in the validation fold come from groups that are\\nnot represented at all in the paired training fold.\\nThe following cross-validation splitters can be used to do that.\\nThe grouping identifier for the samples is specified via the groups\\nparameter.\\n\\n3.1.2.4.1. Group k-fold#\\nGroupKFold is a variation of k-fold which ensures that the same group is\\nnot represented in both testing and training sets. For example if the data is\\nobtained from different subjects with several samples per-subject and if the\\nmodel is flexible enough to learn from highly person specific features it\\ncould fail to generalize to new subjects. GroupKFold makes it possible\\nto detect this kind of overfitting situations.\\nImagine you have three subjects, each with an associated number from 1 to 3:\\n>>> from sklearn.model_selection import GroupKFold\\n---------new doc---------\\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\\n>>> y = [\\\"a\\\", \\\"b\\\", \\\"b\\\", \\\"b\\\", \\\"c\\\", \\\"c\\\", \\\"c\\\", \\\"d\\\", \\\"d\\\", \\\"d\\\"]\\n>>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\\n\\n>>> gkf = GroupKFold(n_splits=3)\\n>>> for train, test in gkf.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[0 1 2 3 4 5] [6 7 8 9]\\n[0 1 2 6 7 8 9] [3 4 5]\\n[3 4 5 6 7 8 9] [0 1 2]\\n\\n\\nEach subject is in a different testing fold, and the same subject is never in\\nboth testing and training. Notice that the folds do not have exactly the same\\nsize due to the imbalance in the data. If class proportions must be balanced\\nacross folds, StratifiedGroupKFold is a better option.\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nSimilar to KFold, the test sets from GroupKFold will form a\\ncomplete partition of all the data.\\nWhile GroupKFold attempts to place the same number of samples in each\\nfold when shuffle=False, when shuffle=True it attempts to place equal\\nnumber of distinct groups in each fold (but doesn not account for group sizes).\\n---------new doc---------\\n3.1.2.4.2. StratifiedGroupKFold#\\nStratifiedGroupKFold is a cross-validation scheme that combines both\\nStratifiedKFold and GroupKFold. The idea is to try to\\npreserve the distribution of classes in each split while keeping each group\\nwithin a single split. That might be useful when you have an unbalanced\\ndataset so that using just GroupKFold might produce skewed splits.\\nExample:\\n>>> from sklearn.model_selection import StratifiedGroupKFold\\n>>> X = list(range(18))\\n>>> y = [1] * 6 + [0] * 12\\n>>> groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]\\n>>> sgkf = StratifiedGroupKFold(n_splits=3)\\n>>> for train, test in sgkf.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]\\n[ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]\\n[ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]\\n\\n\\n\\n\\nImplementation notes#\\n\\nWith the current implementation full shuffle is not possible in most\\nscenarios. When shuffle=True, the following happens:\\n---------new doc---------\\nImplementation notes#\\n\\nWith the current implementation full shuffle is not possible in most\\nscenarios. When shuffle=True, the following happens:\\n\\nAll groups are shuffled.\\nGroups are sorted by standard deviation of classes using stable sort.\\nSorted groups are iterated over and assigned to folds.\\n\\nThat means that only groups with the same standard deviation of class\\ndistribution will be shuffled, which might be useful when each group has only\\na single class.\\n\\nThe algorithm greedily assigns each group to one of n_splits test sets,\\nchoosing the test set that minimises the variance in class distribution\\nacross test sets. Group assignment proceeds from groups with highest to\\nlowest variance in class frequency, i.e. large groups peaked on one or few\\nclasses are assigned first.\\nThis split is suboptimal in a sense that it might produce imbalanced splits\\neven if perfect stratification is possible. If you have relatively close\\ndistribution of classes in each group, using GroupKFold is better.\\n\\n\\nHere is a visualization of cross-validation behavior for uneven groups:\\n---------new doc---------\\nHere is a visualization of cross-validation behavior for uneven groups:\\n\\n\\n\\n\\n\\n\\n3.1.2.4.3. Leave One Group Out#\\nLeaveOneGroupOut is a cross-validation scheme where each split holds\\nout samples belonging to one specific group. Group information is\\nprovided via an array that encodes the group of each sample.\\nEach training set is thus constituted by all the samples except the ones\\nrelated to a specific group. This is the same as LeavePGroupsOut with\\nn_groups=1 and the same as GroupKFold with n_splits equal to the\\nnumber of unique labels passed to the groups parameter.\\nFor example, in the cases of multiple experiments, LeaveOneGroupOut\\ncan be used to create a cross-validation based on the different experiments:\\nwe create a training set using the samples of all the experiments except one:\\n>>> from sklearn.model_selection import LeaveOneGroupOut\\n\\n>>> X = [1, 5, 10, 50, 60, 70, 80]\\n>>> y = [0, 1, 1, 2, 2, 2, 2]\\n>>> groups = [1, 1, 2, 2, 3, 3, 3]\\n>>> logo = LeaveOneGroupOut()\\n>>> for train, test in logo.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[2 3 4 5 6] [0 1]\\n[0 1 4 5 6] [2 3]\\n[0 1 2 3] [4 5 6]\\n\\n\\nAnother common application is to use time information: for instance the\\ngroups could be the year of collection of the samples and thus allow\\nfor cross-validation against time-based splits.\\n---------new doc---------\\nAnother common application is to use time information: for instance the\\ngroups could be the year of collection of the samples and thus allow\\nfor cross-validation against time-based splits.\\n\\n\\n3.1.2.4.4. Leave P Groups Out#\\nLeavePGroupsOut is similar as LeaveOneGroupOut, but removes\\nsamples related to \\\\(P\\\\) groups for each training/test set. All possible\\ncombinations of \\\\(P\\\\) groups are left out, meaning test sets will overlap\\nfor \\\\(P>1\\\\).\\nExample of Leave-2-Group Out:\\n>>> from sklearn.model_selection import LeavePGroupsOut\\n\\n>>> X = np.arange(6)\\n>>> y = [1, 1, 1, 2, 2, 2]\\n>>> groups = [1, 1, 2, 2, 3, 3]\\n>>> lpgo = LeavePGroupsOut(n_groups=2)\\n>>> for train, test in lpgo.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[4 5] [0 1 2 3]\\n[2 3] [0 1 4 5]\\n[0 1] [2 3 4 5]\\n\\n\\n\\n\\n3.1.2.4.5. Group Shuffle Split#\\nThe GroupShuffleSplit iterator behaves as a combination of\\nShuffleSplit and LeavePGroupsOut, and generates a\\nsequence of randomized partitions in which a subset of groups are held\\nout for each split. Each train/test split is performed independently meaning\\nthere is no guaranteed relationship between successive test sets.\\nHere is a usage example:\\n>>> from sklearn.model_selection import GroupShuffleSplit\\n---------new doc---------\\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\\n>>> y = [\\\"a\\\", \\\"b\\\", \\\"b\\\", \\\"b\\\", \\\"c\\\", \\\"c\\\", \\\"c\\\", \\\"a\\\"]\\n>>> groups = [1, 1, 2, 2, 3, 3, 4, 4]\\n>>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\\n>>> for train, test in gss.split(X, y, groups=groups):\\n...     print(\\\"%s %s\\\" % (train, test))\\n...\\n[0 1 2 3] [4 5 6 7]\\n[2 3 6 7] [0 1 4 5]\\n[2 3 4 5] [0 1 6 7]\\n[4 5 6 7] [0 1 2 3]\\n\\n\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nThis class is useful when the behavior of LeavePGroupsOut is\\ndesired, but the number of groups is large enough that generating all\\npossible partitions with \\\\(P\\\\) groups withheld would be prohibitively\\nexpensive. In such a scenario, GroupShuffleSplit provides\\na random sample (with replacement) of the train / test splits\\ngenerated by LeavePGroupsOut.\\n---------new doc---------\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\nThis class is useful when the behavior of LeavePGroupsOut is\\ndesired, but the number of groups is large enough that generating all\\npossible partitions with \\\\(P\\\\) groups withheld would be prohibitively\\nexpensive. In such a scenario, GroupShuffleSplit provides\\na random sample (with replacement) of the train / test splits\\ngenerated by LeavePGroupsOut.\\n\\n\\n\\n3.1.2.5. Using cross-validation iterators to split train and test#\\nThe above group cross-validation functions may also be useful for splitting a\\ndataset into training and testing subsets. Note that the convenience\\nfunction train_test_split is a wrapper around ShuffleSplit\\nand thus only allows for stratified splitting (using the class labels)\\nand cannot account for groups.\\nTo perform the train and test split, use the indices for the train and test\\nsubsets yielded by the generator output by the split() method of the\\ncross-validation splitter. For example:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import GroupShuffleSplit\\n---------new doc---------\\n3.1.2.6. Cross validation of time series data#\\nTime series data is characterized by the correlation between observations\\nthat are near in time (autocorrelation). However, classical\\ncross-validation techniques such as KFold and\\nShuffleSplit assume the samples are independent and\\nidentically distributed, and would result in unreasonable correlation\\nbetween training and testing instances (yielding poor estimates of\\ngeneralization error) on time series data. Therefore, it is very important\\nto evaluate our model for time series data on the “future” observations\\nleast like those that are used to train the model. To achieve this, one\\nsolution is provided by TimeSeriesSplit.\\n\\n3.1.2.6.1. Time Series Split#\\nTimeSeriesSplit is a variation of k-fold which\\nreturns first \\\\(k\\\\) folds as train set and the \\\\((k+1)\\\\) th\\nfold as test set. Note that unlike standard cross-validation methods,\\nsuccessive training sets are supersets of those that come before them.\\nAlso, it adds all surplus data to the first training partition, which\\nis always used to train the model.\\nThis class can be used to cross-validate time series data samples\\nthat are observed at fixed time intervals.\\nExample of 3-split time series cross-validation on a dataset with 6 samples:\\n>>> from sklearn.model_selection import TimeSeriesSplit\\n---------new doc---------\\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\\n>>> y = np.array([1, 2, 3, 4, 5, 6])\\n>>> tscv = TimeSeriesSplit(n_splits=3)\\n>>> print(tscv)\\nTimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)\\n>>> for train, test in tscv.split(X):\\n...     print(\\\"%s %s\\\" % (train, test))\\n[0 1 2] [3]\\n[0 1 2 3] [4]\\n[0 1 2 3 4] [5]\\n\\n\\nHere is a visualization of the cross-validation behavior.\\n\\n\\n\\n\\n\\n\\n\\n\\n3.1.3. A note on shuffling#\\nIf the data ordering is not arbitrary (e.g. samples with the same class label\\nare contiguous), shuffling it first may be essential to get a meaningful cross-\\nvalidation result. However, the opposite may be true if the samples are not\\nindependently and identically distributed. For example, if samples correspond\\nto news articles, and are ordered by their time of publication, then shuffling\\nthe data will likely lead to a model that is overfit and an inflated validation\\nscore: it will be tested on samples that are artificially similar (close in\\ntime) to training samples.\\nSome cross validation iterators, such as KFold, have an inbuilt option\\nto shuffle the data indices before splitting them. Note that:\\n---------new doc---------\\nThis consumes less memory than shuffling the data directly.\\nBy default no shuffling occurs, including for the (stratified) K fold cross-\\nvalidation performed by specifying cv=some_integer to\\ncross_val_score, grid search, etc. Keep in mind that\\ntrain_test_split still returns a random split.\\nThe random_state parameter defaults to None, meaning that the\\nshuffling will be different every time KFold(..., shuffle=True) is\\niterated. However, GridSearchCV will use the same shuffling for each set\\nof parameters validated by a single call to its fit method.\\nTo get identical results for each split, set random_state to an integer.\\n\\nFor more details on how to control the randomness of cv splitters and avoid\\ncommon pitfalls, see Controlling randomness.\\n\\n\\n3.1.4. Cross validation and model selection#\\nCross validation iterators can also be used to directly perform model\\nselection using Grid Search for the optimal hyperparameters of the\\nmodel. This is the topic of the next section: Tuning the hyper-parameters of an estimator.\\n---------new doc---------\\nthe data. In the latter case, using a more appropriate classifier that\\nis able to utilize the structure in the data, would result in a lower\\np-value.\\nCross-validation provides information about how well a classifier generalizes,\\nspecifically the range of expected errors of the classifier. However, a\\nclassifier trained on a high dimensional dataset with no structure may still\\nperform better than expected on cross-validation, just by chance.\\nThis can typically happen with small datasets with less than a few hundred\\nsamples.\\npermutation_test_score provides information\\non whether the classifier has found a real class structure and can help in\\nevaluating the performance of the classifier.\\nIt is important to note that this test has been shown to produce low\\np-values even if there is only weak structure in the data because in the\\ncorresponding permutated datasets there is absolutely no structure. This\\ntest is therefore only able to show when the model reliably outperforms\\nrandom guessing.\\nFinally, permutation_test_score is computed\\nusing brute force and internally fits (n_permutations + 1) * n_cv models.\\nIt is therefore only tractable with small datasets for which fitting an\\nindividual model is very fast.\\nExamples\\n---------new doc---------\\nTest with permutations the significance of a classification score\\n\\n\\n\\nReferences#\\n\\nOjala and Garriga. Permutation Tests for Studying Classifier Performance.\\nJ. Mach. Learn. Res. 2010.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3. Model selection and evaluation\\n\\n\\n\\n\\nnext\\n3.2. Tuning the hyper-parameters of an estimator\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.2. Tuning the hyper-parameters of an estimator\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3.2. Tuning the hyper-parameters of an estimator#\\nHyper-parameters are parameters that are not directly learnt within estimators.\\nIn scikit-learn they are passed as arguments to the constructor of the\\nestimator classes. Typical examples include C, kernel and gamma\\nfor Support Vector Classifier, alpha for Lasso, etc.\\nIt is possible and recommended to search the hyper-parameter space for the\\nbest cross validation score.\\nAny parameter provided when constructing an estimator may be optimized in this\\nmanner. Specifically, to find the names and current values for all parameters\\nfor a given estimator, use:\\nestimator.get_params()\\n\\n\\nA search consists of:\\n\\nan estimator (regressor or classifier such as sklearn.svm.SVC());\\na parameter space;\\na method for searching or sampling candidates;\\na cross-validation scheme; and\\na score function.\\n---------new doc---------\\nAdvanced examples#\\n\\nSee Nested versus non-nested cross-validation\\nfor an example of Grid Search within a cross validation loop on the iris\\ndataset. This is the best practice for evaluating the performance of a\\nmodel with grid search.\\nSee Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV\\nfor an example of GridSearchCV being used to evaluate multiple\\nmetrics simultaneously.\\nSee Balance model complexity and cross-validated score\\nfor an example of using refit=callable interface in\\nGridSearchCV. The example shows how this interface adds certain\\namount of flexibility in identifying the “best” estimator. This interface\\ncan also be used in multiple metrics evaluation.\\nSee Statistical comparison of models using grid search\\nfor an example of how to do a statistical comparison on the outputs of\\nGridSearchCV.\\n\\n\\n\\n\\n3.2.2. Randomized Parameter Optimization#\\nWhile using a grid of parameter settings is currently the most widely used\\nmethod for parameter optimization, other search methods have more\\nfavorable properties.\\nRandomizedSearchCV implements a randomized search over parameters,\\nwhere each setting is sampled from a distribution over possible parameter values.\\nThis has two main benefits over an exhaustive search:\\n\\nA budget can be chosen independent of the number of parameters and possible values.\\nAdding parameters that do not influence the performance does not decrease efficiency.\\n---------new doc---------\\nA budget can be chosen independent of the number of parameters and possible values.\\nAdding parameters that do not influence the performance does not decrease efficiency.\\n\\nSpecifying how parameters should be sampled is done using a dictionary, very\\nsimilar to specifying parameters for GridSearchCV. Additionally,\\na computation budget, being the number of sampled candidates or sampling\\niterations, is specified using the n_iter parameter.\\nFor each parameter, either a distribution over possible values or a list of\\ndiscrete choices (which will be sampled uniformly) can be specified:\\n{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\\n  'kernel': ['rbf'], 'class_weight':['balanced', None]}\\n\\n\\nThis example uses the scipy.stats module, which contains many useful\\ndistributions for sampling parameters, such as expon, gamma,\\nuniform, loguniform or randint.\\nIn principle, any function can be passed that provides a rvs (random\\nvariate sample) method to sample a value. A call to the rvs function should\\nprovide independent random samples from possible parameter values on\\nconsecutive calls.\\n\\nWarning\\nThe distributions in scipy.stats prior to version scipy 0.16\\ndo not allow specifying a random state. Instead, they use the global\\nnumpy random state, that can be seeded via np.random.seed or set\\nusing np.random.set_state. However, beginning scikit-learn 0.18,\\nthe sklearn.model_selection module sets the random state provided\\nby the user if scipy >= 0.16 is also available.\\n---------new doc---------\\nChoosing a resource#\\nBy default, the resource is defined in terms of number of samples. That is,\\neach iteration will use an increasing amount of samples to train on. You can\\nhowever manually specify a parameter to use as the resource with the\\nresource parameter. Here is an example where the resource is defined in\\nterms of the number of estimators of a random forest:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.ensemble import RandomForestClassifier\\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\\n>>> from sklearn.model_selection import HalvingGridSearchCV\\n>>> import pandas as pd\\n>>> param_grid = {'max_depth': [3, 5, 10],\\n...               'min_samples_split': [2, 5, 10]}\\n>>> base_estimator = RandomForestClassifier(random_state=0)\\n>>> X, y = make_classification(n_samples=1000, random_state=0)\\n>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\\n...                          factor=2, resource='n_estimators',\\n...                          max_resources=30).fit(X, y)\\n>>> sh.best_estimator_\\nRandomForestClassifier(max_depth=5, n_estimators=24, random_state=0)\\n---------new doc---------\\n3.2.3.1. Aggressive elimination of candidates#\\nUsing the aggressive_elimination parameter, you can force the search\\nprocess to end up with less than factor candidates at the last\\niteration.\\n\\n\\nCode example of aggressive elimination#\\nIdeally, we want the last iteration to evaluate factor candidates. We\\nthen just have to pick the best one. When the number of available resources is\\nsmall with respect to the number of candidates, the last iteration may have to\\nevaluate more than factor candidates:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.svm import SVC\\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\\n>>> from sklearn.model_selection import HalvingGridSearchCV\\n>>> import pandas as pd\\n>>> param_grid = {'kernel': ('linear', 'rbf'),\\n...               'C': [1, 10, 100]}\\n>>> base_estimator = SVC(gamma='scale')\\n>>> X, y = make_classification(n_samples=1000)\\n>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\\n...                          factor=2, max_resources=40,\\n...                          aggressive_elimination=False).fit(X, y)\\n>>> sh.n_resources_\\n[20, 40]\\n>>> sh.n_candidates_\\n[6, 3]\\n---------new doc---------\\nNotice that we end with 2 candidates at the last iteration since we have\\neliminated enough candidates during the first iterations, using n_resources =\\nmin_resources = 20.\\n\\n\\n\\n3.2.3.2. Analyzing results with the cv_results_ attribute#\\nThe cv_results_ attribute contains useful information for analyzing the\\nresults of a search. It can be converted to a pandas dataframe with df =\\npd.DataFrame(est.cv_results_). The cv_results_ attribute of\\nHalvingGridSearchCV and HalvingRandomSearchCV is similar\\nto that of GridSearchCV and RandomizedSearchCV, with\\nadditional information related to the successive halving process.\\n\\n\\nExample of a (truncated) output dataframe:#\\n\\n\\n\\niter\\nn_resources\\nmean_test_score\\nparams\\n\\n\\n\\n0\\n0\\n125\\n0.983667\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 5}\\n\\n1\\n0\\n125\\n0.983667\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 8, ‘min_samples_split’: 7}\\n\\n2\\n0\\n125\\n0.983667\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\\n---------new doc---------\\nHere, <estimator> is the parameter name of the nested estimator,\\nin this case estimator.\\nIf the meta-estimator is constructed as a collection of estimators as in\\npipeline.Pipeline, then <estimator> refers to the name of the estimator,\\nsee Access to nested parameters. In practice, there can be several\\nlevels of nesting:\\n>>> from sklearn.pipeline import Pipeline\\n>>> from sklearn.feature_selection import SelectKBest\\n>>> pipe = Pipeline([\\n...    ('select', SelectKBest()),\\n...    ('model', calibrated_forest)])\\n>>> param_grid = {\\n...    'select__k': [1, 2],\\n...    'model__estimator__max_depth': [2, 4, 6, 8]}\\n>>> search = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)\\n\\n\\nPlease refer to Pipeline: chaining estimators for performing parameter searches over\\npipelines.\\n\\n\\n3.2.4.4. Model selection: development and evaluation#\\nModel selection by evaluating various parameter settings can be seen as a way\\nto use the labeled data to “train” the parameters of the grid.\\nWhen evaluating the resulting model it is important to do it on\\nheld-out samples that were not seen during the grid search process:\\nit is recommended to split the data into a development set (to\\nbe fed to the GridSearchCV instance) and an evaluation set\\nto compute performance metrics.\\nThis can be done by using the train_test_split\\nutility function.\\n---------new doc---------\\nensemble.RandomForestClassifier([...])\\nA random forest classifier.\\n\\nensemble.RandomForestRegressor([...])\\nA random forest regressor.\\n\\nensemble.ExtraTreesClassifier([...])\\nAn extra-trees classifier.\\n\\nensemble.ExtraTreesRegressor([n_estimators, ...])\\nAn extra-trees regressor.\\n\\nensemble.GradientBoostingClassifier(*[, ...])\\nGradient Boosting for classification.\\n\\nensemble.GradientBoostingRegressor(*[, ...])\\nGradient Boosting for regression.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.1. Cross-validation: evaluating estimator performance\\n\\n\\n\\n\\nnext\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3.3. Tuning the decision threshold for class prediction#\\nClassification is best divided into two parts:\\n\\nthe statistical problem of learning a model to predict, ideally, class probabilities;\\nthe decision problem to take concrete action based on those probability predictions.\\n---------new doc---------\\nLet’s take a straightforward example related to weather forecasting: the first point is\\nrelated to answering “what is the chance that it will rain tomorrow?” while the second\\npoint is related to answering “should I take an umbrella tomorrow?”.\\nWhen it comes to the scikit-learn API, the first point is addressed providing scores\\nusing predict_proba or decision_function. The former returns conditional\\nprobability estimates \\\\(P(y|X)\\\\) for each class, while the latter returns a decision\\nscore for each class.\\nThe decision corresponding to the labels are obtained with predict. In binary\\nclassification, a decision rule or action is then defined by thresholding the scores,\\nleading to the prediction of a single class label for each sample. For binary\\nclassification in scikit-learn, class labels predictions are obtained by hard-coded\\ncut-off rules: a positive class is predicted when the conditional probability\\n\\\\(P(y|X)\\\\) is greater than 0.5 (obtained with predict_proba) or if the\\ndecision score is greater than 0 (obtained with decision_function).\\nHere, we show an example that illustrates the relation between conditional\\nprobability estimates \\\\(P(y|X)\\\\) and class labels:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> X, y = make_classification(random_state=0)\\n>>> classifier = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y)\\n>>> classifier.predict_proba(X[:4])\\narray([[0.94     , 0.06     ],\\n---------new doc---------\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> X, y = make_classification(random_state=0)\\n>>> classifier = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y)\\n>>> classifier.predict_proba(X[:4])\\narray([[0.94     , 0.06     ],\\n       [0.94     , 0.06     ],\\n       [0.0416..., 0.9583...],\\n       [0.0416..., 0.9583...]])\\n>>> classifier.predict(X[:4])\\narray([0, 0, 1, 1])\\n---------new doc---------\\n3.3.1. Post-tuning the decision threshold#\\nOne solution to address the problem stated in the introduction is to tune the decision\\nthreshold of the classifier once the model has been trained. The\\nTunedThresholdClassifierCV tunes this threshold using\\nan internal cross-validation. The optimum threshold is chosen to maximize a given\\nmetric.\\nThe following image illustrates the tuning of the decision threshold for a gradient\\nboosting classifier. While the vanilla and tuned classifiers provide the same\\npredict_proba outputs and thus the same Receiver Operating Characteristic (ROC)\\nand Precision-Recall curves, the class label predictions differ because of the tuned\\ndecision threshold. The vanilla classifier predicts the class of interest for a\\nconditional probability greater than 0.5 while the tuned classifier predicts the class\\nof interest for a very low probability (around 0.02). This decision threshold optimizes\\na utility metric defined by the business (in this case an insurance company).\\n\\n\\n\\n\\n\\n3.3.1.1. Options to tune the decision threshold#\\nThe decision threshold can be tuned through different strategies controlled by the\\nparameter scoring.\\nOne way to tune the threshold is by maximizing a pre-defined scikit-learn metric. These\\nmetrics can be found by calling the function get_scorer_names.\\nBy default, the balanced accuracy is the metric used but be aware that one should choose\\na meaningful metric for their use case.\\n---------new doc---------\\n3.3.1.2. Important notes regarding the internal cross-validation#\\nBy default TunedThresholdClassifierCV uses a 5-fold\\nstratified cross-validation to tune the decision threshold. The parameter cv allows to\\ncontrol the cross-validation strategy. It is possible to bypass cross-validation by\\nsetting cv=\\\"prefit\\\" and providing a fitted classifier. In this case, the decision\\nthreshold is tuned on the data provided to the fit method.\\nHowever, you should be extremely careful when using this option. You should never use\\nthe same data for training the classifier and tuning the decision threshold due to the\\nrisk of overfitting. Refer to the following example section for more details (cf.\\nConsideration regarding model refitting and cross-validation). If you have limited resources, consider using\\na float number for cv to limit to an internal single train-test split.\\nThe option cv=\\\"prefit\\\" should only be used when the provided classifier was already\\ntrained, and you just want to find the best decision threshold using a new validation\\nset.\\n\\n\\n3.3.1.3. Manually setting the decision threshold#\\nThe previous sections discussed strategies to find an optimal decision threshold. It is\\nalso possible to manually set the decision threshold using the class\\nFixedThresholdClassifier. In case that you don’t want\\nto refit the model when calling fit, wrap your sub-estimator with a\\nFrozenEstimator and do\\nFixedThresholdClassifier(FrozenEstimator(estimator), ...).\\n\\n\\n3.3.1.4. Examples#\\n---------new doc---------\\nclass 0       0.67      1.00      0.80         2\\n     class 1       0.00      0.00      0.00         1\\n     class 2       1.00      0.50      0.67         2\\n\\n    accuracy                           0.60         5\\n   macro avg       0.56      0.50      0.49         5\\nweighted avg       0.67      0.60      0.59         5\\n\\n\\nExamples\\n\\nSee Recognizing hand-written digits\\nfor an example of classification report usage for\\nhand-written digits.\\nSee Custom refit strategy of a grid search with cross-validation\\nfor an example of classification report usage for\\ngrid search with nested cross-validation.\\n---------new doc---------\\nExamples\\n\\nSee Multiclass Receiver Operating Characteristic (ROC) for an example of\\nusing ROC to evaluate the quality of the output of a classifier.\\nSee Receiver Operating Characteristic (ROC) with cross validation  for an\\nexample of using ROC to evaluate classifier output quality, using cross-validation.\\nSee Species distribution modeling\\nfor an example of using ROC to model species distribution.\\n\\nReferences\\n\\n\\n[HT2001]\\n(1,2)\\nHand, D.J. and Till, R.J., (2001). A simple generalisation\\nof the area under the ROC curve for multiple class classification problems.\\nMachine learning, 45(2), pp. 171-186.\\n\\n\\n[FC2009]\\nFerri, Cèsar & Hernandez-Orallo, Jose & Modroiu, R. (2009).\\nAn Experimental Comparison of Performance Measures for Classification.\\nPattern Recognition Letters. 30. 27-38.\\n\\n\\n[PD2000]\\nProvost, F., Domingos, P. (2000). Well-trained PETs: Improving\\nprobability estimation trees\\n(Section 6.2), CeDER Working Paper #IS-00-04, Stern School of Business,\\nNew York University.\\n\\n\\n[F2006]\\nFawcett, T., 2006. An introduction to ROC analysis.\\nPattern Recognition Letters, 27(8), pp. 861-874.\\n\\n\\n[F2001]\\nFawcett, T., 2001. Using rule sets to maximize\\nROC performance\\nIn Data Mining, 2001.\\nProceedings IEEE International Conference, pp. 131-138.\\n---------new doc---------\\nSee Effect of transforming the targets in regression model for\\nan example on how to use PredictionErrorDisplay\\nto visualize the prediction quality improvement of a regression model\\nobtained by transforming the target before learning.\\n\\n\\n\\n\\n3.4.7. Clustering metrics#\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions to measure clustering performance. For more information see the\\nClustering performance evaluation section for instance clustering, and\\nBiclustering evaluation for biclustering.\\n\\n\\n3.4.8. Dummy estimators#\\nWhen doing supervised learning, a simple sanity check consists of comparing\\none’s estimator against simple rules of thumb. DummyClassifier\\nimplements several such simple strategies for classification:\\n\\nstratified generates random predictions by respecting the training\\nset class distribution.\\nmost_frequent always predicts the most frequent label in the training set.\\nprior always predicts the class that maximizes the class prior\\n(like most_frequent) and predict_proba returns the class prior.\\nuniform generates predictions uniformly at random.\\n\\nconstant always predicts a constant label that is provided by the user.A major motivation of this method is F1-scoring, when the positive class\\nis in the minority.\\n---------new doc---------\\nconstant always predicts a constant label that is provided by the user.A major motivation of this method is F1-scoring, when the positive class\\nis in the minority.\\n\\n\\n\\n\\nNote that with all these strategies, the predict method completely ignores\\nthe input data!\\nTo illustrate DummyClassifier, first let’s create an imbalanced\\ndataset:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.model_selection import train_test_split\\n>>> X, y = load_iris(return_X_y=True)\\n>>> y[y != 1] = -1\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\n\\nNext, let’s compare the accuracy of SVC and most_frequent:\\n>>> from sklearn.dummy import DummyClassifier\\n>>> from sklearn.svm import SVC\\n>>> clf = SVC(kernel='linear', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.63...\\n>>> clf = DummyClassifier(strategy='most_frequent', random_state=0)\\n>>> clf.fit(X_train, y_train)\\nDummyClassifier(random_state=0, strategy='most_frequent')\\n>>> clf.score(X_test, y_test)\\n0.57...\\n---------new doc---------\\nWe see that SVC doesn’t do much better than a dummy classifier. Now, let’s\\nchange the kernel:\\n>>> clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.94...\\n\\n\\nWe see that the accuracy was boosted to almost 100%.  A cross validation\\nstrategy is recommended for a better estimate of the accuracy, if it\\nis not too CPU costly. For more information see the Cross-validation: evaluating estimator performance\\nsection. Moreover if you want to optimize over the parameter space, it is highly\\nrecommended to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator\\nsection for details.\\nMore generally, when the accuracy of a classifier is too close to random, it\\nprobably means that something went wrong: features are not helpful, a\\nhyperparameter is not correctly tuned, the classifier is suffering from class\\nimbalance, etc…\\nDummyRegressor also implements four simple rules of thumb for regression:\\n\\nmean always predicts the mean of the training targets.\\nmedian always predicts the median of the training targets.\\nquantile always predicts a user provided quantile of the training targets.\\nconstant always predicts a constant value that is provided by the user.\\n\\nIn all these strategies, the predict method completely ignores\\nthe input data.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n\\n\\nnext\\n3.5. Validation curves: plotting scores to evaluate models\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.5. Validation curves: plotting scores to evaluate models\\n---------new doc---------\\nIn all these strategies, the predict method completely ignores\\nthe input data.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.3. Tuning the decision threshold for class prediction\\n\\n\\n\\n\\nnext\\n3.5. Validation curves: plotting scores to evaluate models\\n\\n\\n --- \\n\\n\\n3. Model selection and evaluation\\n3.5. Validation curves: plotting scores to evaluate models\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3.5. Validation curves: plotting scores to evaluate models#\\nEvery estimator has its advantages and drawbacks. Its generalization error\\ncan be decomposed in terms of bias, variance and noise. The bias of an\\nestimator is its average error for different training sets. The variance\\nof an estimator indicates how sensitive it is to varying training sets. Noise\\nis a property of the data.\\nIn the following plot, we see a function \\\\(f(x) = \\\\cos (\\\\frac{3}{2} \\\\pi x)\\\\)\\nand some noisy samples from that function. We use three different estimators\\nto fit the function: linear regression with polynomial features of degree 1,\\n4 and 15. We see that the first estimator can at best provide only a poor fit\\nto the samples and the true function because it is too simple (high bias),\\nthe second estimator approximates it almost perfectly and the last estimator\\napproximates the training data perfectly but does not fit the true function\\nvery well, i.e. it is very sensitive to varying training data (high variance).\\n---------new doc---------\\nUnderfitting vs. Overfitting\\nEffect of model regularization on training and test error\\nPlotting Learning Curves and Checking Models’ Scalability\\n\\n\\n3.5.1. Validation curve#\\nTo validate a model we need a scoring function (see Metrics and scoring: quantifying the quality of predictions),\\nfor example accuracy for classifiers. The proper way of choosing multiple\\nhyperparameters of an estimator is of course grid search or similar methods\\n(see Tuning the hyper-parameters of an estimator) that select the hyperparameter with the maximum score\\non a validation set or multiple validation sets. Note that if we optimize\\nthe hyperparameters based on a validation score the validation score is biased\\nand not a good estimate of the generalization any longer. To get a proper\\nestimate of the generalization we have to compute the score on another test\\nset.\\nHowever, it is sometimes helpful to plot the influence of a single\\nhyperparameter on the training score and the validation score to find out\\nwhether the estimator is overfitting or underfitting for some hyperparameter\\nvalues.\\nThe function validation_curve can help in this case:\\n>>> import numpy as np\\n>>> from sklearn.model_selection import validation_curve\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.svm import SVC\\n\\n>>> np.random.seed(0)\\n>>> X, y = load_iris(return_X_y=True)\\n>>> indices = np.arange(y.shape[0])\\n>>> np.random.shuffle(indices)\\n>>> X, y = X[indices], y[indices]\\n---------new doc---------\\n>>> np.random.seed(0)\\n>>> X, y = load_iris(return_X_y=True)\\n>>> indices = np.arange(y.shape[0])\\n>>> np.random.shuffle(indices)\\n>>> X, y = X[indices], y[indices]\\n\\n>>> train_scores, valid_scores = validation_curve(\\n...     SVC(kernel=\\\"linear\\\"), X, y, param_name=\\\"C\\\", param_range=np.logspace(-7, 3, 3),\\n... )\\n>>> train_scores\\narray([[0.90..., 0.94..., 0.91..., 0.89..., 0.92...],\\n       [0.9... , 0.92..., 0.93..., 0.92..., 0.93...],\\n       [0.97..., 1...   , 0.98..., 0.97..., 0.99...]])\\n>>> valid_scores\\narray([[0.9..., 0.9... , 0.9... , 0.96..., 0.9... ],\\n       [0.9..., 0.83..., 0.96..., 0.96..., 0.93...],\\n       [1.... , 0.93..., 1....  , 1....  , 0.9... ]])\\n---------new doc---------\\nIf you intend to plot the validation curves only, the class\\nValidationCurveDisplay is more direct than\\nusing matplotlib manually on the results of a call to validation_curve.\\nYou can use the method\\nfrom_estimator similarly\\nto validation_curve to generate and plot the validation curve:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import ValidationCurveDisplay\\nfrom sklearn.svm import SVC\\nfrom sklearn.utils import shuffle\\nX, y = load_iris(return_X_y=True)\\nX, y = shuffle(X, y, random_state=0)\\nValidationCurveDisplay.from_estimator(\\n   SVC(kernel=\\\"linear\\\"), X, y, param_name=\\\"C\\\", param_range=np.logspace(-7, 3, 10)\\n)\\n\\n\\n\\n\\n\\nIf the training score and the validation score are both low, the estimator will\\nbe underfitting. If the training score is high and the validation score is low,\\nthe estimator is overfitting and otherwise it is working very well. A low\\ntraining score and a high validation score is usually not possible.\\n---------new doc---------\\nIf the training score and the validation score are both low, the estimator will\\nbe underfitting. If the training score is high and the validation score is low,\\nthe estimator is overfitting and otherwise it is working very well. A low\\ntraining score and a high validation score is usually not possible.\\n\\n\\n3.5.2. Learning curve#\\nA learning curve shows the validation and training score of an estimator\\nfor varying numbers of training samples. It is a tool to find out how much\\nwe benefit from adding more training data and whether the estimator suffers\\nmore from a variance error or a bias error. Consider the following example\\nwhere we plot the learning curve of a naive Bayes classifier and an SVM.\\nFor the naive Bayes, both the validation score and the training score\\nconverge to a value that is quite low with increasing size of the training\\nset. Thus, we will probably not benefit much from more training data.\\nIn contrast, for small amounts of data, the training score of the SVM is\\nmuch greater than the validation score. Adding more training samples will\\nmost likely increase generalization.\\n\\n\\n\\n\\nWe can use the function learning_curve to generate the values\\nthat are required to plot such a learning curve (number of samples\\nthat have been used, the average scores on the training sets and the\\naverage scores on the validation sets):\\n>>> from sklearn.model_selection import learning_curve\\n>>> from sklearn.svm import SVC\\n---------new doc---------\\nWe can use the function learning_curve to generate the values\\nthat are required to plot such a learning curve (number of samples\\nthat have been used, the average scores on the training sets and the\\naverage scores on the validation sets):\\n>>> from sklearn.model_selection import learning_curve\\n>>> from sklearn.svm import SVC\\n\\n>>> train_sizes, train_scores, valid_scores = learning_curve(\\n...     SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)\\n>>> train_sizes\\narray([ 50, 80, 110])\\n>>> train_scores\\narray([[0.98..., 0.98 , 0.98..., 0.98..., 0.98...],\\n       [0.98..., 1.   , 0.98..., 0.98..., 0.98...],\\n       [0.98..., 1.   , 0.98..., 0.98..., 0.99...]])\\n>>> valid_scores\\narray([[1. ,  0.93...,  1. ,  1. ,  0.96...],\\n       [1. ,  0.96...,  1. ,  1. ,  0.96...],\\n       [1. ,  0.96...,  1. ,  1. ,  0.96...]])\\n---------new doc---------\\nIf you intend to plot the learning curves only, the class\\nLearningCurveDisplay will be easier to use.\\nYou can use the method\\nfrom_estimator similarly\\nto learning_curve to generate and plot the learning curve:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import LearningCurveDisplay\\nfrom sklearn.svm import SVC\\nfrom sklearn.utils import shuffle\\nX, y = load_iris(return_X_y=True)\\nX, y = shuffle(X, y, random_state=0)\\nLearningCurveDisplay.from_estimator(\\n   SVC(kernel=\\\"linear\\\"), X, y, train_sizes=[50, 80, 110], cv=5)\\n\\n\\n\\n\\n\\nExamples\\n\\nSee Plotting Learning Curves and Checking Models’ Scalability for an\\nexample of using learning curves to check the scalability of a predictive model.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.4. Metrics and scoring: quantifying the quality of predictions\\n\\n\\n\\n\\nnext\\n4. Inspection\\n\\n\\n --- \\n\\n\\n4. Inspection\\n4.1. Partial Dependence and Individual Conditional Expectation plots\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4.1. Partial Dependence and Individual Conditional Expectation plots#\\nPartial dependence plots (PDP) and individual conditional expectation (ICE)\\nplots can be used to visualize and analyze interaction between the target\\nresponse [1] and a set of input features of interest.\\nBoth PDPs [H2009] and ICEs [G2015] assume that the input features of interest\\nare independent from the complement features, and this assumption is often\\nviolated in practice. Thus, in the case of correlated features, we will\\ncreate absurd data points to compute the PDP/ICE [M2019].\\n---------new doc---------\\nOne-way PDPs tell us about the interaction between the target response and an input\\nfeature of interest (e.g. linear, non-linear). The left plot in the above figure\\nshows the effect of the temperature on the number of bike rentals; we can clearly see\\nthat a higher temperature is related with a higher number of bike rentals. Similarly, we\\ncould analyze the effect of the humidity on the number of bike rentals (middle plot).\\nThus, these interpretations are marginal, considering a feature at a time.\\nPDPs with two input features of interest show the interactions among the two features.\\nFor example, the two-variable PDP in the above figure shows the dependence of the number\\nof bike rentals on joint values of temperature and humidity. We can clearly see an\\ninteraction between the two features: with a temperature higher than 20 degrees Celsius,\\nmainly the humidity has a strong impact on the number of bike rentals. For lower\\ntemperatures, both the temperature and the humidity have an impact on the number of bike\\nrentals.\\nThe sklearn.inspection module provides a convenience function\\nfrom_estimator to create one-way and two-way partial\\ndependence plots. In the below example we show how to create a grid of\\npartial dependence plots: two one-way PDPs for the features 0 and 1\\nand a two-way PDP between the two features:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n>>> from sklearn.inspection import PartialDependenceDisplay\\n---------new doc---------\\nThe same parameter target is used to specify the target in multi-output\\nregression settings.\\n\\nIf you need the raw values of the partial dependence function rather than\\nthe plots, you can use the\\nsklearn.inspection.partial_dependence function:\\n>>> from sklearn.inspection import partial_dependence\\n\\n>>> results = partial_dependence(clf, X, [0])\\n>>> results[\\\"average\\\"]\\narray([[ 2.466...,  2.466..., ...\\n>>> results[\\\"grid_values\\\"]\\n[array([-1.624..., -1.592..., ...\\n\\n\\nThe values at which the partial dependence should be evaluated are directly\\ngenerated from X. For 2-way partial dependence, a 2D-grid of values is\\ngenerated. The values field returned by\\nsklearn.inspection.partial_dependence gives the actual values\\nused in the grid for each input feature of interest. They also correspond to\\nthe axis of the plots.\\n\\n\\n4.1.2. Individual conditional expectation (ICE) plot#\\nSimilar to a PDP, an individual conditional expectation (ICE) plot\\nshows the dependence between the target function and an input feature of\\ninterest. However, unlike a PDP, which shows the average effect of the input\\nfeature, an ICE plot visualizes the dependence of the prediction on a\\nfeature for each sample separately with one line per sample.\\nDue to the limits of human perception, only one input feature of interest is\\nsupported for ICE plots.\\nThe figures below show two ICE plots for the bike sharing dataset,\\nwith a HistGradientBoostingRegressor:.\\nThe figures plot the corresponding PD line overlaid on ICE lines.\\n---------new doc---------\\nWhile the PDPs are good at showing the average effect of the target features,\\nthey can obscure a heterogeneous relationship created by interactions.\\nWhen interactions are present the ICE plot will provide many more insights.\\nFor example, we see that the ICE for the temperature feature gives us some\\nadditional information: Some of the ICE lines are flat while some others\\nshows a decrease of the dependence for temperature above 35 degrees Celsius.\\nWe observe a similar pattern for the humidity feature: some of the ICE\\nlines show a sharp decrease when the humidity is above 80%.\\nThe sklearn.inspection module’s PartialDependenceDisplay.from_estimator\\nconvenience function can be used to create ICE plots by setting\\nkind='individual'. In the example below, we show how to create a grid of\\nICE plots:\\n>>> from sklearn.datasets import make_hastie_10_2\\n>>> from sklearn.ensemble import GradientBoostingClassifier\\n>>> from sklearn.inspection import PartialDependenceDisplay\\n\\n\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\n>>> features = [0, 1]\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind='individual')\\n<...>\\n---------new doc---------\\nbeing weighted by the fraction of training samples that entered that branch.\\nFinally, the partial dependence is given by a weighted average of all the\\nvisited leaves values.\\nWith the ‘brute’ method, the parameter X is used both for generating the\\ngrid of values \\\\(x_S\\\\) and the complement feature values \\\\(x_C\\\\).\\nHowever with the ‘recursion’ method, X is only used for the grid values:\\nimplicitly, the \\\\(x_C\\\\) values are those of the training data.\\nBy default, the ‘recursion’ method is used for plotting PDPs on tree-based\\nestimators that support it, and ‘brute’ is used for the rest.\\n---------new doc---------\\nNote\\nWhile both methods should be close in general, they might differ in some\\nspecific settings. The ‘brute’ method assumes the existence of the\\ndata points \\\\((x_S, x_C^{(i)})\\\\). When the features are correlated,\\nsuch artificial samples may have a very low probability mass. The ‘brute’\\nand ‘recursion’ methods will likely disagree regarding the value of the\\npartial dependence, because they will treat these unlikely\\nsamples differently. Remember, however, that the primary assumption for\\ninterpreting PDPs is that the features should be independent.\\n\\nExamples\\n\\nPartial Dependence and Individual Conditional Expectation Plots\\n\\nFootnotes\\n\\n\\n[1]\\nFor classification, the target response may be the probability of a\\nclass (the positive class for binary classification), or the decision\\nfunction.\\n\\n\\nReferences\\n\\n\\n[H2009]\\nT. Hastie, R. Tibshirani and J. Friedman,\\nThe Elements of Statistical Learning,\\nSecond Edition, Section 10.13.2, Springer, 2009.\\n\\n\\n[M2019]\\nC. Molnar,\\nInterpretable Machine Learning,\\nSection 5.1, 2019.\\n\\n\\n[G2015]\\n(1,2)\\nA. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin,\\n“Peeking Inside the Black Box: Visualizing Statistical\\nLearning With Plots of Individual Conditional Expectation”\\nJournal of Computational and Graphical Statistics,\\n24(1): 44-65, Springer, 2015.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n4. Inspection\\n\\n\\n\\n\\nnext\\n4.2. Permutation feature importance\\n\\n\\n --- \\n\\n\\n4. Inspection\\n4.2. Permutation feature importance\\n---------new doc---------\\nprevious\\n4. Inspection\\n\\n\\n\\n\\nnext\\n4.2. Permutation feature importance\\n\\n\\n --- \\n\\n\\n4. Inspection\\n4.2. Permutation feature importance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4.2. Permutation feature importance#\\nPermutation feature importance is a model inspection technique that measures the\\ncontribution of each feature to a fitted model’s statistical performance\\non a given tabular dataset. This technique is particularly useful for non-linear\\nor opaque estimators, and involves randomly shuffling the values of a\\nsingle feature and observing the resulting degradation of the model’s score\\n[1]. By breaking the relationship between the feature and the target, we\\ndetermine how much the model relies on such particular feature.\\nIn the following figures, we observe the effect of permuting features on the correlation\\nbetween the feature and the target and consequently on the model statistical\\nperformance.\\n---------new doc---------\\nOn the top figure, we observe that permuting a predictive feature breaks the\\ncorrelation between the feature and the target, and consequently the model\\nstatistical performance decreases. On the bottom figure, we observe that permuting\\na non-predictive feature does not significantly degrade the model statistical performance.\\nOne key advantage of permutation feature importance is that it is\\nmodel-agnostic, i.e. it can be applied to any fitted estimator. Moreover, it can\\nbe calculated multiple times with different permutations of the feature, further\\nproviding a measure of the variance in the estimated feature importances for the\\nspecific trained model.\\nThe figure below shows the permutation feature importance of a\\nRandomForestClassifier trained on an augmented\\nversion of the titanic dataset that contains a random_cat and a random_num\\nfeatures, i.e. a categrical and a numerical feature that are not correlated in\\nany way with the target variable:\\n\\n\\n\\n\\n\\nWarning\\nFeatures that are deemed of low importance for a bad model (low\\ncross-validation score) could be very important for a good model.\\nTherefore it is always important to evaluate the predictive power of a model\\nusing a held-out set (or better with cross-validation) prior to computing\\nimportances. Permutation importance does not reflect to the intrinsic\\npredictive value of a feature by itself but how important this feature is\\nfor a particular model.\\n---------new doc---------\\nThe permutation_importance function calculates the feature importance\\nof estimators for a given dataset. The n_repeats parameter sets the\\nnumber of times a feature is randomly shuffled and returns a sample of feature\\nimportances.\\nLet’s consider the following trained regression model:\\n>>> from sklearn.datasets import load_diabetes\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn.linear_model import Ridge\\n>>> diabetes = load_diabetes()\\n>>> X_train, X_val, y_train, y_val = train_test_split(\\n...     diabetes.data, diabetes.target, random_state=0)\\n...\\n>>> model = Ridge(alpha=1e-2).fit(X_train, y_train)\\n>>> model.score(X_val, y_val)\\n0.356...\\n---------new doc---------\\nIts validation performance, measured via the \\\\(R^2\\\\) score, is\\nsignificantly larger than the chance level. This makes it possible to use the\\npermutation_importance function to probe which features are most\\npredictive:\\n>>> from sklearn.inspection import permutation_importance\\n>>> r = permutation_importance(model, X_val, y_val,\\n...                            n_repeats=30,\\n...                            random_state=0)\\n...\\n>>> for i in r.importances_mean.argsort()[::-1]:\\n...     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\\n...         print(f\\\"{diabetes.feature_names[i]:<8}\\\"\\n...               f\\\"{r.importances_mean[i]:.3f}\\\"\\n...               f\\\" +/- {r.importances_std[i]:.3f}\\\")\\n...\\ns5      0.204 +/- 0.050\\nbmi     0.176 +/- 0.048\\nbp      0.088 +/- 0.033\\nsex     0.056 +/- 0.023\\n---------new doc---------\\nNote that the importance values for the top features represent a large\\nfraction of the reference score of 0.356.\\nPermutation importances can be computed either on the training set or on a\\nheld-out testing or validation set. Using a held-out set makes it possible to\\nhighlight which features contribute the most to the generalization power of the\\ninspected model. Features that are important on the training set but not on the\\nheld-out set might cause the model to overfit.\\nThe permutation feature importance depends on the score function that is\\nspecified with the scoring argument. This argument accepts multiple scorers,\\nwhich is more computationally efficient than sequentially calling\\npermutation_importance several times with a different scorer, as it\\nreuses model predictions.\\n---------new doc---------\\nThe ranking of the features is approximately the same for different metrics even\\nif the scales of the importance values are very different. However, this is not\\nguaranteed and different metrics might lead to significantly different feature\\nimportances, in particular for models trained for imbalanced classification problems,\\nfor which the choice of the classification metric can be critical.\\n\\n\\n4.2.1. Outline of the permutation importance algorithm#\\n\\nInputs: fitted predictive model \\\\(m\\\\), tabular dataset (training or\\nvalidation) \\\\(D\\\\).\\nCompute the reference score \\\\(s\\\\) of the model \\\\(m\\\\) on data\\n\\\\(D\\\\) (for instance the accuracy for a classifier or the \\\\(R^2\\\\) for\\na regressor).\\nFor each feature \\\\(j\\\\) (column of \\\\(D\\\\)):\\n\\nFor each repetition \\\\(k\\\\) in \\\\({1, ..., K}\\\\):\\n\\nRandomly shuffle column \\\\(j\\\\) of dataset \\\\(D\\\\) to generate a\\ncorrupted version of the data named \\\\(\\\\tilde{D}_{k,j}\\\\).\\nCompute the score \\\\(s_{k,j}\\\\) of model \\\\(m\\\\) on corrupted data\\n\\\\(\\\\tilde{D}_{k,j}\\\\).\\n\\n\\nCompute importance \\\\(i_j\\\\) for feature \\\\(f_j\\\\) defined as:\\n\\n\\\\[i_j = s - \\\\frac{1}{K} \\\\sum_{k=1}^{K} s_{k,j}\\\\]\\n---------new doc---------\\nCompute importance \\\\(i_j\\\\) for feature \\\\(f_j\\\\) defined as:\\n\\n\\\\[i_j = s - \\\\frac{1}{K} \\\\sum_{k=1}^{K} s_{k,j}\\\\]\\n\\n\\n\\n\\n\\n\\n4.2.2. Relation to impurity-based importance in trees#\\nTree-based models provide an alternative measure of feature importances\\nbased on the mean decrease in impurity\\n(MDI). Impurity is quantified by the splitting criterion of the decision trees\\n(Gini, Log Loss or Mean Squared Error). However, this method can give high\\nimportance to features that may not be predictive on unseen data when the model\\nis overfitting. Permutation-based feature importance, on the other hand, avoids\\nthis issue, since it can be computed on unseen data.\\nFurthermore, impurity-based feature importance for trees are strongly\\nbiased and favor high cardinality features (typically numerical features)\\nover low cardinality features such as binary features or categorical variables\\nwith a small number of possible categories.\\nPermutation-based feature importances do not exhibit such a bias. Additionally,\\nthe permutation feature importance may be computed with any performance metric\\non the model predictions and can be used to analyze any model class (not just\\ntree-based models).\\nThe following example highlights the limitations of impurity-based feature\\nimportance in contrast to permutation-based feature importance:\\nPermutation Importance vs Random Forest Feature Importance (MDI).\\n---------new doc---------\\n4.2.3. Misleading values on strongly correlated features#\\nWhen two features are correlated and one of the features is permuted, the model\\nstill has access to the latter through its correlated feature. This results in a\\nlower reported importance value for both features, though they might actually\\nbe important.\\nThe figure below shows the permutation feature importance of a\\nRandomForestClassifier trained using the\\nBreast cancer wisconsin (diagnostic) dataset, which contains strongly correlated features. A\\nnaive interpretation would suggest that all features are unimportant:\\n\\n\\n\\n\\nOne way to handle the issue is to cluster features that are correlated and only\\nkeep one feature from each cluster.\\n\\n\\n\\n\\nFor more details on such strategy, see the example\\nPermutation Importance with Multicollinear or Correlated Features.\\nExamples\\n\\nPermutation Importance vs Random Forest Feature Importance (MDI)\\nPermutation Importance with Multicollinear or Correlated Features\\n\\nReferences\\n\\n\\n[1]\\nL. Breiman, “Random Forests”,\\nMachine Learning, 45(1), 5-32, 2001.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n4.1. Partial Dependence and Individual Conditional Expectation plots\\n\\n\\n\\n\\nnext\\n5. Visualizations\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.1. Pipelines and composite estimators\\n---------new doc---------\\nReferences\\n\\n\\n[1]\\nL. Breiman, “Random Forests”,\\nMachine Learning, 45(1), 5-32, 2001.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n4.1. Partial Dependence and Individual Conditional Expectation plots\\n\\n\\n\\n\\nnext\\n5. Visualizations\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.1. Pipelines and composite estimators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6.1. Pipelines and composite estimators#\\nTo build a composite estimator, transformers are usually combined with other\\ntransformers or with predictors (such as classifiers or regressors).\\nThe most common tool used for composing estimators is a Pipeline. Pipelines require all steps except the last to be a\\ntransformer. The last step can be anything, a transformer, a\\npredictor, or a clustering estimator which might have or not have a\\n.predict(...) method. A pipeline exposes all methods provided by the last\\nestimator: if the last step provides a transform method, then the pipeline\\nwould have a transform method and behave like a transformer. If the last step\\nprovides a predict method, then the pipeline would expose that method, and\\ngiven a data X, use all steps except the last to transform the data,\\nand then give that transformed data to the predict method of the last step of\\nthe pipeline. The class Pipeline is often used in combination with\\nColumnTransformer or\\nFeatureUnion which concatenate the output of transformers\\ninto a composite feature space.\\nTransformedTargetRegressor\\ndeals with transforming the target (i.e. log-transform y).\\n---------new doc---------\\n6.1.2. Transforming target in regression#\\nTransformedTargetRegressor transforms the\\ntargets y before fitting a regression model. The predictions are mapped\\nback to the original space via an inverse transform. It takes as an argument\\nthe regressor that will be used for prediction, and the transformer that will\\nbe applied to the target variable:\\n>>> import numpy as np\\n>>> from sklearn.datasets import fetch_california_housing\\n>>> from sklearn.compose import TransformedTargetRegressor\\n>>> from sklearn.preprocessing import QuantileTransformer\\n>>> from sklearn.linear_model import LinearRegression\\n>>> from sklearn.model_selection import train_test_split\\n>>> X, y = fetch_california_housing(return_X_y=True)\\n>>> X, y = X[:2000, :], y[:2000]  # select a subset of data\\n>>> transformer = QuantileTransformer(output_distribution='normal')\\n>>> regressor = LinearRegression()\\n>>> regr = TransformedTargetRegressor(regressor=regressor,\\n...                                   transformer=transformer)\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: 0.61\\n---------new doc---------\\n6.3.1. Standardization, or mean removal and variance scaling#\\nStandardization of datasets is a common requirement for many\\nmachine learning estimators implemented in scikit-learn; they might behave\\nbadly if the individual features do not more or less look like standard\\nnormally distributed data: Gaussian with zero mean and unit variance.\\nIn practice we often ignore the shape of the distribution and just\\ntransform the data to center it by removing the mean value of each\\nfeature, then scale it by dividing non-constant features by their\\nstandard deviation.\\nFor instance, many elements used in the objective function of\\na learning algorithm (such as the RBF kernel of Support Vector\\nMachines or the l1 and l2 regularizers of linear models) may assume that\\nall features are centered around zero or have variance in the same\\norder. If a feature has a variance that is orders of magnitude larger\\nthan others, it might dominate the objective function and make the\\nestimator unable to learn from other features correctly as expected.\\nThe preprocessing module provides the\\nStandardScaler utility class, which is a quick and\\neasy way to perform the following operation on an array-like\\ndataset:\\n>>> from sklearn import preprocessing\\n>>> import numpy as np\\n>>> X_train = np.array([[ 1., -1.,  2.],\\n...                     [ 2.,  0.,  0.],\\n...                     [ 0.,  1., -1.]])\\n>>> scaler = preprocessing.StandardScaler().fit(X_train)\\n>>> scaler\\nStandardScaler()\\n---------new doc---------\\n>>> X, y = make_classification(random_state=42)\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\n>>> pipe = make_pipeline(StandardScaler(), LogisticRegression())\\n>>> pipe.fit(X_train, y_train)  # apply scaling on training data\\nPipeline(steps=[('standardscaler', StandardScaler()),\\n                ('logisticregression', LogisticRegression())])\\n\\n>>> pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.\\n0.96\\n\\n\\nIt is possible to disable either centering or scaling by either\\npassing with_mean=False or with_std=False to the constructor\\nof StandardScaler.\\n---------new doc---------\\nNote\\nThis estimator is still experimental for now: default parameters or\\ndetails of behaviour might change without any deprecation cycle. Resolving\\nthe following issues would help stabilize IterativeImputer:\\nconvergence criteria (#14338) and default estimators\\n(#13286). To use it, you need to explicitly import\\nenable_iterative_imputer.\\n\\n>>> import numpy as np\\n>>> from sklearn.experimental import enable_iterative_imputer\\n>>> from sklearn.impute import IterativeImputer\\n>>> imp = IterativeImputer(max_iter=10, random_state=0)\\n>>> imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\\nIterativeImputer(random_state=0)\\n>>> X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\\n>>> # the model learns that the second feature is double the first\\n>>> print(np.round(imp.transform(X_test)))\\n[[ 1.  2.]\\n [ 6. 12.]\\n [ 3.  6.]]\\n\\n\\nBoth SimpleImputer and IterativeImputer can be used in a\\nPipeline as a way to build a composite estimator that supports imputation.\\nSee Imputing missing values before building an estimator.\\n---------new doc---------\\n6.4.3.2. Multiple vs. Single Imputation#\\nIn the statistics community, it is common practice to perform multiple\\nimputations, generating, for example, m separate imputations for a single\\nfeature matrix. Each of these m imputations is then put through the\\nsubsequent analysis pipeline (e.g. feature engineering, clustering, regression,\\nclassification). The m final analysis results (e.g. held-out validation\\nerrors) allow the data scientist to obtain understanding of how analytic\\nresults may differ as a consequence of the inherent uncertainty caused by the\\nmissing values. The above practice is called multiple imputation.\\nOur implementation of IterativeImputer was inspired by the R MICE\\npackage (Multivariate Imputation by Chained Equations) [1], but differs from\\nit by returning a single imputation instead of multiple imputations.  However,\\nIterativeImputer can also be used for multiple imputations by applying\\nit repeatedly to the same dataset with different random seeds when\\nsample_posterior=True. See [2], chapter 4 for more discussion on multiple\\nvs. single imputations.\\nIt is still an open problem as to how useful single vs. multiple imputation is\\nin the context of prediction and classification when the user is not\\ninterested in measuring uncertainty due to missing values.\\nNote that a call to the transform method of IterativeImputer is\\nnot allowed to change the number of samples. Therefore multiple imputations\\ncannot be achieved by a single call to transform.\\n\\n\\n6.4.3.3. References#\\n\\n\\n[1]\\nStef van Buuren, Karin Groothuis-Oudshoorn (2011). “mice: Multivariate\\nImputation by Chained Equations in R”. Journal of Statistical Software 45:\\n1-67.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [5.09s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"This text appears to be a documentation for the scikit-learn library, specifically covering various preprocessing techniques and imputation methods. Here's a summary of the main topics:\\n\\n**Preprocessing**\\n\\n* **StandardScaler**: scales features by removing the mean and scaling to unit variance\\n\\t+ Can be used in pipelines with `with_mean=False` or `with_std=False`\\n\\t+ Note: still experimental, may change behavior without deprecation cycle\\n* **Imputation**\\n\\t+ **SimpleImputer**: replaces missing values with a specified value (e.g., mean, median)\\n\\t+ **IterativeImputer**: iteratively imputes missing values using a chain of estimators\\n\\t\\t- Can be used in pipelines to build composite estimators that support imputation\\n\\t\\t- Note: returns a single imputation instead of multiple imputations like R's MICE package\\n\\n**Multiple vs. Single Imputation**\\n\\n* Multiple imputation involves generating multiple imputations for a single feature matrix and analyzing each one separately\\n* IterativeImputer can be used for multiple imputations by applying it repeatedly with different random seeds when `sample_posterior=True`\\n* However, note that a call to the transform method of IterativeImputer cannot change the number of samples\\n\\n**References**\\n\\n* [1] Stef van Buuren, Karin Groothuis-Oudshoorn (2011). \\\"mice: Multivariate Imputation by Chained Equations in R\\\". Journal of Statistical Software 45: 1-67.\\n\\nOverall, this documentation provides an overview of various preprocessing techniques and imputation methods available in scikit-learn, including the use of pipelines to build composite estimators that support imputation.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b-instruct-q8_0\",\n",
      "          \"created_at\": \"2025-02-17T13:31:18.577308485Z\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"total_duration\": 5080600649,\n",
      "          \"load_duration\": 20818261,\n",
      "          \"prompt_eval_count\": 2048,\n",
      "          \"prompt_eval_duration\": 658000000,\n",
      "          \"eval_count\": 351,\n",
      "          \"eval_duration\": 4398000000,\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\",\n",
      "            \"images\": null,\n",
      "            \"tool_calls\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"This text appears to be a documentation for the scikit-learn library, specifically covering various preprocessing techniques and imputation methods. Here's a summary of the main topics:\\n\\n**Preprocessing**\\n\\n* **StandardScaler**: scales features by removing the mean and scaling to unit variance\\n\\t+ Can be used in pipelines with `with_mean=False` or `with_std=False`\\n\\t+ Note: still experimental, may change behavior without deprecation cycle\\n* **Imputation**\\n\\t+ **SimpleImputer**: replaces missing values with a specified value (e.g., mean, median)\\n\\t+ **IterativeImputer**: iteratively imputes missing values using a chain of estimators\\n\\t\\t- Can be used in pipelines to build composite estimators that support imputation\\n\\t\\t- Note: returns a single imputation instead of multiple imputations like R's MICE package\\n\\n**Multiple vs. Single Imputation**\\n\\n* Multiple imputation involves generating multiple imputations for a single feature matrix and analyzing each one separately\\n* IterativeImputer can be used for multiple imputations by applying it repeatedly with different random seeds when `sample_posterior=True`\\n* However, note that a call to the transform method of IterativeImputer cannot change the number of samples\\n\\n**References**\\n\\n* [1] Stef van Buuren, Karin Groothuis-Oudshoorn (2011). \\\"mice: Multivariate Imputation by Chained Equations in R\\\". Journal of Statistical Software 45: 1-67.\\n\\nOverall, this documentation provides an overview of various preprocessing techniques and imputation methods available in scikit-learn, including the use of pipelines to build composite estimators that support imputation.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b-instruct-q8_0\",\n",
      "              \"created_at\": \"2025-02-17T13:31:18.577308485Z\",\n",
      "              \"done\": true,\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"total_duration\": 5080600649,\n",
      "              \"load_duration\": 20818261,\n",
      "              \"prompt_eval_count\": 2048,\n",
      "              \"prompt_eval_duration\": 658000000,\n",
      "              \"eval_count\": 351,\n",
      "              \"eval_duration\": 4398000000,\n",
      "              \"message\": {\n",
      "                \"lc\": 1,\n",
      "                \"type\": \"not_implemented\",\n",
      "                \"id\": [\n",
      "                  \"ollama\",\n",
      "                  \"_types\",\n",
      "                  \"Message\"\n",
      "                ],\n",
      "                \"repr\": \"Message(role='assistant', content='', images=None, tool_calls=None)\"\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-7f0ba320-8861-4a9f-a07e-3233b00b72c8-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 2048,\n",
      "              \"output_tokens\": 351,\n",
      "              \"total_tokens\": 2399\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"This text appears to be a documentation for the scikit-learn library, specifically covering various preprocessing techniques and imputation methods. Here's a summary of the main topics:\\n\\n**Preprocessing**\\n\\n* **StandardScaler**: scales features by removing the mean and scaling to unit variance\\n\\t+ Can be used in pipelines with `with_mean=False` or `with_std=False`\\n\\t+ Note: still experimental, may change behavior without deprecation cycle\\n* **Imputation**\\n\\t+ **SimpleImputer**: replaces missing values with a specified value (e.g., mean, median)\\n\\t+ **IterativeImputer**: iteratively imputes missing values using a chain of estimators\\n\\t\\t- Can be used in pipelines to build composite estimators that support imputation\\n\\t\\t- Note: returns a single imputation instead of multiple imputations like R's MICE package\\n\\n**Multiple vs. Single Imputation**\\n\\n* Multiple imputation involves generating multiple imputations for a single feature matrix and analyzing each one separately\\n* IterativeImputer can be used for multiple imputations by applying it repeatedly with different random seeds when `sample_posterior=True`\\n* However, note that a call to the transform method of IterativeImputer cannot change the number of samples\\n\\n**References**\\n\\n* [1] Stef van Buuren, Karin Groothuis-Oudshoorn (2011). \\\"mice: Multivariate Imputation by Chained Equations in R\\\". Journal of Statistical Software 45: 1-67.\\n\\nOverall, this documentation provides an overview of various preprocessing techniques and imputation methods available in scikit-learn, including the use of pipelines to build composite estimators that support imputation.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [5.09s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"This text appears to be a documentation for the scikit-learn library, specifically covering various preprocessing techniques and imputation methods. Here's a summary of the main topics:\\n\\n**Preprocessing**\\n\\n* **StandardScaler**: scales features by removing the mean and scaling to unit variance\\n\\t+ Can be used in pipelines with `with_mean=False` or `with_std=False`\\n\\t+ Note: still experimental, may change behavior without deprecation cycle\\n* **Imputation**\\n\\t+ **SimpleImputer**: replaces missing values with a specified value (e.g., mean, median)\\n\\t+ **IterativeImputer**: iteratively imputes missing values using a chain of estimators\\n\\t\\t- Can be used in pipelines to build composite estimators that support imputation\\n\\t\\t- Note: returns a single imputation instead of multiple imputations like R's MICE package\\n\\n**Multiple vs. Single Imputation**\\n\\n* Multiple imputation involves generating multiple imputations for a single feature matrix and analyzing each one separately\\n* IterativeImputer can be used for multiple imputations by applying it repeatedly with different random seeds when `sample_posterior=True`\\n* However, note that a call to the transform method of IterativeImputer cannot change the number of samples\\n\\n**References**\\n\\n* [1] Stef van Buuren, Karin Groothuis-Oudshoorn (2011). \\\"mice: Multivariate Imputation by Chained Equations in R\\\". Journal of Statistical Software 45: 1-67.\\n\\nOverall, this documentation provides an overview of various preprocessing techniques and imputation methods available in scikit-learn, including the use of pipelines to build composite estimators that support imputation.\"\n",
      "}\n",
      "Global cluster: 0, Local cluster: 1, has 144 documents.\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Plot Ridge coefficients as a function of the regularization\\nClassification of text documents using sparse features\\nCommon pitfalls in the interpretation of coefficients of linear models\\n\\n\\n\\n1.1.2.3. Ridge Complexity#\\nThis method has the same order of complexity as\\nOrdinary Least Squares.\\n\\n\\n1.1.2.4. Setting the regularization parameter: leave-one-out Cross-Validation#\\nRidgeCV and RidgeClassifierCV implement ridge\\nregression/classification with built-in cross-validation of the alpha parameter.\\nThey work in the same way as GridSearchCV except\\nthat it defaults to efficient Leave-One-Out cross-validation.\\nWhen using the default cross-validation, alpha cannot be 0 due to the\\nformulation used to calculate Leave-One-Out error. See [RL2007] for details.\\nUsage example:\\n>>> import numpy as np\\n>>> from sklearn import linear_model\\n>>> reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\\n>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\\nRidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\\n      1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\\n>>> reg.alpha_\\n0.01\\n---------new doc---------\\nHere is an example of applying this idea to one-dimensional data, using\\npolynomial features of varying degrees:\\n\\n\\n\\n\\nThis figure is created using the PolynomialFeatures transformer, which\\ntransforms an input data matrix into a new data matrix of a given degree.\\nIt can be used as follows:\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> import numpy as np\\n>>> X = np.arange(6).reshape(3, 2)\\n>>> X\\narray([[0, 1],\\n       [2, 3],\\n       [4, 5]])\\n>>> poly = PolynomialFeatures(degree=2)\\n>>> poly.fit_transform(X)\\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\\n       [ 1.,  4.,  5., 16., 20., 25.]])\\n---------new doc---------\\nThe features of X have been transformed from \\\\([x_1, x_2]\\\\) to\\n\\\\([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\\\\), and can now be used within\\nany linear model.\\nThis sort of preprocessing can be streamlined with the\\nPipeline tools. A single object representing a simple\\npolynomial regression can be created and used as follows:\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> from sklearn.linear_model import LinearRegression\\n>>> from sklearn.pipeline import Pipeline\\n>>> import numpy as np\\n>>> model = Pipeline([('poly', PolynomialFeatures(degree=3)),\\n...                   ('linear', LinearRegression(fit_intercept=False))])\\n>>> # fit to an order-3 polynomial data\\n>>> x = np.arange(5)\\n>>> y = 3 - 2 * x + x ** 2 - x ** 3\\n>>> model = model.fit(x[:, np.newaxis], y)\\n>>> model.named_steps['linear'].coef_\\narray([ 3., -2.,  1., -1.])\\n---------new doc---------\\nThe linear model trained on polynomial features is able to exactly recover\\nthe input polynomial coefficients.\\nIn some cases it’s not necessary to include higher powers of any single feature,\\nbut only the so-called interaction features\\nthat multiply together at most \\\\(d\\\\) distinct features.\\nThese can be gotten from PolynomialFeatures with the setting\\ninteraction_only=True.\\nFor example, when dealing with boolean features,\\n\\\\(x_i^n = x_i\\\\) for all \\\\(n\\\\) and is therefore useless;\\nbut \\\\(x_i x_j\\\\) represents the conjunction of two booleans.\\nThis way, we can solve the XOR problem with a linear classifier:\\n>>> from sklearn.linear_model import Perceptron\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> import numpy as np\\n>>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\\n>>> y = X[:, 0] ^ X[:, 1]\\n>>> y\\narray([0, 1, 1, 0])\\n>>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)\\n>>> X\\narray([[1, 0, 0, 0],\\n       [1, 0, 1, 0],\\n       [1, 1, 0, 0],\\n       [1, 1, 1, 1]])\\n>>> clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,\\n...                  shuffle=False).fit(X, y)\\n---------new doc---------\\n0 to n is “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . .\\n. “n-1 vs n”.\\nThe shape of dual_coef_ is (n_classes-1, n_SV) with\\na somewhat hard to grasp layout.\\nThe columns correspond to the support vectors involved in any\\nof the n_classes * (n_classes - 1) / 2 “one-vs-one” classifiers.\\nEach support vector v has a dual coefficient in each of the\\nn_classes - 1 classifiers comparing the class of v against another class.\\nNote that some, but not all, of these dual coefficients, may be zero.\\nThe n_classes - 1 entries in each column are these dual coefficients,\\nordered by the opposing class.\\nThis might be clearer with an example: consider a three class problem with\\nclass 0 having three support vectors\\n\\\\(v^{0}_0, v^{1}_0, v^{2}_0\\\\) and class 1 and 2 having two support vectors\\n\\\\(v^{0}_1, v^{1}_1\\\\) and \\\\(v^{0}_2, v^{1}_2\\\\) respectively.  For each\\nsupport vector \\\\(v^{j}_i\\\\), there are two dual coefficients.  Let’s call\\nthe coefficient of support vector \\\\(v^{j}_i\\\\) in the classifier between\\nclasses \\\\(i\\\\) and \\\\(k\\\\) \\\\(\\\\alpha^{j}_{i,k}\\\\).\\nThen dual_coef_ looks like this:\\n---------new doc---------\\n1.4.5. Tips on Practical Use#\\n\\nAvoiding data copy: For SVC, SVR, NuSVC and\\nNuSVR, if the data passed to certain methods is not C-ordered\\ncontiguous and double precision, it will be copied before calling the\\nunderlying C implementation. You can check whether a given numpy array is\\nC-contiguous by inspecting its flags attribute.\\nFor LinearSVC (and LogisticRegression) any input passed as a numpy\\narray will be copied and converted to the liblinear internal sparse data\\nrepresentation (double precision floats and int32 indices of non-zero\\ncomponents). If you want to fit a large-scale linear classifier without\\ncopying a dense numpy C-contiguous double precision array as input, we\\nsuggest to use the SGDClassifier class instead.  The objective\\nfunction can be configured to be almost the same as the LinearSVC\\nmodel.\\n\\nKernel cache size: For SVC, SVR, NuSVC and\\nNuSVR, the size of the kernel cache has a strong impact on run\\ntimes for larger problems.  If you have enough RAM available, it is\\nrecommended to set cache_size to a higher value than the default of\\n200(MB), such as 500(MB) or 1000(MB).\\nSetting C: C is 1 by default and it’s a reasonable default\\nchoice.  If you have a lot of noisy observations you should decrease it:\\ndecreasing C corresponds to more regularization.\\nLinearSVC and LinearSVR are less sensitive to C when\\nit becomes large, and prediction results stop improving after a certain\\nthreshold. Meanwhile, larger C values will take more time to train,\\nsometimes up to 10 times longer, as shown in [11].\\n---------new doc---------\\nSupport Vector Machine algorithms are not scale invariant, so it\\nis highly recommended to scale your data. For example, scale each\\nattribute on the input vector X to [0,1] or [-1,+1], or standardize it\\nto have mean 0 and variance 1. Note that the same scaling must be\\napplied to the test vector to obtain meaningful results. This can be done\\neasily by using a Pipeline:\\n>>> from sklearn.pipeline import make_pipeline\\n>>> from sklearn.preprocessing import StandardScaler\\n>>> from sklearn.svm import SVC\\n\\n>>> clf = make_pipeline(StandardScaler(), SVC())\\n\\n\\nSee section Preprocessing data for more details on scaling and\\nnormalization.\\n---------new doc---------\\nNuSVC#\\nThe \\\\(\\\\nu\\\\)-SVC formulation [15] is a reparameterization of the\\n\\\\(C\\\\)-SVC and therefore mathematically equivalent.\\nWe introduce a new parameter \\\\(\\\\nu\\\\) (instead of \\\\(C\\\\)) which\\ncontrols the number of support vectors and margin errors:\\n\\\\(\\\\nu \\\\in (0, 1]\\\\) is an upper bound on the fraction of margin errors and\\na lower bound of the fraction of support vectors. A margin error corresponds\\nto a sample that lies on the wrong side of its margin boundary: it is either\\nmisclassified, or it is correctly classified but does not lie beyond the\\nmargin.\\n\\n\\n\\n1.4.7.2. SVR#\\nGiven training vectors \\\\(x_i \\\\in \\\\mathbb{R}^p\\\\), i=1,…, n, and a\\nvector \\\\(y \\\\in \\\\mathbb{R}^n\\\\) \\\\(\\\\varepsilon\\\\)-SVR solves the following primal problem:\\n---------new doc---------\\nTransforming data#\\nTo transform \\\\(X\\\\) into \\\\(\\\\bar{X}\\\\), we need to find a projection\\nmatrix \\\\(P\\\\) such that \\\\(\\\\bar{X} = XP\\\\). We know that for the\\ntraining data, \\\\(\\\\Xi = XP\\\\), and \\\\(X = \\\\Xi \\\\Gamma^T\\\\). Setting\\n\\\\(P = U(\\\\Gamma^T U)^{-1}\\\\) where \\\\(U\\\\) is the matrix with the\\n\\\\(u_k\\\\) in the columns, we have \\\\(XP = X U(\\\\Gamma^T U)^{-1} = \\\\Xi\\n(\\\\Gamma^T U) (\\\\Gamma^T U)^{-1} = \\\\Xi\\\\) as desired. The rotation matrix\\n\\\\(P\\\\) can be accessed from the x_rotations_ attribute.\\nSimilarly, \\\\(Y\\\\) can be transformed using the rotation matrix\\n\\\\(V(\\\\Delta^T V)^{-1}\\\\), accessed via the y_rotations_ attribute.\\n---------new doc---------\\nOnce trained, you can plot the tree with the plot_tree function:\\n>>> tree.plot_tree(clf)\\n[...]\\n\\n\\n\\n\\n\\n\\n\\n\\nAlternative ways to export trees#\\nWe can also export the tree in Graphviz format using the export_graphviz\\nexporter. If you use the conda package manager, the graphviz binaries\\nand the python package can be installed with conda install python-graphviz.\\nAlternatively binaries for graphviz can be downloaded from the graphviz project homepage,\\nand the Python wrapper installed from pypi with pip install graphviz.\\nBelow is an example graphviz export of the above tree trained on the entire\\niris dataset; the results are saved in an output file iris.pdf:\\n>>> import graphviz \\n>>> dot_data = tree.export_graphviz(clf, out_file=None) \\n>>> graph = graphviz.Source(dot_data) \\n>>> graph.render(\\\"iris\\\")\\n---------new doc---------\\nThe export_graphviz exporter also supports a variety of aesthetic\\noptions, including coloring nodes by their class (or value for regression) and\\nusing explicit variable and class names if desired. Jupyter notebooks also\\nrender these plots inline automatically:\\n>>> dot_data = tree.export_graphviz(clf, out_file=None, \\n...                      feature_names=iris.feature_names,  \\n...                      class_names=iris.target_names,  \\n...                      filled=True, rounded=True,  \\n...                      special_characters=True)  \\n>>> graph = graphviz.Source(dot_data)  \\n>>> graph\\n---------new doc---------\\nAs you can see, the [1, 0] is comfortably classified as 1 since the first\\ntwo samples are ignored due to their sample weights.\\nImplementation detail: taking sample weights into account amounts to\\nmultiplying the gradients (and the hessians) by the sample weights. Note that\\nthe binning stage (specifically the quantiles computation) does not take the\\nweights into account.\\n\\n\\n1.11.1.1.4. Categorical Features Support#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor have native support for categorical\\nfeatures: they can consider splits on non-ordered, categorical data.\\nFor datasets with categorical features, using the native categorical support\\nis often better than relying on one-hot encoding\\n(OneHotEncoder), because one-hot encoding\\nrequires more tree depth to achieve equivalent splits. It is also usually\\nbetter to rely on the native categorical support rather than to treat\\ncategorical features as continuous (ordinal), which happens for ordinal-encoded\\ncategorical data, since categories are nominal quantities where order does not\\nmatter.\\nTo enable categorical support, a boolean mask can be passed to the\\ncategorical_features parameter, indicating which feature is categorical. In\\nthe following, the first feature will be treated as categorical and the\\nsecond feature as numerical:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[True, False])\\n\\n\\nEquivalently, one can pass a list of integers indicating the indices of the\\ncategorical features:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[0])\\n---------new doc---------\\nEquivalently, one can pass a list of integers indicating the indices of the\\ncategorical features:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[0])\\n\\n\\nWhen the input is a DataFrame, it is also possible to pass a list of column\\nnames:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[\\\"site\\\", \\\"manufacturer\\\"])\\n\\n\\nFinally, when the input is a DataFrame we can use\\ncategorical_features=\\\"from_dtype\\\" in which case all columns with a categorical\\ndtype will be treated as categorical features.\\nThe cardinality of each categorical feature must be less than the max_bins\\nparameter. For an example using histogram-based gradient boosting on categorical\\nfeatures, see\\nCategorical Feature Support in Gradient Boosting.\\nIf there are missing values during training, the missing values will be\\ntreated as a proper category. If there are no missing values during training,\\nthen at prediction time, missing values are mapped to the child node that has\\nthe most samples (just like for continuous features). When predicting,\\ncategories that were not seen during fit time will be treated as missing\\nvalues.\\n---------new doc---------\\na monotonic increase constraint is a constraint of the form:\\n\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\leq F(x_1', x_2)\\\\]\\n\\na monotonic decrease constraint is a constraint of the form:\\n\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\geq F(x_1', x_2)\\\\]\\n\\n\\nYou can specify a monotonic constraint on each feature using the\\nmonotonic_cst parameter. For each feature, a value of 0 indicates no\\nconstraint, while 1 and -1 indicate a monotonic increase and\\nmonotonic decrease constraint, respectively:\\n>>> from sklearn.ensemble import HistGradientBoostingRegressor\\n\\n... # monotonic increase, monotonic decrease, and no constraint on the 3 features\\n>>> gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])\\n\\n\\nIn a binary classification context, imposing a monotonic increase (decrease) constraint means that higher values of the feature are supposed\\nto have a positive (negative) effect on the probability of samples\\nto belong to the positive class.\\nNevertheless, monotonic constraints only marginally constrain feature effects on the output.\\nFor instance, monotonic increase and decrease constraints cannot be used to enforce the\\nfollowing modelling constraint:\\n\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\leq F(x_1', x_2')\\\\]\\nAlso, monotonic constraints are not supported for multiclass classification.\\n---------new doc---------\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\leq F(x_1', x_2')\\\\]\\nAlso, monotonic constraints are not supported for multiclass classification.\\n\\nNote\\nSince categories are unordered quantities, it is not possible to enforce\\nmonotonic constraints on categorical features.\\n\\nExamples\\n\\nMonotonic Constraints\\nFeatures in Histogram Gradient Boosting Trees\\n\\n\\n\\n1.11.1.1.6. Interaction constraints#\\nA priori, the histogram gradient boosted trees are allowed to use any feature\\nto split a node into child nodes. This creates so called interactions between\\nfeatures, i.e. usage of different features as split along a branch. Sometimes,\\none wants to restrict the possible interactions, see [Mayer2022]. This can be\\ndone by the parameter interaction_cst, where one can specify the indices\\nof features that are allowed to interact.\\nFor instance, with 3 features in total, interaction_cst=[{0}, {1}, {2}]\\nforbids all interactions.\\nThe constraints [{0, 1}, {1, 2}] specifies two groups of possibly\\ninteracting features. Features 0 and 1 may interact with each other, as well\\nas features 1 and 2. But note that features 0 and 2 are forbidden to interact.\\nThe following depicts a tree and the possible splits of the tree:\\n   1      <- Both constraint groups could be applied from now on\\n  / \\\\\\n 1   2    <- Left split still fulfills both constraint groups.\\n/ \\\\ / \\\\      Right split at feature 2 has only group {1, 2} from now on.\\n---------new doc---------\\nLightGBM uses the same logic for overlapping groups.\\nNote that features not listed in interaction_cst are automatically\\nassigned an interaction group for themselves. With again 3 features, this\\nmeans that [{0}] is equivalent to [{0}, {1, 2}].\\nExamples\\n\\nPartial Dependence and Individual Conditional Expectation Plots\\n\\nReferences\\n\\n\\n[Mayer2022]\\nM. Mayer, S.C. Bourassa, M. Hoesli, and D.F. Scognamiglio.\\n2022. Machine Learning Applications to Land and Structure Valuation.\\nJournal of Risk and Financial Management 15, no. 5: 193\\n\\n\\n\\n\\n1.11.1.1.7. Low-level parallelism#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor use OpenMP\\nfor parallelization through Cython. For more details on how to control the\\nnumber of threads, please refer to our Parallelism notes.\\nThe following parts are parallelized:\\n\\nmapping samples from real values to integer-valued bins (finding the bin\\nthresholds is however sequential)\\nbuilding histograms is parallelized over features\\nfinding the best split point at a node is parallelized over features\\nduring fit, mapping samples into the left and right children is\\nparallelized over samples\\ngradient and hessians computations are parallelized over samples\\npredicting is parallelized over samples\\n---------new doc---------\\n1.12.1.1. Target format#\\nValid multiclass representations for\\ntype_of_target (y) are:\\n\\n1d or column vector containing more than two discrete values. An\\nexample of a vector y for 4 samples:\\n>>> import numpy as np\\n>>> y = np.array(['apple', 'pear', 'apple', 'orange'])\\n>>> print(y)\\n['apple' 'pear' 'apple' 'orange']\\n\\n\\n\\nDense or sparse binary matrix of shape (n_samples, n_classes)\\nwith a single sample per row, where each column represents one class. An\\nexample of both a dense and sparse binary matrix y for 4\\nsamples, where the columns, in order, are apple, orange, and pear:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import LabelBinarizer\\n>>> y = np.array(['apple', 'pear', 'apple', 'orange'])\\n>>> y_dense = LabelBinarizer().fit_transform(y)\\n>>> print(y_dense)\\n[[1 0 0]\\n [0 0 1]\\n [1 0 0]\\n [0 1 0]]\\n>>> from scipy import sparse\\n>>> y_sparse = sparse.csr_matrix(y_dense)\\n>>> print(y_sparse)\\n<Compressed Sparse Row sparse matrix of dtype 'int64'\\n    with 4 stored elements and shape (4, 3)>\\n  Coords    Values\\n  (0, 0)    1\\n  (1, 2)    1\\n  (2, 0)    1\\n  (3, 1)    1\\n---------new doc---------\\n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\n---------new doc---------\\n1.12.1.4. OutputCodeClassifier#\\nError-Correcting Output Code-based strategies are fairly different from\\none-vs-the-rest and one-vs-one. With these strategies, each class is\\nrepresented in a Euclidean space, where each dimension can only be 0 or 1.\\nAnother way to put it is that each class is represented by a binary code (an\\narray of 0 and 1). The matrix which keeps track of the location/code of each\\nclass is called the code book. The code size is the dimensionality of the\\naforementioned space. Intuitively, each class should be represented by a code\\nas unique as possible and a good code book should be designed to optimize\\nclassification accuracy. In this implementation, we simply use a\\nrandomly-generated code book as advocated in [3] although more elaborate\\nmethods may be added in the future.\\nAt fitting time, one binary classifier per bit in the code book is fitted.\\nAt prediction time, the classifiers are used to project new points in the\\nclass space and the class closest to the points is chosen.\\nIn OutputCodeClassifier, the code_size\\nattribute allows the user to control the number of classifiers which will be\\nused. It is a percentage of the total number of classes.\\nA number between 0 and 1 will require fewer classifiers than\\none-vs-the-rest. In theory, log2(n_classes) / n_classes is sufficient to\\nrepresent each class unambiguously. However, in practice, it may not lead to\\ngood accuracy since log2(n_classes) is much smaller than n_classes.\\nA number greater than 1 will require more classifiers than\\none-vs-the-rest. In this case, some classifiers will in theory correct for\\n---------new doc---------\\n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\\n       1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,\\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\n---------new doc---------\\n1.12.2.1. Target format#\\nA valid representation of multilabel y is an either dense or sparse\\nbinary matrix of shape (n_samples, n_classes). Each column\\nrepresents a class. The 1’s in each row denote the positive classes a\\nsample has been labeled with. An example of a dense matrix y for 3\\nsamples:\\n>>> y = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [0, 0, 0, 0]])\\n>>> print(y)\\n[[1 0 0 1]\\n [0 0 1 1]\\n [0 0 0 0]]\\n\\n\\nDense binary matrices can also be created using\\nMultiLabelBinarizer. For more information,\\nrefer to Transforming the prediction target (y).\\nAn example of the same y in sparse matrix form:\\n>>> y_sparse = sparse.csr_matrix(y)\\n>>> print(y_sparse)\\n<Compressed Sparse Row sparse matrix of dtype 'int64'\\n  with 4 stored elements and shape (3, 4)>\\n  Coords      Values\\n  (0, 0)      1\\n  (0, 3)      1\\n  (1, 2)      1\\n  (1, 3)      1\\n---------new doc---------\\n[-141.62745778,   95.02891072, -191.48204257],\\n       [  97.03260883,  165.34867495,  139.52003279],\\n       [ 123.92529176,   21.25719016,   -7.84253   ],\\n       [-122.25193977,  -85.16443186, -107.12274212],\\n       [ -30.170388  ,  -94.80956739,   12.16979946],\\n       [ 140.72667194,  176.50941682,  -17.50447799],\\n       [ 149.37967282,  -81.15699552,   -5.72850319]])\\n---------new doc---------\\n>>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\\narray([[8, 0],\\n       [0, 4]])\\n\\n\\nLabelings that assign all classes members to the same clusters\\nare complete but may not always be pure, hence penalized, and\\nhave some off-diagonal non-zero entries:\\n>>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\\narray([[8, 2],\\n       [0, 2]])\\n\\n\\nThe matrix is not symmetric:\\n>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])\\narray([[8, 0],\\n       [2, 2]])\\n\\n\\nIf classes members are completely split across different clusters, the\\nassignment is totally incomplete, hence the matrix has all zero\\ndiagonal entries:\\n>>> pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])\\narray([[ 0,  0],\\n       [12,  0]])\\n\\n\\n\\n\\nReferences#\\n\\n“Comparing Partitions” L. Hubert and P. Arabie,\\nJournal of Classification 1985\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n2.2. Manifold learning\\n\\n\\n\\n\\nnext\\n2.4. Biclustering\\n\\n\\n --- \\n\\n\\n2. Unsupervised learning\\n2.5. Decomposing signals in components (matrix factorization problems)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2.5. Decomposing signals in components (matrix factorization problems)#\\n---------new doc---------\\nAnother option is to use an iterable yielding (train, test) splits as arrays of\\nindices, for example:\\n>>> def custom_cv_2folds(X):\\n...     n = X.shape[0]\\n...     i = 1\\n...     while i <= 2:\\n...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\\n...         yield idx, idx\\n...         i += 1\\n...\\n>>> custom_cv = custom_cv_2folds(X)\\n>>> cross_val_score(clf, X, y, cv=custom_cv)\\narray([1.        , 0.973...])\\n---------new doc---------\\ntrain -  [30  3]   |   test -  [15  2]\\ntrain -  [30  4]   |   test -  [15  1]\\n>>> kf = KFold(n_splits=3)\\n>>> for train, test in kf.split(X, y):\\n...     print('train -  {}   |   test -  {}'.format(\\n...         np.bincount(y[train]), np.bincount(y[test])))\\ntrain -  [28  5]   |   test -  [17]\\ntrain -  [28  5]   |   test -  [17]\\ntrain -  [34]   |   test -  [11  5]\\n---------new doc---------\\n>>> X = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001])\\n>>> y = np.array([\\\"a\\\", \\\"b\\\", \\\"b\\\", \\\"b\\\", \\\"c\\\", \\\"c\\\", \\\"c\\\", \\\"a\\\"])\\n>>> groups = np.array([1, 1, 2, 2, 3, 3, 4, 4])\\n>>> train_indx, test_indx = next(\\n...     GroupShuffleSplit(random_state=7).split(X, y, groups)\\n... )\\n>>> X_train, X_test, y_train, y_test = \\\\\\n...     X[train_indx], X[test_indx], y[train_indx], y[test_indx]\\n>>> X_train.shape, X_test.shape\\n((6,), (2,))\\n>>> np.unique(groups[train_indx]), np.unique(groups[test_indx])\\n(array([1, 2, 4]), array([3]))\\n---------new doc---------\\n2\\n0\\n125\\n0.983667\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\\n\\n3\\n0\\n125\\n0.983667\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 6, ‘min_samples_split’: 6}\\n\\n…\\n…\\n…\\n…\\n…\\n\\n15\\n2\\n500\\n0.951958\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}\\n\\n16\\n2\\n500\\n0.947958\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\\n\\n17\\n2\\n500\\n0.951958\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 4}\\n\\n18\\n3\\n1000\\n0.961009\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}\\n---------new doc---------\\n[[1, 0],\\n        [0, 1]],\\n\\n       [[0, 1],\\n        [1, 0]]])\\n\\n\\nOr a confusion matrix can be constructed for each sample’s labels:\\n>>> multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\\narray([[[1, 0],\\n        [1, 1]],\\n\\n       [[1, 1],\\n        [0, 1]]])\\n\\n\\nHere is an example demonstrating the use of the\\nmultilabel_confusion_matrix function with\\nmulticlass input:\\n>>> y_true = [\\\"cat\\\", \\\"ant\\\", \\\"cat\\\", \\\"cat\\\", \\\"ant\\\", \\\"bird\\\"]\\n>>> y_pred = [\\\"ant\\\", \\\"ant\\\", \\\"cat\\\", \\\"cat\\\", \\\"ant\\\", \\\"cat\\\"]\\n>>> multilabel_confusion_matrix(y_true, y_pred,\\n...                             labels=[\\\"ant\\\", \\\"bird\\\", \\\"cat\\\"])\\narray([[[3, 1],\\n        [0, 2]],\\n\\n       [[5, 0],\\n        [1, 0]],\\n\\n       [[2, 1],\\n        [1, 2]]])\\n---------new doc---------\\n4.1.1. Partial dependence plots#\\nPartial dependence plots (PDP) show the dependence between the target response\\nand a set of input features of interest, marginalizing over the values\\nof all other input features (the ‘complement’ features). Intuitively, we can\\ninterpret the partial dependence as the expected target response as a\\nfunction of the input features of interest.\\nDue to the limits of human perception, the size of the set of input features of\\ninterest must be small (usually, one or two) thus the input features of interest\\nare usually chosen among the most important features.\\nThe figure below shows two one-way and one two-way partial dependence plots for\\nthe bike sharing dataset, with a\\nHistGradientBoostingRegressor:\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\n>>> features = [0, 1, (0, 1)]\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features)\\n<...>\\n\\n\\nYou can access the newly created figure and Axes objects using plt.gcf()\\nand plt.gca().\\nTo make a partial dependence plot with categorical features, you need to specify\\nwhich features are categorical using the parameter categorical_features. This\\nparameter takes a list of indices, names of the categorical features or a boolean\\nmask. The graphical representation of partial dependence for categorical features is\\na bar plot or a 2D heatmap.\\n\\n\\nPDPs for multi-class classification#\\nFor multi-class classification, you need to set the class label for which\\nthe PDPs should be created via the target argument:\\n>>> from sklearn.datasets import load_iris\\n>>> iris = load_iris()\\n>>> mc_clf = GradientBoostingClassifier(n_estimators=10,\\n...     max_depth=1).fit(iris.data, iris.target)\\n>>> features = [3, 2, (3, 2)]\\n>>> PartialDependenceDisplay.from_estimator(mc_clf, X, features, target=0)\\n<...>\\n\\n\\nThe same parameter target is used to specify the target in multi-output\\nregression settings.\\n---------new doc---------\\nIn ICE plots it might not be easy to see the average effect of the input\\nfeature of interest. Hence, it is recommended to use ICE plots alongside\\nPDPs. They can be plotted together with\\nkind='both'.\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind='both')\\n<...>\\n\\n\\nIf there are too many lines in an ICE plot, it can be difficult to see\\ndifferences between individual samples and interpret the model. Centering the\\nICE at the first value on the x-axis, produces centered Individual Conditional\\nExpectation (cICE) plots [G2015]. This puts emphasis on the divergence of\\nindividual conditional expectations from the mean line, thus making it easier\\nto explore heterogeneous relationships. cICE plots can be plotted by setting\\ncentered=True:\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind='both', centered=True)\\n<...>\\n\\n\\n\\n\\n4.1.3. Mathematical Definition#\\nLet \\\\(X_S\\\\) be the set of input features of interest (i.e. the features\\nparameter) and let \\\\(X_C\\\\) be its complement.\\nThe partial dependence of the response \\\\(f\\\\) at a point \\\\(x_S\\\\) is\\ndefined as:\\n---------new doc---------\\n4.1.3. Mathematical Definition#\\nLet \\\\(X_S\\\\) be the set of input features of interest (i.e. the features\\nparameter) and let \\\\(X_C\\\\) be its complement.\\nThe partial dependence of the response \\\\(f\\\\) at a point \\\\(x_S\\\\) is\\ndefined as:\\n\\n\\\\[\\\\begin{split}pd_{X_S}(x_S) &\\\\overset{def}{=} \\\\mathbb{E}_{X_C}\\\\left[ f(x_S, X_C) \\\\right]\\\\\\\\\\n              &= \\\\int f(x_S, x_C) p(x_C) dx_C,\\\\end{split}\\\\]\\nwhere \\\\(f(x_S, x_C)\\\\) is the response function (predict,\\npredict_proba or decision_function) for a given sample whose\\nvalues are defined by \\\\(x_S\\\\) for the features in \\\\(X_S\\\\), and by\\n\\\\(x_C\\\\) for the features in \\\\(X_C\\\\). Note that \\\\(x_S\\\\) and\\n\\\\(x_C\\\\) may be tuples.\\nComputing this integral for various values of \\\\(x_S\\\\) produces a PDP plot\\nas above. An ICE line is defined as a single \\\\(f(x_{S}, x_{C}^{(i)})\\\\)\\nevaluated at \\\\(x_{S}\\\\).\\n---------new doc---------\\n6.1.1. Pipeline: chaining estimators#\\nPipeline can be used to chain multiple estimators\\ninto one. This is useful as there is often a fixed sequence\\nof steps in processing the data, for example feature selection, normalization\\nand classification. Pipeline serves multiple purposes here:\\n\\nConvenience and encapsulationYou only have to call fit and predict once on your\\ndata to fit a whole sequence of estimators.\\n\\nJoint parameter selectionYou can grid search\\nover parameters of all estimators in the pipeline at once.\\n\\nSafetyPipelines help avoid leaking statistics from your test data into the\\ntrained model in cross-validation, by ensuring that the same samples are\\nused to train the transformers and predictors.\\n\\n\\nAll estimators in a pipeline, except the last one, must be transformers\\n(i.e. must have a transform method).\\nThe last estimator may be any type (transformer, classifier, etc.).\\n\\nNote\\nCalling fit on the pipeline is the same as calling fit on\\neach estimator in turn, transform the input and pass it on to the next step.\\nThe pipeline has all the methods that the last estimator in the pipeline has,\\ni.e. if the last estimator is a classifier, the Pipeline can be used\\nas a classifier. If the last estimator is a transformer, again, so is the\\npipeline.\\n\\n\\n6.1.1.1. Usage#\\n---------new doc---------\\n6.1.1.1. Usage#\\n\\n6.1.1.1.1. Build a pipeline#\\nThe Pipeline is built using a list of (key, value) pairs, where\\nthe key is a string containing the name you want to give this step and value\\nis an estimator object:\\n>>> from sklearn.pipeline import Pipeline\\n>>> from sklearn.svm import SVC\\n>>> from sklearn.decomposition import PCA\\n>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]\\n>>> pipe = Pipeline(estimators)\\n>>> pipe\\nPipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])\\n\\n\\n\\n\\nShorthand version using make_pipeline#\\nThe utility function make_pipeline is a shorthand\\nfor constructing pipelines;\\nit takes a variable number of estimators and returns a pipeline,\\nfilling in the names automatically:\\n>>> from sklearn.pipeline import make_pipeline\\n>>> make_pipeline(PCA(), SVC())\\nPipeline(steps=[('pca', PCA()), ('svc', SVC())])\\n---------new doc---------\\n6.1.1.1.2. Access pipeline steps#\\nThe estimators of a pipeline are stored as a list in the steps attribute.\\nA sub-pipeline can be extracted using the slicing notation commonly used\\nfor Python Sequences such as lists or strings (although only a step of 1 is\\npermitted). This is convenient for performing only some of the transformations\\n(or their inverse):\\n>>> pipe[:1]\\nPipeline(steps=[('reduce_dim', PCA())])\\n>>> pipe[-1:]\\nPipeline(steps=[('clf', SVC())])\\n\\n\\n\\n\\nAccessing a step by name or position#\\nA specific step can also be accessed by index or name by indexing (with [idx]) the\\npipeline:\\n>>> pipe.steps[0]\\n('reduce_dim', PCA())\\n>>> pipe[0]\\nPCA()\\n>>> pipe['reduce_dim']\\nPCA()\\n\\n\\nPipeline’s named_steps attribute allows accessing steps by name with tab\\ncompletion in interactive environments:\\n>>> pipe.named_steps.reduce_dim is pipe['reduce_dim']\\nTrue\\n---------new doc---------\\nPipeline’s named_steps attribute allows accessing steps by name with tab\\ncompletion in interactive environments:\\n>>> pipe.named_steps.reduce_dim is pipe['reduce_dim']\\nTrue\\n\\n\\n\\n\\n\\n6.1.1.1.3. Tracking feature names in a pipeline#\\nTo enable model inspection, Pipeline has a\\nget_feature_names_out() method, just like all transformers. You can use\\npipeline slicing to get the feature names going into each step:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.feature_selection import SelectKBest\\n>>> iris = load_iris()\\n>>> pipe = Pipeline(steps=[\\n...    ('select', SelectKBest(k=2)),\\n...    ('clf', LogisticRegression())])\\n>>> pipe.fit(iris.data, iris.target)\\nPipeline(steps=[('select', SelectKBest(...)), ('clf', LogisticRegression(...))])\\n>>> pipe[:-1].get_feature_names_out()\\narray(['x2', 'x3'], ...)\\n\\n\\n\\n\\nCustomize feature names#\\nYou can also provide custom feature names for the input data using\\nget_feature_names_out:\\n>>> pipe[:-1].get_feature_names_out(iris.feature_names)\\narray(['petal length (cm)', 'petal width (cm)'], ...)\\n---------new doc---------\\nCustomize feature names#\\nYou can also provide custom feature names for the input data using\\nget_feature_names_out:\\n>>> pipe[:-1].get_feature_names_out(iris.feature_names)\\narray(['petal length (cm)', 'petal width (cm)'], ...)\\n\\n\\n\\n\\n\\n6.1.1.1.4. Access to nested parameters#\\nIt is common to adjust the parameters of an estimator within a pipeline. This parameter\\nis therefore nested because it belongs to a particular sub-step. Parameters of the\\nestimators in the pipeline are accessible using the <estimator>__<parameter>\\nsyntax:\\n>>> pipe = Pipeline(steps=[(\\\"reduce_dim\\\", PCA()), (\\\"clf\\\", SVC())])\\n>>> pipe.set_params(clf__C=10)\\nPipeline(steps=[('reduce_dim', PCA()), ('clf', SVC(C=10))])\\n\\n\\n\\n\\nWhen does it matter?#\\nThis is particularly important for doing grid searches:\\n>>> from sklearn.model_selection import GridSearchCV\\n>>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],\\n...                   clf__C=[0.1, 10, 100])\\n>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)\\n---------new doc---------\\nSide effect of caching transformers#\\nUsing a Pipeline without cache enabled, it is possible to\\ninspect the original instance such as:\\n>>> from sklearn.datasets import load_digits\\n>>> X_digits, y_digits = load_digits(return_X_y=True)\\n>>> pca1 = PCA(n_components=10)\\n>>> svm1 = SVC()\\n>>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])\\n>>> pipe.fit(X_digits, y_digits)\\nPipeline(steps=[('reduce_dim', PCA(n_components=10)), ('clf', SVC())])\\n>>> # The pca instance can be inspected directly\\n>>> pca1.components_.shape\\n(10, 64)\\n---------new doc---------\\nFor simple transformations, instead of a Transformer object, a pair of\\nfunctions can be passed, defining the transformation and its inverse mapping:\\n>>> def func(x):\\n...     return np.log(x)\\n>>> def inverse_func(x):\\n...     return np.exp(x)\\n\\n\\nSubsequently, the object is created as:\\n>>> regr = TransformedTargetRegressor(regressor=regressor,\\n...                                   func=func,\\n...                                   inverse_func=inverse_func)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: 0.51\\n---------new doc---------\\nBy default, the provided functions are checked at each fit to be the inverse of\\neach other. However, it is possible to bypass this checking by setting\\ncheck_inverse to False:\\n>>> def inverse_func(x):\\n...     return x\\n>>> regr = TransformedTargetRegressor(regressor=regressor,\\n...                                   func=func,\\n...                                   inverse_func=inverse_func,\\n...                                   check_inverse=False)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: -1.57\\n\\n\\n\\nNote\\nThe transformation can be triggered by setting either transformer or the\\npair of functions func and inverse_func. However, setting both\\noptions will raise an error.\\n\\nExamples\\n\\nEffect of transforming the targets in regression model\\n---------new doc---------\\nNote\\nThe transformation can be triggered by setting either transformer or the\\npair of functions func and inverse_func. However, setting both\\noptions will raise an error.\\n\\nExamples\\n\\nEffect of transforming the targets in regression model\\n\\n\\n\\n6.1.3. FeatureUnion: composite feature spaces#\\nFeatureUnion combines several transformer objects into a new\\ntransformer that combines their output. A FeatureUnion takes\\na list of transformer objects. During fitting, each of these\\nis fit to the data independently. The transformers are applied in parallel,\\nand the feature matrices they output are concatenated side-by-side into a\\nlarger matrix.\\nWhen you want to apply different transformations to each field of the data,\\nsee the related class ColumnTransformer\\n(see user guide).\\nFeatureUnion serves the same purposes as Pipeline -\\nconvenience and joint parameter estimation and validation.\\nFeatureUnion and Pipeline can be combined to\\ncreate complex models.\\n(A FeatureUnion has no way of checking whether two transformers\\nmight produce identical features. It only produces a union when the\\nfeature sets are disjoint, and making sure they are is the caller’s\\nresponsibility.)\\n---------new doc---------\\n6.1.3.1. Usage#\\nA FeatureUnion is built using a list of (key, value) pairs,\\nwhere the key is the name you want to give to a given transformation\\n(an arbitrary string; it only serves as an identifier)\\nand value is an estimator object:\\n>>> from sklearn.pipeline import FeatureUnion\\n>>> from sklearn.decomposition import PCA\\n>>> from sklearn.decomposition import KernelPCA\\n>>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\\n>>> combined = FeatureUnion(estimators)\\n>>> combined\\nFeatureUnion(transformer_list=[('linear_pca', PCA()),\\n                               ('kernel_pca', KernelPCA())])\\n\\n\\nLike pipelines, feature unions have a shorthand constructor called\\nmake_union that does not require explicit naming of the components.\\nLike Pipeline, individual steps may be replaced using set_params,\\nand ignored by setting to 'drop':\\n>>> combined.set_params(kernel_pca='drop')\\nFeatureUnion(transformer_list=[('linear_pca', PCA()),\\n                               ('kernel_pca', 'drop')])\\n\\n\\nExamples\\n\\nConcatenating multiple feature extraction methods\\n---------new doc---------\\nExamples\\n\\nConcatenating multiple feature extraction methods\\n\\n\\n\\n\\n6.1.4. ColumnTransformer for heterogeneous data#\\nMany datasets contain features of different types, say text, floats, and dates,\\nwhere each type of feature requires separate preprocessing or feature\\nextraction steps.  Often it is easiest to preprocess data before applying\\nscikit-learn methods, for example using pandas.\\nProcessing your data before passing it to scikit-learn might be problematic for\\none of the following reasons:\\n\\nIncorporating statistics from test data into the preprocessors makes\\ncross-validation scores unreliable (known as data leakage),\\nfor example in the case of scalers or imputing missing values.\\nYou may want to include the parameters of the preprocessors in a\\nparameter search.\\n---------new doc---------\\nIncorporating statistics from test data into the preprocessors makes\\ncross-validation scores unreliable (known as data leakage),\\nfor example in the case of scalers or imputing missing values.\\nYou may want to include the parameters of the preprocessors in a\\nparameter search.\\n\\nThe ColumnTransformer helps performing different\\ntransformations for different columns of the data, within a\\nPipeline that is safe from data leakage and that can\\nbe parametrized. ColumnTransformer works on\\narrays, sparse matrices, and\\npandas DataFrames.\\nTo each column, a different transformation can be applied, such as\\npreprocessing or a specific feature extraction method:\\n>>> import pandas as pd\\n>>> X = pd.DataFrame(\\n...     {'city': ['London', 'London', 'Paris', 'Sallisaw'],\\n...      'title': [\\\"His Last Bow\\\", \\\"How Watson Learned the Trick\\\",\\n...                \\\"A Moveable Feast\\\", \\\"The Grapes of Wrath\\\"],\\n...      'expert_rating': [5, 3, 4, 5],\\n...      'user_rating': [4, 5, 4, 3]})\\n---------new doc---------\\nFor this data, we might want to encode the 'city' column as a categorical\\nvariable using OneHotEncoder but apply a\\nCountVectorizer to the 'title' column.\\nAs we might use multiple feature extraction methods on the same column, we give\\neach transformer a unique name, say 'city_category' and 'title_bow'.\\nBy default, the remaining rating columns are ignored (remainder='drop'):\\n>>> from sklearn.compose import ColumnTransformer\\n>>> from sklearn.feature_extraction.text import CountVectorizer\\n>>> from sklearn.preprocessing import OneHotEncoder\\n>>> column_trans = ColumnTransformer(\\n...     [('categories', OneHotEncoder(dtype='int'), ['city']),\\n...      ('title_bow', CountVectorizer(), 'title')],\\n...     remainder='drop', verbose_feature_names_out=False)\\n\\n>>> column_trans.fit(X)\\nColumnTransformer(transformers=[('categories', OneHotEncoder(dtype='int'),\\n                                 ['city']),\\n                                ('title_bow', CountVectorizer(), 'title')],\\n                  verbose_feature_names_out=False)\\n---------new doc---------\\n>>> column_trans.get_feature_names_out()\\narray(['city_London', 'city_Paris', 'city_Sallisaw', 'bow', 'feast',\\n'grapes', 'his', 'how', 'last', 'learned', 'moveable', 'of', 'the',\\n 'trick', 'watson', 'wrath'], ...)\\n\\n>>> column_trans.transform(X).toarray()\\narray([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\\n       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0],\\n       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\\n       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]...)\\n---------new doc---------\\nIn the above example, the\\nCountVectorizer expects a 1D array as\\ninput and therefore the columns were specified as a string ('title').\\nHowever, OneHotEncoder\\nas most of other transformers expects 2D data, therefore in that case you need\\nto specify the column as a list of strings (['city']).\\nApart from a scalar or a single item list, the column selection can be specified\\nas a list of multiple items, an integer array, a slice, a boolean mask, or\\nwith a make_column_selector. The\\nmake_column_selector is used to select columns based\\non data type or column name:\\n>>> from sklearn.preprocessing import StandardScaler\\n>>> from sklearn.compose import make_column_selector\\n>>> ct = ColumnTransformer([\\n...       ('scale', StandardScaler(),\\n...       make_column_selector(dtype_include=np.number)),\\n...       ('onehot',\\n...       OneHotEncoder(),\\n...       make_column_selector(pattern='city', dtype_include=object))])\\n>>> ct.fit_transform(X)\\narray([[ 0.904...,  0.      ,  1. ,  0. ,  0. ],\\n       [-1.507...,  1.414...,  1. ,  0. ,  0. ],\\n       [-0.301...,  0.      ,  0. ,  1. ,  0. ],\\n---------new doc---------\\narray([[ 0.904...,  0.      ,  1. ,  0. ,  0. ],\\n       [-1.507...,  1.414...,  1. ,  0. ,  0. ],\\n       [-0.301...,  0.      ,  0. ,  1. ,  0. ],\\n       [ 0.904..., -1.414...,  0. ,  0. ,  1. ]])\\n---------new doc---------\\nStrings can reference columns if the input is a DataFrame, integers are always\\ninterpreted as the positional columns.\\nWe can keep the remaining rating columns by setting\\nremainder='passthrough'. The values are appended to the end of the\\ntransformation:\\n>>> column_trans = ColumnTransformer(\\n...     [('city_category', OneHotEncoder(dtype='int'),['city']),\\n...      ('title_bow', CountVectorizer(), 'title')],\\n...     remainder='passthrough')\\n\\n>>> column_trans.fit_transform(X)\\narray([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],\\n       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],\\n       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4],\\n       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]]...)\\n---------new doc---------\\nThe remainder parameter can be set to an estimator to transform the\\nremaining rating columns. The transformed values are appended to the end of\\nthe transformation:\\n>>> from sklearn.preprocessing import MinMaxScaler\\n>>> column_trans = ColumnTransformer(\\n...     [('city_category', OneHotEncoder(), ['city']),\\n...      ('title_bow', CountVectorizer(), 'title')],\\n...     remainder=MinMaxScaler())\\n\\n>>> column_trans.fit_transform(X)[:, -2:]\\narray([[1. , 0.5],\\n       [0. , 1. ],\\n       [0.5, 0.5],\\n       [1. , 0. ]])\\n---------new doc---------\\n>>> column_trans.fit_transform(X)[:, -2:]\\narray([[1. , 0.5],\\n       [0. , 1. ],\\n       [0.5, 0.5],\\n       [1. , 0. ]])\\n\\n\\nThe make_column_transformer function is available\\nto more easily create a ColumnTransformer object.\\nSpecifically, the names will be given automatically. The equivalent for the\\nabove example would be:\\n>>> from sklearn.compose import make_column_transformer\\n>>> column_trans = make_column_transformer(\\n...     (OneHotEncoder(), ['city']),\\n...     (CountVectorizer(), 'title'),\\n...     remainder=MinMaxScaler())\\n>>> column_trans\\nColumnTransformer(remainder=MinMaxScaler(),\\n                  transformers=[('onehotencoder', OneHotEncoder(), ['city']),\\n                                ('countvectorizer', CountVectorizer(),\\n                                 'title')])\\n---------new doc---------\\nIf ColumnTransformer is fitted with a dataframe\\nand the dataframe only has string column names, then transforming a dataframe\\nwill use the column names to select the columns:\\n>>> ct = ColumnTransformer(\\n...          [(\\\"scale\\\", StandardScaler(), [\\\"expert_rating\\\"])]).fit(X)\\n>>> X_new = pd.DataFrame({\\\"expert_rating\\\": [5, 6, 1],\\n...                       \\\"ignored_new_col\\\": [1.2, 0.3, -0.1]})\\n>>> ct.transform(X_new)\\narray([[ 0.9...],\\n       [ 2.1...],\\n       [-3.9...]])\\n\\n\\n\\n\\n6.1.5. Visualizing Composite Estimators#\\nEstimators are displayed with an HTML representation when shown in a\\njupyter notebook. This is useful to diagnose or visualize a Pipeline with\\nmany estimators. This visualization is activated by default:\\n>>> column_trans  \\n\\n\\nIt can be deactivated by setting the display option in set_config\\nto ‘text’:\\n>>> from sklearn import set_config\\n>>> set_config(display='text')  \\n>>> # displays text representation in a jupyter context\\n>>> column_trans\\n---------new doc---------\\nIt can be deactivated by setting the display option in set_config\\nto ‘text’:\\n>>> from sklearn import set_config\\n>>> set_config(display='text')  \\n>>> # displays text representation in a jupyter context\\n>>> column_trans  \\n\\n\\nAn example of the HTML output can be seen in the\\nHTML representation of Pipeline section of\\nColumn Transformer with Mixed Types.\\nAs an alternative, the HTML can be written to a file using\\nestimator_html_repr:\\n>>> from sklearn.utils import estimator_html_repr\\n>>> with open('my_estimator.html', 'w') as f:  \\n...     f.write(estimator_html_repr(clf))\\n\\n\\nExamples\\n\\nColumn Transformer with Heterogeneous Data Sources\\nColumn Transformer with Mixed Types\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n6. Dataset transformations\\n\\n\\n\\n\\nnext\\n6.2. Feature extraction\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.2. Feature extraction\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6.2. Feature extraction#\\nThe sklearn.feature_extraction module can be used to extract\\nfeatures in a format supported by machine learning algorithms from datasets\\nconsisting of formats such as text and image.\\n\\nNote\\nFeature extraction is very different from Feature selection:\\nthe former consists in transforming arbitrary data, such as text or\\nimages, into numerical features usable for machine learning. The latter\\nis a machine learning technique applied on these features.\\n---------new doc---------\\n6.2. Feature extraction#\\nThe sklearn.feature_extraction module can be used to extract\\nfeatures in a format supported by machine learning algorithms from datasets\\nconsisting of formats such as text and image.\\n\\nNote\\nFeature extraction is very different from Feature selection:\\nthe former consists in transforming arbitrary data, such as text or\\nimages, into numerical features usable for machine learning. The latter\\nis a machine learning technique applied on these features.\\n\\n\\n6.2.1. Loading features from dicts#\\nThe class DictVectorizer can be used to convert feature\\narrays represented as lists of standard Python dict objects to the\\nNumPy/SciPy representation used by scikit-learn estimators.\\nWhile not particularly fast to process, Python’s dict has the\\nadvantages of being convenient to use, being sparse (absent features\\nneed not be stored) and storing feature names in addition to values.\\nDictVectorizer implements what is called one-of-K or “one-hot”\\ncoding for categorical (aka nominal, discrete) features. Categorical\\nfeatures are “attribute-value” pairs where the value is restricted\\nto a list of discrete possibilities without ordering (e.g. topic\\nidentifiers, types of objects, tags, names…).\\nIn the following, “city” is a categorical attribute while “temperature”\\nis a traditional numerical feature:\\n>>> measurements = [\\n...     {'city': 'Dubai', 'temperature': 33.},\\n...     {'city': 'London', 'temperature': 12.},\\n...     {'city': 'San Francisco', 'temperature': 18.},\\n... ]\\n---------new doc---------\\n>>> from sklearn.feature_extraction import DictVectorizer\\n>>> vec = DictVectorizer()\\n\\n>>> vec.fit_transform(measurements).toarray()\\narray([[ 1.,  0.,  0., 33.],\\n       [ 0.,  1.,  0., 12.],\\n       [ 0.,  0.,  1., 18.]])\\n\\n>>> vec.get_feature_names_out()\\narray(['city=Dubai', 'city=London', 'city=San Francisco', 'temperature'], ...)\\n---------new doc---------\\nDictVectorizer accepts multiple string values for one\\nfeature, like, e.g., multiple categories for a movie.\\nAssume a database classifies each movie using some categories (not mandatories)\\nand its year of release.\\n>>> movie_entry = [{'category': ['thriller', 'drama'], 'year': 2003},\\n...                {'category': ['animation', 'family'], 'year': 2011},\\n...                {'year': 1974}]\\n>>> vec.fit_transform(movie_entry).toarray()\\narray([[0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 2.003e+03],\\n       [1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 2.011e+03],\\n       [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.974e+03]])\\n>>> vec.get_feature_names_out()\\narray(['category=animation', 'category=drama', 'category=family',\\n       'category=thriller', 'year'], ...)\\n>>> vec.transform({'category': ['thriller'],\\n...                'unseen_feature': '3'}).toarray()\\narray([[0., 0., 0., 1., 0.]])\\n---------new doc---------\\nDictVectorizer is also a useful representation transformation\\nfor training sequence classifiers in Natural Language Processing models\\nthat typically work by extracting feature windows around a particular\\nword of interest.\\nFor example, suppose that we have a first algorithm that extracts Part of\\nSpeech (PoS) tags that we want to use as complementary tags for training\\na sequence classifier (e.g. a chunker). The following dict could be\\nsuch a window of features extracted around the word ‘sat’ in the sentence\\n‘The cat sat on the mat.’:\\n>>> pos_window = [\\n...     {\\n...         'word-2': 'the',\\n...         'pos-2': 'DT',\\n...         'word-1': 'cat',\\n...         'pos-1': 'NN',\\n...         'word+1': 'on',\\n...         'pos+1': 'PP',\\n...     },\\n...     # in a real application one would extract many such dictionaries\\n... ]\\n---------new doc---------\\nThis description can be vectorized into a sparse two-dimensional matrix\\nsuitable for feeding into a classifier (maybe after being piped into a\\nTfidfTransformer for normalization):\\n>>> vec = DictVectorizer()\\n>>> pos_vectorized = vec.fit_transform(pos_window)\\n>>> pos_vectorized\\n<Compressed Sparse...dtype 'float64'\\n  with 6 stored elements and shape (1, 6)>\\n>>> pos_vectorized.toarray()\\narray([[1., 1., 1., 1., 1., 1.]])\\n>>> vec.get_feature_names_out()\\narray(['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat',\\n       'word-2=the'], ...)\\n\\n\\nAs you can imagine, if one extracts such a context around each individual\\nword of a corpus of documents the resulting matrix will be very wide\\n(many one-hot-features) with most of them being valued to zero most\\nof the time. So as to make the resulting data structure able to fit in\\nmemory the DictVectorizer class uses a scipy.sparse matrix by\\ndefault instead of a numpy.ndarray.\\n---------new doc---------\\n6.2.2. Feature hashing#\\nThe class FeatureHasher is a high-speed, low-memory vectorizer that\\nuses a technique known as\\nfeature hashing,\\nor the “hashing trick”.\\nInstead of building a hash table of the features encountered in training,\\nas the vectorizers do, instances of FeatureHasher\\napply a hash function to the features\\nto determine their column index in sample matrices directly.\\nThe result is increased speed and reduced memory usage,\\nat the expense of inspectability;\\nthe hasher does not remember what the input features looked like\\nand has no inverse_transform method.\\nSince the hash function might cause collisions between (unrelated) features,\\na signed hash function is used and the sign of the hash value\\ndetermines the sign of the value stored in the output matrix for a feature.\\nThis way, collisions are likely to cancel out rather than accumulate error,\\nand the expected mean of any output feature’s value is zero. This mechanism\\nis enabled by default with alternate_sign=True and is particularly useful\\nfor small hash table sizes (n_features < 10000). For large hash table\\nsizes, it can be disabled, to allow the output to be passed to estimators like\\nMultinomialNB or\\nchi2\\nfeature selectors that expect non-negative inputs.\\nFeatureHasher accepts either mappings\\n(like Python’s dict and its variants in the collections module),\\n(feature, value) pairs, or strings,\\ndepending on the constructor parameter input_type.\\nMapping are treated as lists of (feature, value) pairs,\\nwhile single strings have an implicit value of 1,\\nso ['feat1', 'feat2', 'feat3'] is interpreted as\\n[('feat1', 1), ('feat2', 1), ('feat3', 1)].\\n---------new doc---------\\n(like Python’s dict and its variants in the collections module),\\n(feature, value) pairs, or strings,\\ndepending on the constructor parameter input_type.\\nMapping are treated as lists of (feature, value) pairs,\\nwhile single strings have an implicit value of 1,\\nso ['feat1', 'feat2', 'feat3'] is interpreted as\\n[('feat1', 1), ('feat2', 1), ('feat3', 1)].\\nIf a single feature occurs multiple times in a sample,\\nthe associated values will be summed\\n(so ('feat', 2) and ('feat', 3.5) become ('feat', 5.5)).\\nThe output from FeatureHasher is always a scipy.sparse matrix\\nin the CSR format.\\nFeature hashing can be employed in document classification,\\nbut unlike CountVectorizer,\\nFeatureHasher does not do word\\nsplitting or any other preprocessing except Unicode-to-UTF-8 encoding;\\nsee Vectorizing a large text corpus with the hashing trick, below, for a combined tokenizer/hasher.\\nAs an example, consider a word-level natural language processing task\\nthat needs features extracted from (token, part_of_speech) pairs.\\nOne could use a Python generator function to extract features:\\ndef token_features(token, part_of_speech):\\n    if token.isdigit():\\n        yield \\\"numeric\\\"\\n    else:\\n        yield \\\"token={}\\\".format(token.lower())\\n        yield \\\"token,pos={},{}\\\".format(token, part_of_speech)\\n    if token[0].isupper():\\n        yield \\\"uppercase_initial\\\"\\n---------new doc---------\\nyield \\\"numeric\\\"\\n    else:\\n        yield \\\"token={}\\\".format(token.lower())\\n        yield \\\"token,pos={},{}\\\".format(token, part_of_speech)\\n    if token[0].isupper():\\n        yield \\\"uppercase_initial\\\"\\n    if token.isupper():\\n        yield \\\"all_uppercase\\\"\\n    yield \\\"pos={}\\\".format(part_of_speech)\\n---------new doc---------\\nThen, the raw_X to be fed to FeatureHasher.transform\\ncan be constructed using:\\nraw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)\\n\\n\\nand fed to a hasher with:\\nhasher = FeatureHasher(input_type='string')\\nX = hasher.transform(raw_X)\\n\\n\\nto get a scipy.sparse matrix X.\\nNote the use of a generator comprehension,\\nwhich introduces laziness into the feature extraction:\\ntokens are only processed on demand from the hasher.\\n\\n\\nImplementation details#\\nFeatureHasher uses the signed 32-bit variant of MurmurHash3.\\nAs a result (and because of limitations in scipy.sparse),\\nthe maximum number of features supported is currently \\\\(2^{31} - 1\\\\).\\nThe original formulation of the hashing trick by Weinberger et al.\\nused two separate hash functions \\\\(h\\\\) and \\\\(\\\\xi\\\\)\\nto determine the column index and sign of a feature, respectively.\\nThe present implementation works under the assumption\\nthat the sign bit of MurmurHash3 is independent of its other bits.\\nSince a simple modulo is used to transform the hash function to a column index,\\nit is advisable to use a power of two as the n_features parameter;\\notherwise the features will not be mapped evenly to the columns.\\nReferences\\n\\nMurmurHash3.\\n\\n\\nReferences\\n\\nKilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and\\nJosh Attenberg (2009). Feature hashing for large scale multitask learning. Proc. ICML.\\n\\n\\n\\n6.2.3. Text feature extraction#\\n---------new doc---------\\nIn this scheme, features and samples are defined as follows:\\n\\neach individual token occurrence frequency (normalized or not)\\nis treated as a feature.\\nthe vector of all the token frequencies for a given document is\\nconsidered a multivariate sample.\\n\\nA corpus of documents can thus be represented by a matrix with one row\\nper document and one column per token (e.g. word) occurring in the corpus.\\nWe call vectorization the general process of turning a collection\\nof text documents into numerical feature vectors. This specific strategy\\n(tokenization, counting and normalization) is called the Bag of Words\\nor “Bag of n-grams” representation. Documents are described by word\\noccurrences while completely ignoring the relative position information\\nof the words in the document.\\n\\n\\n6.2.3.2. Sparsity#\\nAs most documents will typically use a very small subset of the words used in\\nthe corpus, the resulting matrix will have many feature values that are\\nzeros (typically more than 99% of them).\\nFor instance a collection of 10,000 short text documents (such as emails)\\nwill use a vocabulary with a size in the order of 100,000 unique words in\\ntotal while each document will use 100 to 1000 unique words individually.\\nIn order to be able to store such a matrix in memory but also to speed\\nup algebraic operations matrix / vector, implementations will typically\\nuse a sparse representation such as the implementations available in the\\nscipy.sparse package.\\n\\n\\n6.2.3.3. Common Vectorizer usage#\\nCountVectorizer implements both tokenization and occurrence\\ncounting in a single class:\\n>>> from sklearn.feature_extraction.text import CountVectorizer\\n---------new doc---------\\n6.2.3.3. Common Vectorizer usage#\\nCountVectorizer implements both tokenization and occurrence\\ncounting in a single class:\\n>>> from sklearn.feature_extraction.text import CountVectorizer\\n\\n\\nThis model has many parameters, however the default values are quite\\nreasonable (please see  the reference documentation for the details):\\n>>> vectorizer = CountVectorizer()\\n>>> vectorizer\\nCountVectorizer()\\n\\n\\nLet’s use it to tokenize and count the word occurrences of a minimalistic\\ncorpus of text documents:\\n>>> corpus = [\\n...     'This is the first document.',\\n...     'This is the second second document.',\\n...     'And the third one.',\\n...     'Is this the first document?',\\n... ]\\n>>> X = vectorizer.fit_transform(corpus)\\n>>> X\\n<Compressed Sparse...dtype 'int64'\\n  with 19 stored elements and shape (4, 9)>\\n\\n\\nThe default configuration tokenizes the string by extracting words of\\nat least 2 letters. The specific function that does this step can be\\nrequested explicitly:\\n>>> analyze = vectorizer.build_analyzer()\\n>>> analyze(\\\"This is a text document to analyze.\\\") == (\\n...     ['this', 'is', 'text', 'document', 'to', 'analyze'])\\nTrue\\n---------new doc---------\\nThe default configuration tokenizes the string by extracting words of\\nat least 2 letters. The specific function that does this step can be\\nrequested explicitly:\\n>>> analyze = vectorizer.build_analyzer()\\n>>> analyze(\\\"This is a text document to analyze.\\\") == (\\n...     ['this', 'is', 'text', 'document', 'to', 'analyze'])\\nTrue\\n\\n\\nEach term found by the analyzer during the fit is assigned a unique\\ninteger index corresponding to a column in the resulting matrix. This\\ninterpretation of the columns can be retrieved as follows:\\n>>> vectorizer.get_feature_names_out()\\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the',\\n       'third', 'this'], ...)\\n\\n>>> X.toarray()\\narray([[0, 1, 1, 1, 0, 0, 1, 0, 1],\\n       [0, 1, 0, 1, 0, 2, 1, 0, 1],\\n       [1, 0, 0, 0, 1, 0, 1, 1, 0],\\n       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)\\n\\n\\nThe converse mapping from feature name to column index is stored in the\\nvocabulary_ attribute of the vectorizer:\\n>>> vectorizer.vocabulary_.get('document')\\n1\\n\\n\\nHence words that were not seen in the training corpus will be completely\\nignored in future calls to the transform method:\\n>>> vectorizer.transform(['Something completely new.']).toarray()\\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)\\n---------new doc---------\\nHence words that were not seen in the training corpus will be completely\\nignored in future calls to the transform method:\\n>>> vectorizer.transform(['Something completely new.']).toarray()\\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)\\n\\n\\nNote that in the previous corpus, the first and the last documents have\\nexactly the same words hence are encoded in equal vectors. In particular\\nwe lose the information that the last document is an interrogative form. To\\npreserve some of the local ordering information we can extract 2-grams\\nof words in addition to the 1-grams (individual words):\\n>>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\\n...                                     token_pattern=r'\\\\b\\\\w+\\\\b', min_df=1)\\n>>> analyze = bigram_vectorizer.build_analyzer()\\n>>> analyze('Bi-grams are cool!') == (\\n...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\\nTrue\\n---------new doc---------\\nThe vocabulary extracted by this vectorizer is hence much bigger and\\ncan now resolve ambiguities encoded in local positioning patterns:\\n>>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\\n>>> X_2\\narray([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\\n       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\\n       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\\n       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)\\n\\n\\nIn particular the interrogative form “Is this” is only present in the\\nlast document:\\n>>> feature_index = bigram_vectorizer.vocabulary_.get('is this')\\n>>> X_2[:, feature_index]\\narray([0, 0, 0, 1]...)\\n---------new doc---------\\nIn particular the interrogative form “Is this” is only present in the\\nlast document:\\n>>> feature_index = bigram_vectorizer.vocabulary_.get('is this')\\n>>> X_2[:, feature_index]\\narray([0, 0, 0, 1]...)\\n\\n\\n\\n\\n6.2.3.4. Using stop words#\\nStop words are words like “and”, “the”, “him”, which are presumed to be\\nuninformative in representing the content of a text, and which may be\\nremoved to avoid them being construed as signal for prediction.  Sometimes,\\nhowever, similar words are useful for prediction, such as in classifying\\nwriting style or personality.\\nThere are several known issues in our provided ‘english’ stop word list. It\\ndoes not aim to be a general, ‘one-size-fits-all’ solution as some tasks\\nmay require a more custom solution. See [NQY18] for more details.\\nPlease take care in choosing a stop word list.\\nPopular stop word lists may include words that are highly informative to\\nsome tasks, such as computer.\\nYou should also make sure that the stop word list has had the same\\npreprocessing and tokenization applied as the one used in the vectorizer.\\nThe word we’ve is split into we and ve by CountVectorizer’s default\\ntokenizer, so if we’ve is in stop_words, but ve is not, ve will\\nbe retained from we’ve in transformed text.  Our vectorizers will try to\\nidentify and warn about some kinds of inconsistencies.\\nReferences\\n---------new doc---------\\n[NQY18]\\nJ. Nothman, H. Qin and R. Yurchak (2018).\\n“Stop Word Lists in Free Open-source Software Packages”.\\nIn Proc. Workshop for NLP Open Source Software.\\n---------new doc---------\\n6.2.3.5. Tf–idf term weighting#\\nIn a large text corpus, some words will be very present (e.g. “the”, “a”,\\n“is” in English) hence carrying very little meaningful information about\\nthe actual contents of the document. If we were to feed the direct count\\ndata directly to a classifier those very frequent terms would shadow\\nthe frequencies of rarer yet more interesting terms.\\nIn order to re-weight the count features into floating point values\\nsuitable for usage by a classifier it is very common to use the tf–idf\\ntransform.\\nTf means term-frequency while tf–idf means term-frequency times\\ninverse document-frequency:\\n\\\\(\\\\text{tf-idf(t,d)}=\\\\text{tf(t,d)} \\\\times \\\\text{idf(t)}\\\\).\\nUsing the TfidfTransformer’s default settings,\\nTfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\\nthe term frequency, the number of times a term occurs in a given document,\\nis multiplied with idf component, which is computed as\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{1 + n}{1+\\\\text{df}(t)}} + 1\\\\),\\nwhere \\\\(n\\\\) is the total number of documents in the document set, and\\n\\\\(\\\\text{df}(t)\\\\) is the number of documents in the document set that\\ncontain term \\\\(t\\\\). The resulting tf-idf vectors are then normalized by the\\nEuclidean norm:\\n---------new doc---------\\nwhere \\\\(n\\\\) is the total number of documents in the document set, and\\n\\\\(\\\\text{df}(t)\\\\) is the number of documents in the document set that\\ncontain term \\\\(t\\\\). The resulting tf-idf vectors are then normalized by the\\nEuclidean norm:\\n\\\\(v_{norm} = \\\\frac{v}{||v||_2} = \\\\frac{v}{\\\\sqrt{v{_1}^2 +\\nv{_2}^2 + \\\\dots + v{_n}^2}}\\\\).\\nThis was originally a term weighting scheme developed for information retrieval\\n(as a ranking function for search engines results) that has also found good\\nuse in document classification and clustering.\\nThe following sections contain further explanations and examples that\\nillustrate how the tf-idfs are computed exactly and how the tf-idfs\\ncomputed in scikit-learn’s TfidfTransformer\\nand TfidfVectorizer differ slightly from the standard textbook\\nnotation that defines the idf as\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{n}{1+\\\\text{df}(t)}}.\\\\)\\nIn the TfidfTransformer and TfidfVectorizer\\nwith smooth_idf=False, the\\n“1” count is added to the idf instead of the idf’s denominator:\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{n}{\\\\text{df}(t)}} + 1\\\\)\\nThis normalization is implemented by the TfidfTransformer\\nclass:\\n>>> from sklearn.feature_extraction.text import TfidfTransformer\\n---------new doc---------\\n“1” count is added to the idf instead of the idf’s denominator:\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{n}{\\\\text{df}(t)}} + 1\\\\)\\nThis normalization is implemented by the TfidfTransformer\\nclass:\\n>>> from sklearn.feature_extraction.text import TfidfTransformer\\n>>> transformer = TfidfTransformer(smooth_idf=False)\\n>>> transformer\\nTfidfTransformer(smooth_idf=False)\\n---------new doc---------\\nAgain please see the reference documentation for the details on all the parameters.\\n\\n\\nNumeric example of a tf-idf matrix#\\nLet’s take an example with the following counts. The first term is present\\n100% of the time hence not very interesting. The two other features only\\nin less than 50% of the time hence probably more representative of the\\ncontent of the documents:\\n>>> counts = [[3, 0, 1],\\n...           [2, 0, 0],\\n...           [3, 0, 0],\\n...           [4, 0, 0],\\n...           [3, 2, 0],\\n...           [3, 0, 2]]\\n...\\n>>> tfidf = transformer.fit_transform(counts)\\n>>> tfidf\\n<Compressed Sparse...dtype 'float64'\\n  with 9 stored elements and shape (6, 3)>\\n---------new doc---------\\n>>> tfidf.toarray()\\narray([[0.81940995, 0.        , 0.57320793],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [0.47330339, 0.88089948, 0.        ],\\n      [0.58149261, 0.        , 0.81355169]])\\n---------new doc---------\\nand the vector of raw tf-idfs:\\n\\\\(\\\\text{tf-idf}_{\\\\text{raw}} = [3, 0, 2.0986].\\\\)\\nThen, applying the Euclidean (L2) norm, we obtain the following tf-idfs\\nfor document 1:\\n\\\\(\\\\frac{[3, 0, 2.0986]}{\\\\sqrt{\\\\big(3^2 + 0^2 + 2.0986^2\\\\big)}}\\n= [ 0.819,  0,  0.573].\\\\)\\nFurthermore, the default parameter smooth_idf=True adds “1” to the numerator\\nand  denominator as if an extra document was seen containing every term in the\\ncollection exactly once, which prevents zero divisions:\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{1 + n}{1+\\\\text{df}(t)}} + 1\\\\)\\nUsing this modification, the tf-idf of the third term in document 1 changes to\\n1.8473:\\n\\\\(\\\\text{tf-idf}_{\\\\text{term3}} = 1 \\\\times \\\\log(7/3)+1 \\\\approx 1.8473\\\\)\\nAnd the L2-normalized tf-idf changes to\\n\\\\(\\\\frac{[3, 0, 1.8473]}{\\\\sqrt{\\\\big(3^2 + 0^2 + 1.8473^2\\\\big)}}\\n= [0.8515, 0, 0.5243]\\\\):\\n>>> transformer = TfidfTransformer()\\n>>> transformer.fit_transform(counts).toarray()\\narray([[0.85151335, 0.        , 0.52433293],\\n---------new doc---------\\n= [0.8515, 0, 0.5243]\\\\):\\n>>> transformer = TfidfTransformer()\\n>>> transformer.fit_transform(counts).toarray()\\narray([[0.85151335, 0.        , 0.52433293],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [0.55422893, 0.83236428, 0.        ],\\n      [0.63035731, 0.        , 0.77630514]])\\n---------new doc---------\\nThe weights of each\\nfeature computed by the fit method call are stored in a model\\nattribute:\\n>>> transformer.idf_\\narray([1. ..., 2.25..., 1.84...])\\n\\n\\nAs tf-idf is very often used for text features, there is also another\\nclass called TfidfVectorizer that combines all the options of\\nCountVectorizer and TfidfTransformer in a single model:\\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\\n>>> vectorizer = TfidfVectorizer()\\n>>> vectorizer.fit_transform(corpus)\\n<Compressed Sparse...dtype 'float64'\\n  with 19 stored elements and shape (4, 9)>\\n\\n\\nWhile the tf-idf normalization is often very useful, there might\\nbe cases where the binary occurrence markers might offer better\\nfeatures. This can be achieved by using the binary parameter\\nof CountVectorizer. In particular, some estimators such as\\nBernoulli Naive Bayes explicitly model discrete boolean random\\nvariables. Also, very short texts are likely to have noisy tf-idf values\\nwhile the binary occurrence info is more stable.\\nAs usual the best way to adjust the feature extraction parameters\\nis to use a cross-validated grid search, for instance by pipelining the\\nfeature extractor with a classifier:\\n\\nSample pipeline for text feature extraction and evaluation\\n---------new doc---------\\nSample pipeline for text feature extraction and evaluation\\n\\n\\n\\n\\n6.2.3.6. Decoding text files#\\nText is made of characters, but files are made of bytes. These bytes represent\\ncharacters according to some encoding. To work with text files in Python,\\ntheir bytes must be decoded to a character set called Unicode.\\nCommon encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian)\\nand the universal encodings UTF-8 and UTF-16. Many others exist.\\n\\nNote\\nAn encoding can also be called a ‘character set’,\\nbut this term is less accurate: several encodings can exist\\nfor a single character set.\\n\\nThe text feature extractors in scikit-learn know how to decode text files,\\nbut only if you tell them what encoding the files are in.\\nThe CountVectorizer takes an encoding parameter for this purpose.\\nFor modern text files, the correct encoding is probably UTF-8,\\nwhich is therefore the default (encoding=\\\"utf-8\\\").\\nIf the text you are loading is not actually encoded with UTF-8, however,\\nyou will get a UnicodeDecodeError.\\nThe vectorizers can be told to be silent about decoding errors\\nby setting the decode_error parameter to either \\\"ignore\\\"\\nor \\\"replace\\\". See the documentation for the Python function\\nbytes.decode for more details\\n(type help(bytes.decode) at the Python prompt).\\n\\n\\nTroubleshooting decoding text#\\nIf you are having trouble decoding text, here are some things to try:\\n---------new doc---------\\nTroubleshooting decoding text#\\nIf you are having trouble decoding text, here are some things to try:\\n\\nFind out what the actual encoding of the text is. The file might come\\nwith a header or README that tells you the encoding, or there might be some\\nstandard encoding you can assume based on where the text comes from.\\nYou may be able to find out what kind of encoding it is in general\\nusing the UNIX command file. The Python chardet module comes with\\na script called chardetect.py that will guess the specific encoding,\\nthough you cannot rely on its guess being correct.\\nYou could try UTF-8 and disregard the errors. You can decode byte\\nstrings with bytes.decode(errors='replace') to replace all\\ndecoding errors with a meaningless character, or set\\ndecode_error='replace' in the vectorizer. This may damage the\\nusefulness of your features.\\nReal text may come from a variety of sources that may have used different\\nencodings, or even be sloppily decoded in a different encoding than the\\none it was encoded with. This is common in text retrieved from the Web.\\nThe Python package ftfy\\ncan automatically sort out some classes of\\ndecoding errors, so you could try decoding the unknown text as latin-1\\nand then using ftfy to fix errors.\\nIf the text is in a mish-mash of encodings that is simply too hard to sort\\nout (which is the case for the 20 Newsgroups dataset), you can fall back on\\na simple single-byte encoding such as latin-1. Some text may display\\nincorrectly, but at least the same sequence of bytes will always represent\\nthe same feature.\\n---------new doc---------\\nFor example, the following snippet uses chardet\\n(not shipped with scikit-learn, must be installed separately)\\nto figure out the encoding of three texts.\\nIt then vectorizes the texts and prints the learned vocabulary.\\nThe output is not shown here.\\n>>> import chardet    \\n>>> text1 = b\\\"Sei mir gegr\\\\xc3\\\\xbc\\\\xc3\\\\x9ft mein Sauerkraut\\\"\\n>>> text2 = b\\\"holdselig sind deine Ger\\\\xfcche\\\"\\n>>> text3 = b\\\"\\\\xff\\\\xfeA\\\\x00u\\\\x00f\\\\x00 \\\\x00F\\\\x00l\\\\x00\\\\xfc\\\\x00g\\\\x00e\\\\x00l\\\\x00n\\\\x00 \\\\x00d\\\\x00e\\\\x00s\\\\x00 \\\\x00G\\\\x00e\\\\x00s\\\\x00a\\\\x00n\\\\x00g\\\\x00e\\\\x00s\\\\x00,\\\\x00 \\\\x00H\\\\x00e\\\\x00r\\\\x00z\\\\x00l\\\\x00i\\\\x00e\\\\x00b\\\\x00c\\\\x00h\\\\x00e\\\\x00n\\\\x00,\\\\x00 \\\\x00t\\\\x00r\\\\x00a\\\\x00g\\\\x00 \\\\x00i\\\\x00c\\\\x00h\\\\x00 \\\\x00d\\\\x00i\\\\x00c\\\\x00h\\\\x00 \\\\x00f\\\\x00o\\\\x00r\\\\x00t\\\\x00\\\"\\n>>> decoded = [x.decode(chardet.detect(x)['encoding'])\\n---------new doc---------\\n>>> decoded = [x.decode(chardet.detect(x)['encoding'])\\n...            for x in (text1, text2, text3)]        \\n>>> v = CountVectorizer().fit(decoded).vocabulary_    \\n>>> for term in v: print(v)\\n---------new doc---------\\n(Depending on the version of chardet, it might get the first one wrong.)\\nFor an introduction to Unicode and character encodings in general,\\nsee Joel Spolsky’s Absolute Minimum Every Software Developer Must Know\\nAbout Unicode.\\n\\n\\n\\n6.2.3.7. Applications and examples#\\nThe bag of words representation is quite simplistic but surprisingly\\nuseful in practice.\\nIn particular in a supervised setting it can be successfully combined\\nwith fast and scalable linear models to train document classifiers,\\nfor instance:\\n\\nClassification of text documents using sparse features\\n\\nIn an unsupervised setting it can be used to group similar documents\\ntogether by applying clustering algorithms such as K-means:\\n\\nClustering text documents using k-means\\n\\nFinally it is possible to discover the main topics of a corpus by\\nrelaxing the hard assignment constraint of clustering, for instance by\\nusing Non-negative matrix factorization (NMF or NNMF):\\n\\nTopic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\\n---------new doc---------\\n6.2.3.8. Limitations of the Bag of Words representation#\\nA collection of unigrams (what bag of words is) cannot capture phrases\\nand multi-word expressions, effectively disregarding any word order\\ndependence. Additionally, the bag of words model doesn’t account for potential\\nmisspellings or word derivations.\\nN-grams to the rescue! Instead of building a simple collection of\\nunigrams (n=1), one might prefer a collection of bigrams (n=2), where\\noccurrences of pairs of consecutive words are counted.\\nOne might alternatively consider a collection of character n-grams, a\\nrepresentation resilient against misspellings and derivations.\\nFor example, let’s say we’re dealing with a corpus of two documents:\\n['words', 'wprds']. The second document contains a misspelling\\nof the word ‘words’.\\nA simple bag of words representation would consider these two as\\nvery distinct documents, differing in both of the two possible features.\\nA character 2-gram representation, however, would find the documents\\nmatching in 4 out of 8 features, which may help the preferred classifier\\ndecide better:\\n>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\\n>>> counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\\n>>> ngram_vectorizer.get_feature_names_out()\\narray([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...)\\n>>> counts.toarray().astype(int)\\narray([[1, 1, 1, 0, 1, 1, 1, 0],\\n---------new doc---------\\n>>> ngram_vectorizer.get_feature_names_out()\\narray([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...)\\n>>> counts.toarray().astype(int)\\narray([[1, 1, 1, 0, 1, 1, 1, 0],\\n       [1, 1, 0, 1, 1, 1, 0, 1]])\\n---------new doc---------\\nIn the above example, char_wb analyzer is used, which creates n-grams\\nonly from characters inside word boundaries (padded with space on each\\nside). The char analyzer, alternatively, creates n-grams that\\nspan across words:\\n>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\\n>>> ngram_vectorizer.fit_transform(['jumpy fox'])\\n<Compressed Sparse...dtype 'int64'\\n  with 4 stored elements and shape (1, 4)>\\n\\n>>> ngram_vectorizer.get_feature_names_out()\\narray([' fox ', ' jump', 'jumpy', 'umpy '], ...)\\n\\n>>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5))\\n>>> ngram_vectorizer.fit_transform(['jumpy fox'])\\n<Compressed Sparse...dtype 'int64'\\n  with 5 stored elements and shape (1, 5)>\\n>>> ngram_vectorizer.get_feature_names_out()\\narray(['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'], ...)\\n---------new doc---------\\nThe word boundaries-aware variant char_wb is especially interesting\\nfor languages that use white-spaces for word separation as it generates\\nsignificantly less noisy features than the raw char variant in\\nthat case. For such languages it can increase both the predictive\\naccuracy and convergence speed of classifiers trained using such\\nfeatures while retaining the robustness with regards to misspellings and\\nword derivations.\\nWhile some local positioning information can be preserved by extracting\\nn-grams instead of individual words, bag of words and bag of n-grams\\ndestroy most of the inner structure of the document and hence most of\\nthe meaning carried by that internal structure.\\nIn order to address the wider task of Natural Language Understanding,\\nthe local structure of sentences and paragraphs should thus be taken\\ninto account. Many such models will thus be casted as “Structured output”\\nproblems which are currently outside of the scope of scikit-learn.\\n\\n\\n6.2.3.9. Vectorizing a large text corpus with the hashing trick#\\nThe above vectorization scheme is simple but the fact that it holds an in-\\nmemory mapping from the string tokens to the integer feature indices (the\\nvocabulary_ attribute) causes several problems when dealing with large\\ndatasets:\\n---------new doc---------\\nIt is possible to overcome those limitations by combining the “hashing trick”\\n(Feature hashing) implemented by the\\nFeatureHasher class and the text\\npreprocessing and tokenization features of the CountVectorizer.\\nThis combination is implementing in HashingVectorizer,\\na transformer class that is mostly API compatible with CountVectorizer.\\nHashingVectorizer is stateless,\\nmeaning that you don’t have to call fit on it:\\n>>> from sklearn.feature_extraction.text import HashingVectorizer\\n>>> hv = HashingVectorizer(n_features=10)\\n>>> hv.transform(corpus)\\n<Compressed Sparse...dtype 'float64'\\n  with 16 stored elements and shape (4, 10)>\\n---------new doc---------\\nYou can see that 16 non-zero feature tokens were extracted in the vector\\noutput: this is less than the 19 non-zeros extracted previously by the\\nCountVectorizer on the same toy corpus. The discrepancy comes from\\nhash function collisions because of the low value of the n_features parameter.\\nIn a real world setting, the n_features parameter can be left to its\\ndefault value of 2 ** 20 (roughly one million possible features). If memory\\nor downstream models size is an issue selecting a lower value such as 2 **\\n18 might help without introducing too many additional collisions on typical\\ntext classification tasks.\\nNote that the dimensionality does not affect the CPU training time of\\nalgorithms which operate on CSR matrices (LinearSVC(dual=True),\\nPerceptron, SGDClassifier, PassiveAggressive) but it does for\\nalgorithms that work with CSC matrices (LinearSVC(dual=False), Lasso(),\\netc.).\\nLet’s try again with the default setting:\\n>>> hv = HashingVectorizer()\\n>>> hv.transform(corpus)\\n<Compressed Sparse...dtype 'float64'\\n  with 19 stored elements and shape (4, 1048576)>\\n\\n\\nWe no longer get the collisions, but this comes at the expense of a much larger\\ndimensionality of the output space.\\nOf course, other terms than the 19 used here\\nmight still collide with each other.\\nThe HashingVectorizer also comes with the following limitations:\\n---------new doc---------\\nWe no longer get the collisions, but this comes at the expense of a much larger\\ndimensionality of the output space.\\nOf course, other terms than the 19 used here\\nmight still collide with each other.\\nThe HashingVectorizer also comes with the following limitations:\\n\\nit is not possible to invert the model (no inverse_transform method),\\nnor to access the original string representation of the features,\\nbecause of the one-way nature of the hash function that performs the mapping.\\nit does not provide IDF weighting as that would introduce statefulness in the\\nmodel. A TfidfTransformer can be appended to it in a pipeline if\\nrequired.\\n\\n\\n\\nPerforming out-of-core scaling with HashingVectorizer#\\nAn interesting development of using a HashingVectorizer is the ability\\nto perform out-of-core scaling. This means that we can learn from data that\\ndoes not fit into the computer’s main memory.\\nA strategy to implement out-of-core scaling is to stream data to the estimator\\nin mini-batches. Each mini-batch is vectorized using HashingVectorizer\\nso as to guarantee that the input space of the estimator has always the same\\ndimensionality. The amount of memory used at any time is thus bounded by the\\nsize of a mini-batch. Although there is no limit to the amount of data that can\\nbe ingested using such an approach, from a practical point of view the learning\\ntime is often limited by the CPU time one wants to spend on the task.\\nFor a full-fledged example of out-of-core scaling in a text classification\\ntask see Out-of-core classification of text documents.\\n---------new doc---------\\n6.2.3.10. Customizing the vectorizer classes#\\nIt is possible to customize the behavior by passing a callable\\nto the vectorizer constructor:\\n>>> def my_tokenizer(s):\\n...     return s.split()\\n...\\n>>> vectorizer = CountVectorizer(tokenizer=my_tokenizer)\\n>>> vectorizer.build_analyzer()(u\\\"Some... punctuation!\\\") == (\\n...     ['some...', 'punctuation!'])\\nTrue\\n\\n\\nIn particular we name:\\n\\npreprocessor: a callable that takes an entire document as input (as a\\nsingle string), and returns a possibly transformed version of the document,\\nstill as an entire string. This can be used to remove HTML tags, lowercase\\nthe entire document, etc.\\ntokenizer: a callable that takes the output from the preprocessor\\nand splits it into tokens, then returns a list of these.\\nanalyzer: a callable that replaces the preprocessor and tokenizer.\\nThe default analyzers all call the preprocessor and tokenizer, but custom\\nanalyzers will skip this. N-gram extraction and stop word filtering take\\nplace at the analyzer level, so a custom analyzer may have to reproduce\\nthese steps.\\n\\n(Lucene users might recognize these names, but be aware that scikit-learn\\nconcepts may not map one-to-one onto Lucene concepts.)\\nTo make the preprocessor, tokenizer and analyzers aware of the model\\nparameters it is possible to derive from the class and override the\\nbuild_preprocessor, build_tokenizer and build_analyzer\\nfactory methods instead of passing custom functions.\\n\\n\\nTips and tricks#\\n---------new doc---------\\n(Lucene users might recognize these names, but be aware that scikit-learn\\nconcepts may not map one-to-one onto Lucene concepts.)\\nTo make the preprocessor, tokenizer and analyzers aware of the model\\nparameters it is possible to derive from the class and override the\\nbuild_preprocessor, build_tokenizer and build_analyzer\\nfactory methods instead of passing custom functions.\\n\\n\\nTips and tricks#\\n\\nIf documents are pre-tokenized by an external package, then store them in\\nfiles (or strings) with the tokens separated by whitespace and pass\\nanalyzer=str.split\\nFancy token-level analysis such as stemming, lemmatizing, compound\\nsplitting, filtering based on part-of-speech, etc. are not included in the\\nscikit-learn codebase, but can be added by customizing either the\\ntokenizer or the analyzer.\\nHere’s a CountVectorizer with a tokenizer and lemmatizer using\\nNLTK:\\n>>> from nltk import word_tokenize          \\n>>> from nltk.stem import WordNetLemmatizer \\n>>> class LemmaTokenizer:\\n...     def __init__(self):\\n...         self.wnl = WordNetLemmatizer()\\n...     def __call__(self, doc):\\n...         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\\n...\\n>>> vect = CountVectorizer(tokenizer=LemmaTokenizer())\\n---------new doc---------\\n(Note that this will not filter out punctuation.)\\nThe following example will, for instance, transform some British spelling\\nto American spelling:\\n>>> import re\\n>>> def to_british(tokens):\\n...     for t in tokens:\\n...         t = re.sub(r\\\"(...)our$\\\", r\\\"\\\\1or\\\", t)\\n...         t = re.sub(r\\\"([bt])re$\\\", r\\\"\\\\1er\\\", t)\\n...         t = re.sub(r\\\"([iy])s(e$|ing|ation)\\\", r\\\"\\\\1z\\\\2\\\", t)\\n...         t = re.sub(r\\\"ogue$\\\", \\\"og\\\", t)\\n...         yield t\\n...\\n>>> class CustomVectorizer(CountVectorizer):\\n...     def build_tokenizer(self):\\n...         tokenize = super().build_tokenizer()\\n...         return lambda doc: list(to_british(tokenize(doc)))\\n...\\n>>> print(CustomVectorizer().build_analyzer()(u\\\"color colour\\\"))\\n[...'color', ...'color']\\n\\n\\nfor other styles of preprocessing; examples include stemming, lemmatization,\\nor normalizing numerical tokens, with the latter illustrated in:\\n\\nBiclustering documents with the Spectral Co-clustering algorithm\\n\\n\\n\\nCustomizing the vectorizer can also be useful when handling Asian languages\\nthat do not use an explicit word separator such as whitespace.\\n\\n\\n\\n\\n6.2.4. Image feature extraction#\\n---------new doc---------\\n>>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,\\n...     random_state=0)\\n>>> patches.shape\\n(2, 2, 2, 3)\\n>>> patches[:, :, :, 0]\\narray([[[ 0,  3],\\n        [12, 15]],\\n\\n       [[15, 18],\\n        [27, 30]]])\\n>>> patches = image.extract_patches_2d(one_image, (2, 2))\\n>>> patches.shape\\n(9, 2, 2, 3)\\n>>> patches[4, :, :, 0]\\narray([[15, 18],\\n       [27, 30]])\\n\\n\\nLet us now try to reconstruct the original image from the patches by averaging\\non overlapping areas:\\n>>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))\\n>>> np.testing.assert_array_equal(one_image, reconstructed)\\n\\n\\nThe PatchExtractor class works in the same way as\\nextract_patches_2d, only it supports multiple images as input. It is\\nimplemented as a scikit-learn transformer, so it can be used in pipelines. See:\\n>>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\\n>>> patches = image.PatchExtractor(patch_size=(2, 2)).transform(five_images)\\n>>> patches.shape\\n(45, 2, 2, 3)\\n---------new doc---------\\n>>> scaler.mean_\\narray([1. ..., 0. ..., 0.33...])\\n\\n>>> scaler.scale_\\narray([0.81..., 0.81..., 1.24...])\\n\\n>>> X_scaled = scaler.transform(X_train)\\n>>> X_scaled\\narray([[ 0.  ..., -1.22...,  1.33...],\\n       [ 1.22...,  0.  ..., -0.26...],\\n       [-1.22...,  1.22..., -1.06...]])\\n\\n\\nScaled data has zero mean and unit variance:\\n>>> X_scaled.mean(axis=0)\\narray([0., 0., 0.])\\n\\n>>> X_scaled.std(axis=0)\\narray([1., 1., 1.])\\n\\n\\nThis class implements the Transformer API to compute the mean and\\nstandard deviation on a training set so as to be able to later re-apply the\\nsame transformation on the testing set. This class is hence suitable for\\nuse in the early steps of a Pipeline:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn.pipeline import make_pipeline\\n>>> from sklearn.preprocessing import StandardScaler\\n---------new doc---------\\n6.3.1.1. Scaling features to a range#\\nAn alternative standardization is scaling features to\\nlie between a given minimum and maximum value, often between zero and one,\\nor so that the maximum absolute value of each feature is scaled to unit size.\\nThis can be achieved using MinMaxScaler or MaxAbsScaler,\\nrespectively.\\nThe motivation to use this scaling include robustness to very small\\nstandard deviations of features and preserving zero entries in sparse data.\\nHere is an example to scale a toy data matrix to the [0, 1] range:\\n>>> X_train = np.array([[ 1., -1.,  2.],\\n...                     [ 2.,  0.,  0.],\\n...                     [ 0.,  1., -1.]])\\n...\\n>>> min_max_scaler = preprocessing.MinMaxScaler()\\n>>> X_train_minmax = min_max_scaler.fit_transform(X_train)\\n>>> X_train_minmax\\narray([[0.5       , 0.        , 1.        ],\\n       [1.        , 0.5       , 0.33333333],\\n       [0.        , 1.        , 0.        ]])\\n---------new doc---------\\nThe same instance of the transformer can then be applied to some new test data\\nunseen during the fit call: the same scaling and shifting operations will be\\napplied to be consistent with the transformation performed on the train data:\\n>>> X_test = np.array([[-3., -1.,  4.]])\\n>>> X_test_minmax = min_max_scaler.transform(X_test)\\n>>> X_test_minmax\\narray([[-1.5       ,  0.        ,  1.66666667]])\\n\\n\\nIt is possible to introspect the scaler attributes to find about the exact\\nnature of the transformation learned on the training data:\\n>>> min_max_scaler.scale_\\narray([0.5       , 0.5       , 0.33...])\\n\\n>>> min_max_scaler.min_\\narray([0.        , 0.5       , 0.33...])\\n\\n\\nIf MinMaxScaler is given an explicit feature_range=(min, max) the\\nfull formula is:\\nX_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\\n\\nX_scaled = X_std * (max - min) + min\\n---------new doc---------\\n6.3.1.2. Scaling sparse data#\\nCentering sparse data would destroy the sparseness structure in the data, and\\nthus rarely is a sensible thing to do. However, it can make sense to scale\\nsparse inputs, especially if features are on different scales.\\nMaxAbsScaler was specifically designed for scaling\\nsparse data, and is the recommended way to go about this.\\nHowever, StandardScaler can accept scipy.sparse\\nmatrices  as input, as long as with_mean=False is explicitly passed\\nto the constructor. Otherwise a ValueError will be raised as\\nsilently centering would break the sparsity and would often crash the\\nexecution by allocating excessive amounts of memory unintentionally.\\nRobustScaler cannot be fitted to sparse inputs, but you can use\\nthe transform method on sparse inputs.\\nNote that the scalers accept both Compressed Sparse Rows and Compressed\\nSparse Columns format (see scipy.sparse.csr_matrix and\\nscipy.sparse.csc_matrix). Any other sparse input will be converted to\\nthe Compressed Sparse Rows representation.  To avoid unnecessary memory\\ncopies, it is recommended to choose the CSR or CSC representation upstream.\\nFinally, if the centered data is expected to be small enough, explicitly\\nconverting the input to an array using the toarray method of sparse matrices\\nis another option.\\n\\n\\n6.3.1.3. Scaling data with outliers#\\nIf your data contains many outliers, scaling using the mean and variance\\nof the data is likely to not work very well. In these cases, you can use\\nRobustScaler as a drop-in replacement instead. It uses\\nmore robust estimates for the center and range of your data.\\n---------new doc---------\\n6.3.2.1. Mapping to a Uniform distribution#\\nQuantileTransformer provides a non-parametric\\ntransformation to map the data to a uniform distribution\\nwith values between 0 and 1:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.model_selection import train_test_split\\n>>> X, y = load_iris(return_X_y=True)\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n>>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\\n>>> X_train_trans = quantile_transformer.fit_transform(X_train)\\n>>> X_test_trans = quantile_transformer.transform(X_test)\\n>>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) \\narray([ 4.3,  5.1,  5.8,  6.5,  7.9])\\n\\n\\nThis feature corresponds to the sepal length in cm. Once the quantile\\ntransformation applied, those landmarks approach closely the percentiles\\npreviously defined:\\n>>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])\\n... \\narray([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])\\n---------new doc---------\\nYeo-Johnson transform#\\n\\n\\\\[\\\\begin{split}x_i^{(\\\\lambda)} =\\n\\\\begin{cases}\\n[(x_i + 1)^\\\\lambda - 1] / \\\\lambda & \\\\text{if } \\\\lambda \\\\neq 0, x_i \\\\geq 0, \\\\\\\\[8pt]\\n\\\\ln{(x_i + 1)} & \\\\text{if } \\\\lambda = 0, x_i \\\\geq 0 \\\\\\\\[8pt]\\n-[(-x_i + 1)^{2 - \\\\lambda} - 1] / (2 - \\\\lambda) & \\\\text{if } \\\\lambda \\\\neq 2, x_i < 0, \\\\\\\\[8pt]\\n- \\\\ln (- x_i + 1) & \\\\text{if } \\\\lambda = 2, x_i < 0\\n\\\\end{cases}\\\\end{split}\\\\]\\n\\n\\n\\nBox-Cox transform#\\n---------new doc---------\\nBox-Cox transform#\\n\\n\\\\[\\\\begin{split}x_i^{(\\\\lambda)} =\\n\\\\begin{cases}\\n\\\\dfrac{x_i^\\\\lambda - 1}{\\\\lambda} & \\\\text{if } \\\\lambda \\\\neq 0, \\\\\\\\[8pt]\\n\\\\ln{(x_i)} & \\\\text{if } \\\\lambda = 0,\\n\\\\end{cases}\\\\end{split}\\\\]\\nBox-Cox can only be applied to strictly positive data. In both methods, the\\ntransformation is parameterized by \\\\(\\\\lambda\\\\), which is determined through\\nmaximum likelihood estimation. Here is an example of using Box-Cox to map\\nsamples drawn from a lognormal distribution to a normal distribution:\\n>>> pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\\n>>> X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\\n>>> X_lognormal\\narray([[1.28..., 1.18..., 0.84...],\\n      [0.94..., 1.60..., 0.38...],\\n      [1.35..., 0.21..., 1.09...]])\\n>>> pt.fit_transform(X_lognormal)\\narray([[ 0.49...,  0.17..., -0.15...],\\n      [-0.05...,  0.58..., -0.57...],\\n      [ 0.69..., -0.84...,  0.10...]])\\n---------new doc---------\\nWhile the above example sets the standardize option to False,\\nPowerTransformer will apply zero-mean, unit-variance normalization\\nto the transformed output by default.\\n\\nBelow are examples of Box-Cox and Yeo-Johnson applied to various probability\\ndistributions.  Note that when applied to certain distributions, the power\\ntransforms achieve very Gaussian-like results, but with others, they are\\nineffective. This highlights the importance of visualizing the data before and\\nafter transformation.\\n\\n\\n\\n\\nIt is also possible to map data to a normal distribution using\\nQuantileTransformer by setting output_distribution='normal'.\\nUsing the earlier example with the iris dataset:\\n>>> quantile_transformer = preprocessing.QuantileTransformer(\\n...     output_distribution='normal', random_state=0)\\n>>> X_trans = quantile_transformer.fit_transform(X)\\n>>> quantile_transformer.quantiles_\\narray([[4.3, 2. , 1. , 0.1],\\n       [4.4, 2.2, 1.1, 0.1],\\n       [4.4, 2.2, 1.2, 0.1],\\n       ...,\\n       [7.7, 4.1, 6.7, 2.5],\\n       [7.7, 4.2, 6.7, 2.5],\\n       [7.9, 4.4, 6.9, 2.5]])\\n---------new doc---------\\n>>> X_normalized\\narray([[ 0.40..., -0.40...,  0.81...],\\n       [ 1.  ...,  0.  ...,  0.  ...],\\n       [ 0.  ...,  0.70..., -0.70...]])\\n\\n\\nThe preprocessing module further provides a utility class\\nNormalizer that implements the same operation using the\\nTransformer API (even though the fit method is useless in this case:\\nthe class is stateless as this operation treats samples independently).\\nThis class is hence suitable for use in the early steps of a\\nPipeline:\\n>>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\\n>>> normalizer\\nNormalizer()\\n\\n\\nThe normalizer instance can then be used on sample vectors as any transformer:\\n>>> normalizer.transform(X)\\narray([[ 0.40..., -0.40...,  0.81...],\\n       [ 1.  ...,  0.  ...,  0.  ...],\\n       [ 0.  ...,  0.70..., -0.70...]])\\n\\n>>> normalizer.transform([[-1.,  1., 0.]])\\narray([[-0.70...,  0.70...,  0.  ...]])\\n\\n\\nNote: L2 normalization is also known as spatial sign preprocessing.\\n---------new doc---------\\nSparse input#\\nnormalize and Normalizer accept both dense array-like\\nand sparse matrices from scipy.sparse as input.\\nFor sparse input the data is converted to the Compressed Sparse Rows\\nrepresentation (see scipy.sparse.csr_matrix) before being fed to\\nefficient Cython routines. To avoid unnecessary memory copies, it is\\nrecommended to choose the CSR representation upstream.\\n\\n\\n\\n6.3.4. Encoding categorical features#\\nOften features are not given as continuous values but categorical.\\nFor example a person could have features [\\\"male\\\", \\\"female\\\"],\\n[\\\"from Europe\\\", \\\"from US\\\", \\\"from Asia\\\"],\\n[\\\"uses Firefox\\\", \\\"uses Chrome\\\", \\\"uses Safari\\\", \\\"uses Internet Explorer\\\"].\\nSuch features can be efficiently coded as integers, for instance\\n[\\\"male\\\", \\\"from US\\\", \\\"uses Internet Explorer\\\"] could be expressed as\\n[0, 1, 3] while [\\\"female\\\", \\\"from Asia\\\", \\\"uses Chrome\\\"] would be\\n[1, 2, 1].\\nTo convert categorical features to such integer codes, we can use the\\nOrdinalEncoder. This estimator transforms each categorical feature to one\\nnew feature of integers (0 to n_categories - 1):\\n>>> enc = preprocessing.OrdinalEncoder()\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOrdinalEncoder()\\n>>> enc.transform([['female', 'from US', 'uses Safari']])\\narray([[0., 1., 1.]])\\n---------new doc---------\\nSuch integer representation can, however, not be used directly with all\\nscikit-learn estimators, as these expect continuous input, and would interpret\\nthe categories as being ordered, which is often not desired (i.e. the set of\\nbrowsers was ordered arbitrarily).\\nBy default, OrdinalEncoder will also passthrough missing values that\\nare indicated by np.nan.\\n>>> enc = preprocessing.OrdinalEncoder()\\n>>> X = [['male'], ['female'], [np.nan], ['female']]\\n>>> enc.fit_transform(X)\\narray([[ 1.],\\n       [ 0.],\\n       [nan],\\n       [ 0.]])\\n\\n\\nOrdinalEncoder provides a parameter encoded_missing_value to encode\\nthe missing values without the need to create a pipeline and using\\nSimpleImputer.\\n>>> enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\\n>>> X = [['male'], ['female'], [np.nan], ['female']]\\n>>> enc.fit_transform(X)\\narray([[ 1.],\\n       [ 0.],\\n       [-1.],\\n       [ 0.]])\\n---------new doc---------\\nThe above processing is equivalent to the following pipeline:\\n>>> from sklearn.pipeline import Pipeline\\n>>> from sklearn.impute import SimpleImputer\\n>>> enc = Pipeline(steps=[\\n...     (\\\"encoder\\\", preprocessing.OrdinalEncoder()),\\n...     (\\\"imputer\\\", SimpleImputer(strategy=\\\"constant\\\", fill_value=-1)),\\n... ])\\n>>> enc.fit_transform(X)\\narray([[ 1.],\\n       [ 0.],\\n       [-1.],\\n       [ 0.]])\\n\\n\\nAnother possibility to convert categorical features to features that can be used\\nwith scikit-learn estimators is to use a one-of-K, also known as one-hot or\\ndummy encoding.\\nThis type of encoding can be obtained with the OneHotEncoder,\\nwhich transforms each categorical feature with\\nn_categories possible values into n_categories binary features, with\\none of them 1, and all others 0.\\nContinuing the example above:\\n>>> enc = preprocessing.OneHotEncoder()\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOneHotEncoder()\\n>>> enc.transform([['female', 'from US', 'uses Safari'],\\n...                ['male', 'from Europe', 'uses Safari']]).toarray()\\narray([[1., 0., 0., 1., 0., 1.],\\n       [0., 1., 1., 0., 0., 1.]])\\n---------new doc---------\\nBy default, the values each feature can take is inferred automatically\\nfrom the dataset and can be found in the categories_ attribute:\\n>>> enc.categories_\\n[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)]\\n---------new doc---------\\nIt is possible to specify this explicitly using the parameter categories.\\nThere are two genders, four possible continents and four web browsers in our\\ndataset:\\n>>> genders = ['female', 'male']\\n>>> locations = ['from Africa', 'from Asia', 'from Europe', 'from US']\\n>>> browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\\n>>> enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\\n>>> # Note that for there are missing categorical values for the 2nd and 3rd\\n>>> # feature\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOneHotEncoder(categories=[['female', 'male'],\\n                          ['from Africa', 'from Asia', 'from Europe',\\n                           'from US'],\\n                          ['uses Chrome', 'uses Firefox', 'uses IE',\\n                           'uses Safari']])\\n>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\\narray([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])\\n---------new doc---------\\nIf there is a possibility that the training data might have missing categorical\\nfeatures, it can often be better to specify\\nhandle_unknown='infrequent_if_exist' instead of setting the categories\\nmanually as above. When handle_unknown='infrequent_if_exist' is specified\\nand unknown categories are encountered during transform, no error will be\\nraised but the resulting one-hot encoded columns for this feature will be all\\nzeros or considered as an infrequent category if enabled.\\n(handle_unknown='infrequent_if_exist' is only supported for one-hot\\nencoding):\\n>>> enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist')\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOneHotEncoder(handle_unknown='infrequent_if_exist')\\n>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\\narray([[1., 0., 0., 0., 0., 0.]])\\n---------new doc---------\\nIt is also possible to encode each column into n_categories - 1 columns\\ninstead of n_categories columns by using the drop parameter. This\\nparameter allows the user to specify a category for each feature to be dropped.\\nThis is useful to avoid co-linearity in the input matrix in some classifiers.\\nSuch functionality is useful, for example, when using non-regularized\\nregression (LinearRegression),\\nsince co-linearity would cause the covariance matrix to be non-invertible:\\n>>> X = [['male', 'from US', 'uses Safari'],\\n...      ['female', 'from Europe', 'uses Firefox']]\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)\\n>>> drop_enc.categories_\\n[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object),\\n array(['uses Firefox', 'uses Safari'], dtype=object)]\\n>>> drop_enc.transform(X).toarray()\\narray([[1., 1., 1.],\\n       [0., 0., 0.]])\\n---------new doc---------\\nOne might want to drop one of the two columns only for features with 2\\ncategories. In this case, you can set the parameter drop='if_binary'.\\n>>> X = [['male', 'US', 'Safari'],\\n...      ['female', 'Europe', 'Firefox'],\\n...      ['female', 'Asia', 'Chrome']]\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary').fit(X)\\n>>> drop_enc.categories_\\n[array(['female', 'male'], dtype=object), array(['Asia', 'Europe', 'US'], dtype=object),\\n array(['Chrome', 'Firefox', 'Safari'], dtype=object)]\\n>>> drop_enc.transform(X).toarray()\\narray([[1., 0., 0., 1., 0., 0., 1.],\\n       [0., 0., 1., 0., 0., 1., 0.],\\n       [0., 1., 0., 0., 1., 0., 0.]])\\n---------new doc---------\\nIn the transformed X, the first column is the encoding of the feature with\\ncategories “male”/”female”, while the remaining 6 columns is the encoding of\\nthe 2 features with respectively 3 categories each.\\nWhen handle_unknown='ignore' and drop is not None, unknown categories will\\nbe encoded as all zeros:\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='first',\\n...                                        handle_unknown='ignore').fit(X)\\n>>> X_test = [['unknown', 'America', 'IE']]\\n>>> drop_enc.transform(X_test).toarray()\\narray([[0., 0., 0., 0., 0.]])\\n---------new doc---------\\nAll the categories in X_test are unknown during transform and will be mapped\\nto all zeros. This means that unknown categories will have the same mapping as\\nthe dropped category. OneHotEncoder.inverse_transform will map all zeros\\nto the dropped category if a category is dropped and None if a category is\\nnot dropped:\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse_output=False,\\n...                                        handle_unknown='ignore').fit(X)\\n>>> X_test = [['unknown', 'America', 'IE']]\\n>>> X_trans = drop_enc.transform(X_test)\\n>>> X_trans\\narray([[0., 0., 0., 0., 0., 0., 0.]])\\n>>> drop_enc.inverse_transform(X_trans)\\narray([['female', None, None]], dtype=object)\\n---------new doc---------\\nSupport of categorical features with missing values#\\nOneHotEncoder supports categorical features with missing values by\\nconsidering the missing values as an additional category:\\n>>> X = [['male', 'Safari'],\\n...      ['female', None],\\n...      [np.nan, 'Firefox']]\\n>>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\\n>>> enc.categories_\\n[array(['female', 'male', nan], dtype=object),\\narray(['Firefox', 'Safari', None], dtype=object)]\\n>>> enc.transform(X).toarray()\\narray([[0., 1., 0., 0., 1., 0.],\\n      [1., 0., 0., 0., 0., 1.],\\n      [0., 0., 1., 1., 0., 0.]])\\n\\n\\nIf a feature contains both np.nan and None, they will be considered\\nseparate categories:\\n>>> X = [['Safari'], [None], [np.nan], ['Firefox']]\\n>>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\\n>>> enc.categories_\\n[array(['Firefox', 'Safari', None, nan], dtype=object)]\\n>>> enc.transform(X).toarray()\\narray([[0., 1., 0., 0.],\\n      [0., 0., 1., 0.],\\n      [0., 0., 0., 1.],\\n      [1., 0., 0., 0.]])\\n---------new doc---------\\nSee Loading features from dicts for categorical features that are\\nrepresented as a dict, not as scalars.\\n\\n\\n6.3.4.1. Infrequent categories#\\nOneHotEncoder and OrdinalEncoder support aggregating\\ninfrequent categories into a single output for each feature. The parameters to\\nenable the gathering of infrequent categories are min_frequency and\\nmax_categories.\\n\\nmin_frequency is either an  integer greater or equal to 1, or a float in\\nthe interval (0.0, 1.0). If min_frequency is an integer, categories with\\na cardinality smaller than min_frequency  will be considered infrequent.\\nIf min_frequency is a float, categories with a cardinality smaller than\\nthis fraction of the total number of samples will be considered infrequent.\\nThe default value is 1, which means every category is encoded separately.\\nmax_categories is either None or any integer greater than 1. This\\nparameter sets an upper limit to the number of output features for each\\ninput feature. max_categories includes the feature that combines\\ninfrequent categories.\\n---------new doc---------\\nIn the following example with OrdinalEncoder, the categories 'dog' and\\n'snake' are considered infrequent:\\n>>> X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 +\\n...               ['snake'] * 3], dtype=object).T\\n>>> enc = preprocessing.OrdinalEncoder(min_frequency=6).fit(X)\\n>>> enc.infrequent_categories_\\n[array(['dog', 'snake'], dtype=object)]\\n>>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\\narray([[2.],\\n       [0.],\\n       [1.],\\n       [2.]])\\n---------new doc---------\\nOrdinalEncoder’s max_categories do not take into account missing\\nor unknown categories. Setting unknown_value or encoded_missing_value to an\\ninteger will increase the number of unique integer codes by one each. This can\\nresult in up to max_categories + 2 integer codes. In the following example,\\n“a” and “d” are considered infrequent and grouped together into a single\\ncategory, “b” and “c” are their own categories, unknown values are encoded as 3\\nand missing values are encoded as 4.\\n>>> X_train = np.array(\\n...     [[\\\"a\\\"] * 5 + [\\\"b\\\"] * 20 + [\\\"c\\\"] * 10 + [\\\"d\\\"] * 3 + [np.nan]],\\n...     dtype=object).T\\n>>> enc = preprocessing.OrdinalEncoder(\\n...     handle_unknown=\\\"use_encoded_value\\\", unknown_value=3,\\n...     max_categories=3, encoded_missing_value=4)\\n>>> _ = enc.fit(X_train)\\n>>> X_test = np.array([[\\\"a\\\"], [\\\"b\\\"], [\\\"c\\\"], [\\\"d\\\"], [\\\"e\\\"], [np.nan]], dtype=object)\\n>>> enc.transform(X_test)\\narray([[2.],\\n       [0.],\\n       [1.],\\n       [2.],\\n       [3.],\\n       [4.]])\\n---------new doc---------\\nSimilarity, OneHotEncoder can be configured to group together infrequent\\ncategories:\\n>>> enc = preprocessing.OneHotEncoder(min_frequency=6, sparse_output=False).fit(X)\\n>>> enc.infrequent_categories_\\n[array(['dog', 'snake'], dtype=object)]\\n>>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\\narray([[0., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.],\\n       [0., 0., 1.]])\\n\\n\\nBy setting handle_unknown to 'infrequent_if_exist', unknown categories will\\nbe considered infrequent:\\n>>> enc = preprocessing.OneHotEncoder(\\n...    handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=6)\\n>>> enc = enc.fit(X)\\n>>> enc.transform(np.array([['dragon']]))\\narray([[0., 0., 1.]])\\n\\n\\nOneHotEncoder.get_feature_names_out uses ‘infrequent’ as the infrequent\\nfeature name:\\n>>> enc.get_feature_names_out()\\narray(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)\\n\\n\\nWhen 'handle_unknown' is set to 'infrequent_if_exist' and an unknown\\ncategory is encountered in transform:\\n---------new doc---------\\nWhen 'handle_unknown' is set to 'infrequent_if_exist' and an unknown\\ncategory is encountered in transform:\\n\\nIf infrequent category support was not configured or there was no\\ninfrequent category during training, the resulting one-hot encoded columns\\nfor this feature will be all zeros. In the inverse transform, an unknown\\ncategory will be denoted as None.\\nIf there is an infrequent category during training, the unknown category\\nwill be considered infrequent. In the inverse transform, ‘infrequent_sklearn’\\nwill be used to represent the infrequent category.\\n\\nInfrequent categories can also be configured using max_categories. In the\\nfollowing example, we set max_categories=2 to limit the number of features in\\nthe output. This will result in all but the 'cat' category to be considered\\ninfrequent, leading to two features, one for 'cat' and one for infrequent\\ncategories - which are all the others:\\n>>> enc = preprocessing.OneHotEncoder(max_categories=2, sparse_output=False)\\n>>> enc = enc.fit(X)\\n>>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\\narray([[0., 1.],\\n       [1., 0.],\\n       [0., 1.],\\n       [0., 1.]])\\n---------new doc---------\\nIf both max_categories and min_frequency are non-default values, then\\ncategories are selected based on min_frequency first and max_categories\\ncategories are kept. In the following example, min_frequency=4 considers\\nonly snake to be infrequent, but max_categories=3, forces dog to also be\\ninfrequent:\\n>>> enc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse_output=False)\\n>>> enc = enc.fit(X)\\n>>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\\narray([[0., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.],\\n       [0., 0., 1.]])\\n\\n\\nIf there are infrequent categories with the same cardinality at the cutoff of\\nmax_categories, then then the first max_categories are taken based on lexicon\\nordering. In the following example, “b”, “c”, and “d”, have the same cardinality\\nand with max_categories=2, “b” and “c” are infrequent because they have a higher\\nlexicon order.\\n>>> X = np.asarray([[\\\"a\\\"] * 20 + [\\\"b\\\"] * 10 + [\\\"c\\\"] * 10 + [\\\"d\\\"] * 10], dtype=object).T\\n>>> enc = preprocessing.OneHotEncoder(max_categories=3).fit(X)\\n>>> enc.infrequent_categories_\\n[array(['b', 'c'], dtype=object)]\\n---------new doc---------\\n6.3.4.2. Target Encoder#\\nThe TargetEncoder uses the target mean conditioned on the categorical\\nfeature for encoding unordered categories, i.e. nominal categories [PAR]\\n[MIC]. This encoding scheme is useful with categorical features with high\\ncardinality, where one-hot encoding would inflate the feature space making it\\nmore expensive for a downstream model to process. A classical example of high\\ncardinality categories are location based such as zip code or region.\\n\\n\\nBinary classification targets#\\nFor the binary classification target, the target encoding is given by:\\n\\n\\\\[S_i = \\\\lambda_i\\\\frac{n_{iY}}{n_i} + (1 - \\\\lambda_i)\\\\frac{n_Y}{n}\\\\]\\nwhere \\\\(S_i\\\\) is the encoding for category \\\\(i\\\\), \\\\(n_{iY}\\\\) is the\\nnumber of observations with \\\\(Y=1\\\\) and category \\\\(i\\\\), \\\\(n_i\\\\) is\\nthe number of observations with category \\\\(i\\\\), \\\\(n_Y\\\\) is the number of\\nobservations with \\\\(Y=1\\\\), \\\\(n\\\\) is the number of observations, and\\n\\\\(\\\\lambda_i\\\\) is a shrinkage factor for category \\\\(i\\\\). The shrinkage\\nfactor is given by:\\n---------new doc---------\\nfit_transform also learns a ‘full data’ encoding using\\nthe whole training set. This is never used in\\nfit_transform but is saved to the attribute encodings_,\\nfor use when transform is called. Note that the encodings\\nlearned for each fold during the cross fitting scheme are not saved to\\nan attribute.\\nThe fit method does not use any cross fitting\\nschemes and learns one encoding on the entire training set, which is used to\\nencode categories in transform.\\nThis encoding is the same as the ‘full data’\\nencoding learned in fit_transform.\\n\\nNote\\nTargetEncoder considers missing values, such as np.nan or None,\\nas another category and encodes them like any other category. Categories\\nthat are not seen during fit are encoded with the target mean, i.e.\\ntarget_mean_.\\n\\nExamples\\n\\nComparing Target Encoder with Other Encoders\\nTarget Encoder’s Internal Cross fitting\\n\\nReferences\\n\\n\\n[MIC]\\nMicci-Barreca, Daniele. “A preprocessing scheme for high-cardinality\\ncategorical attributes in classification and prediction problems”\\nSIGKDD Explor. Newsl. 3, 1 (July 2001), 27-32.\\n\\n\\n[PAR]\\nPargent, F., Pfisterer, F., Thomas, J. et al. “Regularized target\\nencoding outperforms traditional methods in supervised machine learning with\\nhigh cardinality features” Comput Stat 37, 2671-2692 (2022)\\n---------new doc---------\\nBy default the output is one-hot encoded into a sparse matrix\\n(See Encoding categorical features)\\nand this can be configured with the encode parameter.\\nFor each feature, the bin edges are computed during fit and together with\\nthe number of bins, they will define the intervals. Therefore, for the current\\nexample, these intervals are defined as:\\n\\nfeature 1: \\\\({[-\\\\infty, -1), [-1, 2), [2, \\\\infty)}\\\\)\\nfeature 2: \\\\({[-\\\\infty, 5), [5, \\\\infty)}\\\\)\\nfeature 3: \\\\({[-\\\\infty, 14), [14, \\\\infty)}\\\\)\\n\\nBased on these bin intervals, X is transformed as follows:\\n>>> est.transform(X)                      \\narray([[ 0., 1., 1.],\\n       [ 1., 1., 1.],\\n       [ 2., 0., 0.]])\\n---------new doc---------\\nThe resulting dataset contains ordinal attributes which can be further used\\nin a Pipeline.\\nDiscretization is similar to constructing histograms for continuous data.\\nHowever, histograms focus on counting features which fall into particular\\nbins, whereas discretization focuses on assigning feature values to these bins.\\nKBinsDiscretizer implements different binning strategies, which can be\\nselected with the strategy parameter. The ‘uniform’ strategy uses\\nconstant-width bins. The ‘quantile’ strategy uses the quantiles values to have\\nequally populated bins in each feature. The ‘kmeans’ strategy defines bins based\\non a k-means clustering procedure performed on each feature independently.\\nBe aware that one can specify custom bins by passing a callable defining the\\ndiscretization strategy to FunctionTransformer.\\nFor instance, we can use the Pandas function pandas.cut:\\n>>> import pandas as pd\\n>>> import numpy as np\\n>>> from sklearn import preprocessing\\n>>>\\n>>> bins = [0, 1, 13, 20, 60, np.inf]\\n>>> labels = ['infant', 'kid', 'teen', 'adult', 'senior citizen']\\n>>> transformer = preprocessing.FunctionTransformer(\\n...     pd.cut, kw_args={'bins': bins, 'labels': labels, 'retbins': False}\\n... )\\n>>> X = np.array([0.2, 2, 15, 25, 97])\\n>>> transformer.fit_transform(X)\\n['infant', 'kid', 'teen', 'adult', 'senior citizen']\\nCategories (5, object): ['infant' < 'kid' < 'teen' < 'adult' < 'senior citizen']\\n\\n\\nExamples\\n---------new doc---------\\nExamples\\n\\nUsing KBinsDiscretizer to discretize continuous features\\nFeature discretization\\nDemonstrating the different strategies of KBinsDiscretizer\\n\\n\\n\\n6.3.5.2. Feature binarization#\\nFeature binarization is the process of thresholding numerical\\nfeatures to get boolean values. This can be useful for downstream\\nprobabilistic estimators that make assumption that the input data\\nis distributed according to a multi-variate Bernoulli distribution. For instance,\\nthis is the case for the BernoulliRBM.\\nIt is also common among the text processing community to use binary\\nfeature values (probably to simplify the probabilistic reasoning) even\\nif normalized counts (a.k.a. term frequencies) or TF-IDF valued features\\noften perform slightly better in practice.\\nAs for the Normalizer, the utility class\\nBinarizer is meant to be used in the early stages of\\nPipeline. The fit method does nothing\\nas each sample is treated independently of others:\\n>>> X = [[ 1., -1.,  2.],\\n...      [ 2.,  0.,  0.],\\n...      [ 0.,  1., -1.]]\\n\\n>>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\\n>>> binarizer\\nBinarizer()\\n\\n>>> binarizer.transform(X)\\narray([[1., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.]])\\n---------new doc---------\\n>>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\\n>>> binarizer\\nBinarizer()\\n\\n>>> binarizer.transform(X)\\narray([[1., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.]])\\n\\n\\nIt is possible to adjust the threshold of the binarizer:\\n>>> binarizer = preprocessing.Binarizer(threshold=1.1)\\n>>> binarizer.transform(X)\\narray([[0., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 0., 0.]])\\n\\n\\nAs for the Normalizer class, the preprocessing module\\nprovides a companion function binarize\\nto be used when the transformer API is not necessary.\\nNote that the Binarizer is similar to the KBinsDiscretizer\\nwhen k = 2, and when the bin edge is at the value threshold.\\n\\nSparse input\\nbinarize and Binarizer accept both dense array-like\\nand sparse matrices from scipy.sparse as input.\\nFor sparse input the data is converted to the Compressed Sparse Rows\\nrepresentation (see scipy.sparse.csr_matrix).\\nTo avoid unnecessary memory copies, it is recommended to choose the CSR\\nrepresentation upstream.\\n\\n\\n\\n\\n6.3.6. Imputation of missing values#\\nTools for imputing missing values are discussed at Imputation of missing values.\\n---------new doc---------\\n6.3.6. Imputation of missing values#\\nTools for imputing missing values are discussed at Imputation of missing values.\\n\\n\\n6.3.7. Generating polynomial features#\\nOften it’s useful to add complexity to a model by considering nonlinear\\nfeatures of the input data. We show two possibilities that are both based on\\npolynomials: The first one uses pure polynomials, the second one uses splines,\\ni.e. piecewise polynomials.\\n\\n6.3.7.1. Polynomial features#\\nA simple and common method to use is polynomial features, which can get\\nfeatures’ high-order and interaction terms. It is implemented in\\nPolynomialFeatures:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> X = np.arange(6).reshape(3, 2)\\n>>> X\\narray([[0, 1],\\n       [2, 3],\\n       [4, 5]])\\n>>> poly = PolynomialFeatures(2)\\n>>> poly.fit_transform(X)\\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\\n       [ 1.,  4.,  5., 16., 20., 25.]])\\n---------new doc---------\\nThe features of X have been transformed from \\\\((X_1, X_2)\\\\) to\\n\\\\((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\\\\).\\nIn some cases, only interaction terms among features are required, and it can\\nbe gotten with the setting interaction_only=True:\\n>>> X = np.arange(9).reshape(3, 3)\\n>>> X\\narray([[0, 1, 2],\\n       [3, 4, 5],\\n       [6, 7, 8]])\\n>>> poly = PolynomialFeatures(degree=3, interaction_only=True)\\n>>> poly.fit_transform(X)\\narray([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],\\n       [  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],\\n       [  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])\\n---------new doc---------\\nThe features of X have been transformed from \\\\((X_1, X_2, X_3)\\\\) to\\n\\\\((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\\\\).\\nNote that polynomial features are used implicitly in kernel methods (e.g., SVC,\\nKernelPCA) when using polynomial Kernel functions.\\nSee Polynomial and Spline interpolation\\nfor Ridge regression using created polynomial features.\\n\\n\\n6.3.7.2. Spline transformer#\\nAnother way to add nonlinear terms instead of pure polynomials of features is\\nto generate spline basis functions for each feature with the\\nSplineTransformer. Splines are piecewise polynomials, parametrized by\\ntheir polynomial degree and the positions of the knots. The\\nSplineTransformer implements a B-spline basis, cf. the references\\nbelow.\\n\\nNote\\nThe SplineTransformer treats each feature separately, i.e. it\\nwon’t give you interaction terms.\\n\\nSome of the advantages of splines over polynomials are:\\n---------new doc---------\\nNote\\nThe SplineTransformer treats each feature separately, i.e. it\\nwon’t give you interaction terms.\\n\\nSome of the advantages of splines over polynomials are:\\n\\nB-splines are very flexible and robust if you keep a fixed low degree,\\nusually 3, and parsimoniously adapt the number of knots. Polynomials\\nwould need a higher degree, which leads to the next point.\\nB-splines do not have oscillatory behaviour at the boundaries as have\\npolynomials (the higher the degree, the worse). This is known as Runge’s\\nphenomenon.\\nB-splines provide good options for extrapolation beyond the boundaries,\\ni.e. beyond the range of fitted values. Have a look at the option\\nextrapolation.\\nB-splines generate a feature matrix with a banded structure. For a single\\nfeature, every row contains only degree + 1 non-zero elements, which\\noccur consecutively and are even positive. This results in a matrix with\\ngood numerical properties, e.g. a low condition number, in sharp contrast\\nto a matrix of polynomials, which goes under the name\\nVandermonde matrix.\\nA low condition number is important for stable algorithms of linear\\nmodels.\\n---------new doc---------\\nThe following code snippet shows splines in action:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import SplineTransformer\\n>>> X = np.arange(5).reshape(5, 1)\\n>>> X\\narray([[0],\\n       [1],\\n       [2],\\n       [3],\\n       [4]])\\n>>> spline = SplineTransformer(degree=2, n_knots=3)\\n>>> spline.fit_transform(X)\\narray([[0.5  , 0.5  , 0.   , 0.   ],\\n       [0.125, 0.75 , 0.125, 0.   ],\\n       [0.   , 0.5  , 0.5  , 0.   ],\\n       [0.   , 0.125, 0.75 , 0.125],\\n       [0.   , 0.   , 0.5  , 0.5  ]])\\n\\n\\nAs the X is sorted, one can easily see the banded matrix output. Only the\\nthree middle diagonals are non-zero for degree=2. The higher the degree,\\nthe more overlapping of the splines.\\nInterestingly, a SplineTransformer of degree=0 is the same as\\nKBinsDiscretizer with\\nencode='onehot-dense' and n_bins = n_knots - 1 if\\nknots = strategy.\\nExamples\\n\\nPolynomial and Spline interpolation\\nTime-related feature engineering\\n\\n\\n\\nReferences#\\n---------new doc---------\\nPolynomial and Spline interpolation\\nTime-related feature engineering\\n\\n\\n\\nReferences#\\n\\nEilers, P., & Marx, B. (1996). Flexible Smoothing with B-splines and\\nPenalties. Statist. Sci. 11 (1996), no. 2, 89–121.\\nPerperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. A review of\\nspline function procedures in R.\\nBMC Med Res Methodol 19, 46 (2019).\\n\\n\\n\\n\\n\\n6.3.8. Custom transformers#\\nOften, you will want to convert an existing Python function into a transformer\\nto assist in data cleaning or processing. You can implement a transformer from\\nan arbitrary function with FunctionTransformer. For example, to build\\na transformer that applies a log transformation in a pipeline, do:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import FunctionTransformer\\n>>> transformer = FunctionTransformer(np.log1p, validate=True)\\n>>> X = np.array([[0, 1], [2, 3]])\\n>>> # Since FunctionTransformer is no-op during fit, we can call transform directly\\n>>> transformer.transform(X)\\narray([[0.        , 0.69314718],\\n       [1.09861229, 1.38629436]])\\n---------new doc---------\\nYou can ensure that func and inverse_func are the inverse of each other\\nby setting check_inverse=True and calling fit before\\ntransform. Please note that a warning is raised and can be turned into an\\nerror with a filterwarnings:\\n>>> import warnings\\n>>> warnings.filterwarnings(\\\"error\\\", message=\\\".*check_inverse*.\\\",\\n...                         category=UserWarning, append=False)\\n\\n\\nFor a full code example that demonstrates using a FunctionTransformer\\nto extract features from text data see\\nColumn Transformer with Heterogeneous Data Sources and\\nTime-related feature engineering.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n6.2. Feature extraction\\n\\n\\n\\n\\nnext\\n6.4. Imputation of missing values\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.4. Imputation of missing values\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6.4. Imputation of missing values#\\nFor various reasons, many real world datasets contain missing values, often\\nencoded as blanks, NaNs or other placeholders. Such datasets however are\\nincompatible with scikit-learn estimators which assume that all values in an\\narray are numerical, and that all have and hold meaning. A basic strategy to\\nuse incomplete datasets is to discard entire rows and/or columns containing\\nmissing values. However, this comes at the price of losing data which may be\\nvaluable (even though incomplete). A better strategy is to impute the missing\\nvalues, i.e., to infer them from the known part of the data. See the\\nglossary entry on imputation.\\n---------new doc---------\\n6.4.1. Univariate vs. Multivariate Imputation#\\nOne type of imputation algorithm is univariate, which imputes values in the\\ni-th feature dimension using only non-missing values in that feature dimension\\n(e.g. SimpleImputer). By contrast, multivariate imputation\\nalgorithms use the entire set of available feature dimensions to estimate the\\nmissing values (e.g. IterativeImputer).\\n\\n\\n6.4.2. Univariate feature imputation#\\nThe SimpleImputer class provides basic strategies for imputing missing\\nvalues. Missing values can be imputed with a provided constant value, or using\\nthe statistics (mean, median or most frequent) of each column in which the\\nmissing values are located. This class also allows for different missing values\\nencodings.\\nThe following snippet demonstrates how to replace missing values,\\nencoded as np.nan, using the mean value of the columns (axis 0)\\nthat contain the missing values:\\n>>> import numpy as np\\n>>> from sklearn.impute import SimpleImputer\\n>>> imp = SimpleImputer(missing_values=np.nan, strategy='mean')\\n>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])\\nSimpleImputer()\\n>>> X = [[np.nan, 2], [6, np.nan], [7, 6]]\\n>>> print(imp.transform(X))\\n[[4.          2.        ]\\n [6.          3.666...]\\n [7.          6.        ]]\\n---------new doc---------\\nThe SimpleImputer class also supports sparse matrices:\\n>>> import scipy.sparse as sp\\n>>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])\\n>>> imp = SimpleImputer(missing_values=-1, strategy='mean')\\n>>> imp.fit(X)\\nSimpleImputer(missing_values=-1)\\n>>> X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])\\n>>> print(imp.transform(X_test).toarray())\\n[[3. 2.]\\n [6. 3.]\\n [7. 6.]]\\n---------new doc---------\\nNote that this format is not meant to be used to implicitly store missing\\nvalues in the matrix because it would densify it at transform time. Missing\\nvalues encoded by 0 must be used with dense input.\\nThe SimpleImputer class also supports categorical data represented as\\nstring values or pandas categoricals when using the 'most_frequent' or\\n'constant' strategy:\\n>>> import pandas as pd\\n>>> df = pd.DataFrame([[\\\"a\\\", \\\"x\\\"],\\n...                    [np.nan, \\\"y\\\"],\\n...                    [\\\"a\\\", np.nan],\\n...                    [\\\"b\\\", \\\"y\\\"]], dtype=\\\"category\\\")\\n...\\n>>> imp = SimpleImputer(strategy=\\\"most_frequent\\\")\\n>>> print(imp.fit_transform(df))\\n[['a' 'x']\\n ['a' 'y']\\n ['a' 'y']\\n ['b' 'y']]\\n\\n\\nFor another example on usage, see Imputing missing values before building an estimator.\\n---------new doc---------\\nFor another example on usage, see Imputing missing values before building an estimator.\\n\\n\\n6.4.3. Multivariate feature imputation#\\nA more sophisticated approach is to use the IterativeImputer class,\\nwhich models each feature with missing values as a function of other features,\\nand uses that estimate for imputation. It does so in an iterated round-robin\\nfashion: at each step, a feature column is designated as output y and the\\nother feature columns are treated as inputs X. A regressor is fit on (X,\\ny) for known y. Then, the regressor is used to predict the missing values\\nof y.  This is done for each feature in an iterative fashion, and then is\\nrepeated for max_iter imputation rounds. The results of the final\\nimputation round are returned.\\n\\nNote\\nThis estimator is still experimental for now: default parameters or\\ndetails of behaviour might change without any deprecation cycle. Resolving\\nthe following issues would help stabilize IterativeImputer:\\nconvergence criteria (#14338) and default estimators\\n(#13286). To use it, you need to explicitly import\\nenable_iterative_imputer.\\n---------new doc---------\\nBoth SimpleImputer and IterativeImputer can be used in a\\nPipeline as a way to build a composite estimator that supports imputation.\\nSee Imputing missing values before building an estimator.\\n\\n6.4.3.1. Flexibility of IterativeImputer#\\nThere are many well-established imputation packages in the R data science\\necosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns\\nout to be a particular instance of different sequential imputation algorithms\\nthat can all be implemented with IterativeImputer by passing in\\ndifferent regressors to be used for predicting missing feature values. In the\\ncase of missForest, this regressor is a Random Forest.\\nSee Imputing missing values with variants of IterativeImputer.\\n---------new doc---------\\n>>> from sklearn.impute import KNNImputer\\n>>> nan = np.nan\\n>>> X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\\n>>> imputer = KNNImputer(n_neighbors=2, weights=\\\"uniform\\\")\\n>>> imputer.fit_transform(X)\\narray([[1. , 2. , 4. ],\\n       [3. , 4. , 3. ],\\n       [5.5, 6. , 5. ],\\n       [8. , 8. , 7. ]])\\n---------new doc---------\\nFor another example on usage, see Imputing missing values before building an estimator.\\nReferences\\n\\n\\n[OL2001]\\nOlga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown,\\nTrevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman,\\nMissing value estimation methods for DNA microarrays, BIOINFORMATICS\\nVol. 17 no. 6, 2001 Pages 520-525.\\n\\n\\n\\n\\n6.4.5. Keeping the number of features constant#\\nBy default, the scikit-learn imputers will drop fully empty features, i.e.\\ncolumns containing only missing values. For instance:\\n>>> imputer = SimpleImputer()\\n>>> X = np.array([[np.nan, 1], [np.nan, 2], [np.nan, 3]])\\n>>> imputer.fit_transform(X)\\narray([[1.],\\n       [2.],\\n       [3.]])\\n\\n\\nThe first feature in X containing only np.nan was dropped after the\\nimputation. While this feature will not help in predictive setting, dropping\\nthe columns will change the shape of X which could be problematic when using\\nimputers in a more complex machine-learning pipeline. The parameter\\nkeep_empty_features offers the option to keep the empty features by imputing\\nwith a constant values. In most of the cases, this constant value is zero:\\n>>> imputer.set_params(keep_empty_features=True)\\nSimpleImputer(keep_empty_features=True)\\n>>> imputer.fit_transform(X)\\narray([[0., 1.],\\n       [0., 2.],\\n       [0., 3.]])\\n---------new doc---------\\n6.4.6. Marking imputed values#\\nThe MissingIndicator transformer is useful to transform a dataset into\\ncorresponding binary matrix indicating the presence of missing values in the\\ndataset. This transformation is useful in conjunction with imputation. When\\nusing imputation, preserving the information about which values had been\\nmissing can be informative. Note that both the SimpleImputer and\\nIterativeImputer have the boolean parameter add_indicator\\n(False by default) which when set to True provides a convenient way of\\nstacking the output of the MissingIndicator transformer with the\\noutput of the imputer.\\nNaN is usually used as the placeholder for missing values. However, it\\nenforces the data type to be float. The parameter missing_values allows to\\nspecify other placeholder such as integer. In the following example, we will\\nuse -1 as missing values:\\n>>> from sklearn.impute import MissingIndicator\\n>>> X = np.array([[-1, -1, 1, 3],\\n...               [4, -1, 0, -1],\\n...               [8, -1, 1, 0]])\\n>>> indicator = MissingIndicator(missing_values=-1)\\n>>> mask_missing_values_only = indicator.fit_transform(X)\\n>>> mask_missing_values_only\\narray([[ True,  True, False],\\n       [False,  True,  True],\\n       [False,  True, False]])\\n---------new doc---------\\nThe features parameter is used to choose the features for which the mask is\\nconstructed. By default, it is 'missing-only' which returns the imputer\\nmask of the features containing missing values at fit time:\\n>>> indicator.features_\\narray([0, 1, 3])\\n\\n\\nThe features parameter can be set to 'all' to return all features\\nwhether or not they contain missing values:\\n>>> indicator = MissingIndicator(missing_values=-1, features=\\\"all\\\")\\n>>> mask_all = indicator.fit_transform(X)\\n>>> mask_all\\narray([[ True,  True, False, False],\\n       [False,  True, False,  True],\\n       [False,  True, False, False]])\\n>>> indicator.features_\\narray([0, 1, 2, 3])\\n---------new doc---------\\nWhen using the MissingIndicator in a\\nPipeline, be sure to use the\\nFeatureUnion or\\nColumnTransformer to add the indicator features to\\nthe regular features. First we obtain the iris dataset, and add some missing\\nvalues to it.\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.impute import SimpleImputer, MissingIndicator\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn.pipeline import FeatureUnion, make_pipeline\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> X, y = load_iris(return_X_y=True)\\n>>> mask = np.random.randint(0, 2, size=X.shape).astype(bool)\\n>>> X[mask] = np.nan\\n>>> X_train, X_test, y_train, _ = train_test_split(X, y, test_size=100,\\n...                                                random_state=0)\\n---------new doc---------\\nNow we create a FeatureUnion. All features will be\\nimputed using SimpleImputer, in order to enable classifiers to work\\nwith this data. Additionally, it adds the indicator variables from\\nMissingIndicator.\\n>>> transformer = FeatureUnion(\\n...     transformer_list=[\\n...         ('features', SimpleImputer(strategy='mean')),\\n...         ('indicators', MissingIndicator())])\\n>>> transformer = transformer.fit(X_train, y_train)\\n>>> results = transformer.transform(X_test)\\n>>> results.shape\\n(100, 8)\\n\\n\\nOf course, we cannot use the transformer to make any predictions. We should\\nwrap this in a Pipeline with a classifier (e.g., a\\nDecisionTreeClassifier) to be able to make predictions.\\n>>> clf = make_pipeline(transformer, DecisionTreeClassifier())\\n>>> clf = clf.fit(X_train, y_train)\\n>>> results = clf.predict(X_test)\\n>>> results.shape\\n(100,)\\n\\n\\n\\n\\n6.4.7. Estimators that handle NaN values#\\nSome estimators are designed to handle NaN values without preprocessing.\\nBelow is the list of these estimators, classified by type\\n(cluster, regressor, classifier, transform):\\n\\nEstimators that allow NaN values for type cluster:\\n\\nHDBSCAN\\n\\n\\nEstimators that allow NaN values for type regressor:\\n\\nBaggingRegressor\\nDecisionTreeRegressor\\nExtraTreeRegressor\\nExtraTreesRegressor\\nHistGradientBoostingRegressor\\nRandomForestRegressor\\nStackingRegressor\\nVotingRegressor\\n---------new doc---------\\n6.6.4. Inverse Transform#\\nThe random projection transformers have compute_inverse_components parameter. When\\nset to True, after creating the random components_ matrix during fitting,\\nthe transformer computes the pseudo-inverse of this matrix and stores it as\\ninverse_components_. The inverse_components_ matrix has shape\\n\\\\(n_{features} \\\\times n_{components}\\\\), and it is always a dense matrix,\\nregardless of whether the components matrix is sparse or dense. So depending on\\nthe number of features and components, it may use a lot of memory.\\nWhen the inverse_transform method is called, it computes the product of the\\ninput X and the transpose of the inverse components. If the inverse components have\\nbeen computed during fit, they are reused at each call to inverse_transform.\\nOtherwise they are recomputed each time, which can be costly. The result is always\\ndense, even if X is sparse.\\nHere a small code example which illustrates how to use the inverse transform\\nfeature:\\n>>> import numpy as np\\n>>> from sklearn.random_projection import SparseRandomProjection\\n>>> X = np.random.rand(100, 10000)\\n>>> transformer = SparseRandomProjection(\\n...   compute_inverse_components=True\\n... )\\n...\\n>>> X_new = transformer.fit_transform(X)\\n>>> X_new.shape\\n(100, 3947)\\n>>> X_new_inversed = transformer.inverse_transform(X_new)\\n>>> X_new_inversed.shape\\n(100, 10000)\\n>>> X_new_again = transformer.transform(X_new_inversed)\\n>>> np.allclose(X_new, X_new_again)\\nTrue\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Plot Ridge coefficients as a function of the regularization\\nClassification of text documents using sparse features\\nCommon pitfalls in the interpretation of coefficients of linear models\\n\\n\\n\\n1.1.2.3. Ridge Complexity#\\nThis method has the same order of complexity as\\nOrdinary Least Squares.\\n\\n\\n1.1.2.4. Setting the regularization parameter: leave-one-out Cross-Validation#\\nRidgeCV and RidgeClassifierCV implement ridge\\nregression/classification with built-in cross-validation of the alpha parameter.\\nThey work in the same way as GridSearchCV except\\nthat it defaults to efficient Leave-One-Out cross-validation.\\nWhen using the default cross-validation, alpha cannot be 0 due to the\\nformulation used to calculate Leave-One-Out error. See [RL2007] for details.\\nUsage example:\\n>>> import numpy as np\\n>>> from sklearn import linear_model\\n>>> reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\\n>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\\nRidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\\n      1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\\n>>> reg.alpha_\\n0.01\\n---------new doc---------\\nHere is an example of applying this idea to one-dimensional data, using\\npolynomial features of varying degrees:\\n\\n\\n\\n\\nThis figure is created using the PolynomialFeatures transformer, which\\ntransforms an input data matrix into a new data matrix of a given degree.\\nIt can be used as follows:\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> import numpy as np\\n>>> X = np.arange(6).reshape(3, 2)\\n>>> X\\narray([[0, 1],\\n       [2, 3],\\n       [4, 5]])\\n>>> poly = PolynomialFeatures(degree=2)\\n>>> poly.fit_transform(X)\\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\\n       [ 1.,  4.,  5., 16., 20., 25.]])\\n---------new doc---------\\nThe features of X have been transformed from \\\\([x_1, x_2]\\\\) to\\n\\\\([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\\\\), and can now be used within\\nany linear model.\\nThis sort of preprocessing can be streamlined with the\\nPipeline tools. A single object representing a simple\\npolynomial regression can be created and used as follows:\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> from sklearn.linear_model import LinearRegression\\n>>> from sklearn.pipeline import Pipeline\\n>>> import numpy as np\\n>>> model = Pipeline([('poly', PolynomialFeatures(degree=3)),\\n...                   ('linear', LinearRegression(fit_intercept=False))])\\n>>> # fit to an order-3 polynomial data\\n>>> x = np.arange(5)\\n>>> y = 3 - 2 * x + x ** 2 - x ** 3\\n>>> model = model.fit(x[:, np.newaxis], y)\\n>>> model.named_steps['linear'].coef_\\narray([ 3., -2.,  1., -1.])\\n---------new doc---------\\nThe linear model trained on polynomial features is able to exactly recover\\nthe input polynomial coefficients.\\nIn some cases it’s not necessary to include higher powers of any single feature,\\nbut only the so-called interaction features\\nthat multiply together at most \\\\(d\\\\) distinct features.\\nThese can be gotten from PolynomialFeatures with the setting\\ninteraction_only=True.\\nFor example, when dealing with boolean features,\\n\\\\(x_i^n = x_i\\\\) for all \\\\(n\\\\) and is therefore useless;\\nbut \\\\(x_i x_j\\\\) represents the conjunction of two booleans.\\nThis way, we can solve the XOR problem with a linear classifier:\\n>>> from sklearn.linear_model import Perceptron\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> import numpy as np\\n>>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\\n>>> y = X[:, 0] ^ X[:, 1]\\n>>> y\\narray([0, 1, 1, 0])\\n>>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)\\n>>> X\\narray([[1, 0, 0, 0],\\n       [1, 0, 1, 0],\\n       [1, 1, 0, 0],\\n       [1, 1, 1, 1]])\\n>>> clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,\\n...                  shuffle=False).fit(X, y)\\n---------new doc---------\\n0 to n is “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . .\\n. “n-1 vs n”.\\nThe shape of dual_coef_ is (n_classes-1, n_SV) with\\na somewhat hard to grasp layout.\\nThe columns correspond to the support vectors involved in any\\nof the n_classes * (n_classes - 1) / 2 “one-vs-one” classifiers.\\nEach support vector v has a dual coefficient in each of the\\nn_classes - 1 classifiers comparing the class of v against another class.\\nNote that some, but not all, of these dual coefficients, may be zero.\\nThe n_classes - 1 entries in each column are these dual coefficients,\\nordered by the opposing class.\\nThis might be clearer with an example: consider a three class problem with\\nclass 0 having three support vectors\\n\\\\(v^{0}_0, v^{1}_0, v^{2}_0\\\\) and class 1 and 2 having two support vectors\\n\\\\(v^{0}_1, v^{1}_1\\\\) and \\\\(v^{0}_2, v^{1}_2\\\\) respectively.  For each\\nsupport vector \\\\(v^{j}_i\\\\), there are two dual coefficients.  Let’s call\\nthe coefficient of support vector \\\\(v^{j}_i\\\\) in the classifier between\\nclasses \\\\(i\\\\) and \\\\(k\\\\) \\\\(\\\\alpha^{j}_{i,k}\\\\).\\nThen dual_coef_ looks like this:\\n---------new doc---------\\n1.4.5. Tips on Practical Use#\\n\\nAvoiding data copy: For SVC, SVR, NuSVC and\\nNuSVR, if the data passed to certain methods is not C-ordered\\ncontiguous and double precision, it will be copied before calling the\\nunderlying C implementation. You can check whether a given numpy array is\\nC-contiguous by inspecting its flags attribute.\\nFor LinearSVC (and LogisticRegression) any input passed as a numpy\\narray will be copied and converted to the liblinear internal sparse data\\nrepresentation (double precision floats and int32 indices of non-zero\\ncomponents). If you want to fit a large-scale linear classifier without\\ncopying a dense numpy C-contiguous double precision array as input, we\\nsuggest to use the SGDClassifier class instead.  The objective\\nfunction can be configured to be almost the same as the LinearSVC\\nmodel.\\n\\nKernel cache size: For SVC, SVR, NuSVC and\\nNuSVR, the size of the kernel cache has a strong impact on run\\ntimes for larger problems.  If you have enough RAM available, it is\\nrecommended to set cache_size to a higher value than the default of\\n200(MB), such as 500(MB) or 1000(MB).\\nSetting C: C is 1 by default and it’s a reasonable default\\nchoice.  If you have a lot of noisy observations you should decrease it:\\ndecreasing C corresponds to more regularization.\\nLinearSVC and LinearSVR are less sensitive to C when\\nit becomes large, and prediction results stop improving after a certain\\nthreshold. Meanwhile, larger C values will take more time to train,\\nsometimes up to 10 times longer, as shown in [11].\\n---------new doc---------\\nSupport Vector Machine algorithms are not scale invariant, so it\\nis highly recommended to scale your data. For example, scale each\\nattribute on the input vector X to [0,1] or [-1,+1], or standardize it\\nto have mean 0 and variance 1. Note that the same scaling must be\\napplied to the test vector to obtain meaningful results. This can be done\\neasily by using a Pipeline:\\n>>> from sklearn.pipeline import make_pipeline\\n>>> from sklearn.preprocessing import StandardScaler\\n>>> from sklearn.svm import SVC\\n\\n>>> clf = make_pipeline(StandardScaler(), SVC())\\n\\n\\nSee section Preprocessing data for more details on scaling and\\nnormalization.\\n---------new doc---------\\nNuSVC#\\nThe \\\\(\\\\nu\\\\)-SVC formulation [15] is a reparameterization of the\\n\\\\(C\\\\)-SVC and therefore mathematically equivalent.\\nWe introduce a new parameter \\\\(\\\\nu\\\\) (instead of \\\\(C\\\\)) which\\ncontrols the number of support vectors and margin errors:\\n\\\\(\\\\nu \\\\in (0, 1]\\\\) is an upper bound on the fraction of margin errors and\\na lower bound of the fraction of support vectors. A margin error corresponds\\nto a sample that lies on the wrong side of its margin boundary: it is either\\nmisclassified, or it is correctly classified but does not lie beyond the\\nmargin.\\n\\n\\n\\n1.4.7.2. SVR#\\nGiven training vectors \\\\(x_i \\\\in \\\\mathbb{R}^p\\\\), i=1,…, n, and a\\nvector \\\\(y \\\\in \\\\mathbb{R}^n\\\\) \\\\(\\\\varepsilon\\\\)-SVR solves the following primal problem:\\n---------new doc---------\\nTransforming data#\\nTo transform \\\\(X\\\\) into \\\\(\\\\bar{X}\\\\), we need to find a projection\\nmatrix \\\\(P\\\\) such that \\\\(\\\\bar{X} = XP\\\\). We know that for the\\ntraining data, \\\\(\\\\Xi = XP\\\\), and \\\\(X = \\\\Xi \\\\Gamma^T\\\\). Setting\\n\\\\(P = U(\\\\Gamma^T U)^{-1}\\\\) where \\\\(U\\\\) is the matrix with the\\n\\\\(u_k\\\\) in the columns, we have \\\\(XP = X U(\\\\Gamma^T U)^{-1} = \\\\Xi\\n(\\\\Gamma^T U) (\\\\Gamma^T U)^{-1} = \\\\Xi\\\\) as desired. The rotation matrix\\n\\\\(P\\\\) can be accessed from the x_rotations_ attribute.\\nSimilarly, \\\\(Y\\\\) can be transformed using the rotation matrix\\n\\\\(V(\\\\Delta^T V)^{-1}\\\\), accessed via the y_rotations_ attribute.\\n---------new doc---------\\nOnce trained, you can plot the tree with the plot_tree function:\\n>>> tree.plot_tree(clf)\\n[...]\\n\\n\\n\\n\\n\\n\\n\\n\\nAlternative ways to export trees#\\nWe can also export the tree in Graphviz format using the export_graphviz\\nexporter. If you use the conda package manager, the graphviz binaries\\nand the python package can be installed with conda install python-graphviz.\\nAlternatively binaries for graphviz can be downloaded from the graphviz project homepage,\\nand the Python wrapper installed from pypi with pip install graphviz.\\nBelow is an example graphviz export of the above tree trained on the entire\\niris dataset; the results are saved in an output file iris.pdf:\\n>>> import graphviz \\n>>> dot_data = tree.export_graphviz(clf, out_file=None) \\n>>> graph = graphviz.Source(dot_data) \\n>>> graph.render(\\\"iris\\\")\\n---------new doc---------\\nThe export_graphviz exporter also supports a variety of aesthetic\\noptions, including coloring nodes by their class (or value for regression) and\\nusing explicit variable and class names if desired. Jupyter notebooks also\\nrender these plots inline automatically:\\n>>> dot_data = tree.export_graphviz(clf, out_file=None, \\n...                      feature_names=iris.feature_names,  \\n...                      class_names=iris.target_names,  \\n...                      filled=True, rounded=True,  \\n...                      special_characters=True)  \\n>>> graph = graphviz.Source(dot_data)  \\n>>> graph\\n---------new doc---------\\nAs you can see, the [1, 0] is comfortably classified as 1 since the first\\ntwo samples are ignored due to their sample weights.\\nImplementation detail: taking sample weights into account amounts to\\nmultiplying the gradients (and the hessians) by the sample weights. Note that\\nthe binning stage (specifically the quantiles computation) does not take the\\nweights into account.\\n\\n\\n1.11.1.1.4. Categorical Features Support#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor have native support for categorical\\nfeatures: they can consider splits on non-ordered, categorical data.\\nFor datasets with categorical features, using the native categorical support\\nis often better than relying on one-hot encoding\\n(OneHotEncoder), because one-hot encoding\\nrequires more tree depth to achieve equivalent splits. It is also usually\\nbetter to rely on the native categorical support rather than to treat\\ncategorical features as continuous (ordinal), which happens for ordinal-encoded\\ncategorical data, since categories are nominal quantities where order does not\\nmatter.\\nTo enable categorical support, a boolean mask can be passed to the\\ncategorical_features parameter, indicating which feature is categorical. In\\nthe following, the first feature will be treated as categorical and the\\nsecond feature as numerical:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[True, False])\\n\\n\\nEquivalently, one can pass a list of integers indicating the indices of the\\ncategorical features:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[0])\\n---------new doc---------\\nEquivalently, one can pass a list of integers indicating the indices of the\\ncategorical features:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[0])\\n\\n\\nWhen the input is a DataFrame, it is also possible to pass a list of column\\nnames:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[\\\"site\\\", \\\"manufacturer\\\"])\\n\\n\\nFinally, when the input is a DataFrame we can use\\ncategorical_features=\\\"from_dtype\\\" in which case all columns with a categorical\\ndtype will be treated as categorical features.\\nThe cardinality of each categorical feature must be less than the max_bins\\nparameter. For an example using histogram-based gradient boosting on categorical\\nfeatures, see\\nCategorical Feature Support in Gradient Boosting.\\nIf there are missing values during training, the missing values will be\\ntreated as a proper category. If there are no missing values during training,\\nthen at prediction time, missing values are mapped to the child node that has\\nthe most samples (just like for continuous features). When predicting,\\ncategories that were not seen during fit time will be treated as missing\\nvalues.\\n---------new doc---------\\na monotonic increase constraint is a constraint of the form:\\n\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\leq F(x_1', x_2)\\\\]\\n\\na monotonic decrease constraint is a constraint of the form:\\n\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\geq F(x_1', x_2)\\\\]\\n\\n\\nYou can specify a monotonic constraint on each feature using the\\nmonotonic_cst parameter. For each feature, a value of 0 indicates no\\nconstraint, while 1 and -1 indicate a monotonic increase and\\nmonotonic decrease constraint, respectively:\\n>>> from sklearn.ensemble import HistGradientBoostingRegressor\\n\\n... # monotonic increase, monotonic decrease, and no constraint on the 3 features\\n>>> gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])\\n\\n\\nIn a binary classification context, imposing a monotonic increase (decrease) constraint means that higher values of the feature are supposed\\nto have a positive (negative) effect on the probability of samples\\nto belong to the positive class.\\nNevertheless, monotonic constraints only marginally constrain feature effects on the output.\\nFor instance, monotonic increase and decrease constraints cannot be used to enforce the\\nfollowing modelling constraint:\\n\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\leq F(x_1', x_2')\\\\]\\nAlso, monotonic constraints are not supported for multiclass classification.\\n---------new doc---------\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\leq F(x_1', x_2')\\\\]\\nAlso, monotonic constraints are not supported for multiclass classification.\\n\\nNote\\nSince categories are unordered quantities, it is not possible to enforce\\nmonotonic constraints on categorical features.\\n\\nExamples\\n\\nMonotonic Constraints\\nFeatures in Histogram Gradient Boosting Trees\\n\\n\\n\\n1.11.1.1.6. Interaction constraints#\\nA priori, the histogram gradient boosted trees are allowed to use any feature\\nto split a node into child nodes. This creates so called interactions between\\nfeatures, i.e. usage of different features as split along a branch. Sometimes,\\none wants to restrict the possible interactions, see [Mayer2022]. This can be\\ndone by the parameter interaction_cst, where one can specify the indices\\nof features that are allowed to interact.\\nFor instance, with 3 features in total, interaction_cst=[{0}, {1}, {2}]\\nforbids all interactions.\\nThe constraints [{0, 1}, {1, 2}] specifies two groups of possibly\\ninteracting features. Features 0 and 1 may interact with each other, as well\\nas features 1 and 2. But note that features 0 and 2 are forbidden to interact.\\nThe following depicts a tree and the possible splits of the tree:\\n   1      <- Both constraint groups could be applied from now on\\n  / \\\\\\n 1   2    <- Left split still fulfills both constraint groups.\\n/ \\\\ / \\\\      Right split at feature 2 has only group {1, 2} from now on.\\n---------new doc---------\\nLightGBM uses the same logic for overlapping groups.\\nNote that features not listed in interaction_cst are automatically\\nassigned an interaction group for themselves. With again 3 features, this\\nmeans that [{0}] is equivalent to [{0}, {1, 2}].\\nExamples\\n\\nPartial Dependence and Individual Conditional Expectation Plots\\n\\nReferences\\n\\n\\n[Mayer2022]\\nM. Mayer, S.C. Bourassa, M. Hoesli, and D.F. Scognamiglio.\\n2022. Machine Learning Applications to Land and Structure Valuation.\\nJournal of Risk and Financial Management 15, no. 5: 193\\n\\n\\n\\n\\n1.11.1.1.7. Low-level parallelism#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor use OpenMP\\nfor parallelization through Cython. For more details on how to control the\\nnumber of threads, please refer to our Parallelism notes.\\nThe following parts are parallelized:\\n\\nmapping samples from real values to integer-valued bins (finding the bin\\nthresholds is however sequential)\\nbuilding histograms is parallelized over features\\nfinding the best split point at a node is parallelized over features\\nduring fit, mapping samples into the left and right children is\\nparallelized over samples\\ngradient and hessians computations are parallelized over samples\\npredicting is parallelized over samples\\n---------new doc---------\\n1.12.1.1. Target format#\\nValid multiclass representations for\\ntype_of_target (y) are:\\n\\n1d or column vector containing more than two discrete values. An\\nexample of a vector y for 4 samples:\\n>>> import numpy as np\\n>>> y = np.array(['apple', 'pear', 'apple', 'orange'])\\n>>> print(y)\\n['apple' 'pear' 'apple' 'orange']\\n\\n\\n\\nDense or sparse binary matrix of shape (n_samples, n_classes)\\nwith a single sample per row, where each column represents one class. An\\nexample of both a dense and sparse binary matrix y for 4\\nsamples, where the columns, in order, are apple, orange, and pear:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import LabelBinarizer\\n>>> y = np.array(['apple', 'pear', 'apple', 'orange'])\\n>>> y_dense = LabelBinarizer().fit_transform(y)\\n>>> print(y_dense)\\n[[1 0 0]\\n [0 0 1]\\n [1 0 0]\\n [0 1 0]]\\n>>> from scipy import sparse\\n>>> y_sparse = sparse.csr_matrix(y_dense)\\n>>> print(y_sparse)\\n<Compressed Sparse Row sparse matrix of dtype 'int64'\\n    with 4 stored elements and shape (4, 3)>\\n  Coords    Values\\n  (0, 0)    1\\n  (1, 2)    1\\n  (2, 0)    1\\n  (3, 1)    1\\n---------new doc---------\\n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\n---------new doc---------\\n1.12.1.4. OutputCodeClassifier#\\nError-Correcting Output Code-based strategies are fairly different from\\none-vs-the-rest and one-vs-one. With these strategies, each class is\\nrepresented in a Euclidean space, where each dimension can only be 0 or 1.\\nAnother way to put it is that each class is represented by a binary code (an\\narray of 0 and 1). The matrix which keeps track of the location/code of each\\nclass is called the code book. The code size is the dimensionality of the\\naforementioned space. Intuitively, each class should be represented by a code\\nas unique as possible and a good code book should be designed to optimize\\nclassification accuracy. In this implementation, we simply use a\\nrandomly-generated code book as advocated in [3] although more elaborate\\nmethods may be added in the future.\\nAt fitting time, one binary classifier per bit in the code book is fitted.\\nAt prediction time, the classifiers are used to project new points in the\\nclass space and the class closest to the points is chosen.\\nIn OutputCodeClassifier, the code_size\\nattribute allows the user to control the number of classifiers which will be\\nused. It is a percentage of the total number of classes.\\nA number between 0 and 1 will require fewer classifiers than\\none-vs-the-rest. In theory, log2(n_classes) / n_classes is sufficient to\\nrepresent each class unambiguously. However, in practice, it may not lead to\\ngood accuracy since log2(n_classes) is much smaller than n_classes.\\nA number greater than 1 will require more classifiers than\\none-vs-the-rest. In this case, some classifiers will in theory correct for\\n---------new doc---------\\n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\\n       1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,\\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\n---------new doc---------\\n1.12.2.1. Target format#\\nA valid representation of multilabel y is an either dense or sparse\\nbinary matrix of shape (n_samples, n_classes). Each column\\nrepresents a class. The 1’s in each row denote the positive classes a\\nsample has been labeled with. An example of a dense matrix y for 3\\nsamples:\\n>>> y = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [0, 0, 0, 0]])\\n>>> print(y)\\n[[1 0 0 1]\\n [0 0 1 1]\\n [0 0 0 0]]\\n\\n\\nDense binary matrices can also be created using\\nMultiLabelBinarizer. For more information,\\nrefer to Transforming the prediction target (y).\\nAn example of the same y in sparse matrix form:\\n>>> y_sparse = sparse.csr_matrix(y)\\n>>> print(y_sparse)\\n<Compressed Sparse Row sparse matrix of dtype 'int64'\\n  with 4 stored elements and shape (3, 4)>\\n  Coords      Values\\n  (0, 0)      1\\n  (0, 3)      1\\n  (1, 2)      1\\n  (1, 3)      1\\n---------new doc---------\\n[-141.62745778,   95.02891072, -191.48204257],\\n       [  97.03260883,  165.34867495,  139.52003279],\\n       [ 123.92529176,   21.25719016,   -7.84253   ],\\n       [-122.25193977,  -85.16443186, -107.12274212],\\n       [ -30.170388  ,  -94.80956739,   12.16979946],\\n       [ 140.72667194,  176.50941682,  -17.50447799],\\n       [ 149.37967282,  -81.15699552,   -5.72850319]])\\n---------new doc---------\\n>>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\\narray([[8, 0],\\n       [0, 4]])\\n\\n\\nLabelings that assign all classes members to the same clusters\\nare complete but may not always be pure, hence penalized, and\\nhave some off-diagonal non-zero entries:\\n>>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\\narray([[8, 2],\\n       [0, 2]])\\n\\n\\nThe matrix is not symmetric:\\n>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])\\narray([[8, 0],\\n       [2, 2]])\\n\\n\\nIf classes members are completely split across different clusters, the\\nassignment is totally incomplete, hence the matrix has all zero\\ndiagonal entries:\\n>>> pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])\\narray([[ 0,  0],\\n       [12,  0]])\\n\\n\\n\\n\\nReferences#\\n\\n“Comparing Partitions” L. Hubert and P. Arabie,\\nJournal of Classification 1985\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n2.2. Manifold learning\\n\\n\\n\\n\\nnext\\n2.4. Biclustering\\n\\n\\n --- \\n\\n\\n2. Unsupervised learning\\n2.5. Decomposing signals in components (matrix factorization problems)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2.5. Decomposing signals in components (matrix factorization problems)#\\n---------new doc---------\\nAnother option is to use an iterable yielding (train, test) splits as arrays of\\nindices, for example:\\n>>> def custom_cv_2folds(X):\\n...     n = X.shape[0]\\n...     i = 1\\n...     while i <= 2:\\n...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\\n...         yield idx, idx\\n...         i += 1\\n...\\n>>> custom_cv = custom_cv_2folds(X)\\n>>> cross_val_score(clf, X, y, cv=custom_cv)\\narray([1.        , 0.973...])\\n---------new doc---------\\ntrain -  [30  3]   |   test -  [15  2]\\ntrain -  [30  4]   |   test -  [15  1]\\n>>> kf = KFold(n_splits=3)\\n>>> for train, test in kf.split(X, y):\\n...     print('train -  {}   |   test -  {}'.format(\\n...         np.bincount(y[train]), np.bincount(y[test])))\\ntrain -  [28  5]   |   test -  [17]\\ntrain -  [28  5]   |   test -  [17]\\ntrain -  [34]   |   test -  [11  5]\\n---------new doc---------\\n>>> X = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001])\\n>>> y = np.array([\\\"a\\\", \\\"b\\\", \\\"b\\\", \\\"b\\\", \\\"c\\\", \\\"c\\\", \\\"c\\\", \\\"a\\\"])\\n>>> groups = np.array([1, 1, 2, 2, 3, 3, 4, 4])\\n>>> train_indx, test_indx = next(\\n...     GroupShuffleSplit(random_state=7).split(X, y, groups)\\n... )\\n>>> X_train, X_test, y_train, y_test = \\\\\\n...     X[train_indx], X[test_indx], y[train_indx], y[test_indx]\\n>>> X_train.shape, X_test.shape\\n((6,), (2,))\\n>>> np.unique(groups[train_indx]), np.unique(groups[test_indx])\\n(array([1, 2, 4]), array([3]))\\n---------new doc---------\\n2\\n0\\n125\\n0.983667\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\\n\\n3\\n0\\n125\\n0.983667\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 6, ‘min_samples_split’: 6}\\n\\n…\\n…\\n…\\n…\\n…\\n\\n15\\n2\\n500\\n0.951958\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}\\n\\n16\\n2\\n500\\n0.947958\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\\n\\n17\\n2\\n500\\n0.951958\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 4}\\n\\n18\\n3\\n1000\\n0.961009\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}\\n---------new doc---------\\n[[1, 0],\\n        [0, 1]],\\n\\n       [[0, 1],\\n        [1, 0]]])\\n\\n\\nOr a confusion matrix can be constructed for each sample’s labels:\\n>>> multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\\narray([[[1, 0],\\n        [1, 1]],\\n\\n       [[1, 1],\\n        [0, 1]]])\\n\\n\\nHere is an example demonstrating the use of the\\nmultilabel_confusion_matrix function with\\nmulticlass input:\\n>>> y_true = [\\\"cat\\\", \\\"ant\\\", \\\"cat\\\", \\\"cat\\\", \\\"ant\\\", \\\"bird\\\"]\\n>>> y_pred = [\\\"ant\\\", \\\"ant\\\", \\\"cat\\\", \\\"cat\\\", \\\"ant\\\", \\\"cat\\\"]\\n>>> multilabel_confusion_matrix(y_true, y_pred,\\n...                             labels=[\\\"ant\\\", \\\"bird\\\", \\\"cat\\\"])\\narray([[[3, 1],\\n        [0, 2]],\\n\\n       [[5, 0],\\n        [1, 0]],\\n\\n       [[2, 1],\\n        [1, 2]]])\\n---------new doc---------\\n4.1.1. Partial dependence plots#\\nPartial dependence plots (PDP) show the dependence between the target response\\nand a set of input features of interest, marginalizing over the values\\nof all other input features (the ‘complement’ features). Intuitively, we can\\ninterpret the partial dependence as the expected target response as a\\nfunction of the input features of interest.\\nDue to the limits of human perception, the size of the set of input features of\\ninterest must be small (usually, one or two) thus the input features of interest\\nare usually chosen among the most important features.\\nThe figure below shows two one-way and one two-way partial dependence plots for\\nthe bike sharing dataset, with a\\nHistGradientBoostingRegressor:\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\n>>> features = [0, 1, (0, 1)]\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features)\\n<...>\\n\\n\\nYou can access the newly created figure and Axes objects using plt.gcf()\\nand plt.gca().\\nTo make a partial dependence plot with categorical features, you need to specify\\nwhich features are categorical using the parameter categorical_features. This\\nparameter takes a list of indices, names of the categorical features or a boolean\\nmask. The graphical representation of partial dependence for categorical features is\\na bar plot or a 2D heatmap.\\n\\n\\nPDPs for multi-class classification#\\nFor multi-class classification, you need to set the class label for which\\nthe PDPs should be created via the target argument:\\n>>> from sklearn.datasets import load_iris\\n>>> iris = load_iris()\\n>>> mc_clf = GradientBoostingClassifier(n_estimators=10,\\n...     max_depth=1).fit(iris.data, iris.target)\\n>>> features = [3, 2, (3, 2)]\\n>>> PartialDependenceDisplay.from_estimator(mc_clf, X, features, target=0)\\n<...>\\n\\n\\nThe same parameter target is used to specify the target in multi-output\\nregression settings.\\n---------new doc---------\\nIn ICE plots it might not be easy to see the average effect of the input\\nfeature of interest. Hence, it is recommended to use ICE plots alongside\\nPDPs. They can be plotted together with\\nkind='both'.\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind='both')\\n<...>\\n\\n\\nIf there are too many lines in an ICE plot, it can be difficult to see\\ndifferences between individual samples and interpret the model. Centering the\\nICE at the first value on the x-axis, produces centered Individual Conditional\\nExpectation (cICE) plots [G2015]. This puts emphasis on the divergence of\\nindividual conditional expectations from the mean line, thus making it easier\\nto explore heterogeneous relationships. cICE plots can be plotted by setting\\ncentered=True:\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind='both', centered=True)\\n<...>\\n\\n\\n\\n\\n4.1.3. Mathematical Definition#\\nLet \\\\(X_S\\\\) be the set of input features of interest (i.e. the features\\nparameter) and let \\\\(X_C\\\\) be its complement.\\nThe partial dependence of the response \\\\(f\\\\) at a point \\\\(x_S\\\\) is\\ndefined as:\\n---------new doc---------\\n4.1.3. Mathematical Definition#\\nLet \\\\(X_S\\\\) be the set of input features of interest (i.e. the features\\nparameter) and let \\\\(X_C\\\\) be its complement.\\nThe partial dependence of the response \\\\(f\\\\) at a point \\\\(x_S\\\\) is\\ndefined as:\\n\\n\\\\[\\\\begin{split}pd_{X_S}(x_S) &\\\\overset{def}{=} \\\\mathbb{E}_{X_C}\\\\left[ f(x_S, X_C) \\\\right]\\\\\\\\\\n              &= \\\\int f(x_S, x_C) p(x_C) dx_C,\\\\end{split}\\\\]\\nwhere \\\\(f(x_S, x_C)\\\\) is the response function (predict,\\npredict_proba or decision_function) for a given sample whose\\nvalues are defined by \\\\(x_S\\\\) for the features in \\\\(X_S\\\\), and by\\n\\\\(x_C\\\\) for the features in \\\\(X_C\\\\). Note that \\\\(x_S\\\\) and\\n\\\\(x_C\\\\) may be tuples.\\nComputing this integral for various values of \\\\(x_S\\\\) produces a PDP plot\\nas above. An ICE line is defined as a single \\\\(f(x_{S}, x_{C}^{(i)})\\\\)\\nevaluated at \\\\(x_{S}\\\\).\\n---------new doc---------\\n6.1.1. Pipeline: chaining estimators#\\nPipeline can be used to chain multiple estimators\\ninto one. This is useful as there is often a fixed sequence\\nof steps in processing the data, for example feature selection, normalization\\nand classification. Pipeline serves multiple purposes here:\\n\\nConvenience and encapsulationYou only have to call fit and predict once on your\\ndata to fit a whole sequence of estimators.\\n\\nJoint parameter selectionYou can grid search\\nover parameters of all estimators in the pipeline at once.\\n\\nSafetyPipelines help avoid leaking statistics from your test data into the\\ntrained model in cross-validation, by ensuring that the same samples are\\nused to train the transformers and predictors.\\n\\n\\nAll estimators in a pipeline, except the last one, must be transformers\\n(i.e. must have a transform method).\\nThe last estimator may be any type (transformer, classifier, etc.).\\n\\nNote\\nCalling fit on the pipeline is the same as calling fit on\\neach estimator in turn, transform the input and pass it on to the next step.\\nThe pipeline has all the methods that the last estimator in the pipeline has,\\ni.e. if the last estimator is a classifier, the Pipeline can be used\\nas a classifier. If the last estimator is a transformer, again, so is the\\npipeline.\\n\\n\\n6.1.1.1. Usage#\\n---------new doc---------\\n6.1.1.1. Usage#\\n\\n6.1.1.1.1. Build a pipeline#\\nThe Pipeline is built using a list of (key, value) pairs, where\\nthe key is a string containing the name you want to give this step and value\\nis an estimator object:\\n>>> from sklearn.pipeline import Pipeline\\n>>> from sklearn.svm import SVC\\n>>> from sklearn.decomposition import PCA\\n>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]\\n>>> pipe = Pipeline(estimators)\\n>>> pipe\\nPipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])\\n\\n\\n\\n\\nShorthand version using make_pipeline#\\nThe utility function make_pipeline is a shorthand\\nfor constructing pipelines;\\nit takes a variable number of estimators and returns a pipeline,\\nfilling in the names automatically:\\n>>> from sklearn.pipeline import make_pipeline\\n>>> make_pipeline(PCA(), SVC())\\nPipeline(steps=[('pca', PCA()), ('svc', SVC())])\\n---------new doc---------\\n6.1.1.1.2. Access pipeline steps#\\nThe estimators of a pipeline are stored as a list in the steps attribute.\\nA sub-pipeline can be extracted using the slicing notation commonly used\\nfor Python Sequences such as lists or strings (although only a step of 1 is\\npermitted). This is convenient for performing only some of the transformations\\n(or their inverse):\\n>>> pipe[:1]\\nPipeline(steps=[('reduce_dim', PCA())])\\n>>> pipe[-1:]\\nPipeline(steps=[('clf', SVC())])\\n\\n\\n\\n\\nAccessing a step by name or position#\\nA specific step can also be accessed by index or name by indexing (with [idx]) the\\npipeline:\\n>>> pipe.steps[0]\\n('reduce_dim', PCA())\\n>>> pipe[0]\\nPCA()\\n>>> pipe['reduce_dim']\\nPCA()\\n\\n\\nPipeline’s named_steps attribute allows accessing steps by name with tab\\ncompletion in interactive environments:\\n>>> pipe.named_steps.reduce_dim is pipe['reduce_dim']\\nTrue\\n---------new doc---------\\nPipeline’s named_steps attribute allows accessing steps by name with tab\\ncompletion in interactive environments:\\n>>> pipe.named_steps.reduce_dim is pipe['reduce_dim']\\nTrue\\n\\n\\n\\n\\n\\n6.1.1.1.3. Tracking feature names in a pipeline#\\nTo enable model inspection, Pipeline has a\\nget_feature_names_out() method, just like all transformers. You can use\\npipeline slicing to get the feature names going into each step:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.feature_selection import SelectKBest\\n>>> iris = load_iris()\\n>>> pipe = Pipeline(steps=[\\n...    ('select', SelectKBest(k=2)),\\n...    ('clf', LogisticRegression())])\\n>>> pipe.fit(iris.data, iris.target)\\nPipeline(steps=[('select', SelectKBest(...)), ('clf', LogisticRegression(...))])\\n>>> pipe[:-1].get_feature_names_out()\\narray(['x2', 'x3'], ...)\\n\\n\\n\\n\\nCustomize feature names#\\nYou can also provide custom feature names for the input data using\\nget_feature_names_out:\\n>>> pipe[:-1].get_feature_names_out(iris.feature_names)\\narray(['petal length (cm)', 'petal width (cm)'], ...)\\n---------new doc---------\\nCustomize feature names#\\nYou can also provide custom feature names for the input data using\\nget_feature_names_out:\\n>>> pipe[:-1].get_feature_names_out(iris.feature_names)\\narray(['petal length (cm)', 'petal width (cm)'], ...)\\n\\n\\n\\n\\n\\n6.1.1.1.4. Access to nested parameters#\\nIt is common to adjust the parameters of an estimator within a pipeline. This parameter\\nis therefore nested because it belongs to a particular sub-step. Parameters of the\\nestimators in the pipeline are accessible using the <estimator>__<parameter>\\nsyntax:\\n>>> pipe = Pipeline(steps=[(\\\"reduce_dim\\\", PCA()), (\\\"clf\\\", SVC())])\\n>>> pipe.set_params(clf__C=10)\\nPipeline(steps=[('reduce_dim', PCA()), ('clf', SVC(C=10))])\\n\\n\\n\\n\\nWhen does it matter?#\\nThis is particularly important for doing grid searches:\\n>>> from sklearn.model_selection import GridSearchCV\\n>>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],\\n...                   clf__C=[0.1, 10, 100])\\n>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)\\n---------new doc---------\\nSide effect of caching transformers#\\nUsing a Pipeline without cache enabled, it is possible to\\ninspect the original instance such as:\\n>>> from sklearn.datasets import load_digits\\n>>> X_digits, y_digits = load_digits(return_X_y=True)\\n>>> pca1 = PCA(n_components=10)\\n>>> svm1 = SVC()\\n>>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])\\n>>> pipe.fit(X_digits, y_digits)\\nPipeline(steps=[('reduce_dim', PCA(n_components=10)), ('clf', SVC())])\\n>>> # The pca instance can be inspected directly\\n>>> pca1.components_.shape\\n(10, 64)\\n---------new doc---------\\nFor simple transformations, instead of a Transformer object, a pair of\\nfunctions can be passed, defining the transformation and its inverse mapping:\\n>>> def func(x):\\n...     return np.log(x)\\n>>> def inverse_func(x):\\n...     return np.exp(x)\\n\\n\\nSubsequently, the object is created as:\\n>>> regr = TransformedTargetRegressor(regressor=regressor,\\n...                                   func=func,\\n...                                   inverse_func=inverse_func)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: 0.51\\n---------new doc---------\\nBy default, the provided functions are checked at each fit to be the inverse of\\neach other. However, it is possible to bypass this checking by setting\\ncheck_inverse to False:\\n>>> def inverse_func(x):\\n...     return x\\n>>> regr = TransformedTargetRegressor(regressor=regressor,\\n...                                   func=func,\\n...                                   inverse_func=inverse_func,\\n...                                   check_inverse=False)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: -1.57\\n\\n\\n\\nNote\\nThe transformation can be triggered by setting either transformer or the\\npair of functions func and inverse_func. However, setting both\\noptions will raise an error.\\n\\nExamples\\n\\nEffect of transforming the targets in regression model\\n---------new doc---------\\nNote\\nThe transformation can be triggered by setting either transformer or the\\npair of functions func and inverse_func. However, setting both\\noptions will raise an error.\\n\\nExamples\\n\\nEffect of transforming the targets in regression model\\n\\n\\n\\n6.1.3. FeatureUnion: composite feature spaces#\\nFeatureUnion combines several transformer objects into a new\\ntransformer that combines their output. A FeatureUnion takes\\na list of transformer objects. During fitting, each of these\\nis fit to the data independently. The transformers are applied in parallel,\\nand the feature matrices they output are concatenated side-by-side into a\\nlarger matrix.\\nWhen you want to apply different transformations to each field of the data,\\nsee the related class ColumnTransformer\\n(see user guide).\\nFeatureUnion serves the same purposes as Pipeline -\\nconvenience and joint parameter estimation and validation.\\nFeatureUnion and Pipeline can be combined to\\ncreate complex models.\\n(A FeatureUnion has no way of checking whether two transformers\\nmight produce identical features. It only produces a union when the\\nfeature sets are disjoint, and making sure they are is the caller’s\\nresponsibility.)\\n---------new doc---------\\n6.1.3.1. Usage#\\nA FeatureUnion is built using a list of (key, value) pairs,\\nwhere the key is the name you want to give to a given transformation\\n(an arbitrary string; it only serves as an identifier)\\nand value is an estimator object:\\n>>> from sklearn.pipeline import FeatureUnion\\n>>> from sklearn.decomposition import PCA\\n>>> from sklearn.decomposition import KernelPCA\\n>>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\\n>>> combined = FeatureUnion(estimators)\\n>>> combined\\nFeatureUnion(transformer_list=[('linear_pca', PCA()),\\n                               ('kernel_pca', KernelPCA())])\\n\\n\\nLike pipelines, feature unions have a shorthand constructor called\\nmake_union that does not require explicit naming of the components.\\nLike Pipeline, individual steps may be replaced using set_params,\\nand ignored by setting to 'drop':\\n>>> combined.set_params(kernel_pca='drop')\\nFeatureUnion(transformer_list=[('linear_pca', PCA()),\\n                               ('kernel_pca', 'drop')])\\n\\n\\nExamples\\n\\nConcatenating multiple feature extraction methods\\n---------new doc---------\\nExamples\\n\\nConcatenating multiple feature extraction methods\\n\\n\\n\\n\\n6.1.4. ColumnTransformer for heterogeneous data#\\nMany datasets contain features of different types, say text, floats, and dates,\\nwhere each type of feature requires separate preprocessing or feature\\nextraction steps.  Often it is easiest to preprocess data before applying\\nscikit-learn methods, for example using pandas.\\nProcessing your data before passing it to scikit-learn might be problematic for\\none of the following reasons:\\n\\nIncorporating statistics from test data into the preprocessors makes\\ncross-validation scores unreliable (known as data leakage),\\nfor example in the case of scalers or imputing missing values.\\nYou may want to include the parameters of the preprocessors in a\\nparameter search.\\n---------new doc---------\\nIncorporating statistics from test data into the preprocessors makes\\ncross-validation scores unreliable (known as data leakage),\\nfor example in the case of scalers or imputing missing values.\\nYou may want to include the parameters of the preprocessors in a\\nparameter search.\\n\\nThe ColumnTransformer helps performing different\\ntransformations for different columns of the data, within a\\nPipeline that is safe from data leakage and that can\\nbe parametrized. ColumnTransformer works on\\narrays, sparse matrices, and\\npandas DataFrames.\\nTo each column, a different transformation can be applied, such as\\npreprocessing or a specific feature extraction method:\\n>>> import pandas as pd\\n>>> X = pd.DataFrame(\\n...     {'city': ['London', 'London', 'Paris', 'Sallisaw'],\\n...      'title': [\\\"His Last Bow\\\", \\\"How Watson Learned the Trick\\\",\\n...                \\\"A Moveable Feast\\\", \\\"The Grapes of Wrath\\\"],\\n...      'expert_rating': [5, 3, 4, 5],\\n...      'user_rating': [4, 5, 4, 3]})\\n---------new doc---------\\nFor this data, we might want to encode the 'city' column as a categorical\\nvariable using OneHotEncoder but apply a\\nCountVectorizer to the 'title' column.\\nAs we might use multiple feature extraction methods on the same column, we give\\neach transformer a unique name, say 'city_category' and 'title_bow'.\\nBy default, the remaining rating columns are ignored (remainder='drop'):\\n>>> from sklearn.compose import ColumnTransformer\\n>>> from sklearn.feature_extraction.text import CountVectorizer\\n>>> from sklearn.preprocessing import OneHotEncoder\\n>>> column_trans = ColumnTransformer(\\n...     [('categories', OneHotEncoder(dtype='int'), ['city']),\\n...      ('title_bow', CountVectorizer(), 'title')],\\n...     remainder='drop', verbose_feature_names_out=False)\\n\\n>>> column_trans.fit(X)\\nColumnTransformer(transformers=[('categories', OneHotEncoder(dtype='int'),\\n                                 ['city']),\\n                                ('title_bow', CountVectorizer(), 'title')],\\n                  verbose_feature_names_out=False)\\n---------new doc---------\\n>>> column_trans.get_feature_names_out()\\narray(['city_London', 'city_Paris', 'city_Sallisaw', 'bow', 'feast',\\n'grapes', 'his', 'how', 'last', 'learned', 'moveable', 'of', 'the',\\n 'trick', 'watson', 'wrath'], ...)\\n\\n>>> column_trans.transform(X).toarray()\\narray([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\\n       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0],\\n       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\\n       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]...)\\n---------new doc---------\\nIn the above example, the\\nCountVectorizer expects a 1D array as\\ninput and therefore the columns were specified as a string ('title').\\nHowever, OneHotEncoder\\nas most of other transformers expects 2D data, therefore in that case you need\\nto specify the column as a list of strings (['city']).\\nApart from a scalar or a single item list, the column selection can be specified\\nas a list of multiple items, an integer array, a slice, a boolean mask, or\\nwith a make_column_selector. The\\nmake_column_selector is used to select columns based\\non data type or column name:\\n>>> from sklearn.preprocessing import StandardScaler\\n>>> from sklearn.compose import make_column_selector\\n>>> ct = ColumnTransformer([\\n...       ('scale', StandardScaler(),\\n...       make_column_selector(dtype_include=np.number)),\\n...       ('onehot',\\n...       OneHotEncoder(),\\n...       make_column_selector(pattern='city', dtype_include=object))])\\n>>> ct.fit_transform(X)\\narray([[ 0.904...,  0.      ,  1. ,  0. ,  0. ],\\n       [-1.507...,  1.414...,  1. ,  0. ,  0. ],\\n       [-0.301...,  0.      ,  0. ,  1. ,  0. ],\\n---------new doc---------\\narray([[ 0.904...,  0.      ,  1. ,  0. ,  0. ],\\n       [-1.507...,  1.414...,  1. ,  0. ,  0. ],\\n       [-0.301...,  0.      ,  0. ,  1. ,  0. ],\\n       [ 0.904..., -1.414...,  0. ,  0. ,  1. ]])\\n---------new doc---------\\nStrings can reference columns if the input is a DataFrame, integers are always\\ninterpreted as the positional columns.\\nWe can keep the remaining rating columns by setting\\nremainder='passthrough'. The values are appended to the end of the\\ntransformation:\\n>>> column_trans = ColumnTransformer(\\n...     [('city_category', OneHotEncoder(dtype='int'),['city']),\\n...      ('title_bow', CountVectorizer(), 'title')],\\n...     remainder='passthrough')\\n\\n>>> column_trans.fit_transform(X)\\narray([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],\\n       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],\\n       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4],\\n       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]]...)\\n---------new doc---------\\nThe remainder parameter can be set to an estimator to transform the\\nremaining rating columns. The transformed values are appended to the end of\\nthe transformation:\\n>>> from sklearn.preprocessing import MinMaxScaler\\n>>> column_trans = ColumnTransformer(\\n...     [('city_category', OneHotEncoder(), ['city']),\\n...      ('title_bow', CountVectorizer(), 'title')],\\n...     remainder=MinMaxScaler())\\n\\n>>> column_trans.fit_transform(X)[:, -2:]\\narray([[1. , 0.5],\\n       [0. , 1. ],\\n       [0.5, 0.5],\\n       [1. , 0. ]])\\n---------new doc---------\\n>>> column_trans.fit_transform(X)[:, -2:]\\narray([[1. , 0.5],\\n       [0. , 1. ],\\n       [0.5, 0.5],\\n       [1. , 0. ]])\\n\\n\\nThe make_column_transformer function is available\\nto more easily create a ColumnTransformer object.\\nSpecifically, the names will be given automatically. The equivalent for the\\nabove example would be:\\n>>> from sklearn.compose import make_column_transformer\\n>>> column_trans = make_column_transformer(\\n...     (OneHotEncoder(), ['city']),\\n...     (CountVectorizer(), 'title'),\\n...     remainder=MinMaxScaler())\\n>>> column_trans\\nColumnTransformer(remainder=MinMaxScaler(),\\n                  transformers=[('onehotencoder', OneHotEncoder(), ['city']),\\n                                ('countvectorizer', CountVectorizer(),\\n                                 'title')])\\n---------new doc---------\\nIf ColumnTransformer is fitted with a dataframe\\nand the dataframe only has string column names, then transforming a dataframe\\nwill use the column names to select the columns:\\n>>> ct = ColumnTransformer(\\n...          [(\\\"scale\\\", StandardScaler(), [\\\"expert_rating\\\"])]).fit(X)\\n>>> X_new = pd.DataFrame({\\\"expert_rating\\\": [5, 6, 1],\\n...                       \\\"ignored_new_col\\\": [1.2, 0.3, -0.1]})\\n>>> ct.transform(X_new)\\narray([[ 0.9...],\\n       [ 2.1...],\\n       [-3.9...]])\\n\\n\\n\\n\\n6.1.5. Visualizing Composite Estimators#\\nEstimators are displayed with an HTML representation when shown in a\\njupyter notebook. This is useful to diagnose or visualize a Pipeline with\\nmany estimators. This visualization is activated by default:\\n>>> column_trans  \\n\\n\\nIt can be deactivated by setting the display option in set_config\\nto ‘text’:\\n>>> from sklearn import set_config\\n>>> set_config(display='text')  \\n>>> # displays text representation in a jupyter context\\n>>> column_trans\\n---------new doc---------\\nIt can be deactivated by setting the display option in set_config\\nto ‘text’:\\n>>> from sklearn import set_config\\n>>> set_config(display='text')  \\n>>> # displays text representation in a jupyter context\\n>>> column_trans  \\n\\n\\nAn example of the HTML output can be seen in the\\nHTML representation of Pipeline section of\\nColumn Transformer with Mixed Types.\\nAs an alternative, the HTML can be written to a file using\\nestimator_html_repr:\\n>>> from sklearn.utils import estimator_html_repr\\n>>> with open('my_estimator.html', 'w') as f:  \\n...     f.write(estimator_html_repr(clf))\\n\\n\\nExamples\\n\\nColumn Transformer with Heterogeneous Data Sources\\nColumn Transformer with Mixed Types\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n6. Dataset transformations\\n\\n\\n\\n\\nnext\\n6.2. Feature extraction\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.2. Feature extraction\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6.2. Feature extraction#\\nThe sklearn.feature_extraction module can be used to extract\\nfeatures in a format supported by machine learning algorithms from datasets\\nconsisting of formats such as text and image.\\n\\nNote\\nFeature extraction is very different from Feature selection:\\nthe former consists in transforming arbitrary data, such as text or\\nimages, into numerical features usable for machine learning. The latter\\nis a machine learning technique applied on these features.\\n---------new doc---------\\n6.2. Feature extraction#\\nThe sklearn.feature_extraction module can be used to extract\\nfeatures in a format supported by machine learning algorithms from datasets\\nconsisting of formats such as text and image.\\n\\nNote\\nFeature extraction is very different from Feature selection:\\nthe former consists in transforming arbitrary data, such as text or\\nimages, into numerical features usable for machine learning. The latter\\nis a machine learning technique applied on these features.\\n\\n\\n6.2.1. Loading features from dicts#\\nThe class DictVectorizer can be used to convert feature\\narrays represented as lists of standard Python dict objects to the\\nNumPy/SciPy representation used by scikit-learn estimators.\\nWhile not particularly fast to process, Python’s dict has the\\nadvantages of being convenient to use, being sparse (absent features\\nneed not be stored) and storing feature names in addition to values.\\nDictVectorizer implements what is called one-of-K or “one-hot”\\ncoding for categorical (aka nominal, discrete) features. Categorical\\nfeatures are “attribute-value” pairs where the value is restricted\\nto a list of discrete possibilities without ordering (e.g. topic\\nidentifiers, types of objects, tags, names…).\\nIn the following, “city” is a categorical attribute while “temperature”\\nis a traditional numerical feature:\\n>>> measurements = [\\n...     {'city': 'Dubai', 'temperature': 33.},\\n...     {'city': 'London', 'temperature': 12.},\\n...     {'city': 'San Francisco', 'temperature': 18.},\\n... ]\\n---------new doc---------\\n>>> from sklearn.feature_extraction import DictVectorizer\\n>>> vec = DictVectorizer()\\n\\n>>> vec.fit_transform(measurements).toarray()\\narray([[ 1.,  0.,  0., 33.],\\n       [ 0.,  1.,  0., 12.],\\n       [ 0.,  0.,  1., 18.]])\\n\\n>>> vec.get_feature_names_out()\\narray(['city=Dubai', 'city=London', 'city=San Francisco', 'temperature'], ...)\\n---------new doc---------\\nDictVectorizer accepts multiple string values for one\\nfeature, like, e.g., multiple categories for a movie.\\nAssume a database classifies each movie using some categories (not mandatories)\\nand its year of release.\\n>>> movie_entry = [{'category': ['thriller', 'drama'], 'year': 2003},\\n...                {'category': ['animation', 'family'], 'year': 2011},\\n...                {'year': 1974}]\\n>>> vec.fit_transform(movie_entry).toarray()\\narray([[0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 2.003e+03],\\n       [1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 2.011e+03],\\n       [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.974e+03]])\\n>>> vec.get_feature_names_out()\\narray(['category=animation', 'category=drama', 'category=family',\\n       'category=thriller', 'year'], ...)\\n>>> vec.transform({'category': ['thriller'],\\n...                'unseen_feature': '3'}).toarray()\\narray([[0., 0., 0., 1., 0.]])\\n---------new doc---------\\nDictVectorizer is also a useful representation transformation\\nfor training sequence classifiers in Natural Language Processing models\\nthat typically work by extracting feature windows around a particular\\nword of interest.\\nFor example, suppose that we have a first algorithm that extracts Part of\\nSpeech (PoS) tags that we want to use as complementary tags for training\\na sequence classifier (e.g. a chunker). The following dict could be\\nsuch a window of features extracted around the word ‘sat’ in the sentence\\n‘The cat sat on the mat.’:\\n>>> pos_window = [\\n...     {\\n...         'word-2': 'the',\\n...         'pos-2': 'DT',\\n...         'word-1': 'cat',\\n...         'pos-1': 'NN',\\n...         'word+1': 'on',\\n...         'pos+1': 'PP',\\n...     },\\n...     # in a real application one would extract many such dictionaries\\n... ]\\n---------new doc---------\\nThis description can be vectorized into a sparse two-dimensional matrix\\nsuitable for feeding into a classifier (maybe after being piped into a\\nTfidfTransformer for normalization):\\n>>> vec = DictVectorizer()\\n>>> pos_vectorized = vec.fit_transform(pos_window)\\n>>> pos_vectorized\\n<Compressed Sparse...dtype 'float64'\\n  with 6 stored elements and shape (1, 6)>\\n>>> pos_vectorized.toarray()\\narray([[1., 1., 1., 1., 1., 1.]])\\n>>> vec.get_feature_names_out()\\narray(['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat',\\n       'word-2=the'], ...)\\n\\n\\nAs you can imagine, if one extracts such a context around each individual\\nword of a corpus of documents the resulting matrix will be very wide\\n(many one-hot-features) with most of them being valued to zero most\\nof the time. So as to make the resulting data structure able to fit in\\nmemory the DictVectorizer class uses a scipy.sparse matrix by\\ndefault instead of a numpy.ndarray.\\n---------new doc---------\\n6.2.2. Feature hashing#\\nThe class FeatureHasher is a high-speed, low-memory vectorizer that\\nuses a technique known as\\nfeature hashing,\\nor the “hashing trick”.\\nInstead of building a hash table of the features encountered in training,\\nas the vectorizers do, instances of FeatureHasher\\napply a hash function to the features\\nto determine their column index in sample matrices directly.\\nThe result is increased speed and reduced memory usage,\\nat the expense of inspectability;\\nthe hasher does not remember what the input features looked like\\nand has no inverse_transform method.\\nSince the hash function might cause collisions between (unrelated) features,\\na signed hash function is used and the sign of the hash value\\ndetermines the sign of the value stored in the output matrix for a feature.\\nThis way, collisions are likely to cancel out rather than accumulate error,\\nand the expected mean of any output feature’s value is zero. This mechanism\\nis enabled by default with alternate_sign=True and is particularly useful\\nfor small hash table sizes (n_features < 10000). For large hash table\\nsizes, it can be disabled, to allow the output to be passed to estimators like\\nMultinomialNB or\\nchi2\\nfeature selectors that expect non-negative inputs.\\nFeatureHasher accepts either mappings\\n(like Python’s dict and its variants in the collections module),\\n(feature, value) pairs, or strings,\\ndepending on the constructor parameter input_type.\\nMapping are treated as lists of (feature, value) pairs,\\nwhile single strings have an implicit value of 1,\\nso ['feat1', 'feat2', 'feat3'] is interpreted as\\n[('feat1', 1), ('feat2', 1), ('feat3', 1)].\\n---------new doc---------\\n(like Python’s dict and its variants in the collections module),\\n(feature, value) pairs, or strings,\\ndepending on the constructor parameter input_type.\\nMapping are treated as lists of (feature, value) pairs,\\nwhile single strings have an implicit value of 1,\\nso ['feat1', 'feat2', 'feat3'] is interpreted as\\n[('feat1', 1), ('feat2', 1), ('feat3', 1)].\\nIf a single feature occurs multiple times in a sample,\\nthe associated values will be summed\\n(so ('feat', 2) and ('feat', 3.5) become ('feat', 5.5)).\\nThe output from FeatureHasher is always a scipy.sparse matrix\\nin the CSR format.\\nFeature hashing can be employed in document classification,\\nbut unlike CountVectorizer,\\nFeatureHasher does not do word\\nsplitting or any other preprocessing except Unicode-to-UTF-8 encoding;\\nsee Vectorizing a large text corpus with the hashing trick, below, for a combined tokenizer/hasher.\\nAs an example, consider a word-level natural language processing task\\nthat needs features extracted from (token, part_of_speech) pairs.\\nOne could use a Python generator function to extract features:\\ndef token_features(token, part_of_speech):\\n    if token.isdigit():\\n        yield \\\"numeric\\\"\\n    else:\\n        yield \\\"token={}\\\".format(token.lower())\\n        yield \\\"token,pos={},{}\\\".format(token, part_of_speech)\\n    if token[0].isupper():\\n        yield \\\"uppercase_initial\\\"\\n---------new doc---------\\nyield \\\"numeric\\\"\\n    else:\\n        yield \\\"token={}\\\".format(token.lower())\\n        yield \\\"token,pos={},{}\\\".format(token, part_of_speech)\\n    if token[0].isupper():\\n        yield \\\"uppercase_initial\\\"\\n    if token.isupper():\\n        yield \\\"all_uppercase\\\"\\n    yield \\\"pos={}\\\".format(part_of_speech)\\n---------new doc---------\\nThen, the raw_X to be fed to FeatureHasher.transform\\ncan be constructed using:\\nraw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)\\n\\n\\nand fed to a hasher with:\\nhasher = FeatureHasher(input_type='string')\\nX = hasher.transform(raw_X)\\n\\n\\nto get a scipy.sparse matrix X.\\nNote the use of a generator comprehension,\\nwhich introduces laziness into the feature extraction:\\ntokens are only processed on demand from the hasher.\\n\\n\\nImplementation details#\\nFeatureHasher uses the signed 32-bit variant of MurmurHash3.\\nAs a result (and because of limitations in scipy.sparse),\\nthe maximum number of features supported is currently \\\\(2^{31} - 1\\\\).\\nThe original formulation of the hashing trick by Weinberger et al.\\nused two separate hash functions \\\\(h\\\\) and \\\\(\\\\xi\\\\)\\nto determine the column index and sign of a feature, respectively.\\nThe present implementation works under the assumption\\nthat the sign bit of MurmurHash3 is independent of its other bits.\\nSince a simple modulo is used to transform the hash function to a column index,\\nit is advisable to use a power of two as the n_features parameter;\\notherwise the features will not be mapped evenly to the columns.\\nReferences\\n\\nMurmurHash3.\\n\\n\\nReferences\\n\\nKilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and\\nJosh Attenberg (2009). Feature hashing for large scale multitask learning. Proc. ICML.\\n\\n\\n\\n6.2.3. Text feature extraction#\\n---------new doc---------\\nIn this scheme, features and samples are defined as follows:\\n\\neach individual token occurrence frequency (normalized or not)\\nis treated as a feature.\\nthe vector of all the token frequencies for a given document is\\nconsidered a multivariate sample.\\n\\nA corpus of documents can thus be represented by a matrix with one row\\nper document and one column per token (e.g. word) occurring in the corpus.\\nWe call vectorization the general process of turning a collection\\nof text documents into numerical feature vectors. This specific strategy\\n(tokenization, counting and normalization) is called the Bag of Words\\nor “Bag of n-grams” representation. Documents are described by word\\noccurrences while completely ignoring the relative position information\\nof the words in the document.\\n\\n\\n6.2.3.2. Sparsity#\\nAs most documents will typically use a very small subset of the words used in\\nthe corpus, the resulting matrix will have many feature values that are\\nzeros (typically more than 99% of them).\\nFor instance a collection of 10,000 short text documents (such as emails)\\nwill use a vocabulary with a size in the order of 100,000 unique words in\\ntotal while each document will use 100 to 1000 unique words individually.\\nIn order to be able to store such a matrix in memory but also to speed\\nup algebraic operations matrix / vector, implementations will typically\\nuse a sparse representation such as the implementations available in the\\nscipy.sparse package.\\n\\n\\n6.2.3.3. Common Vectorizer usage#\\nCountVectorizer implements both tokenization and occurrence\\ncounting in a single class:\\n>>> from sklearn.feature_extraction.text import CountVectorizer\\n---------new doc---------\\n6.2.3.3. Common Vectorizer usage#\\nCountVectorizer implements both tokenization and occurrence\\ncounting in a single class:\\n>>> from sklearn.feature_extraction.text import CountVectorizer\\n\\n\\nThis model has many parameters, however the default values are quite\\nreasonable (please see  the reference documentation for the details):\\n>>> vectorizer = CountVectorizer()\\n>>> vectorizer\\nCountVectorizer()\\n\\n\\nLet’s use it to tokenize and count the word occurrences of a minimalistic\\ncorpus of text documents:\\n>>> corpus = [\\n...     'This is the first document.',\\n...     'This is the second second document.',\\n...     'And the third one.',\\n...     'Is this the first document?',\\n... ]\\n>>> X = vectorizer.fit_transform(corpus)\\n>>> X\\n<Compressed Sparse...dtype 'int64'\\n  with 19 stored elements and shape (4, 9)>\\n\\n\\nThe default configuration tokenizes the string by extracting words of\\nat least 2 letters. The specific function that does this step can be\\nrequested explicitly:\\n>>> analyze = vectorizer.build_analyzer()\\n>>> analyze(\\\"This is a text document to analyze.\\\") == (\\n...     ['this', 'is', 'text', 'document', 'to', 'analyze'])\\nTrue\\n---------new doc---------\\nThe default configuration tokenizes the string by extracting words of\\nat least 2 letters. The specific function that does this step can be\\nrequested explicitly:\\n>>> analyze = vectorizer.build_analyzer()\\n>>> analyze(\\\"This is a text document to analyze.\\\") == (\\n...     ['this', 'is', 'text', 'document', 'to', 'analyze'])\\nTrue\\n\\n\\nEach term found by the analyzer during the fit is assigned a unique\\ninteger index corresponding to a column in the resulting matrix. This\\ninterpretation of the columns can be retrieved as follows:\\n>>> vectorizer.get_feature_names_out()\\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the',\\n       'third', 'this'], ...)\\n\\n>>> X.toarray()\\narray([[0, 1, 1, 1, 0, 0, 1, 0, 1],\\n       [0, 1, 0, 1, 0, 2, 1, 0, 1],\\n       [1, 0, 0, 0, 1, 0, 1, 1, 0],\\n       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)\\n\\n\\nThe converse mapping from feature name to column index is stored in the\\nvocabulary_ attribute of the vectorizer:\\n>>> vectorizer.vocabulary_.get('document')\\n1\\n\\n\\nHence words that were not seen in the training corpus will be completely\\nignored in future calls to the transform method:\\n>>> vectorizer.transform(['Something completely new.']).toarray()\\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)\\n---------new doc---------\\nHence words that were not seen in the training corpus will be completely\\nignored in future calls to the transform method:\\n>>> vectorizer.transform(['Something completely new.']).toarray()\\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)\\n\\n\\nNote that in the previous corpus, the first and the last documents have\\nexactly the same words hence are encoded in equal vectors. In particular\\nwe lose the information that the last document is an interrogative form. To\\npreserve some of the local ordering information we can extract 2-grams\\nof words in addition to the 1-grams (individual words):\\n>>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\\n...                                     token_pattern=r'\\\\b\\\\w+\\\\b', min_df=1)\\n>>> analyze = bigram_vectorizer.build_analyzer()\\n>>> analyze('Bi-grams are cool!') == (\\n...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\\nTrue\\n---------new doc---------\\nThe vocabulary extracted by this vectorizer is hence much bigger and\\ncan now resolve ambiguities encoded in local positioning patterns:\\n>>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\\n>>> X_2\\narray([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\\n       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\\n       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\\n       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)\\n\\n\\nIn particular the interrogative form “Is this” is only present in the\\nlast document:\\n>>> feature_index = bigram_vectorizer.vocabulary_.get('is this')\\n>>> X_2[:, feature_index]\\narray([0, 0, 0, 1]...)\\n---------new doc---------\\nIn particular the interrogative form “Is this” is only present in the\\nlast document:\\n>>> feature_index = bigram_vectorizer.vocabulary_.get('is this')\\n>>> X_2[:, feature_index]\\narray([0, 0, 0, 1]...)\\n\\n\\n\\n\\n6.2.3.4. Using stop words#\\nStop words are words like “and”, “the”, “him”, which are presumed to be\\nuninformative in representing the content of a text, and which may be\\nremoved to avoid them being construed as signal for prediction.  Sometimes,\\nhowever, similar words are useful for prediction, such as in classifying\\nwriting style or personality.\\nThere are several known issues in our provided ‘english’ stop word list. It\\ndoes not aim to be a general, ‘one-size-fits-all’ solution as some tasks\\nmay require a more custom solution. See [NQY18] for more details.\\nPlease take care in choosing a stop word list.\\nPopular stop word lists may include words that are highly informative to\\nsome tasks, such as computer.\\nYou should also make sure that the stop word list has had the same\\npreprocessing and tokenization applied as the one used in the vectorizer.\\nThe word we’ve is split into we and ve by CountVectorizer’s default\\ntokenizer, so if we’ve is in stop_words, but ve is not, ve will\\nbe retained from we’ve in transformed text.  Our vectorizers will try to\\nidentify and warn about some kinds of inconsistencies.\\nReferences\\n---------new doc---------\\n[NQY18]\\nJ. Nothman, H. Qin and R. Yurchak (2018).\\n“Stop Word Lists in Free Open-source Software Packages”.\\nIn Proc. Workshop for NLP Open Source Software.\\n---------new doc---------\\n6.2.3.5. Tf–idf term weighting#\\nIn a large text corpus, some words will be very present (e.g. “the”, “a”,\\n“is” in English) hence carrying very little meaningful information about\\nthe actual contents of the document. If we were to feed the direct count\\ndata directly to a classifier those very frequent terms would shadow\\nthe frequencies of rarer yet more interesting terms.\\nIn order to re-weight the count features into floating point values\\nsuitable for usage by a classifier it is very common to use the tf–idf\\ntransform.\\nTf means term-frequency while tf–idf means term-frequency times\\ninverse document-frequency:\\n\\\\(\\\\text{tf-idf(t,d)}=\\\\text{tf(t,d)} \\\\times \\\\text{idf(t)}\\\\).\\nUsing the TfidfTransformer’s default settings,\\nTfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\\nthe term frequency, the number of times a term occurs in a given document,\\nis multiplied with idf component, which is computed as\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{1 + n}{1+\\\\text{df}(t)}} + 1\\\\),\\nwhere \\\\(n\\\\) is the total number of documents in the document set, and\\n\\\\(\\\\text{df}(t)\\\\) is the number of documents in the document set that\\ncontain term \\\\(t\\\\). The resulting tf-idf vectors are then normalized by the\\nEuclidean norm:\\n---------new doc---------\\nwhere \\\\(n\\\\) is the total number of documents in the document set, and\\n\\\\(\\\\text{df}(t)\\\\) is the number of documents in the document set that\\ncontain term \\\\(t\\\\). The resulting tf-idf vectors are then normalized by the\\nEuclidean norm:\\n\\\\(v_{norm} = \\\\frac{v}{||v||_2} = \\\\frac{v}{\\\\sqrt{v{_1}^2 +\\nv{_2}^2 + \\\\dots + v{_n}^2}}\\\\).\\nThis was originally a term weighting scheme developed for information retrieval\\n(as a ranking function for search engines results) that has also found good\\nuse in document classification and clustering.\\nThe following sections contain further explanations and examples that\\nillustrate how the tf-idfs are computed exactly and how the tf-idfs\\ncomputed in scikit-learn’s TfidfTransformer\\nand TfidfVectorizer differ slightly from the standard textbook\\nnotation that defines the idf as\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{n}{1+\\\\text{df}(t)}}.\\\\)\\nIn the TfidfTransformer and TfidfVectorizer\\nwith smooth_idf=False, the\\n“1” count is added to the idf instead of the idf’s denominator:\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{n}{\\\\text{df}(t)}} + 1\\\\)\\nThis normalization is implemented by the TfidfTransformer\\nclass:\\n>>> from sklearn.feature_extraction.text import TfidfTransformer\\n---------new doc---------\\n“1” count is added to the idf instead of the idf’s denominator:\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{n}{\\\\text{df}(t)}} + 1\\\\)\\nThis normalization is implemented by the TfidfTransformer\\nclass:\\n>>> from sklearn.feature_extraction.text import TfidfTransformer\\n>>> transformer = TfidfTransformer(smooth_idf=False)\\n>>> transformer\\nTfidfTransformer(smooth_idf=False)\\n---------new doc---------\\nAgain please see the reference documentation for the details on all the parameters.\\n\\n\\nNumeric example of a tf-idf matrix#\\nLet’s take an example with the following counts. The first term is present\\n100% of the time hence not very interesting. The two other features only\\nin less than 50% of the time hence probably more representative of the\\ncontent of the documents:\\n>>> counts = [[3, 0, 1],\\n...           [2, 0, 0],\\n...           [3, 0, 0],\\n...           [4, 0, 0],\\n...           [3, 2, 0],\\n...           [3, 0, 2]]\\n...\\n>>> tfidf = transformer.fit_transform(counts)\\n>>> tfidf\\n<Compressed Sparse...dtype 'float64'\\n  with 9 stored elements and shape (6, 3)>\\n---------new doc---------\\n>>> tfidf.toarray()\\narray([[0.81940995, 0.        , 0.57320793],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [0.47330339, 0.88089948, 0.        ],\\n      [0.58149261, 0.        , 0.81355169]])\\n---------new doc---------\\nand the vector of raw tf-idfs:\\n\\\\(\\\\text{tf-idf}_{\\\\text{raw}} = [3, 0, 2.0986].\\\\)\\nThen, applying the Euclidean (L2) norm, we obtain the following tf-idfs\\nfor document 1:\\n\\\\(\\\\frac{[3, 0, 2.0986]}{\\\\sqrt{\\\\big(3^2 + 0^2 + 2.0986^2\\\\big)}}\\n= [ 0.819,  0,  0.573].\\\\)\\nFurthermore, the default parameter smooth_idf=True adds “1” to the numerator\\nand  denominator as if an extra document was seen containing every term in the\\ncollection exactly once, which prevents zero divisions:\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{1 + n}{1+\\\\text{df}(t)}} + 1\\\\)\\nUsing this modification, the tf-idf of the third term in document 1 changes to\\n1.8473:\\n\\\\(\\\\text{tf-idf}_{\\\\text{term3}} = 1 \\\\times \\\\log(7/3)+1 \\\\approx 1.8473\\\\)\\nAnd the L2-normalized tf-idf changes to\\n\\\\(\\\\frac{[3, 0, 1.8473]}{\\\\sqrt{\\\\big(3^2 + 0^2 + 1.8473^2\\\\big)}}\\n= [0.8515, 0, 0.5243]\\\\):\\n>>> transformer = TfidfTransformer()\\n>>> transformer.fit_transform(counts).toarray()\\narray([[0.85151335, 0.        , 0.52433293],\\n---------new doc---------\\n= [0.8515, 0, 0.5243]\\\\):\\n>>> transformer = TfidfTransformer()\\n>>> transformer.fit_transform(counts).toarray()\\narray([[0.85151335, 0.        , 0.52433293],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [0.55422893, 0.83236428, 0.        ],\\n      [0.63035731, 0.        , 0.77630514]])\\n---------new doc---------\\nThe weights of each\\nfeature computed by the fit method call are stored in a model\\nattribute:\\n>>> transformer.idf_\\narray([1. ..., 2.25..., 1.84...])\\n\\n\\nAs tf-idf is very often used for text features, there is also another\\nclass called TfidfVectorizer that combines all the options of\\nCountVectorizer and TfidfTransformer in a single model:\\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\\n>>> vectorizer = TfidfVectorizer()\\n>>> vectorizer.fit_transform(corpus)\\n<Compressed Sparse...dtype 'float64'\\n  with 19 stored elements and shape (4, 9)>\\n\\n\\nWhile the tf-idf normalization is often very useful, there might\\nbe cases where the binary occurrence markers might offer better\\nfeatures. This can be achieved by using the binary parameter\\nof CountVectorizer. In particular, some estimators such as\\nBernoulli Naive Bayes explicitly model discrete boolean random\\nvariables. Also, very short texts are likely to have noisy tf-idf values\\nwhile the binary occurrence info is more stable.\\nAs usual the best way to adjust the feature extraction parameters\\nis to use a cross-validated grid search, for instance by pipelining the\\nfeature extractor with a classifier:\\n\\nSample pipeline for text feature extraction and evaluation\\n---------new doc---------\\nSample pipeline for text feature extraction and evaluation\\n\\n\\n\\n\\n6.2.3.6. Decoding text files#\\nText is made of characters, but files are made of bytes. These bytes represent\\ncharacters according to some encoding. To work with text files in Python,\\ntheir bytes must be decoded to a character set called Unicode.\\nCommon encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian)\\nand the universal encodings UTF-8 and UTF-16. Many others exist.\\n\\nNote\\nAn encoding can also be called a ‘character set’,\\nbut this term is less accurate: several encodings can exist\\nfor a single character set.\\n\\nThe text feature extractors in scikit-learn know how to decode text files,\\nbut only if you tell them what encoding the files are in.\\nThe CountVectorizer takes an encoding parameter for this purpose.\\nFor modern text files, the correct encoding is probably UTF-8,\\nwhich is therefore the default (encoding=\\\"utf-8\\\").\\nIf the text you are loading is not actually encoded with UTF-8, however,\\nyou will get a UnicodeDecodeError.\\nThe vectorizers can be told to be silent about decoding errors\\nby setting the decode_error parameter to either \\\"ignore\\\"\\nor \\\"replace\\\". See the documentation for the Python function\\nbytes.decode for more details\\n(type help(bytes.decode) at the Python prompt).\\n\\n\\nTroubleshooting decoding text#\\nIf you are having trouble decoding text, here are some things to try:\\n---------new doc---------\\nTroubleshooting decoding text#\\nIf you are having trouble decoding text, here are some things to try:\\n\\nFind out what the actual encoding of the text is. The file might come\\nwith a header or README that tells you the encoding, or there might be some\\nstandard encoding you can assume based on where the text comes from.\\nYou may be able to find out what kind of encoding it is in general\\nusing the UNIX command file. The Python chardet module comes with\\na script called chardetect.py that will guess the specific encoding,\\nthough you cannot rely on its guess being correct.\\nYou could try UTF-8 and disregard the errors. You can decode byte\\nstrings with bytes.decode(errors='replace') to replace all\\ndecoding errors with a meaningless character, or set\\ndecode_error='replace' in the vectorizer. This may damage the\\nusefulness of your features.\\nReal text may come from a variety of sources that may have used different\\nencodings, or even be sloppily decoded in a different encoding than the\\none it was encoded with. This is common in text retrieved from the Web.\\nThe Python package ftfy\\ncan automatically sort out some classes of\\ndecoding errors, so you could try decoding the unknown text as latin-1\\nand then using ftfy to fix errors.\\nIf the text is in a mish-mash of encodings that is simply too hard to sort\\nout (which is the case for the 20 Newsgroups dataset), you can fall back on\\na simple single-byte encoding such as latin-1. Some text may display\\nincorrectly, but at least the same sequence of bytes will always represent\\nthe same feature.\\n---------new doc---------\\nFor example, the following snippet uses chardet\\n(not shipped with scikit-learn, must be installed separately)\\nto figure out the encoding of three texts.\\nIt then vectorizes the texts and prints the learned vocabulary.\\nThe output is not shown here.\\n>>> import chardet    \\n>>> text1 = b\\\"Sei mir gegr\\\\xc3\\\\xbc\\\\xc3\\\\x9ft mein Sauerkraut\\\"\\n>>> text2 = b\\\"holdselig sind deine Ger\\\\xfcche\\\"\\n>>> text3 = b\\\"\\\\xff\\\\xfeA\\\\x00u\\\\x00f\\\\x00 \\\\x00F\\\\x00l\\\\x00\\\\xfc\\\\x00g\\\\x00e\\\\x00l\\\\x00n\\\\x00 \\\\x00d\\\\x00e\\\\x00s\\\\x00 \\\\x00G\\\\x00e\\\\x00s\\\\x00a\\\\x00n\\\\x00g\\\\x00e\\\\x00s\\\\x00,\\\\x00 \\\\x00H\\\\x00e\\\\x00r\\\\x00z\\\\x00l\\\\x00i\\\\x00e\\\\x00b\\\\x00c\\\\x00h\\\\x00e\\\\x00n\\\\x00,\\\\x00 \\\\x00t\\\\x00r\\\\x00a\\\\x00g\\\\x00 \\\\x00i\\\\x00c\\\\x00h\\\\x00 \\\\x00d\\\\x00i\\\\x00c\\\\x00h\\\\x00 \\\\x00f\\\\x00o\\\\x00r\\\\x00t\\\\x00\\\"\\n>>> decoded = [x.decode(chardet.detect(x)['encoding'])\\n---------new doc---------\\n>>> decoded = [x.decode(chardet.detect(x)['encoding'])\\n...            for x in (text1, text2, text3)]        \\n>>> v = CountVectorizer().fit(decoded).vocabulary_    \\n>>> for term in v: print(v)\\n---------new doc---------\\n(Depending on the version of chardet, it might get the first one wrong.)\\nFor an introduction to Unicode and character encodings in general,\\nsee Joel Spolsky’s Absolute Minimum Every Software Developer Must Know\\nAbout Unicode.\\n\\n\\n\\n6.2.3.7. Applications and examples#\\nThe bag of words representation is quite simplistic but surprisingly\\nuseful in practice.\\nIn particular in a supervised setting it can be successfully combined\\nwith fast and scalable linear models to train document classifiers,\\nfor instance:\\n\\nClassification of text documents using sparse features\\n\\nIn an unsupervised setting it can be used to group similar documents\\ntogether by applying clustering algorithms such as K-means:\\n\\nClustering text documents using k-means\\n\\nFinally it is possible to discover the main topics of a corpus by\\nrelaxing the hard assignment constraint of clustering, for instance by\\nusing Non-negative matrix factorization (NMF or NNMF):\\n\\nTopic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\\n---------new doc---------\\n6.2.3.8. Limitations of the Bag of Words representation#\\nA collection of unigrams (what bag of words is) cannot capture phrases\\nand multi-word expressions, effectively disregarding any word order\\ndependence. Additionally, the bag of words model doesn’t account for potential\\nmisspellings or word derivations.\\nN-grams to the rescue! Instead of building a simple collection of\\nunigrams (n=1), one might prefer a collection of bigrams (n=2), where\\noccurrences of pairs of consecutive words are counted.\\nOne might alternatively consider a collection of character n-grams, a\\nrepresentation resilient against misspellings and derivations.\\nFor example, let’s say we’re dealing with a corpus of two documents:\\n['words', 'wprds']. The second document contains a misspelling\\nof the word ‘words’.\\nA simple bag of words representation would consider these two as\\nvery distinct documents, differing in both of the two possible features.\\nA character 2-gram representation, however, would find the documents\\nmatching in 4 out of 8 features, which may help the preferred classifier\\ndecide better:\\n>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\\n>>> counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\\n>>> ngram_vectorizer.get_feature_names_out()\\narray([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...)\\n>>> counts.toarray().astype(int)\\narray([[1, 1, 1, 0, 1, 1, 1, 0],\\n---------new doc---------\\n>>> ngram_vectorizer.get_feature_names_out()\\narray([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...)\\n>>> counts.toarray().astype(int)\\narray([[1, 1, 1, 0, 1, 1, 1, 0],\\n       [1, 1, 0, 1, 1, 1, 0, 1]])\\n---------new doc---------\\nIn the above example, char_wb analyzer is used, which creates n-grams\\nonly from characters inside word boundaries (padded with space on each\\nside). The char analyzer, alternatively, creates n-grams that\\nspan across words:\\n>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\\n>>> ngram_vectorizer.fit_transform(['jumpy fox'])\\n<Compressed Sparse...dtype 'int64'\\n  with 4 stored elements and shape (1, 4)>\\n\\n>>> ngram_vectorizer.get_feature_names_out()\\narray([' fox ', ' jump', 'jumpy', 'umpy '], ...)\\n\\n>>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5))\\n>>> ngram_vectorizer.fit_transform(['jumpy fox'])\\n<Compressed Sparse...dtype 'int64'\\n  with 5 stored elements and shape (1, 5)>\\n>>> ngram_vectorizer.get_feature_names_out()\\narray(['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'], ...)\\n---------new doc---------\\nThe word boundaries-aware variant char_wb is especially interesting\\nfor languages that use white-spaces for word separation as it generates\\nsignificantly less noisy features than the raw char variant in\\nthat case. For such languages it can increase both the predictive\\naccuracy and convergence speed of classifiers trained using such\\nfeatures while retaining the robustness with regards to misspellings and\\nword derivations.\\nWhile some local positioning information can be preserved by extracting\\nn-grams instead of individual words, bag of words and bag of n-grams\\ndestroy most of the inner structure of the document and hence most of\\nthe meaning carried by that internal structure.\\nIn order to address the wider task of Natural Language Understanding,\\nthe local structure of sentences and paragraphs should thus be taken\\ninto account. Many such models will thus be casted as “Structured output”\\nproblems which are currently outside of the scope of scikit-learn.\\n\\n\\n6.2.3.9. Vectorizing a large text corpus with the hashing trick#\\nThe above vectorization scheme is simple but the fact that it holds an in-\\nmemory mapping from the string tokens to the integer feature indices (the\\nvocabulary_ attribute) causes several problems when dealing with large\\ndatasets:\\n---------new doc---------\\nIt is possible to overcome those limitations by combining the “hashing trick”\\n(Feature hashing) implemented by the\\nFeatureHasher class and the text\\npreprocessing and tokenization features of the CountVectorizer.\\nThis combination is implementing in HashingVectorizer,\\na transformer class that is mostly API compatible with CountVectorizer.\\nHashingVectorizer is stateless,\\nmeaning that you don’t have to call fit on it:\\n>>> from sklearn.feature_extraction.text import HashingVectorizer\\n>>> hv = HashingVectorizer(n_features=10)\\n>>> hv.transform(corpus)\\n<Compressed Sparse...dtype 'float64'\\n  with 16 stored elements and shape (4, 10)>\\n---------new doc---------\\nYou can see that 16 non-zero feature tokens were extracted in the vector\\noutput: this is less than the 19 non-zeros extracted previously by the\\nCountVectorizer on the same toy corpus. The discrepancy comes from\\nhash function collisions because of the low value of the n_features parameter.\\nIn a real world setting, the n_features parameter can be left to its\\ndefault value of 2 ** 20 (roughly one million possible features). If memory\\nor downstream models size is an issue selecting a lower value such as 2 **\\n18 might help without introducing too many additional collisions on typical\\ntext classification tasks.\\nNote that the dimensionality does not affect the CPU training time of\\nalgorithms which operate on CSR matrices (LinearSVC(dual=True),\\nPerceptron, SGDClassifier, PassiveAggressive) but it does for\\nalgorithms that work with CSC matrices (LinearSVC(dual=False), Lasso(),\\netc.).\\nLet’s try again with the default setting:\\n>>> hv = HashingVectorizer()\\n>>> hv.transform(corpus)\\n<Compressed Sparse...dtype 'float64'\\n  with 19 stored elements and shape (4, 1048576)>\\n\\n\\nWe no longer get the collisions, but this comes at the expense of a much larger\\ndimensionality of the output space.\\nOf course, other terms than the 19 used here\\nmight still collide with each other.\\nThe HashingVectorizer also comes with the following limitations:\\n---------new doc---------\\nWe no longer get the collisions, but this comes at the expense of a much larger\\ndimensionality of the output space.\\nOf course, other terms than the 19 used here\\nmight still collide with each other.\\nThe HashingVectorizer also comes with the following limitations:\\n\\nit is not possible to invert the model (no inverse_transform method),\\nnor to access the original string representation of the features,\\nbecause of the one-way nature of the hash function that performs the mapping.\\nit does not provide IDF weighting as that would introduce statefulness in the\\nmodel. A TfidfTransformer can be appended to it in a pipeline if\\nrequired.\\n\\n\\n\\nPerforming out-of-core scaling with HashingVectorizer#\\nAn interesting development of using a HashingVectorizer is the ability\\nto perform out-of-core scaling. This means that we can learn from data that\\ndoes not fit into the computer’s main memory.\\nA strategy to implement out-of-core scaling is to stream data to the estimator\\nin mini-batches. Each mini-batch is vectorized using HashingVectorizer\\nso as to guarantee that the input space of the estimator has always the same\\ndimensionality. The amount of memory used at any time is thus bounded by the\\nsize of a mini-batch. Although there is no limit to the amount of data that can\\nbe ingested using such an approach, from a practical point of view the learning\\ntime is often limited by the CPU time one wants to spend on the task.\\nFor a full-fledged example of out-of-core scaling in a text classification\\ntask see Out-of-core classification of text documents.\\n---------new doc---------\\n6.2.3.10. Customizing the vectorizer classes#\\nIt is possible to customize the behavior by passing a callable\\nto the vectorizer constructor:\\n>>> def my_tokenizer(s):\\n...     return s.split()\\n...\\n>>> vectorizer = CountVectorizer(tokenizer=my_tokenizer)\\n>>> vectorizer.build_analyzer()(u\\\"Some... punctuation!\\\") == (\\n...     ['some...', 'punctuation!'])\\nTrue\\n\\n\\nIn particular we name:\\n\\npreprocessor: a callable that takes an entire document as input (as a\\nsingle string), and returns a possibly transformed version of the document,\\nstill as an entire string. This can be used to remove HTML tags, lowercase\\nthe entire document, etc.\\ntokenizer: a callable that takes the output from the preprocessor\\nand splits it into tokens, then returns a list of these.\\nanalyzer: a callable that replaces the preprocessor and tokenizer.\\nThe default analyzers all call the preprocessor and tokenizer, but custom\\nanalyzers will skip this. N-gram extraction and stop word filtering take\\nplace at the analyzer level, so a custom analyzer may have to reproduce\\nthese steps.\\n\\n(Lucene users might recognize these names, but be aware that scikit-learn\\nconcepts may not map one-to-one onto Lucene concepts.)\\nTo make the preprocessor, tokenizer and analyzers aware of the model\\nparameters it is possible to derive from the class and override the\\nbuild_preprocessor, build_tokenizer and build_analyzer\\nfactory methods instead of passing custom functions.\\n\\n\\nTips and tricks#\\n---------new doc---------\\n(Lucene users might recognize these names, but be aware that scikit-learn\\nconcepts may not map one-to-one onto Lucene concepts.)\\nTo make the preprocessor, tokenizer and analyzers aware of the model\\nparameters it is possible to derive from the class and override the\\nbuild_preprocessor, build_tokenizer and build_analyzer\\nfactory methods instead of passing custom functions.\\n\\n\\nTips and tricks#\\n\\nIf documents are pre-tokenized by an external package, then store them in\\nfiles (or strings) with the tokens separated by whitespace and pass\\nanalyzer=str.split\\nFancy token-level analysis such as stemming, lemmatizing, compound\\nsplitting, filtering based on part-of-speech, etc. are not included in the\\nscikit-learn codebase, but can be added by customizing either the\\ntokenizer or the analyzer.\\nHere’s a CountVectorizer with a tokenizer and lemmatizer using\\nNLTK:\\n>>> from nltk import word_tokenize          \\n>>> from nltk.stem import WordNetLemmatizer \\n>>> class LemmaTokenizer:\\n...     def __init__(self):\\n...         self.wnl = WordNetLemmatizer()\\n...     def __call__(self, doc):\\n...         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\\n...\\n>>> vect = CountVectorizer(tokenizer=LemmaTokenizer())\\n---------new doc---------\\n(Note that this will not filter out punctuation.)\\nThe following example will, for instance, transform some British spelling\\nto American spelling:\\n>>> import re\\n>>> def to_british(tokens):\\n...     for t in tokens:\\n...         t = re.sub(r\\\"(...)our$\\\", r\\\"\\\\1or\\\", t)\\n...         t = re.sub(r\\\"([bt])re$\\\", r\\\"\\\\1er\\\", t)\\n...         t = re.sub(r\\\"([iy])s(e$|ing|ation)\\\", r\\\"\\\\1z\\\\2\\\", t)\\n...         t = re.sub(r\\\"ogue$\\\", \\\"og\\\", t)\\n...         yield t\\n...\\n>>> class CustomVectorizer(CountVectorizer):\\n...     def build_tokenizer(self):\\n...         tokenize = super().build_tokenizer()\\n...         return lambda doc: list(to_british(tokenize(doc)))\\n...\\n>>> print(CustomVectorizer().build_analyzer()(u\\\"color colour\\\"))\\n[...'color', ...'color']\\n\\n\\nfor other styles of preprocessing; examples include stemming, lemmatization,\\nor normalizing numerical tokens, with the latter illustrated in:\\n\\nBiclustering documents with the Spectral Co-clustering algorithm\\n\\n\\n\\nCustomizing the vectorizer can also be useful when handling Asian languages\\nthat do not use an explicit word separator such as whitespace.\\n\\n\\n\\n\\n6.2.4. Image feature extraction#\\n---------new doc---------\\n>>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,\\n...     random_state=0)\\n>>> patches.shape\\n(2, 2, 2, 3)\\n>>> patches[:, :, :, 0]\\narray([[[ 0,  3],\\n        [12, 15]],\\n\\n       [[15, 18],\\n        [27, 30]]])\\n>>> patches = image.extract_patches_2d(one_image, (2, 2))\\n>>> patches.shape\\n(9, 2, 2, 3)\\n>>> patches[4, :, :, 0]\\narray([[15, 18],\\n       [27, 30]])\\n\\n\\nLet us now try to reconstruct the original image from the patches by averaging\\non overlapping areas:\\n>>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))\\n>>> np.testing.assert_array_equal(one_image, reconstructed)\\n\\n\\nThe PatchExtractor class works in the same way as\\nextract_patches_2d, only it supports multiple images as input. It is\\nimplemented as a scikit-learn transformer, so it can be used in pipelines. See:\\n>>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\\n>>> patches = image.PatchExtractor(patch_size=(2, 2)).transform(five_images)\\n>>> patches.shape\\n(45, 2, 2, 3)\\n---------new doc---------\\n>>> scaler.mean_\\narray([1. ..., 0. ..., 0.33...])\\n\\n>>> scaler.scale_\\narray([0.81..., 0.81..., 1.24...])\\n\\n>>> X_scaled = scaler.transform(X_train)\\n>>> X_scaled\\narray([[ 0.  ..., -1.22...,  1.33...],\\n       [ 1.22...,  0.  ..., -0.26...],\\n       [-1.22...,  1.22..., -1.06...]])\\n\\n\\nScaled data has zero mean and unit variance:\\n>>> X_scaled.mean(axis=0)\\narray([0., 0., 0.])\\n\\n>>> X_scaled.std(axis=0)\\narray([1., 1., 1.])\\n\\n\\nThis class implements the Transformer API to compute the mean and\\nstandard deviation on a training set so as to be able to later re-apply the\\nsame transformation on the testing set. This class is hence suitable for\\nuse in the early steps of a Pipeline:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn.pipeline import make_pipeline\\n>>> from sklearn.preprocessing import StandardScaler\\n---------new doc---------\\n6.3.1.1. Scaling features to a range#\\nAn alternative standardization is scaling features to\\nlie between a given minimum and maximum value, often between zero and one,\\nor so that the maximum absolute value of each feature is scaled to unit size.\\nThis can be achieved using MinMaxScaler or MaxAbsScaler,\\nrespectively.\\nThe motivation to use this scaling include robustness to very small\\nstandard deviations of features and preserving zero entries in sparse data.\\nHere is an example to scale a toy data matrix to the [0, 1] range:\\n>>> X_train = np.array([[ 1., -1.,  2.],\\n...                     [ 2.,  0.,  0.],\\n...                     [ 0.,  1., -1.]])\\n...\\n>>> min_max_scaler = preprocessing.MinMaxScaler()\\n>>> X_train_minmax = min_max_scaler.fit_transform(X_train)\\n>>> X_train_minmax\\narray([[0.5       , 0.        , 1.        ],\\n       [1.        , 0.5       , 0.33333333],\\n       [0.        , 1.        , 0.        ]])\\n---------new doc---------\\nThe same instance of the transformer can then be applied to some new test data\\nunseen during the fit call: the same scaling and shifting operations will be\\napplied to be consistent with the transformation performed on the train data:\\n>>> X_test = np.array([[-3., -1.,  4.]])\\n>>> X_test_minmax = min_max_scaler.transform(X_test)\\n>>> X_test_minmax\\narray([[-1.5       ,  0.        ,  1.66666667]])\\n\\n\\nIt is possible to introspect the scaler attributes to find about the exact\\nnature of the transformation learned on the training data:\\n>>> min_max_scaler.scale_\\narray([0.5       , 0.5       , 0.33...])\\n\\n>>> min_max_scaler.min_\\narray([0.        , 0.5       , 0.33...])\\n\\n\\nIf MinMaxScaler is given an explicit feature_range=(min, max) the\\nfull formula is:\\nX_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\\n\\nX_scaled = X_std * (max - min) + min\\n---------new doc---------\\n6.3.1.2. Scaling sparse data#\\nCentering sparse data would destroy the sparseness structure in the data, and\\nthus rarely is a sensible thing to do. However, it can make sense to scale\\nsparse inputs, especially if features are on different scales.\\nMaxAbsScaler was specifically designed for scaling\\nsparse data, and is the recommended way to go about this.\\nHowever, StandardScaler can accept scipy.sparse\\nmatrices  as input, as long as with_mean=False is explicitly passed\\nto the constructor. Otherwise a ValueError will be raised as\\nsilently centering would break the sparsity and would often crash the\\nexecution by allocating excessive amounts of memory unintentionally.\\nRobustScaler cannot be fitted to sparse inputs, but you can use\\nthe transform method on sparse inputs.\\nNote that the scalers accept both Compressed Sparse Rows and Compressed\\nSparse Columns format (see scipy.sparse.csr_matrix and\\nscipy.sparse.csc_matrix). Any other sparse input will be converted to\\nthe Compressed Sparse Rows representation.  To avoid unnecessary memory\\ncopies, it is recommended to choose the CSR or CSC representation upstream.\\nFinally, if the centered data is expected to be small enough, explicitly\\nconverting the input to an array using the toarray method of sparse matrices\\nis another option.\\n\\n\\n6.3.1.3. Scaling data with outliers#\\nIf your data contains many outliers, scaling using the mean and variance\\nof the data is likely to not work very well. In these cases, you can use\\nRobustScaler as a drop-in replacement instead. It uses\\nmore robust estimates for the center and range of your data.\\n---------new doc---------\\n6.3.2.1. Mapping to a Uniform distribution#\\nQuantileTransformer provides a non-parametric\\ntransformation to map the data to a uniform distribution\\nwith values between 0 and 1:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.model_selection import train_test_split\\n>>> X, y = load_iris(return_X_y=True)\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n>>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\\n>>> X_train_trans = quantile_transformer.fit_transform(X_train)\\n>>> X_test_trans = quantile_transformer.transform(X_test)\\n>>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) \\narray([ 4.3,  5.1,  5.8,  6.5,  7.9])\\n\\n\\nThis feature corresponds to the sepal length in cm. Once the quantile\\ntransformation applied, those landmarks approach closely the percentiles\\npreviously defined:\\n>>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])\\n... \\narray([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])\\n---------new doc---------\\nYeo-Johnson transform#\\n\\n\\\\[\\\\begin{split}x_i^{(\\\\lambda)} =\\n\\\\begin{cases}\\n[(x_i + 1)^\\\\lambda - 1] / \\\\lambda & \\\\text{if } \\\\lambda \\\\neq 0, x_i \\\\geq 0, \\\\\\\\[8pt]\\n\\\\ln{(x_i + 1)} & \\\\text{if } \\\\lambda = 0, x_i \\\\geq 0 \\\\\\\\[8pt]\\n-[(-x_i + 1)^{2 - \\\\lambda} - 1] / (2 - \\\\lambda) & \\\\text{if } \\\\lambda \\\\neq 2, x_i < 0, \\\\\\\\[8pt]\\n- \\\\ln (- x_i + 1) & \\\\text{if } \\\\lambda = 2, x_i < 0\\n\\\\end{cases}\\\\end{split}\\\\]\\n\\n\\n\\nBox-Cox transform#\\n---------new doc---------\\nBox-Cox transform#\\n\\n\\\\[\\\\begin{split}x_i^{(\\\\lambda)} =\\n\\\\begin{cases}\\n\\\\dfrac{x_i^\\\\lambda - 1}{\\\\lambda} & \\\\text{if } \\\\lambda \\\\neq 0, \\\\\\\\[8pt]\\n\\\\ln{(x_i)} & \\\\text{if } \\\\lambda = 0,\\n\\\\end{cases}\\\\end{split}\\\\]\\nBox-Cox can only be applied to strictly positive data. In both methods, the\\ntransformation is parameterized by \\\\(\\\\lambda\\\\), which is determined through\\nmaximum likelihood estimation. Here is an example of using Box-Cox to map\\nsamples drawn from a lognormal distribution to a normal distribution:\\n>>> pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\\n>>> X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\\n>>> X_lognormal\\narray([[1.28..., 1.18..., 0.84...],\\n      [0.94..., 1.60..., 0.38...],\\n      [1.35..., 0.21..., 1.09...]])\\n>>> pt.fit_transform(X_lognormal)\\narray([[ 0.49...,  0.17..., -0.15...],\\n      [-0.05...,  0.58..., -0.57...],\\n      [ 0.69..., -0.84...,  0.10...]])\\n---------new doc---------\\nWhile the above example sets the standardize option to False,\\nPowerTransformer will apply zero-mean, unit-variance normalization\\nto the transformed output by default.\\n\\nBelow are examples of Box-Cox and Yeo-Johnson applied to various probability\\ndistributions.  Note that when applied to certain distributions, the power\\ntransforms achieve very Gaussian-like results, but with others, they are\\nineffective. This highlights the importance of visualizing the data before and\\nafter transformation.\\n\\n\\n\\n\\nIt is also possible to map data to a normal distribution using\\nQuantileTransformer by setting output_distribution='normal'.\\nUsing the earlier example with the iris dataset:\\n>>> quantile_transformer = preprocessing.QuantileTransformer(\\n...     output_distribution='normal', random_state=0)\\n>>> X_trans = quantile_transformer.fit_transform(X)\\n>>> quantile_transformer.quantiles_\\narray([[4.3, 2. , 1. , 0.1],\\n       [4.4, 2.2, 1.1, 0.1],\\n       [4.4, 2.2, 1.2, 0.1],\\n       ...,\\n       [7.7, 4.1, 6.7, 2.5],\\n       [7.7, 4.2, 6.7, 2.5],\\n       [7.9, 4.4, 6.9, 2.5]])\\n---------new doc---------\\n>>> X_normalized\\narray([[ 0.40..., -0.40...,  0.81...],\\n       [ 1.  ...,  0.  ...,  0.  ...],\\n       [ 0.  ...,  0.70..., -0.70...]])\\n\\n\\nThe preprocessing module further provides a utility class\\nNormalizer that implements the same operation using the\\nTransformer API (even though the fit method is useless in this case:\\nthe class is stateless as this operation treats samples independently).\\nThis class is hence suitable for use in the early steps of a\\nPipeline:\\n>>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\\n>>> normalizer\\nNormalizer()\\n\\n\\nThe normalizer instance can then be used on sample vectors as any transformer:\\n>>> normalizer.transform(X)\\narray([[ 0.40..., -0.40...,  0.81...],\\n       [ 1.  ...,  0.  ...,  0.  ...],\\n       [ 0.  ...,  0.70..., -0.70...]])\\n\\n>>> normalizer.transform([[-1.,  1., 0.]])\\narray([[-0.70...,  0.70...,  0.  ...]])\\n\\n\\nNote: L2 normalization is also known as spatial sign preprocessing.\\n---------new doc---------\\nSparse input#\\nnormalize and Normalizer accept both dense array-like\\nand sparse matrices from scipy.sparse as input.\\nFor sparse input the data is converted to the Compressed Sparse Rows\\nrepresentation (see scipy.sparse.csr_matrix) before being fed to\\nefficient Cython routines. To avoid unnecessary memory copies, it is\\nrecommended to choose the CSR representation upstream.\\n\\n\\n\\n6.3.4. Encoding categorical features#\\nOften features are not given as continuous values but categorical.\\nFor example a person could have features [\\\"male\\\", \\\"female\\\"],\\n[\\\"from Europe\\\", \\\"from US\\\", \\\"from Asia\\\"],\\n[\\\"uses Firefox\\\", \\\"uses Chrome\\\", \\\"uses Safari\\\", \\\"uses Internet Explorer\\\"].\\nSuch features can be efficiently coded as integers, for instance\\n[\\\"male\\\", \\\"from US\\\", \\\"uses Internet Explorer\\\"] could be expressed as\\n[0, 1, 3] while [\\\"female\\\", \\\"from Asia\\\", \\\"uses Chrome\\\"] would be\\n[1, 2, 1].\\nTo convert categorical features to such integer codes, we can use the\\nOrdinalEncoder. This estimator transforms each categorical feature to one\\nnew feature of integers (0 to n_categories - 1):\\n>>> enc = preprocessing.OrdinalEncoder()\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOrdinalEncoder()\\n>>> enc.transform([['female', 'from US', 'uses Safari']])\\narray([[0., 1., 1.]])\\n---------new doc---------\\nSuch integer representation can, however, not be used directly with all\\nscikit-learn estimators, as these expect continuous input, and would interpret\\nthe categories as being ordered, which is often not desired (i.e. the set of\\nbrowsers was ordered arbitrarily).\\nBy default, OrdinalEncoder will also passthrough missing values that\\nare indicated by np.nan.\\n>>> enc = preprocessing.OrdinalEncoder()\\n>>> X = [['male'], ['female'], [np.nan], ['female']]\\n>>> enc.fit_transform(X)\\narray([[ 1.],\\n       [ 0.],\\n       [nan],\\n       [ 0.]])\\n\\n\\nOrdinalEncoder provides a parameter encoded_missing_value to encode\\nthe missing values without the need to create a pipeline and using\\nSimpleImputer.\\n>>> enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\\n>>> X = [['male'], ['female'], [np.nan], ['female']]\\n>>> enc.fit_transform(X)\\narray([[ 1.],\\n       [ 0.],\\n       [-1.],\\n       [ 0.]])\\n---------new doc---------\\nThe above processing is equivalent to the following pipeline:\\n>>> from sklearn.pipeline import Pipeline\\n>>> from sklearn.impute import SimpleImputer\\n>>> enc = Pipeline(steps=[\\n...     (\\\"encoder\\\", preprocessing.OrdinalEncoder()),\\n...     (\\\"imputer\\\", SimpleImputer(strategy=\\\"constant\\\", fill_value=-1)),\\n... ])\\n>>> enc.fit_transform(X)\\narray([[ 1.],\\n       [ 0.],\\n       [-1.],\\n       [ 0.]])\\n\\n\\nAnother possibility to convert categorical features to features that can be used\\nwith scikit-learn estimators is to use a one-of-K, also known as one-hot or\\ndummy encoding.\\nThis type of encoding can be obtained with the OneHotEncoder,\\nwhich transforms each categorical feature with\\nn_categories possible values into n_categories binary features, with\\none of them 1, and all others 0.\\nContinuing the example above:\\n>>> enc = preprocessing.OneHotEncoder()\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOneHotEncoder()\\n>>> enc.transform([['female', 'from US', 'uses Safari'],\\n...                ['male', 'from Europe', 'uses Safari']]).toarray()\\narray([[1., 0., 0., 1., 0., 1.],\\n       [0., 1., 1., 0., 0., 1.]])\\n---------new doc---------\\nBy default, the values each feature can take is inferred automatically\\nfrom the dataset and can be found in the categories_ attribute:\\n>>> enc.categories_\\n[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)]\\n---------new doc---------\\nIt is possible to specify this explicitly using the parameter categories.\\nThere are two genders, four possible continents and four web browsers in our\\ndataset:\\n>>> genders = ['female', 'male']\\n>>> locations = ['from Africa', 'from Asia', 'from Europe', 'from US']\\n>>> browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\\n>>> enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\\n>>> # Note that for there are missing categorical values for the 2nd and 3rd\\n>>> # feature\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOneHotEncoder(categories=[['female', 'male'],\\n                          ['from Africa', 'from Asia', 'from Europe',\\n                           'from US'],\\n                          ['uses Chrome', 'uses Firefox', 'uses IE',\\n                           'uses Safari']])\\n>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\\narray([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])\\n---------new doc---------\\nIf there is a possibility that the training data might have missing categorical\\nfeatures, it can often be better to specify\\nhandle_unknown='infrequent_if_exist' instead of setting the categories\\nmanually as above. When handle_unknown='infrequent_if_exist' is specified\\nand unknown categories are encountered during transform, no error will be\\nraised but the resulting one-hot encoded columns for this feature will be all\\nzeros or considered as an infrequent category if enabled.\\n(handle_unknown='infrequent_if_exist' is only supported for one-hot\\nencoding):\\n>>> enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist')\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOneHotEncoder(handle_unknown='infrequent_if_exist')\\n>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\\narray([[1., 0., 0., 0., 0., 0.]])\\n---------new doc---------\\nIt is also possible to encode each column into n_categories - 1 columns\\ninstead of n_categories columns by using the drop parameter. This\\nparameter allows the user to specify a category for each feature to be dropped.\\nThis is useful to avoid co-linearity in the input matrix in some classifiers.\\nSuch functionality is useful, for example, when using non-regularized\\nregression (LinearRegression),\\nsince co-linearity would cause the covariance matrix to be non-invertible:\\n>>> X = [['male', 'from US', 'uses Safari'],\\n...      ['female', 'from Europe', 'uses Firefox']]\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)\\n>>> drop_enc.categories_\\n[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object),\\n array(['uses Firefox', 'uses Safari'], dtype=object)]\\n>>> drop_enc.transform(X).toarray()\\narray([[1., 1., 1.],\\n       [0., 0., 0.]])\\n---------new doc---------\\nOne might want to drop one of the two columns only for features with 2\\ncategories. In this case, you can set the parameter drop='if_binary'.\\n>>> X = [['male', 'US', 'Safari'],\\n...      ['female', 'Europe', 'Firefox'],\\n...      ['female', 'Asia', 'Chrome']]\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary').fit(X)\\n>>> drop_enc.categories_\\n[array(['female', 'male'], dtype=object), array(['Asia', 'Europe', 'US'], dtype=object),\\n array(['Chrome', 'Firefox', 'Safari'], dtype=object)]\\n>>> drop_enc.transform(X).toarray()\\narray([[1., 0., 0., 1., 0., 0., 1.],\\n       [0., 0., 1., 0., 0., 1., 0.],\\n       [0., 1., 0., 0., 1., 0., 0.]])\\n---------new doc---------\\nIn the transformed X, the first column is the encoding of the feature with\\ncategories “male”/”female”, while the remaining 6 columns is the encoding of\\nthe 2 features with respectively 3 categories each.\\nWhen handle_unknown='ignore' and drop is not None, unknown categories will\\nbe encoded as all zeros:\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='first',\\n...                                        handle_unknown='ignore').fit(X)\\n>>> X_test = [['unknown', 'America', 'IE']]\\n>>> drop_enc.transform(X_test).toarray()\\narray([[0., 0., 0., 0., 0.]])\\n---------new doc---------\\nAll the categories in X_test are unknown during transform and will be mapped\\nto all zeros. This means that unknown categories will have the same mapping as\\nthe dropped category. OneHotEncoder.inverse_transform will map all zeros\\nto the dropped category if a category is dropped and None if a category is\\nnot dropped:\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse_output=False,\\n...                                        handle_unknown='ignore').fit(X)\\n>>> X_test = [['unknown', 'America', 'IE']]\\n>>> X_trans = drop_enc.transform(X_test)\\n>>> X_trans\\narray([[0., 0., 0., 0., 0., 0., 0.]])\\n>>> drop_enc.inverse_transform(X_trans)\\narray([['female', None, None]], dtype=object)\\n---------new doc---------\\nSupport of categorical features with missing values#\\nOneHotEncoder supports categorical features with missing values by\\nconsidering the missing values as an additional category:\\n>>> X = [['male', 'Safari'],\\n...      ['female', None],\\n...      [np.nan, 'Firefox']]\\n>>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\\n>>> enc.categories_\\n[array(['female', 'male', nan], dtype=object),\\narray(['Firefox', 'Safari', None], dtype=object)]\\n>>> enc.transform(X).toarray()\\narray([[0., 1., 0., 0., 1., 0.],\\n      [1., 0., 0., 0., 0., 1.],\\n      [0., 0., 1., 1., 0., 0.]])\\n\\n\\nIf a feature contains both np.nan and None, they will be considered\\nseparate categories:\\n>>> X = [['Safari'], [None], [np.nan], ['Firefox']]\\n>>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\\n>>> enc.categories_\\n[array(['Firefox', 'Safari', None, nan], dtype=object)]\\n>>> enc.transform(X).toarray()\\narray([[0., 1., 0., 0.],\\n      [0., 0., 1., 0.],\\n      [0., 0., 0., 1.],\\n      [1., 0., 0., 0.]])\\n---------new doc---------\\nSee Loading features from dicts for categorical features that are\\nrepresented as a dict, not as scalars.\\n\\n\\n6.3.4.1. Infrequent categories#\\nOneHotEncoder and OrdinalEncoder support aggregating\\ninfrequent categories into a single output for each feature. The parameters to\\nenable the gathering of infrequent categories are min_frequency and\\nmax_categories.\\n\\nmin_frequency is either an  integer greater or equal to 1, or a float in\\nthe interval (0.0, 1.0). If min_frequency is an integer, categories with\\na cardinality smaller than min_frequency  will be considered infrequent.\\nIf min_frequency is a float, categories with a cardinality smaller than\\nthis fraction of the total number of samples will be considered infrequent.\\nThe default value is 1, which means every category is encoded separately.\\nmax_categories is either None or any integer greater than 1. This\\nparameter sets an upper limit to the number of output features for each\\ninput feature. max_categories includes the feature that combines\\ninfrequent categories.\\n---------new doc---------\\nIn the following example with OrdinalEncoder, the categories 'dog' and\\n'snake' are considered infrequent:\\n>>> X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 +\\n...               ['snake'] * 3], dtype=object).T\\n>>> enc = preprocessing.OrdinalEncoder(min_frequency=6).fit(X)\\n>>> enc.infrequent_categories_\\n[array(['dog', 'snake'], dtype=object)]\\n>>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\\narray([[2.],\\n       [0.],\\n       [1.],\\n       [2.]])\\n---------new doc---------\\nOrdinalEncoder’s max_categories do not take into account missing\\nor unknown categories. Setting unknown_value or encoded_missing_value to an\\ninteger will increase the number of unique integer codes by one each. This can\\nresult in up to max_categories + 2 integer codes. In the following example,\\n“a” and “d” are considered infrequent and grouped together into a single\\ncategory, “b” and “c” are their own categories, unknown values are encoded as 3\\nand missing values are encoded as 4.\\n>>> X_train = np.array(\\n...     [[\\\"a\\\"] * 5 + [\\\"b\\\"] * 20 + [\\\"c\\\"] * 10 + [\\\"d\\\"] * 3 + [np.nan]],\\n...     dtype=object).T\\n>>> enc = preprocessing.OrdinalEncoder(\\n...     handle_unknown=\\\"use_encoded_value\\\", unknown_value=3,\\n...     max_categories=3, encoded_missing_value=4)\\n>>> _ = enc.fit(X_train)\\n>>> X_test = np.array([[\\\"a\\\"], [\\\"b\\\"], [\\\"c\\\"], [\\\"d\\\"], [\\\"e\\\"], [np.nan]], dtype=object)\\n>>> enc.transform(X_test)\\narray([[2.],\\n       [0.],\\n       [1.],\\n       [2.],\\n       [3.],\\n       [4.]])\\n---------new doc---------\\nSimilarity, OneHotEncoder can be configured to group together infrequent\\ncategories:\\n>>> enc = preprocessing.OneHotEncoder(min_frequency=6, sparse_output=False).fit(X)\\n>>> enc.infrequent_categories_\\n[array(['dog', 'snake'], dtype=object)]\\n>>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\\narray([[0., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.],\\n       [0., 0., 1.]])\\n\\n\\nBy setting handle_unknown to 'infrequent_if_exist', unknown categories will\\nbe considered infrequent:\\n>>> enc = preprocessing.OneHotEncoder(\\n...    handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=6)\\n>>> enc = enc.fit(X)\\n>>> enc.transform(np.array([['dragon']]))\\narray([[0., 0., 1.]])\\n\\n\\nOneHotEncoder.get_feature_names_out uses ‘infrequent’ as the infrequent\\nfeature name:\\n>>> enc.get_feature_names_out()\\narray(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)\\n\\n\\nWhen 'handle_unknown' is set to 'infrequent_if_exist' and an unknown\\ncategory is encountered in transform:\\n---------new doc---------\\nWhen 'handle_unknown' is set to 'infrequent_if_exist' and an unknown\\ncategory is encountered in transform:\\n\\nIf infrequent category support was not configured or there was no\\ninfrequent category during training, the resulting one-hot encoded columns\\nfor this feature will be all zeros. In the inverse transform, an unknown\\ncategory will be denoted as None.\\nIf there is an infrequent category during training, the unknown category\\nwill be considered infrequent. In the inverse transform, ‘infrequent_sklearn’\\nwill be used to represent the infrequent category.\\n\\nInfrequent categories can also be configured using max_categories. In the\\nfollowing example, we set max_categories=2 to limit the number of features in\\nthe output. This will result in all but the 'cat' category to be considered\\ninfrequent, leading to two features, one for 'cat' and one for infrequent\\ncategories - which are all the others:\\n>>> enc = preprocessing.OneHotEncoder(max_categories=2, sparse_output=False)\\n>>> enc = enc.fit(X)\\n>>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\\narray([[0., 1.],\\n       [1., 0.],\\n       [0., 1.],\\n       [0., 1.]])\\n---------new doc---------\\nIf both max_categories and min_frequency are non-default values, then\\ncategories are selected based on min_frequency first and max_categories\\ncategories are kept. In the following example, min_frequency=4 considers\\nonly snake to be infrequent, but max_categories=3, forces dog to also be\\ninfrequent:\\n>>> enc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse_output=False)\\n>>> enc = enc.fit(X)\\n>>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\\narray([[0., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.],\\n       [0., 0., 1.]])\\n\\n\\nIf there are infrequent categories with the same cardinality at the cutoff of\\nmax_categories, then then the first max_categories are taken based on lexicon\\nordering. In the following example, “b”, “c”, and “d”, have the same cardinality\\nand with max_categories=2, “b” and “c” are infrequent because they have a higher\\nlexicon order.\\n>>> X = np.asarray([[\\\"a\\\"] * 20 + [\\\"b\\\"] * 10 + [\\\"c\\\"] * 10 + [\\\"d\\\"] * 10], dtype=object).T\\n>>> enc = preprocessing.OneHotEncoder(max_categories=3).fit(X)\\n>>> enc.infrequent_categories_\\n[array(['b', 'c'], dtype=object)]\\n---------new doc---------\\n6.3.4.2. Target Encoder#\\nThe TargetEncoder uses the target mean conditioned on the categorical\\nfeature for encoding unordered categories, i.e. nominal categories [PAR]\\n[MIC]. This encoding scheme is useful with categorical features with high\\ncardinality, where one-hot encoding would inflate the feature space making it\\nmore expensive for a downstream model to process. A classical example of high\\ncardinality categories are location based such as zip code or region.\\n\\n\\nBinary classification targets#\\nFor the binary classification target, the target encoding is given by:\\n\\n\\\\[S_i = \\\\lambda_i\\\\frac{n_{iY}}{n_i} + (1 - \\\\lambda_i)\\\\frac{n_Y}{n}\\\\]\\nwhere \\\\(S_i\\\\) is the encoding for category \\\\(i\\\\), \\\\(n_{iY}\\\\) is the\\nnumber of observations with \\\\(Y=1\\\\) and category \\\\(i\\\\), \\\\(n_i\\\\) is\\nthe number of observations with category \\\\(i\\\\), \\\\(n_Y\\\\) is the number of\\nobservations with \\\\(Y=1\\\\), \\\\(n\\\\) is the number of observations, and\\n\\\\(\\\\lambda_i\\\\) is a shrinkage factor for category \\\\(i\\\\). The shrinkage\\nfactor is given by:\\n---------new doc---------\\nfit_transform also learns a ‘full data’ encoding using\\nthe whole training set. This is never used in\\nfit_transform but is saved to the attribute encodings_,\\nfor use when transform is called. Note that the encodings\\nlearned for each fold during the cross fitting scheme are not saved to\\nan attribute.\\nThe fit method does not use any cross fitting\\nschemes and learns one encoding on the entire training set, which is used to\\nencode categories in transform.\\nThis encoding is the same as the ‘full data’\\nencoding learned in fit_transform.\\n\\nNote\\nTargetEncoder considers missing values, such as np.nan or None,\\nas another category and encodes them like any other category. Categories\\nthat are not seen during fit are encoded with the target mean, i.e.\\ntarget_mean_.\\n\\nExamples\\n\\nComparing Target Encoder with Other Encoders\\nTarget Encoder’s Internal Cross fitting\\n\\nReferences\\n\\n\\n[MIC]\\nMicci-Barreca, Daniele. “A preprocessing scheme for high-cardinality\\ncategorical attributes in classification and prediction problems”\\nSIGKDD Explor. Newsl. 3, 1 (July 2001), 27-32.\\n\\n\\n[PAR]\\nPargent, F., Pfisterer, F., Thomas, J. et al. “Regularized target\\nencoding outperforms traditional methods in supervised machine learning with\\nhigh cardinality features” Comput Stat 37, 2671-2692 (2022)\\n---------new doc---------\\nBy default the output is one-hot encoded into a sparse matrix\\n(See Encoding categorical features)\\nand this can be configured with the encode parameter.\\nFor each feature, the bin edges are computed during fit and together with\\nthe number of bins, they will define the intervals. Therefore, for the current\\nexample, these intervals are defined as:\\n\\nfeature 1: \\\\({[-\\\\infty, -1), [-1, 2), [2, \\\\infty)}\\\\)\\nfeature 2: \\\\({[-\\\\infty, 5), [5, \\\\infty)}\\\\)\\nfeature 3: \\\\({[-\\\\infty, 14), [14, \\\\infty)}\\\\)\\n\\nBased on these bin intervals, X is transformed as follows:\\n>>> est.transform(X)                      \\narray([[ 0., 1., 1.],\\n       [ 1., 1., 1.],\\n       [ 2., 0., 0.]])\\n---------new doc---------\\nThe resulting dataset contains ordinal attributes which can be further used\\nin a Pipeline.\\nDiscretization is similar to constructing histograms for continuous data.\\nHowever, histograms focus on counting features which fall into particular\\nbins, whereas discretization focuses on assigning feature values to these bins.\\nKBinsDiscretizer implements different binning strategies, which can be\\nselected with the strategy parameter. The ‘uniform’ strategy uses\\nconstant-width bins. The ‘quantile’ strategy uses the quantiles values to have\\nequally populated bins in each feature. The ‘kmeans’ strategy defines bins based\\non a k-means clustering procedure performed on each feature independently.\\nBe aware that one can specify custom bins by passing a callable defining the\\ndiscretization strategy to FunctionTransformer.\\nFor instance, we can use the Pandas function pandas.cut:\\n>>> import pandas as pd\\n>>> import numpy as np\\n>>> from sklearn import preprocessing\\n>>>\\n>>> bins = [0, 1, 13, 20, 60, np.inf]\\n>>> labels = ['infant', 'kid', 'teen', 'adult', 'senior citizen']\\n>>> transformer = preprocessing.FunctionTransformer(\\n...     pd.cut, kw_args={'bins': bins, 'labels': labels, 'retbins': False}\\n... )\\n>>> X = np.array([0.2, 2, 15, 25, 97])\\n>>> transformer.fit_transform(X)\\n['infant', 'kid', 'teen', 'adult', 'senior citizen']\\nCategories (5, object): ['infant' < 'kid' < 'teen' < 'adult' < 'senior citizen']\\n\\n\\nExamples\\n---------new doc---------\\nExamples\\n\\nUsing KBinsDiscretizer to discretize continuous features\\nFeature discretization\\nDemonstrating the different strategies of KBinsDiscretizer\\n\\n\\n\\n6.3.5.2. Feature binarization#\\nFeature binarization is the process of thresholding numerical\\nfeatures to get boolean values. This can be useful for downstream\\nprobabilistic estimators that make assumption that the input data\\nis distributed according to a multi-variate Bernoulli distribution. For instance,\\nthis is the case for the BernoulliRBM.\\nIt is also common among the text processing community to use binary\\nfeature values (probably to simplify the probabilistic reasoning) even\\nif normalized counts (a.k.a. term frequencies) or TF-IDF valued features\\noften perform slightly better in practice.\\nAs for the Normalizer, the utility class\\nBinarizer is meant to be used in the early stages of\\nPipeline. The fit method does nothing\\nas each sample is treated independently of others:\\n>>> X = [[ 1., -1.,  2.],\\n...      [ 2.,  0.,  0.],\\n...      [ 0.,  1., -1.]]\\n\\n>>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\\n>>> binarizer\\nBinarizer()\\n\\n>>> binarizer.transform(X)\\narray([[1., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.]])\\n---------new doc---------\\n>>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\\n>>> binarizer\\nBinarizer()\\n\\n>>> binarizer.transform(X)\\narray([[1., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.]])\\n\\n\\nIt is possible to adjust the threshold of the binarizer:\\n>>> binarizer = preprocessing.Binarizer(threshold=1.1)\\n>>> binarizer.transform(X)\\narray([[0., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 0., 0.]])\\n\\n\\nAs for the Normalizer class, the preprocessing module\\nprovides a companion function binarize\\nto be used when the transformer API is not necessary.\\nNote that the Binarizer is similar to the KBinsDiscretizer\\nwhen k = 2, and when the bin edge is at the value threshold.\\n\\nSparse input\\nbinarize and Binarizer accept both dense array-like\\nand sparse matrices from scipy.sparse as input.\\nFor sparse input the data is converted to the Compressed Sparse Rows\\nrepresentation (see scipy.sparse.csr_matrix).\\nTo avoid unnecessary memory copies, it is recommended to choose the CSR\\nrepresentation upstream.\\n\\n\\n\\n\\n6.3.6. Imputation of missing values#\\nTools for imputing missing values are discussed at Imputation of missing values.\\n---------new doc---------\\n6.3.6. Imputation of missing values#\\nTools for imputing missing values are discussed at Imputation of missing values.\\n\\n\\n6.3.7. Generating polynomial features#\\nOften it’s useful to add complexity to a model by considering nonlinear\\nfeatures of the input data. We show two possibilities that are both based on\\npolynomials: The first one uses pure polynomials, the second one uses splines,\\ni.e. piecewise polynomials.\\n\\n6.3.7.1. Polynomial features#\\nA simple and common method to use is polynomial features, which can get\\nfeatures’ high-order and interaction terms. It is implemented in\\nPolynomialFeatures:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> X = np.arange(6).reshape(3, 2)\\n>>> X\\narray([[0, 1],\\n       [2, 3],\\n       [4, 5]])\\n>>> poly = PolynomialFeatures(2)\\n>>> poly.fit_transform(X)\\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\\n       [ 1.,  4.,  5., 16., 20., 25.]])\\n---------new doc---------\\nThe features of X have been transformed from \\\\((X_1, X_2)\\\\) to\\n\\\\((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\\\\).\\nIn some cases, only interaction terms among features are required, and it can\\nbe gotten with the setting interaction_only=True:\\n>>> X = np.arange(9).reshape(3, 3)\\n>>> X\\narray([[0, 1, 2],\\n       [3, 4, 5],\\n       [6, 7, 8]])\\n>>> poly = PolynomialFeatures(degree=3, interaction_only=True)\\n>>> poly.fit_transform(X)\\narray([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],\\n       [  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],\\n       [  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])\\n---------new doc---------\\nThe features of X have been transformed from \\\\((X_1, X_2, X_3)\\\\) to\\n\\\\((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\\\\).\\nNote that polynomial features are used implicitly in kernel methods (e.g., SVC,\\nKernelPCA) when using polynomial Kernel functions.\\nSee Polynomial and Spline interpolation\\nfor Ridge regression using created polynomial features.\\n\\n\\n6.3.7.2. Spline transformer#\\nAnother way to add nonlinear terms instead of pure polynomials of features is\\nto generate spline basis functions for each feature with the\\nSplineTransformer. Splines are piecewise polynomials, parametrized by\\ntheir polynomial degree and the positions of the knots. The\\nSplineTransformer implements a B-spline basis, cf. the references\\nbelow.\\n\\nNote\\nThe SplineTransformer treats each feature separately, i.e. it\\nwon’t give you interaction terms.\\n\\nSome of the advantages of splines over polynomials are:\\n---------new doc---------\\nNote\\nThe SplineTransformer treats each feature separately, i.e. it\\nwon’t give you interaction terms.\\n\\nSome of the advantages of splines over polynomials are:\\n\\nB-splines are very flexible and robust if you keep a fixed low degree,\\nusually 3, and parsimoniously adapt the number of knots. Polynomials\\nwould need a higher degree, which leads to the next point.\\nB-splines do not have oscillatory behaviour at the boundaries as have\\npolynomials (the higher the degree, the worse). This is known as Runge’s\\nphenomenon.\\nB-splines provide good options for extrapolation beyond the boundaries,\\ni.e. beyond the range of fitted values. Have a look at the option\\nextrapolation.\\nB-splines generate a feature matrix with a banded structure. For a single\\nfeature, every row contains only degree + 1 non-zero elements, which\\noccur consecutively and are even positive. This results in a matrix with\\ngood numerical properties, e.g. a low condition number, in sharp contrast\\nto a matrix of polynomials, which goes under the name\\nVandermonde matrix.\\nA low condition number is important for stable algorithms of linear\\nmodels.\\n---------new doc---------\\nThe following code snippet shows splines in action:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import SplineTransformer\\n>>> X = np.arange(5).reshape(5, 1)\\n>>> X\\narray([[0],\\n       [1],\\n       [2],\\n       [3],\\n       [4]])\\n>>> spline = SplineTransformer(degree=2, n_knots=3)\\n>>> spline.fit_transform(X)\\narray([[0.5  , 0.5  , 0.   , 0.   ],\\n       [0.125, 0.75 , 0.125, 0.   ],\\n       [0.   , 0.5  , 0.5  , 0.   ],\\n       [0.   , 0.125, 0.75 , 0.125],\\n       [0.   , 0.   , 0.5  , 0.5  ]])\\n\\n\\nAs the X is sorted, one can easily see the banded matrix output. Only the\\nthree middle diagonals are non-zero for degree=2. The higher the degree,\\nthe more overlapping of the splines.\\nInterestingly, a SplineTransformer of degree=0 is the same as\\nKBinsDiscretizer with\\nencode='onehot-dense' and n_bins = n_knots - 1 if\\nknots = strategy.\\nExamples\\n\\nPolynomial and Spline interpolation\\nTime-related feature engineering\\n\\n\\n\\nReferences#\\n---------new doc---------\\nPolynomial and Spline interpolation\\nTime-related feature engineering\\n\\n\\n\\nReferences#\\n\\nEilers, P., & Marx, B. (1996). Flexible Smoothing with B-splines and\\nPenalties. Statist. Sci. 11 (1996), no. 2, 89–121.\\nPerperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. A review of\\nspline function procedures in R.\\nBMC Med Res Methodol 19, 46 (2019).\\n\\n\\n\\n\\n\\n6.3.8. Custom transformers#\\nOften, you will want to convert an existing Python function into a transformer\\nto assist in data cleaning or processing. You can implement a transformer from\\nan arbitrary function with FunctionTransformer. For example, to build\\na transformer that applies a log transformation in a pipeline, do:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import FunctionTransformer\\n>>> transformer = FunctionTransformer(np.log1p, validate=True)\\n>>> X = np.array([[0, 1], [2, 3]])\\n>>> # Since FunctionTransformer is no-op during fit, we can call transform directly\\n>>> transformer.transform(X)\\narray([[0.        , 0.69314718],\\n       [1.09861229, 1.38629436]])\\n---------new doc---------\\nYou can ensure that func and inverse_func are the inverse of each other\\nby setting check_inverse=True and calling fit before\\ntransform. Please note that a warning is raised and can be turned into an\\nerror with a filterwarnings:\\n>>> import warnings\\n>>> warnings.filterwarnings(\\\"error\\\", message=\\\".*check_inverse*.\\\",\\n...                         category=UserWarning, append=False)\\n\\n\\nFor a full code example that demonstrates using a FunctionTransformer\\nto extract features from text data see\\nColumn Transformer with Heterogeneous Data Sources and\\nTime-related feature engineering.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n6.2. Feature extraction\\n\\n\\n\\n\\nnext\\n6.4. Imputation of missing values\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.4. Imputation of missing values\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6.4. Imputation of missing values#\\nFor various reasons, many real world datasets contain missing values, often\\nencoded as blanks, NaNs or other placeholders. Such datasets however are\\nincompatible with scikit-learn estimators which assume that all values in an\\narray are numerical, and that all have and hold meaning. A basic strategy to\\nuse incomplete datasets is to discard entire rows and/or columns containing\\nmissing values. However, this comes at the price of losing data which may be\\nvaluable (even though incomplete). A better strategy is to impute the missing\\nvalues, i.e., to infer them from the known part of the data. See the\\nglossary entry on imputation.\\n---------new doc---------\\n6.4.1. Univariate vs. Multivariate Imputation#\\nOne type of imputation algorithm is univariate, which imputes values in the\\ni-th feature dimension using only non-missing values in that feature dimension\\n(e.g. SimpleImputer). By contrast, multivariate imputation\\nalgorithms use the entire set of available feature dimensions to estimate the\\nmissing values (e.g. IterativeImputer).\\n\\n\\n6.4.2. Univariate feature imputation#\\nThe SimpleImputer class provides basic strategies for imputing missing\\nvalues. Missing values can be imputed with a provided constant value, or using\\nthe statistics (mean, median or most frequent) of each column in which the\\nmissing values are located. This class also allows for different missing values\\nencodings.\\nThe following snippet demonstrates how to replace missing values,\\nencoded as np.nan, using the mean value of the columns (axis 0)\\nthat contain the missing values:\\n>>> import numpy as np\\n>>> from sklearn.impute import SimpleImputer\\n>>> imp = SimpleImputer(missing_values=np.nan, strategy='mean')\\n>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])\\nSimpleImputer()\\n>>> X = [[np.nan, 2], [6, np.nan], [7, 6]]\\n>>> print(imp.transform(X))\\n[[4.          2.        ]\\n [6.          3.666...]\\n [7.          6.        ]]\\n---------new doc---------\\nThe SimpleImputer class also supports sparse matrices:\\n>>> import scipy.sparse as sp\\n>>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])\\n>>> imp = SimpleImputer(missing_values=-1, strategy='mean')\\n>>> imp.fit(X)\\nSimpleImputer(missing_values=-1)\\n>>> X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])\\n>>> print(imp.transform(X_test).toarray())\\n[[3. 2.]\\n [6. 3.]\\n [7. 6.]]\\n---------new doc---------\\nNote that this format is not meant to be used to implicitly store missing\\nvalues in the matrix because it would densify it at transform time. Missing\\nvalues encoded by 0 must be used with dense input.\\nThe SimpleImputer class also supports categorical data represented as\\nstring values or pandas categoricals when using the 'most_frequent' or\\n'constant' strategy:\\n>>> import pandas as pd\\n>>> df = pd.DataFrame([[\\\"a\\\", \\\"x\\\"],\\n...                    [np.nan, \\\"y\\\"],\\n...                    [\\\"a\\\", np.nan],\\n...                    [\\\"b\\\", \\\"y\\\"]], dtype=\\\"category\\\")\\n...\\n>>> imp = SimpleImputer(strategy=\\\"most_frequent\\\")\\n>>> print(imp.fit_transform(df))\\n[['a' 'x']\\n ['a' 'y']\\n ['a' 'y']\\n ['b' 'y']]\\n\\n\\nFor another example on usage, see Imputing missing values before building an estimator.\\n---------new doc---------\\nFor another example on usage, see Imputing missing values before building an estimator.\\n\\n\\n6.4.3. Multivariate feature imputation#\\nA more sophisticated approach is to use the IterativeImputer class,\\nwhich models each feature with missing values as a function of other features,\\nand uses that estimate for imputation. It does so in an iterated round-robin\\nfashion: at each step, a feature column is designated as output y and the\\nother feature columns are treated as inputs X. A regressor is fit on (X,\\ny) for known y. Then, the regressor is used to predict the missing values\\nof y.  This is done for each feature in an iterative fashion, and then is\\nrepeated for max_iter imputation rounds. The results of the final\\nimputation round are returned.\\n\\nNote\\nThis estimator is still experimental for now: default parameters or\\ndetails of behaviour might change without any deprecation cycle. Resolving\\nthe following issues would help stabilize IterativeImputer:\\nconvergence criteria (#14338) and default estimators\\n(#13286). To use it, you need to explicitly import\\nenable_iterative_imputer.\\n---------new doc---------\\nBoth SimpleImputer and IterativeImputer can be used in a\\nPipeline as a way to build a composite estimator that supports imputation.\\nSee Imputing missing values before building an estimator.\\n\\n6.4.3.1. Flexibility of IterativeImputer#\\nThere are many well-established imputation packages in the R data science\\necosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns\\nout to be a particular instance of different sequential imputation algorithms\\nthat can all be implemented with IterativeImputer by passing in\\ndifferent regressors to be used for predicting missing feature values. In the\\ncase of missForest, this regressor is a Random Forest.\\nSee Imputing missing values with variants of IterativeImputer.\\n---------new doc---------\\n>>> from sklearn.impute import KNNImputer\\n>>> nan = np.nan\\n>>> X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\\n>>> imputer = KNNImputer(n_neighbors=2, weights=\\\"uniform\\\")\\n>>> imputer.fit_transform(X)\\narray([[1. , 2. , 4. ],\\n       [3. , 4. , 3. ],\\n       [5.5, 6. , 5. ],\\n       [8. , 8. , 7. ]])\\n---------new doc---------\\nFor another example on usage, see Imputing missing values before building an estimator.\\nReferences\\n\\n\\n[OL2001]\\nOlga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown,\\nTrevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman,\\nMissing value estimation methods for DNA microarrays, BIOINFORMATICS\\nVol. 17 no. 6, 2001 Pages 520-525.\\n\\n\\n\\n\\n6.4.5. Keeping the number of features constant#\\nBy default, the scikit-learn imputers will drop fully empty features, i.e.\\ncolumns containing only missing values. For instance:\\n>>> imputer = SimpleImputer()\\n>>> X = np.array([[np.nan, 1], [np.nan, 2], [np.nan, 3]])\\n>>> imputer.fit_transform(X)\\narray([[1.],\\n       [2.],\\n       [3.]])\\n\\n\\nThe first feature in X containing only np.nan was dropped after the\\nimputation. While this feature will not help in predictive setting, dropping\\nthe columns will change the shape of X which could be problematic when using\\nimputers in a more complex machine-learning pipeline. The parameter\\nkeep_empty_features offers the option to keep the empty features by imputing\\nwith a constant values. In most of the cases, this constant value is zero:\\n>>> imputer.set_params(keep_empty_features=True)\\nSimpleImputer(keep_empty_features=True)\\n>>> imputer.fit_transform(X)\\narray([[0., 1.],\\n       [0., 2.],\\n       [0., 3.]])\\n---------new doc---------\\n6.4.6. Marking imputed values#\\nThe MissingIndicator transformer is useful to transform a dataset into\\ncorresponding binary matrix indicating the presence of missing values in the\\ndataset. This transformation is useful in conjunction with imputation. When\\nusing imputation, preserving the information about which values had been\\nmissing can be informative. Note that both the SimpleImputer and\\nIterativeImputer have the boolean parameter add_indicator\\n(False by default) which when set to True provides a convenient way of\\nstacking the output of the MissingIndicator transformer with the\\noutput of the imputer.\\nNaN is usually used as the placeholder for missing values. However, it\\nenforces the data type to be float. The parameter missing_values allows to\\nspecify other placeholder such as integer. In the following example, we will\\nuse -1 as missing values:\\n>>> from sklearn.impute import MissingIndicator\\n>>> X = np.array([[-1, -1, 1, 3],\\n...               [4, -1, 0, -1],\\n...               [8, -1, 1, 0]])\\n>>> indicator = MissingIndicator(missing_values=-1)\\n>>> mask_missing_values_only = indicator.fit_transform(X)\\n>>> mask_missing_values_only\\narray([[ True,  True, False],\\n       [False,  True,  True],\\n       [False,  True, False]])\\n---------new doc---------\\nThe features parameter is used to choose the features for which the mask is\\nconstructed. By default, it is 'missing-only' which returns the imputer\\nmask of the features containing missing values at fit time:\\n>>> indicator.features_\\narray([0, 1, 3])\\n\\n\\nThe features parameter can be set to 'all' to return all features\\nwhether or not they contain missing values:\\n>>> indicator = MissingIndicator(missing_values=-1, features=\\\"all\\\")\\n>>> mask_all = indicator.fit_transform(X)\\n>>> mask_all\\narray([[ True,  True, False, False],\\n       [False,  True, False,  True],\\n       [False,  True, False, False]])\\n>>> indicator.features_\\narray([0, 1, 2, 3])\\n---------new doc---------\\nWhen using the MissingIndicator in a\\nPipeline, be sure to use the\\nFeatureUnion or\\nColumnTransformer to add the indicator features to\\nthe regular features. First we obtain the iris dataset, and add some missing\\nvalues to it.\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.impute import SimpleImputer, MissingIndicator\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn.pipeline import FeatureUnion, make_pipeline\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> X, y = load_iris(return_X_y=True)\\n>>> mask = np.random.randint(0, 2, size=X.shape).astype(bool)\\n>>> X[mask] = np.nan\\n>>> X_train, X_test, y_train, _ = train_test_split(X, y, test_size=100,\\n...                                                random_state=0)\\n---------new doc---------\\nNow we create a FeatureUnion. All features will be\\nimputed using SimpleImputer, in order to enable classifiers to work\\nwith this data. Additionally, it adds the indicator variables from\\nMissingIndicator.\\n>>> transformer = FeatureUnion(\\n...     transformer_list=[\\n...         ('features', SimpleImputer(strategy='mean')),\\n...         ('indicators', MissingIndicator())])\\n>>> transformer = transformer.fit(X_train, y_train)\\n>>> results = transformer.transform(X_test)\\n>>> results.shape\\n(100, 8)\\n\\n\\nOf course, we cannot use the transformer to make any predictions. We should\\nwrap this in a Pipeline with a classifier (e.g., a\\nDecisionTreeClassifier) to be able to make predictions.\\n>>> clf = make_pipeline(transformer, DecisionTreeClassifier())\\n>>> clf = clf.fit(X_train, y_train)\\n>>> results = clf.predict(X_test)\\n>>> results.shape\\n(100,)\\n\\n\\n\\n\\n6.4.7. Estimators that handle NaN values#\\nSome estimators are designed to handle NaN values without preprocessing.\\nBelow is the list of these estimators, classified by type\\n(cluster, regressor, classifier, transform):\\n\\nEstimators that allow NaN values for type cluster:\\n\\nHDBSCAN\\n\\n\\nEstimators that allow NaN values for type regressor:\\n\\nBaggingRegressor\\nDecisionTreeRegressor\\nExtraTreeRegressor\\nExtraTreesRegressor\\nHistGradientBoostingRegressor\\nRandomForestRegressor\\nStackingRegressor\\nVotingRegressor\\n---------new doc---------\\n6.6.4. Inverse Transform#\\nThe random projection transformers have compute_inverse_components parameter. When\\nset to True, after creating the random components_ matrix during fitting,\\nthe transformer computes the pseudo-inverse of this matrix and stores it as\\ninverse_components_. The inverse_components_ matrix has shape\\n\\\\(n_{features} \\\\times n_{components}\\\\), and it is always a dense matrix,\\nregardless of whether the components matrix is sparse or dense. So depending on\\nthe number of features and components, it may use a lot of memory.\\nWhen the inverse_transform method is called, it computes the product of the\\ninput X and the transpose of the inverse components. If the inverse components have\\nbeen computed during fit, they are reused at each call to inverse_transform.\\nOtherwise they are recomputed each time, which can be costly. The result is always\\ndense, even if X is sparse.\\nHere a small code example which illustrates how to use the inverse transform\\nfeature:\\n>>> import numpy as np\\n>>> from sklearn.random_projection import SparseRandomProjection\\n>>> X = np.random.rand(100, 10000)\\n>>> transformer = SparseRandomProjection(\\n...   compute_inverse_components=True\\n... )\\n...\\n>>> X_new = transformer.fit_transform(X)\\n>>> X_new.shape\\n(100, 3947)\\n>>> X_new_inversed = transformer.inverse_transform(X_new)\\n>>> X_new_inversed.shape\\n(100, 10000)\\n>>> X_new_again = transformer.transform(X_new_inversed)\\n>>> np.allclose(X_new, X_new_again)\\nTrue\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Your job is to give a detailed summary of the following documents:\\n\\nDocuments:\\nPlot Ridge coefficients as a function of the regularization\\nClassification of text documents using sparse features\\nCommon pitfalls in the interpretation of coefficients of linear models\\n\\n\\n\\n1.1.2.3. Ridge Complexity#\\nThis method has the same order of complexity as\\nOrdinary Least Squares.\\n\\n\\n1.1.2.4. Setting the regularization parameter: leave-one-out Cross-Validation#\\nRidgeCV and RidgeClassifierCV implement ridge\\nregression/classification with built-in cross-validation of the alpha parameter.\\nThey work in the same way as GridSearchCV except\\nthat it defaults to efficient Leave-One-Out cross-validation.\\nWhen using the default cross-validation, alpha cannot be 0 due to the\\nformulation used to calculate Leave-One-Out error. See [RL2007] for details.\\nUsage example:\\n>>> import numpy as np\\n>>> from sklearn import linear_model\\n>>> reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\\n>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\\nRidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\\n      1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\\n>>> reg.alpha_\\n0.01\\n---------new doc---------\\nHere is an example of applying this idea to one-dimensional data, using\\npolynomial features of varying degrees:\\n\\n\\n\\n\\nThis figure is created using the PolynomialFeatures transformer, which\\ntransforms an input data matrix into a new data matrix of a given degree.\\nIt can be used as follows:\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> import numpy as np\\n>>> X = np.arange(6).reshape(3, 2)\\n>>> X\\narray([[0, 1],\\n       [2, 3],\\n       [4, 5]])\\n>>> poly = PolynomialFeatures(degree=2)\\n>>> poly.fit_transform(X)\\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\\n       [ 1.,  4.,  5., 16., 20., 25.]])\\n---------new doc---------\\nThe features of X have been transformed from \\\\([x_1, x_2]\\\\) to\\n\\\\([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\\\\), and can now be used within\\nany linear model.\\nThis sort of preprocessing can be streamlined with the\\nPipeline tools. A single object representing a simple\\npolynomial regression can be created and used as follows:\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> from sklearn.linear_model import LinearRegression\\n>>> from sklearn.pipeline import Pipeline\\n>>> import numpy as np\\n>>> model = Pipeline([('poly', PolynomialFeatures(degree=3)),\\n...                   ('linear', LinearRegression(fit_intercept=False))])\\n>>> # fit to an order-3 polynomial data\\n>>> x = np.arange(5)\\n>>> y = 3 - 2 * x + x ** 2 - x ** 3\\n>>> model = model.fit(x[:, np.newaxis], y)\\n>>> model.named_steps['linear'].coef_\\narray([ 3., -2.,  1., -1.])\\n---------new doc---------\\nThe linear model trained on polynomial features is able to exactly recover\\nthe input polynomial coefficients.\\nIn some cases it’s not necessary to include higher powers of any single feature,\\nbut only the so-called interaction features\\nthat multiply together at most \\\\(d\\\\) distinct features.\\nThese can be gotten from PolynomialFeatures with the setting\\ninteraction_only=True.\\nFor example, when dealing with boolean features,\\n\\\\(x_i^n = x_i\\\\) for all \\\\(n\\\\) and is therefore useless;\\nbut \\\\(x_i x_j\\\\) represents the conjunction of two booleans.\\nThis way, we can solve the XOR problem with a linear classifier:\\n>>> from sklearn.linear_model import Perceptron\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> import numpy as np\\n>>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\\n>>> y = X[:, 0] ^ X[:, 1]\\n>>> y\\narray([0, 1, 1, 0])\\n>>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)\\n>>> X\\narray([[1, 0, 0, 0],\\n       [1, 0, 1, 0],\\n       [1, 1, 0, 0],\\n       [1, 1, 1, 1]])\\n>>> clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,\\n...                  shuffle=False).fit(X, y)\\n---------new doc---------\\n0 to n is “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . .\\n. “n-1 vs n”.\\nThe shape of dual_coef_ is (n_classes-1, n_SV) with\\na somewhat hard to grasp layout.\\nThe columns correspond to the support vectors involved in any\\nof the n_classes * (n_classes - 1) / 2 “one-vs-one” classifiers.\\nEach support vector v has a dual coefficient in each of the\\nn_classes - 1 classifiers comparing the class of v against another class.\\nNote that some, but not all, of these dual coefficients, may be zero.\\nThe n_classes - 1 entries in each column are these dual coefficients,\\nordered by the opposing class.\\nThis might be clearer with an example: consider a three class problem with\\nclass 0 having three support vectors\\n\\\\(v^{0}_0, v^{1}_0, v^{2}_0\\\\) and class 1 and 2 having two support vectors\\n\\\\(v^{0}_1, v^{1}_1\\\\) and \\\\(v^{0}_2, v^{1}_2\\\\) respectively.  For each\\nsupport vector \\\\(v^{j}_i\\\\), there are two dual coefficients.  Let’s call\\nthe coefficient of support vector \\\\(v^{j}_i\\\\) in the classifier between\\nclasses \\\\(i\\\\) and \\\\(k\\\\) \\\\(\\\\alpha^{j}_{i,k}\\\\).\\nThen dual_coef_ looks like this:\\n---------new doc---------\\n1.4.5. Tips on Practical Use#\\n\\nAvoiding data copy: For SVC, SVR, NuSVC and\\nNuSVR, if the data passed to certain methods is not C-ordered\\ncontiguous and double precision, it will be copied before calling the\\nunderlying C implementation. You can check whether a given numpy array is\\nC-contiguous by inspecting its flags attribute.\\nFor LinearSVC (and LogisticRegression) any input passed as a numpy\\narray will be copied and converted to the liblinear internal sparse data\\nrepresentation (double precision floats and int32 indices of non-zero\\ncomponents). If you want to fit a large-scale linear classifier without\\ncopying a dense numpy C-contiguous double precision array as input, we\\nsuggest to use the SGDClassifier class instead.  The objective\\nfunction can be configured to be almost the same as the LinearSVC\\nmodel.\\n\\nKernel cache size: For SVC, SVR, NuSVC and\\nNuSVR, the size of the kernel cache has a strong impact on run\\ntimes for larger problems.  If you have enough RAM available, it is\\nrecommended to set cache_size to a higher value than the default of\\n200(MB), such as 500(MB) or 1000(MB).\\nSetting C: C is 1 by default and it’s a reasonable default\\nchoice.  If you have a lot of noisy observations you should decrease it:\\ndecreasing C corresponds to more regularization.\\nLinearSVC and LinearSVR are less sensitive to C when\\nit becomes large, and prediction results stop improving after a certain\\nthreshold. Meanwhile, larger C values will take more time to train,\\nsometimes up to 10 times longer, as shown in [11].\\n---------new doc---------\\nSupport Vector Machine algorithms are not scale invariant, so it\\nis highly recommended to scale your data. For example, scale each\\nattribute on the input vector X to [0,1] or [-1,+1], or standardize it\\nto have mean 0 and variance 1. Note that the same scaling must be\\napplied to the test vector to obtain meaningful results. This can be done\\neasily by using a Pipeline:\\n>>> from sklearn.pipeline import make_pipeline\\n>>> from sklearn.preprocessing import StandardScaler\\n>>> from sklearn.svm import SVC\\n\\n>>> clf = make_pipeline(StandardScaler(), SVC())\\n\\n\\nSee section Preprocessing data for more details on scaling and\\nnormalization.\\n---------new doc---------\\nNuSVC#\\nThe \\\\(\\\\nu\\\\)-SVC formulation [15] is a reparameterization of the\\n\\\\(C\\\\)-SVC and therefore mathematically equivalent.\\nWe introduce a new parameter \\\\(\\\\nu\\\\) (instead of \\\\(C\\\\)) which\\ncontrols the number of support vectors and margin errors:\\n\\\\(\\\\nu \\\\in (0, 1]\\\\) is an upper bound on the fraction of margin errors and\\na lower bound of the fraction of support vectors. A margin error corresponds\\nto a sample that lies on the wrong side of its margin boundary: it is either\\nmisclassified, or it is correctly classified but does not lie beyond the\\nmargin.\\n\\n\\n\\n1.4.7.2. SVR#\\nGiven training vectors \\\\(x_i \\\\in \\\\mathbb{R}^p\\\\), i=1,…, n, and a\\nvector \\\\(y \\\\in \\\\mathbb{R}^n\\\\) \\\\(\\\\varepsilon\\\\)-SVR solves the following primal problem:\\n---------new doc---------\\nTransforming data#\\nTo transform \\\\(X\\\\) into \\\\(\\\\bar{X}\\\\), we need to find a projection\\nmatrix \\\\(P\\\\) such that \\\\(\\\\bar{X} = XP\\\\). We know that for the\\ntraining data, \\\\(\\\\Xi = XP\\\\), and \\\\(X = \\\\Xi \\\\Gamma^T\\\\). Setting\\n\\\\(P = U(\\\\Gamma^T U)^{-1}\\\\) where \\\\(U\\\\) is the matrix with the\\n\\\\(u_k\\\\) in the columns, we have \\\\(XP = X U(\\\\Gamma^T U)^{-1} = \\\\Xi\\n(\\\\Gamma^T U) (\\\\Gamma^T U)^{-1} = \\\\Xi\\\\) as desired. The rotation matrix\\n\\\\(P\\\\) can be accessed from the x_rotations_ attribute.\\nSimilarly, \\\\(Y\\\\) can be transformed using the rotation matrix\\n\\\\(V(\\\\Delta^T V)^{-1}\\\\), accessed via the y_rotations_ attribute.\\n---------new doc---------\\nOnce trained, you can plot the tree with the plot_tree function:\\n>>> tree.plot_tree(clf)\\n[...]\\n\\n\\n\\n\\n\\n\\n\\n\\nAlternative ways to export trees#\\nWe can also export the tree in Graphviz format using the export_graphviz\\nexporter. If you use the conda package manager, the graphviz binaries\\nand the python package can be installed with conda install python-graphviz.\\nAlternatively binaries for graphviz can be downloaded from the graphviz project homepage,\\nand the Python wrapper installed from pypi with pip install graphviz.\\nBelow is an example graphviz export of the above tree trained on the entire\\niris dataset; the results are saved in an output file iris.pdf:\\n>>> import graphviz \\n>>> dot_data = tree.export_graphviz(clf, out_file=None) \\n>>> graph = graphviz.Source(dot_data) \\n>>> graph.render(\\\"iris\\\")\\n---------new doc---------\\nThe export_graphviz exporter also supports a variety of aesthetic\\noptions, including coloring nodes by their class (or value for regression) and\\nusing explicit variable and class names if desired. Jupyter notebooks also\\nrender these plots inline automatically:\\n>>> dot_data = tree.export_graphviz(clf, out_file=None, \\n...                      feature_names=iris.feature_names,  \\n...                      class_names=iris.target_names,  \\n...                      filled=True, rounded=True,  \\n...                      special_characters=True)  \\n>>> graph = graphviz.Source(dot_data)  \\n>>> graph\\n---------new doc---------\\nAs you can see, the [1, 0] is comfortably classified as 1 since the first\\ntwo samples are ignored due to their sample weights.\\nImplementation detail: taking sample weights into account amounts to\\nmultiplying the gradients (and the hessians) by the sample weights. Note that\\nthe binning stage (specifically the quantiles computation) does not take the\\nweights into account.\\n\\n\\n1.11.1.1.4. Categorical Features Support#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor have native support for categorical\\nfeatures: they can consider splits on non-ordered, categorical data.\\nFor datasets with categorical features, using the native categorical support\\nis often better than relying on one-hot encoding\\n(OneHotEncoder), because one-hot encoding\\nrequires more tree depth to achieve equivalent splits. It is also usually\\nbetter to rely on the native categorical support rather than to treat\\ncategorical features as continuous (ordinal), which happens for ordinal-encoded\\ncategorical data, since categories are nominal quantities where order does not\\nmatter.\\nTo enable categorical support, a boolean mask can be passed to the\\ncategorical_features parameter, indicating which feature is categorical. In\\nthe following, the first feature will be treated as categorical and the\\nsecond feature as numerical:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[True, False])\\n\\n\\nEquivalently, one can pass a list of integers indicating the indices of the\\ncategorical features:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[0])\\n---------new doc---------\\nEquivalently, one can pass a list of integers indicating the indices of the\\ncategorical features:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[0])\\n\\n\\nWhen the input is a DataFrame, it is also possible to pass a list of column\\nnames:\\n>>> gbdt = HistGradientBoostingClassifier(categorical_features=[\\\"site\\\", \\\"manufacturer\\\"])\\n\\n\\nFinally, when the input is a DataFrame we can use\\ncategorical_features=\\\"from_dtype\\\" in which case all columns with a categorical\\ndtype will be treated as categorical features.\\nThe cardinality of each categorical feature must be less than the max_bins\\nparameter. For an example using histogram-based gradient boosting on categorical\\nfeatures, see\\nCategorical Feature Support in Gradient Boosting.\\nIf there are missing values during training, the missing values will be\\ntreated as a proper category. If there are no missing values during training,\\nthen at prediction time, missing values are mapped to the child node that has\\nthe most samples (just like for continuous features). When predicting,\\ncategories that were not seen during fit time will be treated as missing\\nvalues.\\n---------new doc---------\\na monotonic increase constraint is a constraint of the form:\\n\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\leq F(x_1', x_2)\\\\]\\n\\na monotonic decrease constraint is a constraint of the form:\\n\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\geq F(x_1', x_2)\\\\]\\n\\n\\nYou can specify a monotonic constraint on each feature using the\\nmonotonic_cst parameter. For each feature, a value of 0 indicates no\\nconstraint, while 1 and -1 indicate a monotonic increase and\\nmonotonic decrease constraint, respectively:\\n>>> from sklearn.ensemble import HistGradientBoostingRegressor\\n\\n... # monotonic increase, monotonic decrease, and no constraint on the 3 features\\n>>> gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])\\n\\n\\nIn a binary classification context, imposing a monotonic increase (decrease) constraint means that higher values of the feature are supposed\\nto have a positive (negative) effect on the probability of samples\\nto belong to the positive class.\\nNevertheless, monotonic constraints only marginally constrain feature effects on the output.\\nFor instance, monotonic increase and decrease constraints cannot be used to enforce the\\nfollowing modelling constraint:\\n\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\leq F(x_1', x_2')\\\\]\\nAlso, monotonic constraints are not supported for multiclass classification.\\n---------new doc---------\\n\\\\[x_1 \\\\leq x_1' \\\\implies F(x_1, x_2) \\\\leq F(x_1', x_2')\\\\]\\nAlso, monotonic constraints are not supported for multiclass classification.\\n\\nNote\\nSince categories are unordered quantities, it is not possible to enforce\\nmonotonic constraints on categorical features.\\n\\nExamples\\n\\nMonotonic Constraints\\nFeatures in Histogram Gradient Boosting Trees\\n\\n\\n\\n1.11.1.1.6. Interaction constraints#\\nA priori, the histogram gradient boosted trees are allowed to use any feature\\nto split a node into child nodes. This creates so called interactions between\\nfeatures, i.e. usage of different features as split along a branch. Sometimes,\\none wants to restrict the possible interactions, see [Mayer2022]. This can be\\ndone by the parameter interaction_cst, where one can specify the indices\\nof features that are allowed to interact.\\nFor instance, with 3 features in total, interaction_cst=[{0}, {1}, {2}]\\nforbids all interactions.\\nThe constraints [{0, 1}, {1, 2}] specifies two groups of possibly\\ninteracting features. Features 0 and 1 may interact with each other, as well\\nas features 1 and 2. But note that features 0 and 2 are forbidden to interact.\\nThe following depicts a tree and the possible splits of the tree:\\n   1      <- Both constraint groups could be applied from now on\\n  / \\\\\\n 1   2    <- Left split still fulfills both constraint groups.\\n/ \\\\ / \\\\      Right split at feature 2 has only group {1, 2} from now on.\\n---------new doc---------\\nLightGBM uses the same logic for overlapping groups.\\nNote that features not listed in interaction_cst are automatically\\nassigned an interaction group for themselves. With again 3 features, this\\nmeans that [{0}] is equivalent to [{0}, {1, 2}].\\nExamples\\n\\nPartial Dependence and Individual Conditional Expectation Plots\\n\\nReferences\\n\\n\\n[Mayer2022]\\nM. Mayer, S.C. Bourassa, M. Hoesli, and D.F. Scognamiglio.\\n2022. Machine Learning Applications to Land and Structure Valuation.\\nJournal of Risk and Financial Management 15, no. 5: 193\\n\\n\\n\\n\\n1.11.1.1.7. Low-level parallelism#\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor use OpenMP\\nfor parallelization through Cython. For more details on how to control the\\nnumber of threads, please refer to our Parallelism notes.\\nThe following parts are parallelized:\\n\\nmapping samples from real values to integer-valued bins (finding the bin\\nthresholds is however sequential)\\nbuilding histograms is parallelized over features\\nfinding the best split point at a node is parallelized over features\\nduring fit, mapping samples into the left and right children is\\nparallelized over samples\\ngradient and hessians computations are parallelized over samples\\npredicting is parallelized over samples\\n---------new doc---------\\n1.12.1.1. Target format#\\nValid multiclass representations for\\ntype_of_target (y) are:\\n\\n1d or column vector containing more than two discrete values. An\\nexample of a vector y for 4 samples:\\n>>> import numpy as np\\n>>> y = np.array(['apple', 'pear', 'apple', 'orange'])\\n>>> print(y)\\n['apple' 'pear' 'apple' 'orange']\\n\\n\\n\\nDense or sparse binary matrix of shape (n_samples, n_classes)\\nwith a single sample per row, where each column represents one class. An\\nexample of both a dense and sparse binary matrix y for 4\\nsamples, where the columns, in order, are apple, orange, and pear:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import LabelBinarizer\\n>>> y = np.array(['apple', 'pear', 'apple', 'orange'])\\n>>> y_dense = LabelBinarizer().fit_transform(y)\\n>>> print(y_dense)\\n[[1 0 0]\\n [0 0 1]\\n [1 0 0]\\n [0 1 0]]\\n>>> from scipy import sparse\\n>>> y_sparse = sparse.csr_matrix(y_dense)\\n>>> print(y_sparse)\\n<Compressed Sparse Row sparse matrix of dtype 'int64'\\n    with 4 stored elements and shape (4, 3)>\\n  Coords    Values\\n  (0, 0)    1\\n  (1, 2)    1\\n  (2, 0)    1\\n  (3, 1)    1\\n---------new doc---------\\n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\n---------new doc---------\\n1.12.1.4. OutputCodeClassifier#\\nError-Correcting Output Code-based strategies are fairly different from\\none-vs-the-rest and one-vs-one. With these strategies, each class is\\nrepresented in a Euclidean space, where each dimension can only be 0 or 1.\\nAnother way to put it is that each class is represented by a binary code (an\\narray of 0 and 1). The matrix which keeps track of the location/code of each\\nclass is called the code book. The code size is the dimensionality of the\\naforementioned space. Intuitively, each class should be represented by a code\\nas unique as possible and a good code book should be designed to optimize\\nclassification accuracy. In this implementation, we simply use a\\nrandomly-generated code book as advocated in [3] although more elaborate\\nmethods may be added in the future.\\nAt fitting time, one binary classifier per bit in the code book is fitted.\\nAt prediction time, the classifiers are used to project new points in the\\nclass space and the class closest to the points is chosen.\\nIn OutputCodeClassifier, the code_size\\nattribute allows the user to control the number of classifiers which will be\\nused. It is a percentage of the total number of classes.\\nA number between 0 and 1 will require fewer classifiers than\\none-vs-the-rest. In theory, log2(n_classes) / n_classes is sufficient to\\nrepresent each class unambiguously. However, in practice, it may not lead to\\ngood accuracy since log2(n_classes) is much smaller than n_classes.\\nA number greater than 1 will require more classifiers than\\none-vs-the-rest. In this case, some classifiers will in theory correct for\\n---------new doc---------\\n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\\n       1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,\\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,\\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\n---------new doc---------\\n1.12.2.1. Target format#\\nA valid representation of multilabel y is an either dense or sparse\\nbinary matrix of shape (n_samples, n_classes). Each column\\nrepresents a class. The 1’s in each row denote the positive classes a\\nsample has been labeled with. An example of a dense matrix y for 3\\nsamples:\\n>>> y = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [0, 0, 0, 0]])\\n>>> print(y)\\n[[1 0 0 1]\\n [0 0 1 1]\\n [0 0 0 0]]\\n\\n\\nDense binary matrices can also be created using\\nMultiLabelBinarizer. For more information,\\nrefer to Transforming the prediction target (y).\\nAn example of the same y in sparse matrix form:\\n>>> y_sparse = sparse.csr_matrix(y)\\n>>> print(y_sparse)\\n<Compressed Sparse Row sparse matrix of dtype 'int64'\\n  with 4 stored elements and shape (3, 4)>\\n  Coords      Values\\n  (0, 0)      1\\n  (0, 3)      1\\n  (1, 2)      1\\n  (1, 3)      1\\n---------new doc---------\\n[-141.62745778,   95.02891072, -191.48204257],\\n       [  97.03260883,  165.34867495,  139.52003279],\\n       [ 123.92529176,   21.25719016,   -7.84253   ],\\n       [-122.25193977,  -85.16443186, -107.12274212],\\n       [ -30.170388  ,  -94.80956739,   12.16979946],\\n       [ 140.72667194,  176.50941682,  -17.50447799],\\n       [ 149.37967282,  -81.15699552,   -5.72850319]])\\n---------new doc---------\\n>>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\\narray([[8, 0],\\n       [0, 4]])\\n\\n\\nLabelings that assign all classes members to the same clusters\\nare complete but may not always be pure, hence penalized, and\\nhave some off-diagonal non-zero entries:\\n>>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\\narray([[8, 2],\\n       [0, 2]])\\n\\n\\nThe matrix is not symmetric:\\n>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])\\narray([[8, 0],\\n       [2, 2]])\\n\\n\\nIf classes members are completely split across different clusters, the\\nassignment is totally incomplete, hence the matrix has all zero\\ndiagonal entries:\\n>>> pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])\\narray([[ 0,  0],\\n       [12,  0]])\\n\\n\\n\\n\\nReferences#\\n\\n“Comparing Partitions” L. Hubert and P. Arabie,\\nJournal of Classification 1985\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n2.2. Manifold learning\\n\\n\\n\\n\\nnext\\n2.4. Biclustering\\n\\n\\n --- \\n\\n\\n2. Unsupervised learning\\n2.5. Decomposing signals in components (matrix factorization problems)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2.5. Decomposing signals in components (matrix factorization problems)#\\n---------new doc---------\\nAnother option is to use an iterable yielding (train, test) splits as arrays of\\nindices, for example:\\n>>> def custom_cv_2folds(X):\\n...     n = X.shape[0]\\n...     i = 1\\n...     while i <= 2:\\n...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\\n...         yield idx, idx\\n...         i += 1\\n...\\n>>> custom_cv = custom_cv_2folds(X)\\n>>> cross_val_score(clf, X, y, cv=custom_cv)\\narray([1.        , 0.973...])\\n---------new doc---------\\ntrain -  [30  3]   |   test -  [15  2]\\ntrain -  [30  4]   |   test -  [15  1]\\n>>> kf = KFold(n_splits=3)\\n>>> for train, test in kf.split(X, y):\\n...     print('train -  {}   |   test -  {}'.format(\\n...         np.bincount(y[train]), np.bincount(y[test])))\\ntrain -  [28  5]   |   test -  [17]\\ntrain -  [28  5]   |   test -  [17]\\ntrain -  [34]   |   test -  [11  5]\\n---------new doc---------\\n>>> X = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001])\\n>>> y = np.array([\\\"a\\\", \\\"b\\\", \\\"b\\\", \\\"b\\\", \\\"c\\\", \\\"c\\\", \\\"c\\\", \\\"a\\\"])\\n>>> groups = np.array([1, 1, 2, 2, 3, 3, 4, 4])\\n>>> train_indx, test_indx = next(\\n...     GroupShuffleSplit(random_state=7).split(X, y, groups)\\n... )\\n>>> X_train, X_test, y_train, y_test = \\\\\\n...     X[train_indx], X[test_indx], y[train_indx], y[test_indx]\\n>>> X_train.shape, X_test.shape\\n((6,), (2,))\\n>>> np.unique(groups[train_indx]), np.unique(groups[test_indx])\\n(array([1, 2, 4]), array([3]))\\n---------new doc---------\\n2\\n0\\n125\\n0.983667\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\\n\\n3\\n0\\n125\\n0.983667\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 6, ‘min_samples_split’: 6}\\n\\n…\\n…\\n…\\n…\\n…\\n\\n15\\n2\\n500\\n0.951958\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}\\n\\n16\\n2\\n500\\n0.947958\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\\n\\n17\\n2\\n500\\n0.951958\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 4}\\n\\n18\\n3\\n1000\\n0.961009\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}\\n---------new doc---------\\n[[1, 0],\\n        [0, 1]],\\n\\n       [[0, 1],\\n        [1, 0]]])\\n\\n\\nOr a confusion matrix can be constructed for each sample’s labels:\\n>>> multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\\narray([[[1, 0],\\n        [1, 1]],\\n\\n       [[1, 1],\\n        [0, 1]]])\\n\\n\\nHere is an example demonstrating the use of the\\nmultilabel_confusion_matrix function with\\nmulticlass input:\\n>>> y_true = [\\\"cat\\\", \\\"ant\\\", \\\"cat\\\", \\\"cat\\\", \\\"ant\\\", \\\"bird\\\"]\\n>>> y_pred = [\\\"ant\\\", \\\"ant\\\", \\\"cat\\\", \\\"cat\\\", \\\"ant\\\", \\\"cat\\\"]\\n>>> multilabel_confusion_matrix(y_true, y_pred,\\n...                             labels=[\\\"ant\\\", \\\"bird\\\", \\\"cat\\\"])\\narray([[[3, 1],\\n        [0, 2]],\\n\\n       [[5, 0],\\n        [1, 0]],\\n\\n       [[2, 1],\\n        [1, 2]]])\\n---------new doc---------\\n4.1.1. Partial dependence plots#\\nPartial dependence plots (PDP) show the dependence between the target response\\nand a set of input features of interest, marginalizing over the values\\nof all other input features (the ‘complement’ features). Intuitively, we can\\ninterpret the partial dependence as the expected target response as a\\nfunction of the input features of interest.\\nDue to the limits of human perception, the size of the set of input features of\\ninterest must be small (usually, one or two) thus the input features of interest\\nare usually chosen among the most important features.\\nThe figure below shows two one-way and one two-way partial dependence plots for\\nthe bike sharing dataset, with a\\nHistGradientBoostingRegressor:\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\n>>> features = [0, 1, (0, 1)]\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features)\\n<...>\\n\\n\\nYou can access the newly created figure and Axes objects using plt.gcf()\\nand plt.gca().\\nTo make a partial dependence plot with categorical features, you need to specify\\nwhich features are categorical using the parameter categorical_features. This\\nparameter takes a list of indices, names of the categorical features or a boolean\\nmask. The graphical representation of partial dependence for categorical features is\\na bar plot or a 2D heatmap.\\n\\n\\nPDPs for multi-class classification#\\nFor multi-class classification, you need to set the class label for which\\nthe PDPs should be created via the target argument:\\n>>> from sklearn.datasets import load_iris\\n>>> iris = load_iris()\\n>>> mc_clf = GradientBoostingClassifier(n_estimators=10,\\n...     max_depth=1).fit(iris.data, iris.target)\\n>>> features = [3, 2, (3, 2)]\\n>>> PartialDependenceDisplay.from_estimator(mc_clf, X, features, target=0)\\n<...>\\n\\n\\nThe same parameter target is used to specify the target in multi-output\\nregression settings.\\n---------new doc---------\\nIn ICE plots it might not be easy to see the average effect of the input\\nfeature of interest. Hence, it is recommended to use ICE plots alongside\\nPDPs. They can be plotted together with\\nkind='both'.\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind='both')\\n<...>\\n\\n\\nIf there are too many lines in an ICE plot, it can be difficult to see\\ndifferences between individual samples and interpret the model. Centering the\\nICE at the first value on the x-axis, produces centered Individual Conditional\\nExpectation (cICE) plots [G2015]. This puts emphasis on the divergence of\\nindividual conditional expectations from the mean line, thus making it easier\\nto explore heterogeneous relationships. cICE plots can be plotted by setting\\ncentered=True:\\n>>> PartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind='both', centered=True)\\n<...>\\n\\n\\n\\n\\n4.1.3. Mathematical Definition#\\nLet \\\\(X_S\\\\) be the set of input features of interest (i.e. the features\\nparameter) and let \\\\(X_C\\\\) be its complement.\\nThe partial dependence of the response \\\\(f\\\\) at a point \\\\(x_S\\\\) is\\ndefined as:\\n---------new doc---------\\n4.1.3. Mathematical Definition#\\nLet \\\\(X_S\\\\) be the set of input features of interest (i.e. the features\\nparameter) and let \\\\(X_C\\\\) be its complement.\\nThe partial dependence of the response \\\\(f\\\\) at a point \\\\(x_S\\\\) is\\ndefined as:\\n\\n\\\\[\\\\begin{split}pd_{X_S}(x_S) &\\\\overset{def}{=} \\\\mathbb{E}_{X_C}\\\\left[ f(x_S, X_C) \\\\right]\\\\\\\\\\n              &= \\\\int f(x_S, x_C) p(x_C) dx_C,\\\\end{split}\\\\]\\nwhere \\\\(f(x_S, x_C)\\\\) is the response function (predict,\\npredict_proba or decision_function) for a given sample whose\\nvalues are defined by \\\\(x_S\\\\) for the features in \\\\(X_S\\\\), and by\\n\\\\(x_C\\\\) for the features in \\\\(X_C\\\\). Note that \\\\(x_S\\\\) and\\n\\\\(x_C\\\\) may be tuples.\\nComputing this integral for various values of \\\\(x_S\\\\) produces a PDP plot\\nas above. An ICE line is defined as a single \\\\(f(x_{S}, x_{C}^{(i)})\\\\)\\nevaluated at \\\\(x_{S}\\\\).\\n---------new doc---------\\n6.1.1. Pipeline: chaining estimators#\\nPipeline can be used to chain multiple estimators\\ninto one. This is useful as there is often a fixed sequence\\nof steps in processing the data, for example feature selection, normalization\\nand classification. Pipeline serves multiple purposes here:\\n\\nConvenience and encapsulationYou only have to call fit and predict once on your\\ndata to fit a whole sequence of estimators.\\n\\nJoint parameter selectionYou can grid search\\nover parameters of all estimators in the pipeline at once.\\n\\nSafetyPipelines help avoid leaking statistics from your test data into the\\ntrained model in cross-validation, by ensuring that the same samples are\\nused to train the transformers and predictors.\\n\\n\\nAll estimators in a pipeline, except the last one, must be transformers\\n(i.e. must have a transform method).\\nThe last estimator may be any type (transformer, classifier, etc.).\\n\\nNote\\nCalling fit on the pipeline is the same as calling fit on\\neach estimator in turn, transform the input and pass it on to the next step.\\nThe pipeline has all the methods that the last estimator in the pipeline has,\\ni.e. if the last estimator is a classifier, the Pipeline can be used\\nas a classifier. If the last estimator is a transformer, again, so is the\\npipeline.\\n\\n\\n6.1.1.1. Usage#\\n---------new doc---------\\n6.1.1.1. Usage#\\n\\n6.1.1.1.1. Build a pipeline#\\nThe Pipeline is built using a list of (key, value) pairs, where\\nthe key is a string containing the name you want to give this step and value\\nis an estimator object:\\n>>> from sklearn.pipeline import Pipeline\\n>>> from sklearn.svm import SVC\\n>>> from sklearn.decomposition import PCA\\n>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]\\n>>> pipe = Pipeline(estimators)\\n>>> pipe\\nPipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])\\n\\n\\n\\n\\nShorthand version using make_pipeline#\\nThe utility function make_pipeline is a shorthand\\nfor constructing pipelines;\\nit takes a variable number of estimators and returns a pipeline,\\nfilling in the names automatically:\\n>>> from sklearn.pipeline import make_pipeline\\n>>> make_pipeline(PCA(), SVC())\\nPipeline(steps=[('pca', PCA()), ('svc', SVC())])\\n---------new doc---------\\n6.1.1.1.2. Access pipeline steps#\\nThe estimators of a pipeline are stored as a list in the steps attribute.\\nA sub-pipeline can be extracted using the slicing notation commonly used\\nfor Python Sequences such as lists or strings (although only a step of 1 is\\npermitted). This is convenient for performing only some of the transformations\\n(or their inverse):\\n>>> pipe[:1]\\nPipeline(steps=[('reduce_dim', PCA())])\\n>>> pipe[-1:]\\nPipeline(steps=[('clf', SVC())])\\n\\n\\n\\n\\nAccessing a step by name or position#\\nA specific step can also be accessed by index or name by indexing (with [idx]) the\\npipeline:\\n>>> pipe.steps[0]\\n('reduce_dim', PCA())\\n>>> pipe[0]\\nPCA()\\n>>> pipe['reduce_dim']\\nPCA()\\n\\n\\nPipeline’s named_steps attribute allows accessing steps by name with tab\\ncompletion in interactive environments:\\n>>> pipe.named_steps.reduce_dim is pipe['reduce_dim']\\nTrue\\n---------new doc---------\\nPipeline’s named_steps attribute allows accessing steps by name with tab\\ncompletion in interactive environments:\\n>>> pipe.named_steps.reduce_dim is pipe['reduce_dim']\\nTrue\\n\\n\\n\\n\\n\\n6.1.1.1.3. Tracking feature names in a pipeline#\\nTo enable model inspection, Pipeline has a\\nget_feature_names_out() method, just like all transformers. You can use\\npipeline slicing to get the feature names going into each step:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.feature_selection import SelectKBest\\n>>> iris = load_iris()\\n>>> pipe = Pipeline(steps=[\\n...    ('select', SelectKBest(k=2)),\\n...    ('clf', LogisticRegression())])\\n>>> pipe.fit(iris.data, iris.target)\\nPipeline(steps=[('select', SelectKBest(...)), ('clf', LogisticRegression(...))])\\n>>> pipe[:-1].get_feature_names_out()\\narray(['x2', 'x3'], ...)\\n\\n\\n\\n\\nCustomize feature names#\\nYou can also provide custom feature names for the input data using\\nget_feature_names_out:\\n>>> pipe[:-1].get_feature_names_out(iris.feature_names)\\narray(['petal length (cm)', 'petal width (cm)'], ...)\\n---------new doc---------\\nCustomize feature names#\\nYou can also provide custom feature names for the input data using\\nget_feature_names_out:\\n>>> pipe[:-1].get_feature_names_out(iris.feature_names)\\narray(['petal length (cm)', 'petal width (cm)'], ...)\\n\\n\\n\\n\\n\\n6.1.1.1.4. Access to nested parameters#\\nIt is common to adjust the parameters of an estimator within a pipeline. This parameter\\nis therefore nested because it belongs to a particular sub-step. Parameters of the\\nestimators in the pipeline are accessible using the <estimator>__<parameter>\\nsyntax:\\n>>> pipe = Pipeline(steps=[(\\\"reduce_dim\\\", PCA()), (\\\"clf\\\", SVC())])\\n>>> pipe.set_params(clf__C=10)\\nPipeline(steps=[('reduce_dim', PCA()), ('clf', SVC(C=10))])\\n\\n\\n\\n\\nWhen does it matter?#\\nThis is particularly important for doing grid searches:\\n>>> from sklearn.model_selection import GridSearchCV\\n>>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],\\n...                   clf__C=[0.1, 10, 100])\\n>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)\\n---------new doc---------\\nSide effect of caching transformers#\\nUsing a Pipeline without cache enabled, it is possible to\\ninspect the original instance such as:\\n>>> from sklearn.datasets import load_digits\\n>>> X_digits, y_digits = load_digits(return_X_y=True)\\n>>> pca1 = PCA(n_components=10)\\n>>> svm1 = SVC()\\n>>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])\\n>>> pipe.fit(X_digits, y_digits)\\nPipeline(steps=[('reduce_dim', PCA(n_components=10)), ('clf', SVC())])\\n>>> # The pca instance can be inspected directly\\n>>> pca1.components_.shape\\n(10, 64)\\n---------new doc---------\\nFor simple transformations, instead of a Transformer object, a pair of\\nfunctions can be passed, defining the transformation and its inverse mapping:\\n>>> def func(x):\\n...     return np.log(x)\\n>>> def inverse_func(x):\\n...     return np.exp(x)\\n\\n\\nSubsequently, the object is created as:\\n>>> regr = TransformedTargetRegressor(regressor=regressor,\\n...                                   func=func,\\n...                                   inverse_func=inverse_func)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: 0.51\\n---------new doc---------\\nBy default, the provided functions are checked at each fit to be the inverse of\\neach other. However, it is possible to bypass this checking by setting\\ncheck_inverse to False:\\n>>> def inverse_func(x):\\n...     return x\\n>>> regr = TransformedTargetRegressor(regressor=regressor,\\n...                                   func=func,\\n...                                   inverse_func=inverse_func,\\n...                                   check_inverse=False)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: -1.57\\n\\n\\n\\nNote\\nThe transformation can be triggered by setting either transformer or the\\npair of functions func and inverse_func. However, setting both\\noptions will raise an error.\\n\\nExamples\\n\\nEffect of transforming the targets in regression model\\n---------new doc---------\\nNote\\nThe transformation can be triggered by setting either transformer or the\\npair of functions func and inverse_func. However, setting both\\noptions will raise an error.\\n\\nExamples\\n\\nEffect of transforming the targets in regression model\\n\\n\\n\\n6.1.3. FeatureUnion: composite feature spaces#\\nFeatureUnion combines several transformer objects into a new\\ntransformer that combines their output. A FeatureUnion takes\\na list of transformer objects. During fitting, each of these\\nis fit to the data independently. The transformers are applied in parallel,\\nand the feature matrices they output are concatenated side-by-side into a\\nlarger matrix.\\nWhen you want to apply different transformations to each field of the data,\\nsee the related class ColumnTransformer\\n(see user guide).\\nFeatureUnion serves the same purposes as Pipeline -\\nconvenience and joint parameter estimation and validation.\\nFeatureUnion and Pipeline can be combined to\\ncreate complex models.\\n(A FeatureUnion has no way of checking whether two transformers\\nmight produce identical features. It only produces a union when the\\nfeature sets are disjoint, and making sure they are is the caller’s\\nresponsibility.)\\n---------new doc---------\\n6.1.3.1. Usage#\\nA FeatureUnion is built using a list of (key, value) pairs,\\nwhere the key is the name you want to give to a given transformation\\n(an arbitrary string; it only serves as an identifier)\\nand value is an estimator object:\\n>>> from sklearn.pipeline import FeatureUnion\\n>>> from sklearn.decomposition import PCA\\n>>> from sklearn.decomposition import KernelPCA\\n>>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\\n>>> combined = FeatureUnion(estimators)\\n>>> combined\\nFeatureUnion(transformer_list=[('linear_pca', PCA()),\\n                               ('kernel_pca', KernelPCA())])\\n\\n\\nLike pipelines, feature unions have a shorthand constructor called\\nmake_union that does not require explicit naming of the components.\\nLike Pipeline, individual steps may be replaced using set_params,\\nand ignored by setting to 'drop':\\n>>> combined.set_params(kernel_pca='drop')\\nFeatureUnion(transformer_list=[('linear_pca', PCA()),\\n                               ('kernel_pca', 'drop')])\\n\\n\\nExamples\\n\\nConcatenating multiple feature extraction methods\\n---------new doc---------\\nExamples\\n\\nConcatenating multiple feature extraction methods\\n\\n\\n\\n\\n6.1.4. ColumnTransformer for heterogeneous data#\\nMany datasets contain features of different types, say text, floats, and dates,\\nwhere each type of feature requires separate preprocessing or feature\\nextraction steps.  Often it is easiest to preprocess data before applying\\nscikit-learn methods, for example using pandas.\\nProcessing your data before passing it to scikit-learn might be problematic for\\none of the following reasons:\\n\\nIncorporating statistics from test data into the preprocessors makes\\ncross-validation scores unreliable (known as data leakage),\\nfor example in the case of scalers or imputing missing values.\\nYou may want to include the parameters of the preprocessors in a\\nparameter search.\\n---------new doc---------\\nIncorporating statistics from test data into the preprocessors makes\\ncross-validation scores unreliable (known as data leakage),\\nfor example in the case of scalers or imputing missing values.\\nYou may want to include the parameters of the preprocessors in a\\nparameter search.\\n\\nThe ColumnTransformer helps performing different\\ntransformations for different columns of the data, within a\\nPipeline that is safe from data leakage and that can\\nbe parametrized. ColumnTransformer works on\\narrays, sparse matrices, and\\npandas DataFrames.\\nTo each column, a different transformation can be applied, such as\\npreprocessing or a specific feature extraction method:\\n>>> import pandas as pd\\n>>> X = pd.DataFrame(\\n...     {'city': ['London', 'London', 'Paris', 'Sallisaw'],\\n...      'title': [\\\"His Last Bow\\\", \\\"How Watson Learned the Trick\\\",\\n...                \\\"A Moveable Feast\\\", \\\"The Grapes of Wrath\\\"],\\n...      'expert_rating': [5, 3, 4, 5],\\n...      'user_rating': [4, 5, 4, 3]})\\n---------new doc---------\\nFor this data, we might want to encode the 'city' column as a categorical\\nvariable using OneHotEncoder but apply a\\nCountVectorizer to the 'title' column.\\nAs we might use multiple feature extraction methods on the same column, we give\\neach transformer a unique name, say 'city_category' and 'title_bow'.\\nBy default, the remaining rating columns are ignored (remainder='drop'):\\n>>> from sklearn.compose import ColumnTransformer\\n>>> from sklearn.feature_extraction.text import CountVectorizer\\n>>> from sklearn.preprocessing import OneHotEncoder\\n>>> column_trans = ColumnTransformer(\\n...     [('categories', OneHotEncoder(dtype='int'), ['city']),\\n...      ('title_bow', CountVectorizer(), 'title')],\\n...     remainder='drop', verbose_feature_names_out=False)\\n\\n>>> column_trans.fit(X)\\nColumnTransformer(transformers=[('categories', OneHotEncoder(dtype='int'),\\n                                 ['city']),\\n                                ('title_bow', CountVectorizer(), 'title')],\\n                  verbose_feature_names_out=False)\\n---------new doc---------\\n>>> column_trans.get_feature_names_out()\\narray(['city_London', 'city_Paris', 'city_Sallisaw', 'bow', 'feast',\\n'grapes', 'his', 'how', 'last', 'learned', 'moveable', 'of', 'the',\\n 'trick', 'watson', 'wrath'], ...)\\n\\n>>> column_trans.transform(X).toarray()\\narray([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\\n       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0],\\n       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\\n       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]...)\\n---------new doc---------\\nIn the above example, the\\nCountVectorizer expects a 1D array as\\ninput and therefore the columns were specified as a string ('title').\\nHowever, OneHotEncoder\\nas most of other transformers expects 2D data, therefore in that case you need\\nto specify the column as a list of strings (['city']).\\nApart from a scalar or a single item list, the column selection can be specified\\nas a list of multiple items, an integer array, a slice, a boolean mask, or\\nwith a make_column_selector. The\\nmake_column_selector is used to select columns based\\non data type or column name:\\n>>> from sklearn.preprocessing import StandardScaler\\n>>> from sklearn.compose import make_column_selector\\n>>> ct = ColumnTransformer([\\n...       ('scale', StandardScaler(),\\n...       make_column_selector(dtype_include=np.number)),\\n...       ('onehot',\\n...       OneHotEncoder(),\\n...       make_column_selector(pattern='city', dtype_include=object))])\\n>>> ct.fit_transform(X)\\narray([[ 0.904...,  0.      ,  1. ,  0. ,  0. ],\\n       [-1.507...,  1.414...,  1. ,  0. ,  0. ],\\n       [-0.301...,  0.      ,  0. ,  1. ,  0. ],\\n---------new doc---------\\narray([[ 0.904...,  0.      ,  1. ,  0. ,  0. ],\\n       [-1.507...,  1.414...,  1. ,  0. ,  0. ],\\n       [-0.301...,  0.      ,  0. ,  1. ,  0. ],\\n       [ 0.904..., -1.414...,  0. ,  0. ,  1. ]])\\n---------new doc---------\\nStrings can reference columns if the input is a DataFrame, integers are always\\ninterpreted as the positional columns.\\nWe can keep the remaining rating columns by setting\\nremainder='passthrough'. The values are appended to the end of the\\ntransformation:\\n>>> column_trans = ColumnTransformer(\\n...     [('city_category', OneHotEncoder(dtype='int'),['city']),\\n...      ('title_bow', CountVectorizer(), 'title')],\\n...     remainder='passthrough')\\n\\n>>> column_trans.fit_transform(X)\\narray([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],\\n       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],\\n       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4],\\n       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]]...)\\n---------new doc---------\\nThe remainder parameter can be set to an estimator to transform the\\nremaining rating columns. The transformed values are appended to the end of\\nthe transformation:\\n>>> from sklearn.preprocessing import MinMaxScaler\\n>>> column_trans = ColumnTransformer(\\n...     [('city_category', OneHotEncoder(), ['city']),\\n...      ('title_bow', CountVectorizer(), 'title')],\\n...     remainder=MinMaxScaler())\\n\\n>>> column_trans.fit_transform(X)[:, -2:]\\narray([[1. , 0.5],\\n       [0. , 1. ],\\n       [0.5, 0.5],\\n       [1. , 0. ]])\\n---------new doc---------\\n>>> column_trans.fit_transform(X)[:, -2:]\\narray([[1. , 0.5],\\n       [0. , 1. ],\\n       [0.5, 0.5],\\n       [1. , 0. ]])\\n\\n\\nThe make_column_transformer function is available\\nto more easily create a ColumnTransformer object.\\nSpecifically, the names will be given automatically. The equivalent for the\\nabove example would be:\\n>>> from sklearn.compose import make_column_transformer\\n>>> column_trans = make_column_transformer(\\n...     (OneHotEncoder(), ['city']),\\n...     (CountVectorizer(), 'title'),\\n...     remainder=MinMaxScaler())\\n>>> column_trans\\nColumnTransformer(remainder=MinMaxScaler(),\\n                  transformers=[('onehotencoder', OneHotEncoder(), ['city']),\\n                                ('countvectorizer', CountVectorizer(),\\n                                 'title')])\\n---------new doc---------\\nIf ColumnTransformer is fitted with a dataframe\\nand the dataframe only has string column names, then transforming a dataframe\\nwill use the column names to select the columns:\\n>>> ct = ColumnTransformer(\\n...          [(\\\"scale\\\", StandardScaler(), [\\\"expert_rating\\\"])]).fit(X)\\n>>> X_new = pd.DataFrame({\\\"expert_rating\\\": [5, 6, 1],\\n...                       \\\"ignored_new_col\\\": [1.2, 0.3, -0.1]})\\n>>> ct.transform(X_new)\\narray([[ 0.9...],\\n       [ 2.1...],\\n       [-3.9...]])\\n\\n\\n\\n\\n6.1.5. Visualizing Composite Estimators#\\nEstimators are displayed with an HTML representation when shown in a\\njupyter notebook. This is useful to diagnose or visualize a Pipeline with\\nmany estimators. This visualization is activated by default:\\n>>> column_trans  \\n\\n\\nIt can be deactivated by setting the display option in set_config\\nto ‘text’:\\n>>> from sklearn import set_config\\n>>> set_config(display='text')  \\n>>> # displays text representation in a jupyter context\\n>>> column_trans\\n---------new doc---------\\nIt can be deactivated by setting the display option in set_config\\nto ‘text’:\\n>>> from sklearn import set_config\\n>>> set_config(display='text')  \\n>>> # displays text representation in a jupyter context\\n>>> column_trans  \\n\\n\\nAn example of the HTML output can be seen in the\\nHTML representation of Pipeline section of\\nColumn Transformer with Mixed Types.\\nAs an alternative, the HTML can be written to a file using\\nestimator_html_repr:\\n>>> from sklearn.utils import estimator_html_repr\\n>>> with open('my_estimator.html', 'w') as f:  \\n...     f.write(estimator_html_repr(clf))\\n\\n\\nExamples\\n\\nColumn Transformer with Heterogeneous Data Sources\\nColumn Transformer with Mixed Types\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n6. Dataset transformations\\n\\n\\n\\n\\nnext\\n6.2. Feature extraction\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.2. Feature extraction\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6.2. Feature extraction#\\nThe sklearn.feature_extraction module can be used to extract\\nfeatures in a format supported by machine learning algorithms from datasets\\nconsisting of formats such as text and image.\\n\\nNote\\nFeature extraction is very different from Feature selection:\\nthe former consists in transforming arbitrary data, such as text or\\nimages, into numerical features usable for machine learning. The latter\\nis a machine learning technique applied on these features.\\n---------new doc---------\\n6.2. Feature extraction#\\nThe sklearn.feature_extraction module can be used to extract\\nfeatures in a format supported by machine learning algorithms from datasets\\nconsisting of formats such as text and image.\\n\\nNote\\nFeature extraction is very different from Feature selection:\\nthe former consists in transforming arbitrary data, such as text or\\nimages, into numerical features usable for machine learning. The latter\\nis a machine learning technique applied on these features.\\n\\n\\n6.2.1. Loading features from dicts#\\nThe class DictVectorizer can be used to convert feature\\narrays represented as lists of standard Python dict objects to the\\nNumPy/SciPy representation used by scikit-learn estimators.\\nWhile not particularly fast to process, Python’s dict has the\\nadvantages of being convenient to use, being sparse (absent features\\nneed not be stored) and storing feature names in addition to values.\\nDictVectorizer implements what is called one-of-K or “one-hot”\\ncoding for categorical (aka nominal, discrete) features. Categorical\\nfeatures are “attribute-value” pairs where the value is restricted\\nto a list of discrete possibilities without ordering (e.g. topic\\nidentifiers, types of objects, tags, names…).\\nIn the following, “city” is a categorical attribute while “temperature”\\nis a traditional numerical feature:\\n>>> measurements = [\\n...     {'city': 'Dubai', 'temperature': 33.},\\n...     {'city': 'London', 'temperature': 12.},\\n...     {'city': 'San Francisco', 'temperature': 18.},\\n... ]\\n---------new doc---------\\n>>> from sklearn.feature_extraction import DictVectorizer\\n>>> vec = DictVectorizer()\\n\\n>>> vec.fit_transform(measurements).toarray()\\narray([[ 1.,  0.,  0., 33.],\\n       [ 0.,  1.,  0., 12.],\\n       [ 0.,  0.,  1., 18.]])\\n\\n>>> vec.get_feature_names_out()\\narray(['city=Dubai', 'city=London', 'city=San Francisco', 'temperature'], ...)\\n---------new doc---------\\nDictVectorizer accepts multiple string values for one\\nfeature, like, e.g., multiple categories for a movie.\\nAssume a database classifies each movie using some categories (not mandatories)\\nand its year of release.\\n>>> movie_entry = [{'category': ['thriller', 'drama'], 'year': 2003},\\n...                {'category': ['animation', 'family'], 'year': 2011},\\n...                {'year': 1974}]\\n>>> vec.fit_transform(movie_entry).toarray()\\narray([[0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 2.003e+03],\\n       [1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 2.011e+03],\\n       [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.974e+03]])\\n>>> vec.get_feature_names_out()\\narray(['category=animation', 'category=drama', 'category=family',\\n       'category=thriller', 'year'], ...)\\n>>> vec.transform({'category': ['thriller'],\\n...                'unseen_feature': '3'}).toarray()\\narray([[0., 0., 0., 1., 0.]])\\n---------new doc---------\\nDictVectorizer is also a useful representation transformation\\nfor training sequence classifiers in Natural Language Processing models\\nthat typically work by extracting feature windows around a particular\\nword of interest.\\nFor example, suppose that we have a first algorithm that extracts Part of\\nSpeech (PoS) tags that we want to use as complementary tags for training\\na sequence classifier (e.g. a chunker). The following dict could be\\nsuch a window of features extracted around the word ‘sat’ in the sentence\\n‘The cat sat on the mat.’:\\n>>> pos_window = [\\n...     {\\n...         'word-2': 'the',\\n...         'pos-2': 'DT',\\n...         'word-1': 'cat',\\n...         'pos-1': 'NN',\\n...         'word+1': 'on',\\n...         'pos+1': 'PP',\\n...     },\\n...     # in a real application one would extract many such dictionaries\\n... ]\\n---------new doc---------\\nThis description can be vectorized into a sparse two-dimensional matrix\\nsuitable for feeding into a classifier (maybe after being piped into a\\nTfidfTransformer for normalization):\\n>>> vec = DictVectorizer()\\n>>> pos_vectorized = vec.fit_transform(pos_window)\\n>>> pos_vectorized\\n<Compressed Sparse...dtype 'float64'\\n  with 6 stored elements and shape (1, 6)>\\n>>> pos_vectorized.toarray()\\narray([[1., 1., 1., 1., 1., 1.]])\\n>>> vec.get_feature_names_out()\\narray(['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat',\\n       'word-2=the'], ...)\\n\\n\\nAs you can imagine, if one extracts such a context around each individual\\nword of a corpus of documents the resulting matrix will be very wide\\n(many one-hot-features) with most of them being valued to zero most\\nof the time. So as to make the resulting data structure able to fit in\\nmemory the DictVectorizer class uses a scipy.sparse matrix by\\ndefault instead of a numpy.ndarray.\\n---------new doc---------\\n6.2.2. Feature hashing#\\nThe class FeatureHasher is a high-speed, low-memory vectorizer that\\nuses a technique known as\\nfeature hashing,\\nor the “hashing trick”.\\nInstead of building a hash table of the features encountered in training,\\nas the vectorizers do, instances of FeatureHasher\\napply a hash function to the features\\nto determine their column index in sample matrices directly.\\nThe result is increased speed and reduced memory usage,\\nat the expense of inspectability;\\nthe hasher does not remember what the input features looked like\\nand has no inverse_transform method.\\nSince the hash function might cause collisions between (unrelated) features,\\na signed hash function is used and the sign of the hash value\\ndetermines the sign of the value stored in the output matrix for a feature.\\nThis way, collisions are likely to cancel out rather than accumulate error,\\nand the expected mean of any output feature’s value is zero. This mechanism\\nis enabled by default with alternate_sign=True and is particularly useful\\nfor small hash table sizes (n_features < 10000). For large hash table\\nsizes, it can be disabled, to allow the output to be passed to estimators like\\nMultinomialNB or\\nchi2\\nfeature selectors that expect non-negative inputs.\\nFeatureHasher accepts either mappings\\n(like Python’s dict and its variants in the collections module),\\n(feature, value) pairs, or strings,\\ndepending on the constructor parameter input_type.\\nMapping are treated as lists of (feature, value) pairs,\\nwhile single strings have an implicit value of 1,\\nso ['feat1', 'feat2', 'feat3'] is interpreted as\\n[('feat1', 1), ('feat2', 1), ('feat3', 1)].\\n---------new doc---------\\n(like Python’s dict and its variants in the collections module),\\n(feature, value) pairs, or strings,\\ndepending on the constructor parameter input_type.\\nMapping are treated as lists of (feature, value) pairs,\\nwhile single strings have an implicit value of 1,\\nso ['feat1', 'feat2', 'feat3'] is interpreted as\\n[('feat1', 1), ('feat2', 1), ('feat3', 1)].\\nIf a single feature occurs multiple times in a sample,\\nthe associated values will be summed\\n(so ('feat', 2) and ('feat', 3.5) become ('feat', 5.5)).\\nThe output from FeatureHasher is always a scipy.sparse matrix\\nin the CSR format.\\nFeature hashing can be employed in document classification,\\nbut unlike CountVectorizer,\\nFeatureHasher does not do word\\nsplitting or any other preprocessing except Unicode-to-UTF-8 encoding;\\nsee Vectorizing a large text corpus with the hashing trick, below, for a combined tokenizer/hasher.\\nAs an example, consider a word-level natural language processing task\\nthat needs features extracted from (token, part_of_speech) pairs.\\nOne could use a Python generator function to extract features:\\ndef token_features(token, part_of_speech):\\n    if token.isdigit():\\n        yield \\\"numeric\\\"\\n    else:\\n        yield \\\"token={}\\\".format(token.lower())\\n        yield \\\"token,pos={},{}\\\".format(token, part_of_speech)\\n    if token[0].isupper():\\n        yield \\\"uppercase_initial\\\"\\n---------new doc---------\\nyield \\\"numeric\\\"\\n    else:\\n        yield \\\"token={}\\\".format(token.lower())\\n        yield \\\"token,pos={},{}\\\".format(token, part_of_speech)\\n    if token[0].isupper():\\n        yield \\\"uppercase_initial\\\"\\n    if token.isupper():\\n        yield \\\"all_uppercase\\\"\\n    yield \\\"pos={}\\\".format(part_of_speech)\\n---------new doc---------\\nThen, the raw_X to be fed to FeatureHasher.transform\\ncan be constructed using:\\nraw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)\\n\\n\\nand fed to a hasher with:\\nhasher = FeatureHasher(input_type='string')\\nX = hasher.transform(raw_X)\\n\\n\\nto get a scipy.sparse matrix X.\\nNote the use of a generator comprehension,\\nwhich introduces laziness into the feature extraction:\\ntokens are only processed on demand from the hasher.\\n\\n\\nImplementation details#\\nFeatureHasher uses the signed 32-bit variant of MurmurHash3.\\nAs a result (and because of limitations in scipy.sparse),\\nthe maximum number of features supported is currently \\\\(2^{31} - 1\\\\).\\nThe original formulation of the hashing trick by Weinberger et al.\\nused two separate hash functions \\\\(h\\\\) and \\\\(\\\\xi\\\\)\\nto determine the column index and sign of a feature, respectively.\\nThe present implementation works under the assumption\\nthat the sign bit of MurmurHash3 is independent of its other bits.\\nSince a simple modulo is used to transform the hash function to a column index,\\nit is advisable to use a power of two as the n_features parameter;\\notherwise the features will not be mapped evenly to the columns.\\nReferences\\n\\nMurmurHash3.\\n\\n\\nReferences\\n\\nKilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and\\nJosh Attenberg (2009). Feature hashing for large scale multitask learning. Proc. ICML.\\n\\n\\n\\n6.2.3. Text feature extraction#\\n---------new doc---------\\nIn this scheme, features and samples are defined as follows:\\n\\neach individual token occurrence frequency (normalized or not)\\nis treated as a feature.\\nthe vector of all the token frequencies for a given document is\\nconsidered a multivariate sample.\\n\\nA corpus of documents can thus be represented by a matrix with one row\\nper document and one column per token (e.g. word) occurring in the corpus.\\nWe call vectorization the general process of turning a collection\\nof text documents into numerical feature vectors. This specific strategy\\n(tokenization, counting and normalization) is called the Bag of Words\\nor “Bag of n-grams” representation. Documents are described by word\\noccurrences while completely ignoring the relative position information\\nof the words in the document.\\n\\n\\n6.2.3.2. Sparsity#\\nAs most documents will typically use a very small subset of the words used in\\nthe corpus, the resulting matrix will have many feature values that are\\nzeros (typically more than 99% of them).\\nFor instance a collection of 10,000 short text documents (such as emails)\\nwill use a vocabulary with a size in the order of 100,000 unique words in\\ntotal while each document will use 100 to 1000 unique words individually.\\nIn order to be able to store such a matrix in memory but also to speed\\nup algebraic operations matrix / vector, implementations will typically\\nuse a sparse representation such as the implementations available in the\\nscipy.sparse package.\\n\\n\\n6.2.3.3. Common Vectorizer usage#\\nCountVectorizer implements both tokenization and occurrence\\ncounting in a single class:\\n>>> from sklearn.feature_extraction.text import CountVectorizer\\n---------new doc---------\\n6.2.3.3. Common Vectorizer usage#\\nCountVectorizer implements both tokenization and occurrence\\ncounting in a single class:\\n>>> from sklearn.feature_extraction.text import CountVectorizer\\n\\n\\nThis model has many parameters, however the default values are quite\\nreasonable (please see  the reference documentation for the details):\\n>>> vectorizer = CountVectorizer()\\n>>> vectorizer\\nCountVectorizer()\\n\\n\\nLet’s use it to tokenize and count the word occurrences of a minimalistic\\ncorpus of text documents:\\n>>> corpus = [\\n...     'This is the first document.',\\n...     'This is the second second document.',\\n...     'And the third one.',\\n...     'Is this the first document?',\\n... ]\\n>>> X = vectorizer.fit_transform(corpus)\\n>>> X\\n<Compressed Sparse...dtype 'int64'\\n  with 19 stored elements and shape (4, 9)>\\n\\n\\nThe default configuration tokenizes the string by extracting words of\\nat least 2 letters. The specific function that does this step can be\\nrequested explicitly:\\n>>> analyze = vectorizer.build_analyzer()\\n>>> analyze(\\\"This is a text document to analyze.\\\") == (\\n...     ['this', 'is', 'text', 'document', 'to', 'analyze'])\\nTrue\\n---------new doc---------\\nThe default configuration tokenizes the string by extracting words of\\nat least 2 letters. The specific function that does this step can be\\nrequested explicitly:\\n>>> analyze = vectorizer.build_analyzer()\\n>>> analyze(\\\"This is a text document to analyze.\\\") == (\\n...     ['this', 'is', 'text', 'document', 'to', 'analyze'])\\nTrue\\n\\n\\nEach term found by the analyzer during the fit is assigned a unique\\ninteger index corresponding to a column in the resulting matrix. This\\ninterpretation of the columns can be retrieved as follows:\\n>>> vectorizer.get_feature_names_out()\\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the',\\n       'third', 'this'], ...)\\n\\n>>> X.toarray()\\narray([[0, 1, 1, 1, 0, 0, 1, 0, 1],\\n       [0, 1, 0, 1, 0, 2, 1, 0, 1],\\n       [1, 0, 0, 0, 1, 0, 1, 1, 0],\\n       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)\\n\\n\\nThe converse mapping from feature name to column index is stored in the\\nvocabulary_ attribute of the vectorizer:\\n>>> vectorizer.vocabulary_.get('document')\\n1\\n\\n\\nHence words that were not seen in the training corpus will be completely\\nignored in future calls to the transform method:\\n>>> vectorizer.transform(['Something completely new.']).toarray()\\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)\\n---------new doc---------\\nHence words that were not seen in the training corpus will be completely\\nignored in future calls to the transform method:\\n>>> vectorizer.transform(['Something completely new.']).toarray()\\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)\\n\\n\\nNote that in the previous corpus, the first and the last documents have\\nexactly the same words hence are encoded in equal vectors. In particular\\nwe lose the information that the last document is an interrogative form. To\\npreserve some of the local ordering information we can extract 2-grams\\nof words in addition to the 1-grams (individual words):\\n>>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\\n...                                     token_pattern=r'\\\\b\\\\w+\\\\b', min_df=1)\\n>>> analyze = bigram_vectorizer.build_analyzer()\\n>>> analyze('Bi-grams are cool!') == (\\n...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\\nTrue\\n---------new doc---------\\nThe vocabulary extracted by this vectorizer is hence much bigger and\\ncan now resolve ambiguities encoded in local positioning patterns:\\n>>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\\n>>> X_2\\narray([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\\n       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\\n       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\\n       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)\\n\\n\\nIn particular the interrogative form “Is this” is only present in the\\nlast document:\\n>>> feature_index = bigram_vectorizer.vocabulary_.get('is this')\\n>>> X_2[:, feature_index]\\narray([0, 0, 0, 1]...)\\n---------new doc---------\\nIn particular the interrogative form “Is this” is only present in the\\nlast document:\\n>>> feature_index = bigram_vectorizer.vocabulary_.get('is this')\\n>>> X_2[:, feature_index]\\narray([0, 0, 0, 1]...)\\n\\n\\n\\n\\n6.2.3.4. Using stop words#\\nStop words are words like “and”, “the”, “him”, which are presumed to be\\nuninformative in representing the content of a text, and which may be\\nremoved to avoid them being construed as signal for prediction.  Sometimes,\\nhowever, similar words are useful for prediction, such as in classifying\\nwriting style or personality.\\nThere are several known issues in our provided ‘english’ stop word list. It\\ndoes not aim to be a general, ‘one-size-fits-all’ solution as some tasks\\nmay require a more custom solution. See [NQY18] for more details.\\nPlease take care in choosing a stop word list.\\nPopular stop word lists may include words that are highly informative to\\nsome tasks, such as computer.\\nYou should also make sure that the stop word list has had the same\\npreprocessing and tokenization applied as the one used in the vectorizer.\\nThe word we’ve is split into we and ve by CountVectorizer’s default\\ntokenizer, so if we’ve is in stop_words, but ve is not, ve will\\nbe retained from we’ve in transformed text.  Our vectorizers will try to\\nidentify and warn about some kinds of inconsistencies.\\nReferences\\n---------new doc---------\\n[NQY18]\\nJ. Nothman, H. Qin and R. Yurchak (2018).\\n“Stop Word Lists in Free Open-source Software Packages”.\\nIn Proc. Workshop for NLP Open Source Software.\\n---------new doc---------\\n6.2.3.5. Tf–idf term weighting#\\nIn a large text corpus, some words will be very present (e.g. “the”, “a”,\\n“is” in English) hence carrying very little meaningful information about\\nthe actual contents of the document. If we were to feed the direct count\\ndata directly to a classifier those very frequent terms would shadow\\nthe frequencies of rarer yet more interesting terms.\\nIn order to re-weight the count features into floating point values\\nsuitable for usage by a classifier it is very common to use the tf–idf\\ntransform.\\nTf means term-frequency while tf–idf means term-frequency times\\ninverse document-frequency:\\n\\\\(\\\\text{tf-idf(t,d)}=\\\\text{tf(t,d)} \\\\times \\\\text{idf(t)}\\\\).\\nUsing the TfidfTransformer’s default settings,\\nTfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\\nthe term frequency, the number of times a term occurs in a given document,\\nis multiplied with idf component, which is computed as\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{1 + n}{1+\\\\text{df}(t)}} + 1\\\\),\\nwhere \\\\(n\\\\) is the total number of documents in the document set, and\\n\\\\(\\\\text{df}(t)\\\\) is the number of documents in the document set that\\ncontain term \\\\(t\\\\). The resulting tf-idf vectors are then normalized by the\\nEuclidean norm:\\n---------new doc---------\\nwhere \\\\(n\\\\) is the total number of documents in the document set, and\\n\\\\(\\\\text{df}(t)\\\\) is the number of documents in the document set that\\ncontain term \\\\(t\\\\). The resulting tf-idf vectors are then normalized by the\\nEuclidean norm:\\n\\\\(v_{norm} = \\\\frac{v}{||v||_2} = \\\\frac{v}{\\\\sqrt{v{_1}^2 +\\nv{_2}^2 + \\\\dots + v{_n}^2}}\\\\).\\nThis was originally a term weighting scheme developed for information retrieval\\n(as a ranking function for search engines results) that has also found good\\nuse in document classification and clustering.\\nThe following sections contain further explanations and examples that\\nillustrate how the tf-idfs are computed exactly and how the tf-idfs\\ncomputed in scikit-learn’s TfidfTransformer\\nand TfidfVectorizer differ slightly from the standard textbook\\nnotation that defines the idf as\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{n}{1+\\\\text{df}(t)}}.\\\\)\\nIn the TfidfTransformer and TfidfVectorizer\\nwith smooth_idf=False, the\\n“1” count is added to the idf instead of the idf’s denominator:\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{n}{\\\\text{df}(t)}} + 1\\\\)\\nThis normalization is implemented by the TfidfTransformer\\nclass:\\n>>> from sklearn.feature_extraction.text import TfidfTransformer\\n---------new doc---------\\n“1” count is added to the idf instead of the idf’s denominator:\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{n}{\\\\text{df}(t)}} + 1\\\\)\\nThis normalization is implemented by the TfidfTransformer\\nclass:\\n>>> from sklearn.feature_extraction.text import TfidfTransformer\\n>>> transformer = TfidfTransformer(smooth_idf=False)\\n>>> transformer\\nTfidfTransformer(smooth_idf=False)\\n---------new doc---------\\nAgain please see the reference documentation for the details on all the parameters.\\n\\n\\nNumeric example of a tf-idf matrix#\\nLet’s take an example with the following counts. The first term is present\\n100% of the time hence not very interesting. The two other features only\\nin less than 50% of the time hence probably more representative of the\\ncontent of the documents:\\n>>> counts = [[3, 0, 1],\\n...           [2, 0, 0],\\n...           [3, 0, 0],\\n...           [4, 0, 0],\\n...           [3, 2, 0],\\n...           [3, 0, 2]]\\n...\\n>>> tfidf = transformer.fit_transform(counts)\\n>>> tfidf\\n<Compressed Sparse...dtype 'float64'\\n  with 9 stored elements and shape (6, 3)>\\n---------new doc---------\\n>>> tfidf.toarray()\\narray([[0.81940995, 0.        , 0.57320793],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [0.47330339, 0.88089948, 0.        ],\\n      [0.58149261, 0.        , 0.81355169]])\\n---------new doc---------\\nand the vector of raw tf-idfs:\\n\\\\(\\\\text{tf-idf}_{\\\\text{raw}} = [3, 0, 2.0986].\\\\)\\nThen, applying the Euclidean (L2) norm, we obtain the following tf-idfs\\nfor document 1:\\n\\\\(\\\\frac{[3, 0, 2.0986]}{\\\\sqrt{\\\\big(3^2 + 0^2 + 2.0986^2\\\\big)}}\\n= [ 0.819,  0,  0.573].\\\\)\\nFurthermore, the default parameter smooth_idf=True adds “1” to the numerator\\nand  denominator as if an extra document was seen containing every term in the\\ncollection exactly once, which prevents zero divisions:\\n\\\\(\\\\text{idf}(t) = \\\\log{\\\\frac{1 + n}{1+\\\\text{df}(t)}} + 1\\\\)\\nUsing this modification, the tf-idf of the third term in document 1 changes to\\n1.8473:\\n\\\\(\\\\text{tf-idf}_{\\\\text{term3}} = 1 \\\\times \\\\log(7/3)+1 \\\\approx 1.8473\\\\)\\nAnd the L2-normalized tf-idf changes to\\n\\\\(\\\\frac{[3, 0, 1.8473]}{\\\\sqrt{\\\\big(3^2 + 0^2 + 1.8473^2\\\\big)}}\\n= [0.8515, 0, 0.5243]\\\\):\\n>>> transformer = TfidfTransformer()\\n>>> transformer.fit_transform(counts).toarray()\\narray([[0.85151335, 0.        , 0.52433293],\\n---------new doc---------\\n= [0.8515, 0, 0.5243]\\\\):\\n>>> transformer = TfidfTransformer()\\n>>> transformer.fit_transform(counts).toarray()\\narray([[0.85151335, 0.        , 0.52433293],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [1.        , 0.        , 0.        ],\\n      [0.55422893, 0.83236428, 0.        ],\\n      [0.63035731, 0.        , 0.77630514]])\\n---------new doc---------\\nThe weights of each\\nfeature computed by the fit method call are stored in a model\\nattribute:\\n>>> transformer.idf_\\narray([1. ..., 2.25..., 1.84...])\\n\\n\\nAs tf-idf is very often used for text features, there is also another\\nclass called TfidfVectorizer that combines all the options of\\nCountVectorizer and TfidfTransformer in a single model:\\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\\n>>> vectorizer = TfidfVectorizer()\\n>>> vectorizer.fit_transform(corpus)\\n<Compressed Sparse...dtype 'float64'\\n  with 19 stored elements and shape (4, 9)>\\n\\n\\nWhile the tf-idf normalization is often very useful, there might\\nbe cases where the binary occurrence markers might offer better\\nfeatures. This can be achieved by using the binary parameter\\nof CountVectorizer. In particular, some estimators such as\\nBernoulli Naive Bayes explicitly model discrete boolean random\\nvariables. Also, very short texts are likely to have noisy tf-idf values\\nwhile the binary occurrence info is more stable.\\nAs usual the best way to adjust the feature extraction parameters\\nis to use a cross-validated grid search, for instance by pipelining the\\nfeature extractor with a classifier:\\n\\nSample pipeline for text feature extraction and evaluation\\n---------new doc---------\\nSample pipeline for text feature extraction and evaluation\\n\\n\\n\\n\\n6.2.3.6. Decoding text files#\\nText is made of characters, but files are made of bytes. These bytes represent\\ncharacters according to some encoding. To work with text files in Python,\\ntheir bytes must be decoded to a character set called Unicode.\\nCommon encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian)\\nand the universal encodings UTF-8 and UTF-16. Many others exist.\\n\\nNote\\nAn encoding can also be called a ‘character set’,\\nbut this term is less accurate: several encodings can exist\\nfor a single character set.\\n\\nThe text feature extractors in scikit-learn know how to decode text files,\\nbut only if you tell them what encoding the files are in.\\nThe CountVectorizer takes an encoding parameter for this purpose.\\nFor modern text files, the correct encoding is probably UTF-8,\\nwhich is therefore the default (encoding=\\\"utf-8\\\").\\nIf the text you are loading is not actually encoded with UTF-8, however,\\nyou will get a UnicodeDecodeError.\\nThe vectorizers can be told to be silent about decoding errors\\nby setting the decode_error parameter to either \\\"ignore\\\"\\nor \\\"replace\\\". See the documentation for the Python function\\nbytes.decode for more details\\n(type help(bytes.decode) at the Python prompt).\\n\\n\\nTroubleshooting decoding text#\\nIf you are having trouble decoding text, here are some things to try:\\n---------new doc---------\\nTroubleshooting decoding text#\\nIf you are having trouble decoding text, here are some things to try:\\n\\nFind out what the actual encoding of the text is. The file might come\\nwith a header or README that tells you the encoding, or there might be some\\nstandard encoding you can assume based on where the text comes from.\\nYou may be able to find out what kind of encoding it is in general\\nusing the UNIX command file. The Python chardet module comes with\\na script called chardetect.py that will guess the specific encoding,\\nthough you cannot rely on its guess being correct.\\nYou could try UTF-8 and disregard the errors. You can decode byte\\nstrings with bytes.decode(errors='replace') to replace all\\ndecoding errors with a meaningless character, or set\\ndecode_error='replace' in the vectorizer. This may damage the\\nusefulness of your features.\\nReal text may come from a variety of sources that may have used different\\nencodings, or even be sloppily decoded in a different encoding than the\\none it was encoded with. This is common in text retrieved from the Web.\\nThe Python package ftfy\\ncan automatically sort out some classes of\\ndecoding errors, so you could try decoding the unknown text as latin-1\\nand then using ftfy to fix errors.\\nIf the text is in a mish-mash of encodings that is simply too hard to sort\\nout (which is the case for the 20 Newsgroups dataset), you can fall back on\\na simple single-byte encoding such as latin-1. Some text may display\\nincorrectly, but at least the same sequence of bytes will always represent\\nthe same feature.\\n---------new doc---------\\nFor example, the following snippet uses chardet\\n(not shipped with scikit-learn, must be installed separately)\\nto figure out the encoding of three texts.\\nIt then vectorizes the texts and prints the learned vocabulary.\\nThe output is not shown here.\\n>>> import chardet    \\n>>> text1 = b\\\"Sei mir gegr\\\\xc3\\\\xbc\\\\xc3\\\\x9ft mein Sauerkraut\\\"\\n>>> text2 = b\\\"holdselig sind deine Ger\\\\xfcche\\\"\\n>>> text3 = b\\\"\\\\xff\\\\xfeA\\\\x00u\\\\x00f\\\\x00 \\\\x00F\\\\x00l\\\\x00\\\\xfc\\\\x00g\\\\x00e\\\\x00l\\\\x00n\\\\x00 \\\\x00d\\\\x00e\\\\x00s\\\\x00 \\\\x00G\\\\x00e\\\\x00s\\\\x00a\\\\x00n\\\\x00g\\\\x00e\\\\x00s\\\\x00,\\\\x00 \\\\x00H\\\\x00e\\\\x00r\\\\x00z\\\\x00l\\\\x00i\\\\x00e\\\\x00b\\\\x00c\\\\x00h\\\\x00e\\\\x00n\\\\x00,\\\\x00 \\\\x00t\\\\x00r\\\\x00a\\\\x00g\\\\x00 \\\\x00i\\\\x00c\\\\x00h\\\\x00 \\\\x00d\\\\x00i\\\\x00c\\\\x00h\\\\x00 \\\\x00f\\\\x00o\\\\x00r\\\\x00t\\\\x00\\\"\\n>>> decoded = [x.decode(chardet.detect(x)['encoding'])\\n---------new doc---------\\n>>> decoded = [x.decode(chardet.detect(x)['encoding'])\\n...            for x in (text1, text2, text3)]        \\n>>> v = CountVectorizer().fit(decoded).vocabulary_    \\n>>> for term in v: print(v)\\n---------new doc---------\\n(Depending on the version of chardet, it might get the first one wrong.)\\nFor an introduction to Unicode and character encodings in general,\\nsee Joel Spolsky’s Absolute Minimum Every Software Developer Must Know\\nAbout Unicode.\\n\\n\\n\\n6.2.3.7. Applications and examples#\\nThe bag of words representation is quite simplistic but surprisingly\\nuseful in practice.\\nIn particular in a supervised setting it can be successfully combined\\nwith fast and scalable linear models to train document classifiers,\\nfor instance:\\n\\nClassification of text documents using sparse features\\n\\nIn an unsupervised setting it can be used to group similar documents\\ntogether by applying clustering algorithms such as K-means:\\n\\nClustering text documents using k-means\\n\\nFinally it is possible to discover the main topics of a corpus by\\nrelaxing the hard assignment constraint of clustering, for instance by\\nusing Non-negative matrix factorization (NMF or NNMF):\\n\\nTopic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\\n---------new doc---------\\n6.2.3.8. Limitations of the Bag of Words representation#\\nA collection of unigrams (what bag of words is) cannot capture phrases\\nand multi-word expressions, effectively disregarding any word order\\ndependence. Additionally, the bag of words model doesn’t account for potential\\nmisspellings or word derivations.\\nN-grams to the rescue! Instead of building a simple collection of\\nunigrams (n=1), one might prefer a collection of bigrams (n=2), where\\noccurrences of pairs of consecutive words are counted.\\nOne might alternatively consider a collection of character n-grams, a\\nrepresentation resilient against misspellings and derivations.\\nFor example, let’s say we’re dealing with a corpus of two documents:\\n['words', 'wprds']. The second document contains a misspelling\\nof the word ‘words’.\\nA simple bag of words representation would consider these two as\\nvery distinct documents, differing in both of the two possible features.\\nA character 2-gram representation, however, would find the documents\\nmatching in 4 out of 8 features, which may help the preferred classifier\\ndecide better:\\n>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\\n>>> counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\\n>>> ngram_vectorizer.get_feature_names_out()\\narray([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...)\\n>>> counts.toarray().astype(int)\\narray([[1, 1, 1, 0, 1, 1, 1, 0],\\n---------new doc---------\\n>>> ngram_vectorizer.get_feature_names_out()\\narray([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...)\\n>>> counts.toarray().astype(int)\\narray([[1, 1, 1, 0, 1, 1, 1, 0],\\n       [1, 1, 0, 1, 1, 1, 0, 1]])\\n---------new doc---------\\nIn the above example, char_wb analyzer is used, which creates n-grams\\nonly from characters inside word boundaries (padded with space on each\\nside). The char analyzer, alternatively, creates n-grams that\\nspan across words:\\n>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\\n>>> ngram_vectorizer.fit_transform(['jumpy fox'])\\n<Compressed Sparse...dtype 'int64'\\n  with 4 stored elements and shape (1, 4)>\\n\\n>>> ngram_vectorizer.get_feature_names_out()\\narray([' fox ', ' jump', 'jumpy', 'umpy '], ...)\\n\\n>>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5))\\n>>> ngram_vectorizer.fit_transform(['jumpy fox'])\\n<Compressed Sparse...dtype 'int64'\\n  with 5 stored elements and shape (1, 5)>\\n>>> ngram_vectorizer.get_feature_names_out()\\narray(['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'], ...)\\n---------new doc---------\\nThe word boundaries-aware variant char_wb is especially interesting\\nfor languages that use white-spaces for word separation as it generates\\nsignificantly less noisy features than the raw char variant in\\nthat case. For such languages it can increase both the predictive\\naccuracy and convergence speed of classifiers trained using such\\nfeatures while retaining the robustness with regards to misspellings and\\nword derivations.\\nWhile some local positioning information can be preserved by extracting\\nn-grams instead of individual words, bag of words and bag of n-grams\\ndestroy most of the inner structure of the document and hence most of\\nthe meaning carried by that internal structure.\\nIn order to address the wider task of Natural Language Understanding,\\nthe local structure of sentences and paragraphs should thus be taken\\ninto account. Many such models will thus be casted as “Structured output”\\nproblems which are currently outside of the scope of scikit-learn.\\n\\n\\n6.2.3.9. Vectorizing a large text corpus with the hashing trick#\\nThe above vectorization scheme is simple but the fact that it holds an in-\\nmemory mapping from the string tokens to the integer feature indices (the\\nvocabulary_ attribute) causes several problems when dealing with large\\ndatasets:\\n---------new doc---------\\nIt is possible to overcome those limitations by combining the “hashing trick”\\n(Feature hashing) implemented by the\\nFeatureHasher class and the text\\npreprocessing and tokenization features of the CountVectorizer.\\nThis combination is implementing in HashingVectorizer,\\na transformer class that is mostly API compatible with CountVectorizer.\\nHashingVectorizer is stateless,\\nmeaning that you don’t have to call fit on it:\\n>>> from sklearn.feature_extraction.text import HashingVectorizer\\n>>> hv = HashingVectorizer(n_features=10)\\n>>> hv.transform(corpus)\\n<Compressed Sparse...dtype 'float64'\\n  with 16 stored elements and shape (4, 10)>\\n---------new doc---------\\nYou can see that 16 non-zero feature tokens were extracted in the vector\\noutput: this is less than the 19 non-zeros extracted previously by the\\nCountVectorizer on the same toy corpus. The discrepancy comes from\\nhash function collisions because of the low value of the n_features parameter.\\nIn a real world setting, the n_features parameter can be left to its\\ndefault value of 2 ** 20 (roughly one million possible features). If memory\\nor downstream models size is an issue selecting a lower value such as 2 **\\n18 might help without introducing too many additional collisions on typical\\ntext classification tasks.\\nNote that the dimensionality does not affect the CPU training time of\\nalgorithms which operate on CSR matrices (LinearSVC(dual=True),\\nPerceptron, SGDClassifier, PassiveAggressive) but it does for\\nalgorithms that work with CSC matrices (LinearSVC(dual=False), Lasso(),\\netc.).\\nLet’s try again with the default setting:\\n>>> hv = HashingVectorizer()\\n>>> hv.transform(corpus)\\n<Compressed Sparse...dtype 'float64'\\n  with 19 stored elements and shape (4, 1048576)>\\n\\n\\nWe no longer get the collisions, but this comes at the expense of a much larger\\ndimensionality of the output space.\\nOf course, other terms than the 19 used here\\nmight still collide with each other.\\nThe HashingVectorizer also comes with the following limitations:\\n---------new doc---------\\nWe no longer get the collisions, but this comes at the expense of a much larger\\ndimensionality of the output space.\\nOf course, other terms than the 19 used here\\nmight still collide with each other.\\nThe HashingVectorizer also comes with the following limitations:\\n\\nit is not possible to invert the model (no inverse_transform method),\\nnor to access the original string representation of the features,\\nbecause of the one-way nature of the hash function that performs the mapping.\\nit does not provide IDF weighting as that would introduce statefulness in the\\nmodel. A TfidfTransformer can be appended to it in a pipeline if\\nrequired.\\n\\n\\n\\nPerforming out-of-core scaling with HashingVectorizer#\\nAn interesting development of using a HashingVectorizer is the ability\\nto perform out-of-core scaling. This means that we can learn from data that\\ndoes not fit into the computer’s main memory.\\nA strategy to implement out-of-core scaling is to stream data to the estimator\\nin mini-batches. Each mini-batch is vectorized using HashingVectorizer\\nso as to guarantee that the input space of the estimator has always the same\\ndimensionality. The amount of memory used at any time is thus bounded by the\\nsize of a mini-batch. Although there is no limit to the amount of data that can\\nbe ingested using such an approach, from a practical point of view the learning\\ntime is often limited by the CPU time one wants to spend on the task.\\nFor a full-fledged example of out-of-core scaling in a text classification\\ntask see Out-of-core classification of text documents.\\n---------new doc---------\\n6.2.3.10. Customizing the vectorizer classes#\\nIt is possible to customize the behavior by passing a callable\\nto the vectorizer constructor:\\n>>> def my_tokenizer(s):\\n...     return s.split()\\n...\\n>>> vectorizer = CountVectorizer(tokenizer=my_tokenizer)\\n>>> vectorizer.build_analyzer()(u\\\"Some... punctuation!\\\") == (\\n...     ['some...', 'punctuation!'])\\nTrue\\n\\n\\nIn particular we name:\\n\\npreprocessor: a callable that takes an entire document as input (as a\\nsingle string), and returns a possibly transformed version of the document,\\nstill as an entire string. This can be used to remove HTML tags, lowercase\\nthe entire document, etc.\\ntokenizer: a callable that takes the output from the preprocessor\\nand splits it into tokens, then returns a list of these.\\nanalyzer: a callable that replaces the preprocessor and tokenizer.\\nThe default analyzers all call the preprocessor and tokenizer, but custom\\nanalyzers will skip this. N-gram extraction and stop word filtering take\\nplace at the analyzer level, so a custom analyzer may have to reproduce\\nthese steps.\\n\\n(Lucene users might recognize these names, but be aware that scikit-learn\\nconcepts may not map one-to-one onto Lucene concepts.)\\nTo make the preprocessor, tokenizer and analyzers aware of the model\\nparameters it is possible to derive from the class and override the\\nbuild_preprocessor, build_tokenizer and build_analyzer\\nfactory methods instead of passing custom functions.\\n\\n\\nTips and tricks#\\n---------new doc---------\\n(Lucene users might recognize these names, but be aware that scikit-learn\\nconcepts may not map one-to-one onto Lucene concepts.)\\nTo make the preprocessor, tokenizer and analyzers aware of the model\\nparameters it is possible to derive from the class and override the\\nbuild_preprocessor, build_tokenizer and build_analyzer\\nfactory methods instead of passing custom functions.\\n\\n\\nTips and tricks#\\n\\nIf documents are pre-tokenized by an external package, then store them in\\nfiles (or strings) with the tokens separated by whitespace and pass\\nanalyzer=str.split\\nFancy token-level analysis such as stemming, lemmatizing, compound\\nsplitting, filtering based on part-of-speech, etc. are not included in the\\nscikit-learn codebase, but can be added by customizing either the\\ntokenizer or the analyzer.\\nHere’s a CountVectorizer with a tokenizer and lemmatizer using\\nNLTK:\\n>>> from nltk import word_tokenize          \\n>>> from nltk.stem import WordNetLemmatizer \\n>>> class LemmaTokenizer:\\n...     def __init__(self):\\n...         self.wnl = WordNetLemmatizer()\\n...     def __call__(self, doc):\\n...         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\\n...\\n>>> vect = CountVectorizer(tokenizer=LemmaTokenizer())\\n---------new doc---------\\n(Note that this will not filter out punctuation.)\\nThe following example will, for instance, transform some British spelling\\nto American spelling:\\n>>> import re\\n>>> def to_british(tokens):\\n...     for t in tokens:\\n...         t = re.sub(r\\\"(...)our$\\\", r\\\"\\\\1or\\\", t)\\n...         t = re.sub(r\\\"([bt])re$\\\", r\\\"\\\\1er\\\", t)\\n...         t = re.sub(r\\\"([iy])s(e$|ing|ation)\\\", r\\\"\\\\1z\\\\2\\\", t)\\n...         t = re.sub(r\\\"ogue$\\\", \\\"og\\\", t)\\n...         yield t\\n...\\n>>> class CustomVectorizer(CountVectorizer):\\n...     def build_tokenizer(self):\\n...         tokenize = super().build_tokenizer()\\n...         return lambda doc: list(to_british(tokenize(doc)))\\n...\\n>>> print(CustomVectorizer().build_analyzer()(u\\\"color colour\\\"))\\n[...'color', ...'color']\\n\\n\\nfor other styles of preprocessing; examples include stemming, lemmatization,\\nor normalizing numerical tokens, with the latter illustrated in:\\n\\nBiclustering documents with the Spectral Co-clustering algorithm\\n\\n\\n\\nCustomizing the vectorizer can also be useful when handling Asian languages\\nthat do not use an explicit word separator such as whitespace.\\n\\n\\n\\n\\n6.2.4. Image feature extraction#\\n---------new doc---------\\n>>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,\\n...     random_state=0)\\n>>> patches.shape\\n(2, 2, 2, 3)\\n>>> patches[:, :, :, 0]\\narray([[[ 0,  3],\\n        [12, 15]],\\n\\n       [[15, 18],\\n        [27, 30]]])\\n>>> patches = image.extract_patches_2d(one_image, (2, 2))\\n>>> patches.shape\\n(9, 2, 2, 3)\\n>>> patches[4, :, :, 0]\\narray([[15, 18],\\n       [27, 30]])\\n\\n\\nLet us now try to reconstruct the original image from the patches by averaging\\non overlapping areas:\\n>>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))\\n>>> np.testing.assert_array_equal(one_image, reconstructed)\\n\\n\\nThe PatchExtractor class works in the same way as\\nextract_patches_2d, only it supports multiple images as input. It is\\nimplemented as a scikit-learn transformer, so it can be used in pipelines. See:\\n>>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\\n>>> patches = image.PatchExtractor(patch_size=(2, 2)).transform(five_images)\\n>>> patches.shape\\n(45, 2, 2, 3)\\n---------new doc---------\\n>>> scaler.mean_\\narray([1. ..., 0. ..., 0.33...])\\n\\n>>> scaler.scale_\\narray([0.81..., 0.81..., 1.24...])\\n\\n>>> X_scaled = scaler.transform(X_train)\\n>>> X_scaled\\narray([[ 0.  ..., -1.22...,  1.33...],\\n       [ 1.22...,  0.  ..., -0.26...],\\n       [-1.22...,  1.22..., -1.06...]])\\n\\n\\nScaled data has zero mean and unit variance:\\n>>> X_scaled.mean(axis=0)\\narray([0., 0., 0.])\\n\\n>>> X_scaled.std(axis=0)\\narray([1., 1., 1.])\\n\\n\\nThis class implements the Transformer API to compute the mean and\\nstandard deviation on a training set so as to be able to later re-apply the\\nsame transformation on the testing set. This class is hence suitable for\\nuse in the early steps of a Pipeline:\\n>>> from sklearn.datasets import make_classification\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn.pipeline import make_pipeline\\n>>> from sklearn.preprocessing import StandardScaler\\n---------new doc---------\\n6.3.1.1. Scaling features to a range#\\nAn alternative standardization is scaling features to\\nlie between a given minimum and maximum value, often between zero and one,\\nor so that the maximum absolute value of each feature is scaled to unit size.\\nThis can be achieved using MinMaxScaler or MaxAbsScaler,\\nrespectively.\\nThe motivation to use this scaling include robustness to very small\\nstandard deviations of features and preserving zero entries in sparse data.\\nHere is an example to scale a toy data matrix to the [0, 1] range:\\n>>> X_train = np.array([[ 1., -1.,  2.],\\n...                     [ 2.,  0.,  0.],\\n...                     [ 0.,  1., -1.]])\\n...\\n>>> min_max_scaler = preprocessing.MinMaxScaler()\\n>>> X_train_minmax = min_max_scaler.fit_transform(X_train)\\n>>> X_train_minmax\\narray([[0.5       , 0.        , 1.        ],\\n       [1.        , 0.5       , 0.33333333],\\n       [0.        , 1.        , 0.        ]])\\n---------new doc---------\\nThe same instance of the transformer can then be applied to some new test data\\nunseen during the fit call: the same scaling and shifting operations will be\\napplied to be consistent with the transformation performed on the train data:\\n>>> X_test = np.array([[-3., -1.,  4.]])\\n>>> X_test_minmax = min_max_scaler.transform(X_test)\\n>>> X_test_minmax\\narray([[-1.5       ,  0.        ,  1.66666667]])\\n\\n\\nIt is possible to introspect the scaler attributes to find about the exact\\nnature of the transformation learned on the training data:\\n>>> min_max_scaler.scale_\\narray([0.5       , 0.5       , 0.33...])\\n\\n>>> min_max_scaler.min_\\narray([0.        , 0.5       , 0.33...])\\n\\n\\nIf MinMaxScaler is given an explicit feature_range=(min, max) the\\nfull formula is:\\nX_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\\n\\nX_scaled = X_std * (max - min) + min\\n---------new doc---------\\n6.3.1.2. Scaling sparse data#\\nCentering sparse data would destroy the sparseness structure in the data, and\\nthus rarely is a sensible thing to do. However, it can make sense to scale\\nsparse inputs, especially if features are on different scales.\\nMaxAbsScaler was specifically designed for scaling\\nsparse data, and is the recommended way to go about this.\\nHowever, StandardScaler can accept scipy.sparse\\nmatrices  as input, as long as with_mean=False is explicitly passed\\nto the constructor. Otherwise a ValueError will be raised as\\nsilently centering would break the sparsity and would often crash the\\nexecution by allocating excessive amounts of memory unintentionally.\\nRobustScaler cannot be fitted to sparse inputs, but you can use\\nthe transform method on sparse inputs.\\nNote that the scalers accept both Compressed Sparse Rows and Compressed\\nSparse Columns format (see scipy.sparse.csr_matrix and\\nscipy.sparse.csc_matrix). Any other sparse input will be converted to\\nthe Compressed Sparse Rows representation.  To avoid unnecessary memory\\ncopies, it is recommended to choose the CSR or CSC representation upstream.\\nFinally, if the centered data is expected to be small enough, explicitly\\nconverting the input to an array using the toarray method of sparse matrices\\nis another option.\\n\\n\\n6.3.1.3. Scaling data with outliers#\\nIf your data contains many outliers, scaling using the mean and variance\\nof the data is likely to not work very well. In these cases, you can use\\nRobustScaler as a drop-in replacement instead. It uses\\nmore robust estimates for the center and range of your data.\\n---------new doc---------\\n6.3.2.1. Mapping to a Uniform distribution#\\nQuantileTransformer provides a non-parametric\\ntransformation to map the data to a uniform distribution\\nwith values between 0 and 1:\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.model_selection import train_test_split\\n>>> X, y = load_iris(return_X_y=True)\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n>>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\\n>>> X_train_trans = quantile_transformer.fit_transform(X_train)\\n>>> X_test_trans = quantile_transformer.transform(X_test)\\n>>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) \\narray([ 4.3,  5.1,  5.8,  6.5,  7.9])\\n\\n\\nThis feature corresponds to the sepal length in cm. Once the quantile\\ntransformation applied, those landmarks approach closely the percentiles\\npreviously defined:\\n>>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])\\n... \\narray([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])\\n---------new doc---------\\nYeo-Johnson transform#\\n\\n\\\\[\\\\begin{split}x_i^{(\\\\lambda)} =\\n\\\\begin{cases}\\n[(x_i + 1)^\\\\lambda - 1] / \\\\lambda & \\\\text{if } \\\\lambda \\\\neq 0, x_i \\\\geq 0, \\\\\\\\[8pt]\\n\\\\ln{(x_i + 1)} & \\\\text{if } \\\\lambda = 0, x_i \\\\geq 0 \\\\\\\\[8pt]\\n-[(-x_i + 1)^{2 - \\\\lambda} - 1] / (2 - \\\\lambda) & \\\\text{if } \\\\lambda \\\\neq 2, x_i < 0, \\\\\\\\[8pt]\\n- \\\\ln (- x_i + 1) & \\\\text{if } \\\\lambda = 2, x_i < 0\\n\\\\end{cases}\\\\end{split}\\\\]\\n\\n\\n\\nBox-Cox transform#\\n---------new doc---------\\nBox-Cox transform#\\n\\n\\\\[\\\\begin{split}x_i^{(\\\\lambda)} =\\n\\\\begin{cases}\\n\\\\dfrac{x_i^\\\\lambda - 1}{\\\\lambda} & \\\\text{if } \\\\lambda \\\\neq 0, \\\\\\\\[8pt]\\n\\\\ln{(x_i)} & \\\\text{if } \\\\lambda = 0,\\n\\\\end{cases}\\\\end{split}\\\\]\\nBox-Cox can only be applied to strictly positive data. In both methods, the\\ntransformation is parameterized by \\\\(\\\\lambda\\\\), which is determined through\\nmaximum likelihood estimation. Here is an example of using Box-Cox to map\\nsamples drawn from a lognormal distribution to a normal distribution:\\n>>> pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\\n>>> X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\\n>>> X_lognormal\\narray([[1.28..., 1.18..., 0.84...],\\n      [0.94..., 1.60..., 0.38...],\\n      [1.35..., 0.21..., 1.09...]])\\n>>> pt.fit_transform(X_lognormal)\\narray([[ 0.49...,  0.17..., -0.15...],\\n      [-0.05...,  0.58..., -0.57...],\\n      [ 0.69..., -0.84...,  0.10...]])\\n---------new doc---------\\nWhile the above example sets the standardize option to False,\\nPowerTransformer will apply zero-mean, unit-variance normalization\\nto the transformed output by default.\\n\\nBelow are examples of Box-Cox and Yeo-Johnson applied to various probability\\ndistributions.  Note that when applied to certain distributions, the power\\ntransforms achieve very Gaussian-like results, but with others, they are\\nineffective. This highlights the importance of visualizing the data before and\\nafter transformation.\\n\\n\\n\\n\\nIt is also possible to map data to a normal distribution using\\nQuantileTransformer by setting output_distribution='normal'.\\nUsing the earlier example with the iris dataset:\\n>>> quantile_transformer = preprocessing.QuantileTransformer(\\n...     output_distribution='normal', random_state=0)\\n>>> X_trans = quantile_transformer.fit_transform(X)\\n>>> quantile_transformer.quantiles_\\narray([[4.3, 2. , 1. , 0.1],\\n       [4.4, 2.2, 1.1, 0.1],\\n       [4.4, 2.2, 1.2, 0.1],\\n       ...,\\n       [7.7, 4.1, 6.7, 2.5],\\n       [7.7, 4.2, 6.7, 2.5],\\n       [7.9, 4.4, 6.9, 2.5]])\\n---------new doc---------\\n>>> X_normalized\\narray([[ 0.40..., -0.40...,  0.81...],\\n       [ 1.  ...,  0.  ...,  0.  ...],\\n       [ 0.  ...,  0.70..., -0.70...]])\\n\\n\\nThe preprocessing module further provides a utility class\\nNormalizer that implements the same operation using the\\nTransformer API (even though the fit method is useless in this case:\\nthe class is stateless as this operation treats samples independently).\\nThis class is hence suitable for use in the early steps of a\\nPipeline:\\n>>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\\n>>> normalizer\\nNormalizer()\\n\\n\\nThe normalizer instance can then be used on sample vectors as any transformer:\\n>>> normalizer.transform(X)\\narray([[ 0.40..., -0.40...,  0.81...],\\n       [ 1.  ...,  0.  ...,  0.  ...],\\n       [ 0.  ...,  0.70..., -0.70...]])\\n\\n>>> normalizer.transform([[-1.,  1., 0.]])\\narray([[-0.70...,  0.70...,  0.  ...]])\\n\\n\\nNote: L2 normalization is also known as spatial sign preprocessing.\\n---------new doc---------\\nSparse input#\\nnormalize and Normalizer accept both dense array-like\\nand sparse matrices from scipy.sparse as input.\\nFor sparse input the data is converted to the Compressed Sparse Rows\\nrepresentation (see scipy.sparse.csr_matrix) before being fed to\\nefficient Cython routines. To avoid unnecessary memory copies, it is\\nrecommended to choose the CSR representation upstream.\\n\\n\\n\\n6.3.4. Encoding categorical features#\\nOften features are not given as continuous values but categorical.\\nFor example a person could have features [\\\"male\\\", \\\"female\\\"],\\n[\\\"from Europe\\\", \\\"from US\\\", \\\"from Asia\\\"],\\n[\\\"uses Firefox\\\", \\\"uses Chrome\\\", \\\"uses Safari\\\", \\\"uses Internet Explorer\\\"].\\nSuch features can be efficiently coded as integers, for instance\\n[\\\"male\\\", \\\"from US\\\", \\\"uses Internet Explorer\\\"] could be expressed as\\n[0, 1, 3] while [\\\"female\\\", \\\"from Asia\\\", \\\"uses Chrome\\\"] would be\\n[1, 2, 1].\\nTo convert categorical features to such integer codes, we can use the\\nOrdinalEncoder. This estimator transforms each categorical feature to one\\nnew feature of integers (0 to n_categories - 1):\\n>>> enc = preprocessing.OrdinalEncoder()\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOrdinalEncoder()\\n>>> enc.transform([['female', 'from US', 'uses Safari']])\\narray([[0., 1., 1.]])\\n---------new doc---------\\nSuch integer representation can, however, not be used directly with all\\nscikit-learn estimators, as these expect continuous input, and would interpret\\nthe categories as being ordered, which is often not desired (i.e. the set of\\nbrowsers was ordered arbitrarily).\\nBy default, OrdinalEncoder will also passthrough missing values that\\nare indicated by np.nan.\\n>>> enc = preprocessing.OrdinalEncoder()\\n>>> X = [['male'], ['female'], [np.nan], ['female']]\\n>>> enc.fit_transform(X)\\narray([[ 1.],\\n       [ 0.],\\n       [nan],\\n       [ 0.]])\\n\\n\\nOrdinalEncoder provides a parameter encoded_missing_value to encode\\nthe missing values without the need to create a pipeline and using\\nSimpleImputer.\\n>>> enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\\n>>> X = [['male'], ['female'], [np.nan], ['female']]\\n>>> enc.fit_transform(X)\\narray([[ 1.],\\n       [ 0.],\\n       [-1.],\\n       [ 0.]])\\n---------new doc---------\\nThe above processing is equivalent to the following pipeline:\\n>>> from sklearn.pipeline import Pipeline\\n>>> from sklearn.impute import SimpleImputer\\n>>> enc = Pipeline(steps=[\\n...     (\\\"encoder\\\", preprocessing.OrdinalEncoder()),\\n...     (\\\"imputer\\\", SimpleImputer(strategy=\\\"constant\\\", fill_value=-1)),\\n... ])\\n>>> enc.fit_transform(X)\\narray([[ 1.],\\n       [ 0.],\\n       [-1.],\\n       [ 0.]])\\n\\n\\nAnother possibility to convert categorical features to features that can be used\\nwith scikit-learn estimators is to use a one-of-K, also known as one-hot or\\ndummy encoding.\\nThis type of encoding can be obtained with the OneHotEncoder,\\nwhich transforms each categorical feature with\\nn_categories possible values into n_categories binary features, with\\none of them 1, and all others 0.\\nContinuing the example above:\\n>>> enc = preprocessing.OneHotEncoder()\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOneHotEncoder()\\n>>> enc.transform([['female', 'from US', 'uses Safari'],\\n...                ['male', 'from Europe', 'uses Safari']]).toarray()\\narray([[1., 0., 0., 1., 0., 1.],\\n       [0., 1., 1., 0., 0., 1.]])\\n---------new doc---------\\nBy default, the values each feature can take is inferred automatically\\nfrom the dataset and can be found in the categories_ attribute:\\n>>> enc.categories_\\n[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)]\\n---------new doc---------\\nIt is possible to specify this explicitly using the parameter categories.\\nThere are two genders, four possible continents and four web browsers in our\\ndataset:\\n>>> genders = ['female', 'male']\\n>>> locations = ['from Africa', 'from Asia', 'from Europe', 'from US']\\n>>> browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\\n>>> enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\\n>>> # Note that for there are missing categorical values for the 2nd and 3rd\\n>>> # feature\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOneHotEncoder(categories=[['female', 'male'],\\n                          ['from Africa', 'from Asia', 'from Europe',\\n                           'from US'],\\n                          ['uses Chrome', 'uses Firefox', 'uses IE',\\n                           'uses Safari']])\\n>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\\narray([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])\\n---------new doc---------\\nIf there is a possibility that the training data might have missing categorical\\nfeatures, it can often be better to specify\\nhandle_unknown='infrequent_if_exist' instead of setting the categories\\nmanually as above. When handle_unknown='infrequent_if_exist' is specified\\nand unknown categories are encountered during transform, no error will be\\nraised but the resulting one-hot encoded columns for this feature will be all\\nzeros or considered as an infrequent category if enabled.\\n(handle_unknown='infrequent_if_exist' is only supported for one-hot\\nencoding):\\n>>> enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist')\\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\\n>>> enc.fit(X)\\nOneHotEncoder(handle_unknown='infrequent_if_exist')\\n>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\\narray([[1., 0., 0., 0., 0., 0.]])\\n---------new doc---------\\nIt is also possible to encode each column into n_categories - 1 columns\\ninstead of n_categories columns by using the drop parameter. This\\nparameter allows the user to specify a category for each feature to be dropped.\\nThis is useful to avoid co-linearity in the input matrix in some classifiers.\\nSuch functionality is useful, for example, when using non-regularized\\nregression (LinearRegression),\\nsince co-linearity would cause the covariance matrix to be non-invertible:\\n>>> X = [['male', 'from US', 'uses Safari'],\\n...      ['female', 'from Europe', 'uses Firefox']]\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)\\n>>> drop_enc.categories_\\n[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object),\\n array(['uses Firefox', 'uses Safari'], dtype=object)]\\n>>> drop_enc.transform(X).toarray()\\narray([[1., 1., 1.],\\n       [0., 0., 0.]])\\n---------new doc---------\\nOne might want to drop one of the two columns only for features with 2\\ncategories. In this case, you can set the parameter drop='if_binary'.\\n>>> X = [['male', 'US', 'Safari'],\\n...      ['female', 'Europe', 'Firefox'],\\n...      ['female', 'Asia', 'Chrome']]\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary').fit(X)\\n>>> drop_enc.categories_\\n[array(['female', 'male'], dtype=object), array(['Asia', 'Europe', 'US'], dtype=object),\\n array(['Chrome', 'Firefox', 'Safari'], dtype=object)]\\n>>> drop_enc.transform(X).toarray()\\narray([[1., 0., 0., 1., 0., 0., 1.],\\n       [0., 0., 1., 0., 0., 1., 0.],\\n       [0., 1., 0., 0., 1., 0., 0.]])\\n---------new doc---------\\nIn the transformed X, the first column is the encoding of the feature with\\ncategories “male”/”female”, while the remaining 6 columns is the encoding of\\nthe 2 features with respectively 3 categories each.\\nWhen handle_unknown='ignore' and drop is not None, unknown categories will\\nbe encoded as all zeros:\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='first',\\n...                                        handle_unknown='ignore').fit(X)\\n>>> X_test = [['unknown', 'America', 'IE']]\\n>>> drop_enc.transform(X_test).toarray()\\narray([[0., 0., 0., 0., 0.]])\\n---------new doc---------\\nAll the categories in X_test are unknown during transform and will be mapped\\nto all zeros. This means that unknown categories will have the same mapping as\\nthe dropped category. OneHotEncoder.inverse_transform will map all zeros\\nto the dropped category if a category is dropped and None if a category is\\nnot dropped:\\n>>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse_output=False,\\n...                                        handle_unknown='ignore').fit(X)\\n>>> X_test = [['unknown', 'America', 'IE']]\\n>>> X_trans = drop_enc.transform(X_test)\\n>>> X_trans\\narray([[0., 0., 0., 0., 0., 0., 0.]])\\n>>> drop_enc.inverse_transform(X_trans)\\narray([['female', None, None]], dtype=object)\\n---------new doc---------\\nSupport of categorical features with missing values#\\nOneHotEncoder supports categorical features with missing values by\\nconsidering the missing values as an additional category:\\n>>> X = [['male', 'Safari'],\\n...      ['female', None],\\n...      [np.nan, 'Firefox']]\\n>>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\\n>>> enc.categories_\\n[array(['female', 'male', nan], dtype=object),\\narray(['Firefox', 'Safari', None], dtype=object)]\\n>>> enc.transform(X).toarray()\\narray([[0., 1., 0., 0., 1., 0.],\\n      [1., 0., 0., 0., 0., 1.],\\n      [0., 0., 1., 1., 0., 0.]])\\n\\n\\nIf a feature contains both np.nan and None, they will be considered\\nseparate categories:\\n>>> X = [['Safari'], [None], [np.nan], ['Firefox']]\\n>>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\\n>>> enc.categories_\\n[array(['Firefox', 'Safari', None, nan], dtype=object)]\\n>>> enc.transform(X).toarray()\\narray([[0., 1., 0., 0.],\\n      [0., 0., 1., 0.],\\n      [0., 0., 0., 1.],\\n      [1., 0., 0., 0.]])\\n---------new doc---------\\nSee Loading features from dicts for categorical features that are\\nrepresented as a dict, not as scalars.\\n\\n\\n6.3.4.1. Infrequent categories#\\nOneHotEncoder and OrdinalEncoder support aggregating\\ninfrequent categories into a single output for each feature. The parameters to\\nenable the gathering of infrequent categories are min_frequency and\\nmax_categories.\\n\\nmin_frequency is either an  integer greater or equal to 1, or a float in\\nthe interval (0.0, 1.0). If min_frequency is an integer, categories with\\na cardinality smaller than min_frequency  will be considered infrequent.\\nIf min_frequency is a float, categories with a cardinality smaller than\\nthis fraction of the total number of samples will be considered infrequent.\\nThe default value is 1, which means every category is encoded separately.\\nmax_categories is either None or any integer greater than 1. This\\nparameter sets an upper limit to the number of output features for each\\ninput feature. max_categories includes the feature that combines\\ninfrequent categories.\\n---------new doc---------\\nIn the following example with OrdinalEncoder, the categories 'dog' and\\n'snake' are considered infrequent:\\n>>> X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 +\\n...               ['snake'] * 3], dtype=object).T\\n>>> enc = preprocessing.OrdinalEncoder(min_frequency=6).fit(X)\\n>>> enc.infrequent_categories_\\n[array(['dog', 'snake'], dtype=object)]\\n>>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\\narray([[2.],\\n       [0.],\\n       [1.],\\n       [2.]])\\n---------new doc---------\\nOrdinalEncoder’s max_categories do not take into account missing\\nor unknown categories. Setting unknown_value or encoded_missing_value to an\\ninteger will increase the number of unique integer codes by one each. This can\\nresult in up to max_categories + 2 integer codes. In the following example,\\n“a” and “d” are considered infrequent and grouped together into a single\\ncategory, “b” and “c” are their own categories, unknown values are encoded as 3\\nand missing values are encoded as 4.\\n>>> X_train = np.array(\\n...     [[\\\"a\\\"] * 5 + [\\\"b\\\"] * 20 + [\\\"c\\\"] * 10 + [\\\"d\\\"] * 3 + [np.nan]],\\n...     dtype=object).T\\n>>> enc = preprocessing.OrdinalEncoder(\\n...     handle_unknown=\\\"use_encoded_value\\\", unknown_value=3,\\n...     max_categories=3, encoded_missing_value=4)\\n>>> _ = enc.fit(X_train)\\n>>> X_test = np.array([[\\\"a\\\"], [\\\"b\\\"], [\\\"c\\\"], [\\\"d\\\"], [\\\"e\\\"], [np.nan]], dtype=object)\\n>>> enc.transform(X_test)\\narray([[2.],\\n       [0.],\\n       [1.],\\n       [2.],\\n       [3.],\\n       [4.]])\\n---------new doc---------\\nSimilarity, OneHotEncoder can be configured to group together infrequent\\ncategories:\\n>>> enc = preprocessing.OneHotEncoder(min_frequency=6, sparse_output=False).fit(X)\\n>>> enc.infrequent_categories_\\n[array(['dog', 'snake'], dtype=object)]\\n>>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\\narray([[0., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.],\\n       [0., 0., 1.]])\\n\\n\\nBy setting handle_unknown to 'infrequent_if_exist', unknown categories will\\nbe considered infrequent:\\n>>> enc = preprocessing.OneHotEncoder(\\n...    handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=6)\\n>>> enc = enc.fit(X)\\n>>> enc.transform(np.array([['dragon']]))\\narray([[0., 0., 1.]])\\n\\n\\nOneHotEncoder.get_feature_names_out uses ‘infrequent’ as the infrequent\\nfeature name:\\n>>> enc.get_feature_names_out()\\narray(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)\\n\\n\\nWhen 'handle_unknown' is set to 'infrequent_if_exist' and an unknown\\ncategory is encountered in transform:\\n---------new doc---------\\nWhen 'handle_unknown' is set to 'infrequent_if_exist' and an unknown\\ncategory is encountered in transform:\\n\\nIf infrequent category support was not configured or there was no\\ninfrequent category during training, the resulting one-hot encoded columns\\nfor this feature will be all zeros. In the inverse transform, an unknown\\ncategory will be denoted as None.\\nIf there is an infrequent category during training, the unknown category\\nwill be considered infrequent. In the inverse transform, ‘infrequent_sklearn’\\nwill be used to represent the infrequent category.\\n\\nInfrequent categories can also be configured using max_categories. In the\\nfollowing example, we set max_categories=2 to limit the number of features in\\nthe output. This will result in all but the 'cat' category to be considered\\ninfrequent, leading to two features, one for 'cat' and one for infrequent\\ncategories - which are all the others:\\n>>> enc = preprocessing.OneHotEncoder(max_categories=2, sparse_output=False)\\n>>> enc = enc.fit(X)\\n>>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\\narray([[0., 1.],\\n       [1., 0.],\\n       [0., 1.],\\n       [0., 1.]])\\n---------new doc---------\\nIf both max_categories and min_frequency are non-default values, then\\ncategories are selected based on min_frequency first and max_categories\\ncategories are kept. In the following example, min_frequency=4 considers\\nonly snake to be infrequent, but max_categories=3, forces dog to also be\\ninfrequent:\\n>>> enc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse_output=False)\\n>>> enc = enc.fit(X)\\n>>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\\narray([[0., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.],\\n       [0., 0., 1.]])\\n\\n\\nIf there are infrequent categories with the same cardinality at the cutoff of\\nmax_categories, then then the first max_categories are taken based on lexicon\\nordering. In the following example, “b”, “c”, and “d”, have the same cardinality\\nand with max_categories=2, “b” and “c” are infrequent because they have a higher\\nlexicon order.\\n>>> X = np.asarray([[\\\"a\\\"] * 20 + [\\\"b\\\"] * 10 + [\\\"c\\\"] * 10 + [\\\"d\\\"] * 10], dtype=object).T\\n>>> enc = preprocessing.OneHotEncoder(max_categories=3).fit(X)\\n>>> enc.infrequent_categories_\\n[array(['b', 'c'], dtype=object)]\\n---------new doc---------\\n6.3.4.2. Target Encoder#\\nThe TargetEncoder uses the target mean conditioned on the categorical\\nfeature for encoding unordered categories, i.e. nominal categories [PAR]\\n[MIC]. This encoding scheme is useful with categorical features with high\\ncardinality, where one-hot encoding would inflate the feature space making it\\nmore expensive for a downstream model to process. A classical example of high\\ncardinality categories are location based such as zip code or region.\\n\\n\\nBinary classification targets#\\nFor the binary classification target, the target encoding is given by:\\n\\n\\\\[S_i = \\\\lambda_i\\\\frac{n_{iY}}{n_i} + (1 - \\\\lambda_i)\\\\frac{n_Y}{n}\\\\]\\nwhere \\\\(S_i\\\\) is the encoding for category \\\\(i\\\\), \\\\(n_{iY}\\\\) is the\\nnumber of observations with \\\\(Y=1\\\\) and category \\\\(i\\\\), \\\\(n_i\\\\) is\\nthe number of observations with category \\\\(i\\\\), \\\\(n_Y\\\\) is the number of\\nobservations with \\\\(Y=1\\\\), \\\\(n\\\\) is the number of observations, and\\n\\\\(\\\\lambda_i\\\\) is a shrinkage factor for category \\\\(i\\\\). The shrinkage\\nfactor is given by:\\n---------new doc---------\\nfit_transform also learns a ‘full data’ encoding using\\nthe whole training set. This is never used in\\nfit_transform but is saved to the attribute encodings_,\\nfor use when transform is called. Note that the encodings\\nlearned for each fold during the cross fitting scheme are not saved to\\nan attribute.\\nThe fit method does not use any cross fitting\\nschemes and learns one encoding on the entire training set, which is used to\\nencode categories in transform.\\nThis encoding is the same as the ‘full data’\\nencoding learned in fit_transform.\\n\\nNote\\nTargetEncoder considers missing values, such as np.nan or None,\\nas another category and encodes them like any other category. Categories\\nthat are not seen during fit are encoded with the target mean, i.e.\\ntarget_mean_.\\n\\nExamples\\n\\nComparing Target Encoder with Other Encoders\\nTarget Encoder’s Internal Cross fitting\\n\\nReferences\\n\\n\\n[MIC]\\nMicci-Barreca, Daniele. “A preprocessing scheme for high-cardinality\\ncategorical attributes in classification and prediction problems”\\nSIGKDD Explor. Newsl. 3, 1 (July 2001), 27-32.\\n\\n\\n[PAR]\\nPargent, F., Pfisterer, F., Thomas, J. et al. “Regularized target\\nencoding outperforms traditional methods in supervised machine learning with\\nhigh cardinality features” Comput Stat 37, 2671-2692 (2022)\\n---------new doc---------\\nBy default the output is one-hot encoded into a sparse matrix\\n(See Encoding categorical features)\\nand this can be configured with the encode parameter.\\nFor each feature, the bin edges are computed during fit and together with\\nthe number of bins, they will define the intervals. Therefore, for the current\\nexample, these intervals are defined as:\\n\\nfeature 1: \\\\({[-\\\\infty, -1), [-1, 2), [2, \\\\infty)}\\\\)\\nfeature 2: \\\\({[-\\\\infty, 5), [5, \\\\infty)}\\\\)\\nfeature 3: \\\\({[-\\\\infty, 14), [14, \\\\infty)}\\\\)\\n\\nBased on these bin intervals, X is transformed as follows:\\n>>> est.transform(X)                      \\narray([[ 0., 1., 1.],\\n       [ 1., 1., 1.],\\n       [ 2., 0., 0.]])\\n---------new doc---------\\nThe resulting dataset contains ordinal attributes which can be further used\\nin a Pipeline.\\nDiscretization is similar to constructing histograms for continuous data.\\nHowever, histograms focus on counting features which fall into particular\\nbins, whereas discretization focuses on assigning feature values to these bins.\\nKBinsDiscretizer implements different binning strategies, which can be\\nselected with the strategy parameter. The ‘uniform’ strategy uses\\nconstant-width bins. The ‘quantile’ strategy uses the quantiles values to have\\nequally populated bins in each feature. The ‘kmeans’ strategy defines bins based\\non a k-means clustering procedure performed on each feature independently.\\nBe aware that one can specify custom bins by passing a callable defining the\\ndiscretization strategy to FunctionTransformer.\\nFor instance, we can use the Pandas function pandas.cut:\\n>>> import pandas as pd\\n>>> import numpy as np\\n>>> from sklearn import preprocessing\\n>>>\\n>>> bins = [0, 1, 13, 20, 60, np.inf]\\n>>> labels = ['infant', 'kid', 'teen', 'adult', 'senior citizen']\\n>>> transformer = preprocessing.FunctionTransformer(\\n...     pd.cut, kw_args={'bins': bins, 'labels': labels, 'retbins': False}\\n... )\\n>>> X = np.array([0.2, 2, 15, 25, 97])\\n>>> transformer.fit_transform(X)\\n['infant', 'kid', 'teen', 'adult', 'senior citizen']\\nCategories (5, object): ['infant' < 'kid' < 'teen' < 'adult' < 'senior citizen']\\n\\n\\nExamples\\n---------new doc---------\\nExamples\\n\\nUsing KBinsDiscretizer to discretize continuous features\\nFeature discretization\\nDemonstrating the different strategies of KBinsDiscretizer\\n\\n\\n\\n6.3.5.2. Feature binarization#\\nFeature binarization is the process of thresholding numerical\\nfeatures to get boolean values. This can be useful for downstream\\nprobabilistic estimators that make assumption that the input data\\nis distributed according to a multi-variate Bernoulli distribution. For instance,\\nthis is the case for the BernoulliRBM.\\nIt is also common among the text processing community to use binary\\nfeature values (probably to simplify the probabilistic reasoning) even\\nif normalized counts (a.k.a. term frequencies) or TF-IDF valued features\\noften perform slightly better in practice.\\nAs for the Normalizer, the utility class\\nBinarizer is meant to be used in the early stages of\\nPipeline. The fit method does nothing\\nas each sample is treated independently of others:\\n>>> X = [[ 1., -1.,  2.],\\n...      [ 2.,  0.,  0.],\\n...      [ 0.,  1., -1.]]\\n\\n>>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\\n>>> binarizer\\nBinarizer()\\n\\n>>> binarizer.transform(X)\\narray([[1., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.]])\\n---------new doc---------\\n>>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\\n>>> binarizer\\nBinarizer()\\n\\n>>> binarizer.transform(X)\\narray([[1., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 1., 0.]])\\n\\n\\nIt is possible to adjust the threshold of the binarizer:\\n>>> binarizer = preprocessing.Binarizer(threshold=1.1)\\n>>> binarizer.transform(X)\\narray([[0., 0., 1.],\\n       [1., 0., 0.],\\n       [0., 0., 0.]])\\n\\n\\nAs for the Normalizer class, the preprocessing module\\nprovides a companion function binarize\\nto be used when the transformer API is not necessary.\\nNote that the Binarizer is similar to the KBinsDiscretizer\\nwhen k = 2, and when the bin edge is at the value threshold.\\n\\nSparse input\\nbinarize and Binarizer accept both dense array-like\\nand sparse matrices from scipy.sparse as input.\\nFor sparse input the data is converted to the Compressed Sparse Rows\\nrepresentation (see scipy.sparse.csr_matrix).\\nTo avoid unnecessary memory copies, it is recommended to choose the CSR\\nrepresentation upstream.\\n\\n\\n\\n\\n6.3.6. Imputation of missing values#\\nTools for imputing missing values are discussed at Imputation of missing values.\\n---------new doc---------\\n6.3.6. Imputation of missing values#\\nTools for imputing missing values are discussed at Imputation of missing values.\\n\\n\\n6.3.7. Generating polynomial features#\\nOften it’s useful to add complexity to a model by considering nonlinear\\nfeatures of the input data. We show two possibilities that are both based on\\npolynomials: The first one uses pure polynomials, the second one uses splines,\\ni.e. piecewise polynomials.\\n\\n6.3.7.1. Polynomial features#\\nA simple and common method to use is polynomial features, which can get\\nfeatures’ high-order and interaction terms. It is implemented in\\nPolynomialFeatures:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> X = np.arange(6).reshape(3, 2)\\n>>> X\\narray([[0, 1],\\n       [2, 3],\\n       [4, 5]])\\n>>> poly = PolynomialFeatures(2)\\n>>> poly.fit_transform(X)\\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\\n       [ 1.,  4.,  5., 16., 20., 25.]])\\n---------new doc---------\\nThe features of X have been transformed from \\\\((X_1, X_2)\\\\) to\\n\\\\((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\\\\).\\nIn some cases, only interaction terms among features are required, and it can\\nbe gotten with the setting interaction_only=True:\\n>>> X = np.arange(9).reshape(3, 3)\\n>>> X\\narray([[0, 1, 2],\\n       [3, 4, 5],\\n       [6, 7, 8]])\\n>>> poly = PolynomialFeatures(degree=3, interaction_only=True)\\n>>> poly.fit_transform(X)\\narray([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],\\n       [  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],\\n       [  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])\\n---------new doc---------\\nThe features of X have been transformed from \\\\((X_1, X_2, X_3)\\\\) to\\n\\\\((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\\\\).\\nNote that polynomial features are used implicitly in kernel methods (e.g., SVC,\\nKernelPCA) when using polynomial Kernel functions.\\nSee Polynomial and Spline interpolation\\nfor Ridge regression using created polynomial features.\\n\\n\\n6.3.7.2. Spline transformer#\\nAnother way to add nonlinear terms instead of pure polynomials of features is\\nto generate spline basis functions for each feature with the\\nSplineTransformer. Splines are piecewise polynomials, parametrized by\\ntheir polynomial degree and the positions of the knots. The\\nSplineTransformer implements a B-spline basis, cf. the references\\nbelow.\\n\\nNote\\nThe SplineTransformer treats each feature separately, i.e. it\\nwon’t give you interaction terms.\\n\\nSome of the advantages of splines over polynomials are:\\n---------new doc---------\\nNote\\nThe SplineTransformer treats each feature separately, i.e. it\\nwon’t give you interaction terms.\\n\\nSome of the advantages of splines over polynomials are:\\n\\nB-splines are very flexible and robust if you keep a fixed low degree,\\nusually 3, and parsimoniously adapt the number of knots. Polynomials\\nwould need a higher degree, which leads to the next point.\\nB-splines do not have oscillatory behaviour at the boundaries as have\\npolynomials (the higher the degree, the worse). This is known as Runge’s\\nphenomenon.\\nB-splines provide good options for extrapolation beyond the boundaries,\\ni.e. beyond the range of fitted values. Have a look at the option\\nextrapolation.\\nB-splines generate a feature matrix with a banded structure. For a single\\nfeature, every row contains only degree + 1 non-zero elements, which\\noccur consecutively and are even positive. This results in a matrix with\\ngood numerical properties, e.g. a low condition number, in sharp contrast\\nto a matrix of polynomials, which goes under the name\\nVandermonde matrix.\\nA low condition number is important for stable algorithms of linear\\nmodels.\\n---------new doc---------\\nThe following code snippet shows splines in action:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import SplineTransformer\\n>>> X = np.arange(5).reshape(5, 1)\\n>>> X\\narray([[0],\\n       [1],\\n       [2],\\n       [3],\\n       [4]])\\n>>> spline = SplineTransformer(degree=2, n_knots=3)\\n>>> spline.fit_transform(X)\\narray([[0.5  , 0.5  , 0.   , 0.   ],\\n       [0.125, 0.75 , 0.125, 0.   ],\\n       [0.   , 0.5  , 0.5  , 0.   ],\\n       [0.   , 0.125, 0.75 , 0.125],\\n       [0.   , 0.   , 0.5  , 0.5  ]])\\n\\n\\nAs the X is sorted, one can easily see the banded matrix output. Only the\\nthree middle diagonals are non-zero for degree=2. The higher the degree,\\nthe more overlapping of the splines.\\nInterestingly, a SplineTransformer of degree=0 is the same as\\nKBinsDiscretizer with\\nencode='onehot-dense' and n_bins = n_knots - 1 if\\nknots = strategy.\\nExamples\\n\\nPolynomial and Spline interpolation\\nTime-related feature engineering\\n\\n\\n\\nReferences#\\n---------new doc---------\\nPolynomial and Spline interpolation\\nTime-related feature engineering\\n\\n\\n\\nReferences#\\n\\nEilers, P., & Marx, B. (1996). Flexible Smoothing with B-splines and\\nPenalties. Statist. Sci. 11 (1996), no. 2, 89–121.\\nPerperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. A review of\\nspline function procedures in R.\\nBMC Med Res Methodol 19, 46 (2019).\\n\\n\\n\\n\\n\\n6.3.8. Custom transformers#\\nOften, you will want to convert an existing Python function into a transformer\\nto assist in data cleaning or processing. You can implement a transformer from\\nan arbitrary function with FunctionTransformer. For example, to build\\na transformer that applies a log transformation in a pipeline, do:\\n>>> import numpy as np\\n>>> from sklearn.preprocessing import FunctionTransformer\\n>>> transformer = FunctionTransformer(np.log1p, validate=True)\\n>>> X = np.array([[0, 1], [2, 3]])\\n>>> # Since FunctionTransformer is no-op during fit, we can call transform directly\\n>>> transformer.transform(X)\\narray([[0.        , 0.69314718],\\n       [1.09861229, 1.38629436]])\\n---------new doc---------\\nYou can ensure that func and inverse_func are the inverse of each other\\nby setting check_inverse=True and calling fit before\\ntransform. Please note that a warning is raised and can be turned into an\\nerror with a filterwarnings:\\n>>> import warnings\\n>>> warnings.filterwarnings(\\\"error\\\", message=\\\".*check_inverse*.\\\",\\n...                         category=UserWarning, append=False)\\n\\n\\nFor a full code example that demonstrates using a FunctionTransformer\\nto extract features from text data see\\nColumn Transformer with Heterogeneous Data Sources and\\nTime-related feature engineering.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n6.2. Feature extraction\\n\\n\\n\\n\\nnext\\n6.4. Imputation of missing values\\n\\n\\n --- \\n\\n\\n6. Dataset transformations\\n6.4. Imputation of missing values\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6.4. Imputation of missing values#\\nFor various reasons, many real world datasets contain missing values, often\\nencoded as blanks, NaNs or other placeholders. Such datasets however are\\nincompatible with scikit-learn estimators which assume that all values in an\\narray are numerical, and that all have and hold meaning. A basic strategy to\\nuse incomplete datasets is to discard entire rows and/or columns containing\\nmissing values. However, this comes at the price of losing data which may be\\nvaluable (even though incomplete). A better strategy is to impute the missing\\nvalues, i.e., to infer them from the known part of the data. See the\\nglossary entry on imputation.\\n---------new doc---------\\n6.4.1. Univariate vs. Multivariate Imputation#\\nOne type of imputation algorithm is univariate, which imputes values in the\\ni-th feature dimension using only non-missing values in that feature dimension\\n(e.g. SimpleImputer). By contrast, multivariate imputation\\nalgorithms use the entire set of available feature dimensions to estimate the\\nmissing values (e.g. IterativeImputer).\\n\\n\\n6.4.2. Univariate feature imputation#\\nThe SimpleImputer class provides basic strategies for imputing missing\\nvalues. Missing values can be imputed with a provided constant value, or using\\nthe statistics (mean, median or most frequent) of each column in which the\\nmissing values are located. This class also allows for different missing values\\nencodings.\\nThe following snippet demonstrates how to replace missing values,\\nencoded as np.nan, using the mean value of the columns (axis 0)\\nthat contain the missing values:\\n>>> import numpy as np\\n>>> from sklearn.impute import SimpleImputer\\n>>> imp = SimpleImputer(missing_values=np.nan, strategy='mean')\\n>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])\\nSimpleImputer()\\n>>> X = [[np.nan, 2], [6, np.nan], [7, 6]]\\n>>> print(imp.transform(X))\\n[[4.          2.        ]\\n [6.          3.666...]\\n [7.          6.        ]]\\n---------new doc---------\\nThe SimpleImputer class also supports sparse matrices:\\n>>> import scipy.sparse as sp\\n>>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])\\n>>> imp = SimpleImputer(missing_values=-1, strategy='mean')\\n>>> imp.fit(X)\\nSimpleImputer(missing_values=-1)\\n>>> X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])\\n>>> print(imp.transform(X_test).toarray())\\n[[3. 2.]\\n [6. 3.]\\n [7. 6.]]\\n---------new doc---------\\nNote that this format is not meant to be used to implicitly store missing\\nvalues in the matrix because it would densify it at transform time. Missing\\nvalues encoded by 0 must be used with dense input.\\nThe SimpleImputer class also supports categorical data represented as\\nstring values or pandas categoricals when using the 'most_frequent' or\\n'constant' strategy:\\n>>> import pandas as pd\\n>>> df = pd.DataFrame([[\\\"a\\\", \\\"x\\\"],\\n...                    [np.nan, \\\"y\\\"],\\n...                    [\\\"a\\\", np.nan],\\n...                    [\\\"b\\\", \\\"y\\\"]], dtype=\\\"category\\\")\\n...\\n>>> imp = SimpleImputer(strategy=\\\"most_frequent\\\")\\n>>> print(imp.fit_transform(df))\\n[['a' 'x']\\n ['a' 'y']\\n ['a' 'y']\\n ['b' 'y']]\\n\\n\\nFor another example on usage, see Imputing missing values before building an estimator.\\n---------new doc---------\\nFor another example on usage, see Imputing missing values before building an estimator.\\n\\n\\n6.4.3. Multivariate feature imputation#\\nA more sophisticated approach is to use the IterativeImputer class,\\nwhich models each feature with missing values as a function of other features,\\nand uses that estimate for imputation. It does so in an iterated round-robin\\nfashion: at each step, a feature column is designated as output y and the\\nother feature columns are treated as inputs X. A regressor is fit on (X,\\ny) for known y. Then, the regressor is used to predict the missing values\\nof y.  This is done for each feature in an iterative fashion, and then is\\nrepeated for max_iter imputation rounds. The results of the final\\nimputation round are returned.\\n\\nNote\\nThis estimator is still experimental for now: default parameters or\\ndetails of behaviour might change without any deprecation cycle. Resolving\\nthe following issues would help stabilize IterativeImputer:\\nconvergence criteria (#14338) and default estimators\\n(#13286). To use it, you need to explicitly import\\nenable_iterative_imputer.\\n---------new doc---------\\nBoth SimpleImputer and IterativeImputer can be used in a\\nPipeline as a way to build a composite estimator that supports imputation.\\nSee Imputing missing values before building an estimator.\\n\\n6.4.3.1. Flexibility of IterativeImputer#\\nThere are many well-established imputation packages in the R data science\\necosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns\\nout to be a particular instance of different sequential imputation algorithms\\nthat can all be implemented with IterativeImputer by passing in\\ndifferent regressors to be used for predicting missing feature values. In the\\ncase of missForest, this regressor is a Random Forest.\\nSee Imputing missing values with variants of IterativeImputer.\\n---------new doc---------\\n>>> from sklearn.impute import KNNImputer\\n>>> nan = np.nan\\n>>> X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\\n>>> imputer = KNNImputer(n_neighbors=2, weights=\\\"uniform\\\")\\n>>> imputer.fit_transform(X)\\narray([[1. , 2. , 4. ],\\n       [3. , 4. , 3. ],\\n       [5.5, 6. , 5. ],\\n       [8. , 8. , 7. ]])\\n---------new doc---------\\nFor another example on usage, see Imputing missing values before building an estimator.\\nReferences\\n\\n\\n[OL2001]\\nOlga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown,\\nTrevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman,\\nMissing value estimation methods for DNA microarrays, BIOINFORMATICS\\nVol. 17 no. 6, 2001 Pages 520-525.\\n\\n\\n\\n\\n6.4.5. Keeping the number of features constant#\\nBy default, the scikit-learn imputers will drop fully empty features, i.e.\\ncolumns containing only missing values. For instance:\\n>>> imputer = SimpleImputer()\\n>>> X = np.array([[np.nan, 1], [np.nan, 2], [np.nan, 3]])\\n>>> imputer.fit_transform(X)\\narray([[1.],\\n       [2.],\\n       [3.]])\\n\\n\\nThe first feature in X containing only np.nan was dropped after the\\nimputation. While this feature will not help in predictive setting, dropping\\nthe columns will change the shape of X which could be problematic when using\\nimputers in a more complex machine-learning pipeline. The parameter\\nkeep_empty_features offers the option to keep the empty features by imputing\\nwith a constant values. In most of the cases, this constant value is zero:\\n>>> imputer.set_params(keep_empty_features=True)\\nSimpleImputer(keep_empty_features=True)\\n>>> imputer.fit_transform(X)\\narray([[0., 1.],\\n       [0., 2.],\\n       [0., 3.]])\\n---------new doc---------\\n6.4.6. Marking imputed values#\\nThe MissingIndicator transformer is useful to transform a dataset into\\ncorresponding binary matrix indicating the presence of missing values in the\\ndataset. This transformation is useful in conjunction with imputation. When\\nusing imputation, preserving the information about which values had been\\nmissing can be informative. Note that both the SimpleImputer and\\nIterativeImputer have the boolean parameter add_indicator\\n(False by default) which when set to True provides a convenient way of\\nstacking the output of the MissingIndicator transformer with the\\noutput of the imputer.\\nNaN is usually used as the placeholder for missing values. However, it\\nenforces the data type to be float. The parameter missing_values allows to\\nspecify other placeholder such as integer. In the following example, we will\\nuse -1 as missing values:\\n>>> from sklearn.impute import MissingIndicator\\n>>> X = np.array([[-1, -1, 1, 3],\\n...               [4, -1, 0, -1],\\n...               [8, -1, 1, 0]])\\n>>> indicator = MissingIndicator(missing_values=-1)\\n>>> mask_missing_values_only = indicator.fit_transform(X)\\n>>> mask_missing_values_only\\narray([[ True,  True, False],\\n       [False,  True,  True],\\n       [False,  True, False]])\\n---------new doc---------\\nThe features parameter is used to choose the features for which the mask is\\nconstructed. By default, it is 'missing-only' which returns the imputer\\nmask of the features containing missing values at fit time:\\n>>> indicator.features_\\narray([0, 1, 3])\\n\\n\\nThe features parameter can be set to 'all' to return all features\\nwhether or not they contain missing values:\\n>>> indicator = MissingIndicator(missing_values=-1, features=\\\"all\\\")\\n>>> mask_all = indicator.fit_transform(X)\\n>>> mask_all\\narray([[ True,  True, False, False],\\n       [False,  True, False,  True],\\n       [False,  True, False, False]])\\n>>> indicator.features_\\narray([0, 1, 2, 3])\\n---------new doc---------\\nWhen using the MissingIndicator in a\\nPipeline, be sure to use the\\nFeatureUnion or\\nColumnTransformer to add the indicator features to\\nthe regular features. First we obtain the iris dataset, and add some missing\\nvalues to it.\\n>>> from sklearn.datasets import load_iris\\n>>> from sklearn.impute import SimpleImputer, MissingIndicator\\n>>> from sklearn.model_selection import train_test_split\\n>>> from sklearn.pipeline import FeatureUnion, make_pipeline\\n>>> from sklearn.tree import DecisionTreeClassifier\\n>>> X, y = load_iris(return_X_y=True)\\n>>> mask = np.random.randint(0, 2, size=X.shape).astype(bool)\\n>>> X[mask] = np.nan\\n>>> X_train, X_test, y_train, _ = train_test_split(X, y, test_size=100,\\n...                                                random_state=0)\\n---------new doc---------\\nNow we create a FeatureUnion. All features will be\\nimputed using SimpleImputer, in order to enable classifiers to work\\nwith this data. Additionally, it adds the indicator variables from\\nMissingIndicator.\\n>>> transformer = FeatureUnion(\\n...     transformer_list=[\\n...         ('features', SimpleImputer(strategy='mean')),\\n...         ('indicators', MissingIndicator())])\\n>>> transformer = transformer.fit(X_train, y_train)\\n>>> results = transformer.transform(X_test)\\n>>> results.shape\\n(100, 8)\\n\\n\\nOf course, we cannot use the transformer to make any predictions. We should\\nwrap this in a Pipeline with a classifier (e.g., a\\nDecisionTreeClassifier) to be able to make predictions.\\n>>> clf = make_pipeline(transformer, DecisionTreeClassifier())\\n>>> clf = clf.fit(X_train, y_train)\\n>>> results = clf.predict(X_test)\\n>>> results.shape\\n(100,)\\n\\n\\n\\n\\n6.4.7. Estimators that handle NaN values#\\nSome estimators are designed to handle NaN values without preprocessing.\\nBelow is the list of these estimators, classified by type\\n(cluster, regressor, classifier, transform):\\n\\nEstimators that allow NaN values for type cluster:\\n\\nHDBSCAN\\n\\n\\nEstimators that allow NaN values for type regressor:\\n\\nBaggingRegressor\\nDecisionTreeRegressor\\nExtraTreeRegressor\\nExtraTreesRegressor\\nHistGradientBoostingRegressor\\nRandomForestRegressor\\nStackingRegressor\\nVotingRegressor\\n---------new doc---------\\n6.6.4. Inverse Transform#\\nThe random projection transformers have compute_inverse_components parameter. When\\nset to True, after creating the random components_ matrix during fitting,\\nthe transformer computes the pseudo-inverse of this matrix and stores it as\\ninverse_components_. The inverse_components_ matrix has shape\\n\\\\(n_{features} \\\\times n_{components}\\\\), and it is always a dense matrix,\\nregardless of whether the components matrix is sparse or dense. So depending on\\nthe number of features and components, it may use a lot of memory.\\nWhen the inverse_transform method is called, it computes the product of the\\ninput X and the transpose of the inverse components. If the inverse components have\\nbeen computed during fit, they are reused at each call to inverse_transform.\\nOtherwise they are recomputed each time, which can be costly. The result is always\\ndense, even if X is sparse.\\nHere a small code example which illustrates how to use the inverse transform\\nfeature:\\n>>> import numpy as np\\n>>> from sklearn.random_projection import SparseRandomProjection\\n>>> X = np.random.rand(100, 10000)\\n>>> transformer = SparseRandomProjection(\\n...   compute_inverse_components=True\\n... )\\n...\\n>>> X_new = transformer.fit_transform(X)\\n>>> X_new.shape\\n(100, 3947)\\n>>> X_new_inversed = transformer.inverse_transform(X_new)\\n>>> X_new_inversed.shape\\n(100, 10000)\\n>>> X_new_again = transformer.transform(X_new_inversed)\\n>>> np.allclose(X_new, X_new_again)\\nTrue\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [4.18s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The provided text appears to be a documentation for the scikit-learn library in Python. It covers various topics related to handling missing values (NaN) and transforming data.\\n\\nHere's a summary of the main points:\\n\\n**Handling Missing Values**\\n\\n* The `SimpleImputer` class can impute missing values using different strategies, such as mean, median, or constant.\\n* The `MissingIndicator` class creates a mask indicating which features contain missing values.\\n* Some estimators, like `DecisionTreeRegressor`, allow NaN values without preprocessing.\\n\\n**Transforming Data**\\n\\n* The `FeatureUnion` and `ColumnTransformer` classes can combine multiple transformers to transform data.\\n* The `RandomProjection` transformer reduces the dimensionality of data using random projections.\\n* The `SparseRandomProjection` transformer is a variant that uses sparse matrices.\\n\\n**Inverse Transform**\\n\\n* The `compute_inverse_components` parameter in `RandomProjection` allows computing the pseudo-inverse of the transformation matrix.\\n* This can be useful for inverse transformations, but may consume memory depending on the number of features and components.\\n\\nSome key takeaways:\\n\\n* Missing values should be handled before training models to avoid biased results.\\n* Different imputation strategies can affect model performance.\\n* Some estimators are designed to handle NaN values without preprocessing.\\n* Feature unions and column transformers can combine multiple transformations for data preprocessing.\\n* Random projection and sparse random projection can reduce dimensionality of data.\\n\\nOverall, this documentation provides a comprehensive overview of handling missing values and transforming data in scikit-learn.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b-instruct-q8_0\",\n",
      "          \"created_at\": \"2025-02-17T13:31:22.758598711Z\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"total_duration\": 4174189032,\n",
      "          \"load_duration\": 12305365,\n",
      "          \"prompt_eval_count\": 2048,\n",
      "          \"prompt_eval_duration\": 379000000,\n",
      "          \"eval_count\": 307,\n",
      "          \"eval_duration\": 3781000000,\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\",\n",
      "            \"images\": null,\n",
      "            \"tool_calls\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The provided text appears to be a documentation for the scikit-learn library in Python. It covers various topics related to handling missing values (NaN) and transforming data.\\n\\nHere's a summary of the main points:\\n\\n**Handling Missing Values**\\n\\n* The `SimpleImputer` class can impute missing values using different strategies, such as mean, median, or constant.\\n* The `MissingIndicator` class creates a mask indicating which features contain missing values.\\n* Some estimators, like `DecisionTreeRegressor`, allow NaN values without preprocessing.\\n\\n**Transforming Data**\\n\\n* The `FeatureUnion` and `ColumnTransformer` classes can combine multiple transformers to transform data.\\n* The `RandomProjection` transformer reduces the dimensionality of data using random projections.\\n* The `SparseRandomProjection` transformer is a variant that uses sparse matrices.\\n\\n**Inverse Transform**\\n\\n* The `compute_inverse_components` parameter in `RandomProjection` allows computing the pseudo-inverse of the transformation matrix.\\n* This can be useful for inverse transformations, but may consume memory depending on the number of features and components.\\n\\nSome key takeaways:\\n\\n* Missing values should be handled before training models to avoid biased results.\\n* Different imputation strategies can affect model performance.\\n* Some estimators are designed to handle NaN values without preprocessing.\\n* Feature unions and column transformers can combine multiple transformations for data preprocessing.\\n* Random projection and sparse random projection can reduce dimensionality of data.\\n\\nOverall, this documentation provides a comprehensive overview of handling missing values and transforming data in scikit-learn.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b-instruct-q8_0\",\n",
      "              \"created_at\": \"2025-02-17T13:31:22.758598711Z\",\n",
      "              \"done\": true,\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"total_duration\": 4174189032,\n",
      "              \"load_duration\": 12305365,\n",
      "              \"prompt_eval_count\": 2048,\n",
      "              \"prompt_eval_duration\": 379000000,\n",
      "              \"eval_count\": 307,\n",
      "              \"eval_duration\": 3781000000,\n",
      "              \"message\": {\n",
      "                \"lc\": 1,\n",
      "                \"type\": \"not_implemented\",\n",
      "                \"id\": [\n",
      "                  \"ollama\",\n",
      "                  \"_types\",\n",
      "                  \"Message\"\n",
      "                ],\n",
      "                \"repr\": \"Message(role='assistant', content='', images=None, tool_calls=None)\"\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3835fb40-ebaf-4132-bd97-423888748506-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 2048,\n",
      "              \"output_tokens\": 307,\n",
      "              \"total_tokens\": 2355\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The provided text appears to be a documentation for the scikit-learn library in Python. It covers various topics related to handling missing values (NaN) and transforming data.\\n\\nHere's a summary of the main points:\\n\\n**Handling Missing Values**\\n\\n* The `SimpleImputer` class can impute missing values using different strategies, such as mean, median, or constant.\\n* The `MissingIndicator` class creates a mask indicating which features contain missing values.\\n* Some estimators, like `DecisionTreeRegressor`, allow NaN values without preprocessing.\\n\\n**Transforming Data**\\n\\n* The `FeatureUnion` and `ColumnTransformer` classes can combine multiple transformers to transform data.\\n* The `RandomProjection` transformer reduces the dimensionality of data using random projections.\\n* The `SparseRandomProjection` transformer is a variant that uses sparse matrices.\\n\\n**Inverse Transform**\\n\\n* The `compute_inverse_components` parameter in `RandomProjection` allows computing the pseudo-inverse of the transformation matrix.\\n* This can be useful for inverse transformations, but may consume memory depending on the number of features and components.\\n\\nSome key takeaways:\\n\\n* Missing values should be handled before training models to avoid biased results.\\n* Different imputation strategies can affect model performance.\\n* Some estimators are designed to handle NaN values without preprocessing.\\n* Feature unions and column transformers can combine multiple transformations for data preprocessing.\\n* Random projection and sparse random projection can reduce dimensionality of data.\\n\\nOverall, this documentation provides a comprehensive overview of handling missing values and transforming data in scikit-learn.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [4.18s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The provided text appears to be a documentation for the scikit-learn library in Python. It covers various topics related to handling missing values (NaN) and transforming data.\\n\\nHere's a summary of the main points:\\n\\n**Handling Missing Values**\\n\\n* The `SimpleImputer` class can impute missing values using different strategies, such as mean, median, or constant.\\n* The `MissingIndicator` class creates a mask indicating which features contain missing values.\\n* Some estimators, like `DecisionTreeRegressor`, allow NaN values without preprocessing.\\n\\n**Transforming Data**\\n\\n* The `FeatureUnion` and `ColumnTransformer` classes can combine multiple transformers to transform data.\\n* The `RandomProjection` transformer reduces the dimensionality of data using random projections.\\n* The `SparseRandomProjection` transformer is a variant that uses sparse matrices.\\n\\n**Inverse Transform**\\n\\n* The `compute_inverse_components` parameter in `RandomProjection` allows computing the pseudo-inverse of the transformation matrix.\\n* This can be useful for inverse transformations, but may consume memory depending on the number of features and components.\\n\\nSome key takeaways:\\n\\n* Missing values should be handled before training models to avoid biased results.\\n* Different imputation strategies can affect model performance.\\n* Some estimators are designed to handle NaN values without preprocessing.\\n* Feature unions and column transformers can combine multiple transformations for data preprocessing.\\n* Random projection and sparse random projection can reduce dimensionality of data.\\n\\nOverall, this documentation provides a comprehensive overview of handling missing values and transforming data in scikit-learn.\"\n",
      "}\n",
      "Global cluster: 0, Local cluster: 2, has 129 documents.\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"PDF of a random variable Y following Poisson, Tweedie (power=1.5) and Gamma\\ndistributions with different mean values (\\\\(\\\\mu\\\\)). Observe the point\\nmass at \\\\(Y=0\\\\) for the Poisson distribution and the Tweedie (power=1.5)\\ndistribution, but not for the Gamma distribution which has a strictly\\npositive target domain.#\\n\\n\\nThe Bernoulli distribution is a discrete probability distribution modelling a\\nBernoulli trial - an event that has only two mutually exclusive outcomes.\\nThe Categorical distribution is a generalization of the Bernoulli distribution\\nfor a categorical random variable. While a random variable in a Bernoulli\\ndistribution has two possible outcomes, a Categorical random variable can take\\non one of K possible categories, with the probability of each category\\nspecified separately.\\nThe choice of the distribution depends on the problem at hand:\\n\\nIf the target values \\\\(y\\\\) are counts (non-negative integer valued) or\\nrelative frequencies (non-negative), you might use a Poisson distribution\\nwith a log-link.\\nIf the target values are positive valued and skewed, you might try a Gamma\\ndistribution with a log-link.\\nIf the target values seem to be heavier tailed than a Gamma distribution, you\\nmight try an Inverse Gaussian distribution (or even higher variance powers of\\nthe Tweedie family).\\nIf the target values \\\\(y\\\\) are probabilities, you can use the Bernoulli\\ndistribution. The Bernoulli distribution with a logit link can be used for\\nbinary classification. The Categorical distribution with a softmax link can be\\nused for multiclass classification.\\n\\n\\n\\nExamples of use cases#\\n---------new doc---------\\nExamples of use cases#\\n\\nAgriculture / weather modeling:  number of rain events per year (Poisson),\\namount of rainfall per event (Gamma), total rainfall per year (Tweedie /\\nCompound Poisson Gamma).\\nRisk modeling / insurance policy pricing:  number of claim events /\\npolicyholder per year (Poisson), cost per event (Gamma), total cost per\\npolicyholder per year (Tweedie / Compound Poisson Gamma).\\nCredit Default: probability that a loan can’t be paid back (Bernoulli).\\nFraud Detection: probability that a financial transaction like a cash transfer\\nis a fraudulent transaction (Bernoulli).\\nPredictive maintenance: number of production interruption events per year\\n(Poisson), duration of interruption (Gamma), total interruption time per year\\n(Tweedie / Compound Poisson Gamma).\\nMedical Drug Testing: probability of curing a patient in a set of trials or\\nprobability that a patient will experience side effects (Bernoulli).\\nNews Classification: classification of news articles into three categories\\nnamely Business News, Politics and Entertainment news (Categorical).\\n\\n\\nReferences\\n\\n\\n[10]\\nMcCullagh, Peter; Nelder, John (1989). Generalized Linear Models,\\nSecond Edition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.\\n\\n\\n[11]\\nJørgensen, B. (1992). The theory of exponential dispersion models\\nand analysis of deviance. Monografias de matemática, no. 51.  See also\\nExponential dispersion model.\\n---------new doc---------\\nExamples\\n\\nPoisson regression and non-normal loss\\nTweedie regression on insurance claims\\n\\n\\n\\nPractical considerations#\\nThe feature matrix X should be standardized before fitting. This ensures\\nthat the penalty treats features equally.\\nSince the linear predictor \\\\(Xw\\\\) can be negative and Poisson,\\nGamma and Inverse Gaussian distributions don’t support negative values, it\\nis necessary to apply an inverse link function that guarantees the\\nnon-negativeness. For example with link='log', the inverse link function\\nbecomes \\\\(h(Xw)=\\\\exp(Xw)\\\\).\\nIf you want to model a relative frequency, i.e. counts per exposure (time,\\nvolume, …) you can do so by using a Poisson distribution and passing\\n\\\\(y=\\\\frac{\\\\mathrm{counts}}{\\\\mathrm{exposure}}\\\\) as target values\\ntogether with \\\\(\\\\mathrm{exposure}\\\\) as sample weights. For a concrete\\nexample see e.g.\\nTweedie regression on insurance claims.\\nWhen performing cross-validation for the power parameter of\\nTweedieRegressor, it is advisable to specify an explicit scoring function,\\nbecause the default scorer TweedieRegressor.score is a function of\\npower itself.\\n---------new doc---------\\nPeter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale\\nestimates, p. 172.\\n\\n\\nThe HuberRegressor differs from using SGDRegressor with loss set to huber\\nin the following ways.\\n\\nHuberRegressor is scaling invariant. Once epsilon is set, scaling X and y\\ndown or up by different values would produce the same robustness to outliers as before.\\nas compared to SGDRegressor where epsilon has to be set again when X and y are\\nscaled.\\nHuberRegressor should be more efficient to use on data with small number of\\nsamples while SGDRegressor needs a number of passes on the training data to\\nproduce the same robustness.\\n\\nNote that this estimator is different from the R implementation of Robust\\nRegression  because the R\\nimplementation does a weighted least squares implementation with weights given to each\\nsample on the basis of how much the residual is greater than a certain threshold.\\n\\n\\n\\n1.1.17. Quantile Regression#\\nQuantile regression estimates the median or other quantiles of \\\\(y\\\\)\\nconditional on \\\\(X\\\\), while ordinary least squares (OLS) estimates the\\nconditional mean.\\nQuantile regression may be useful if one is interested in predicting an\\ninterval instead of point prediction. Sometimes, prediction intervals are\\ncalculated based on the assumption that prediction error is distributed\\nnormally with zero mean and constant variance. Quantile regression provides\\nsensible prediction intervals even for errors with non-constant (but\\npredictable) variance or non-normal distribution.\\n---------new doc---------\\nBased on minimizing the pinball loss, conditional quantiles can also be\\nestimated by models other than linear models. For example,\\nGradientBoostingRegressor can predict conditional\\nquantiles if its parameter loss is set to \\\"quantile\\\" and parameter\\nalpha is set to the quantile that should be predicted. See the example in\\nPrediction Intervals for Gradient Boosting Regression.\\nMost implementations of quantile regression are based on linear programming\\nproblem. The current implementation is based on\\nscipy.optimize.linprog.\\nExamples\\n\\nQuantile regression\\n\\n\\n\\nMathematical details#\\nAs a linear model, the QuantileRegressor gives linear predictions\\n\\\\(\\\\hat{y}(w, X) = Xw\\\\) for the \\\\(q\\\\)-th quantile, \\\\(q \\\\in (0, 1)\\\\).\\nThe weights or coefficients \\\\(w\\\\) are then found by the following\\nminimization problem:\\n\\n\\\\[\\\\min_{w} {\\\\frac{1}{n_{\\\\text{samples}}}\\n\\\\sum_i PB_q(y_i - X_i w) + \\\\alpha ||w||_1}.\\\\]\\nThis consists of the pinball loss (also known as linear loss),\\nsee also mean_pinball_loss,\\n---------new doc---------\\n... stratify=y, test_size=0.7, random_state=42)\\n>>> nca = NeighborhoodComponentsAnalysis(random_state=42)\\n>>> knn = KNeighborsClassifier(n_neighbors=3)\\n>>> nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\\n>>> nca_pipe.fit(X_train, y_train)\\nPipeline(...)\\n>>> print(nca_pipe.score(X_test, y_test))\\n0.96190476...\\n---------new doc---------\\nReferences#\\n\\nRennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\\nTackling the poor assumptions of naive bayes text classifiers.\\nIn ICML (Vol. 3, pp. 616-623).\\n\\n\\n\\n\\n1.9.4. Bernoulli Naive Bayes#\\nBernoulliNB implements the naive Bayes training and classification\\nalgorithms for data that is distributed according to multivariate Bernoulli\\ndistributions; i.e., there may be multiple features but each one is assumed\\nto be a binary-valued (Bernoulli, boolean) variable.\\nTherefore, this class requires samples to be represented as binary-valued\\nfeature vectors; if handed any other kind of data, a BernoulliNB instance\\nmay binarize its input (depending on the binarize parameter).\\nThe decision rule for Bernoulli naive Bayes is based on\\n\\n\\\\[P(x_i \\\\mid y) = P(x_i = 1 \\\\mid y) x_i + (1 - P(x_i = 1 \\\\mid y)) (1 - x_i)\\\\]\\nwhich differs from multinomial NB’s rule\\nin that it explicitly penalizes the non-occurrence of a feature \\\\(i\\\\)\\nthat is an indicator for class \\\\(y\\\\),\\nwhere the multinomial variant would simply ignore a non-occurring feature.\\nIn the case of text classification, word occurrence vectors (rather than word\\ncount vectors) may be used to train and use this classifier. BernoulliNB\\nmight perform better on some datasets, especially those with shorter documents.\\nIt is advisable to evaluate both models, if time permits.\\n\\n\\nReferences#\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> X_train, X_test = X[:2000], X[2000:]\\n>>> y_train, y_test = y[:2000], y[2000:]\\n\\n>>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.8965\\n\\n\\nAvailable losses for regression are:\\n\\n‘squared_error’, which is the default loss;\\n‘absolute_error’, which is less sensitive to outliers than the squared error;\\n‘gamma’, which is well suited to model strictly positive outcomes;\\n‘poisson’, which is well suited to model counts and frequencies;\\n‘quantile’, which allows for estimating a conditional quantile that can later\\nbe used to obtain prediction intervals.\\n---------new doc---------\\nNote\\nFor some losses, e.g. 'absolute_error' where the gradients\\nare \\\\(\\\\pm 1\\\\), the values predicted by a fitted \\\\(h_m\\\\) are not\\naccurate enough: the tree can only output integer values. As a result, the\\nleaves values of the tree \\\\(h_m\\\\) are modified once the tree is\\nfitted, such that the leaves values minimize the loss \\\\(L_m\\\\). The\\nupdate is loss-dependent: for the absolute error loss, the value of\\na leaf is updated to the median of the samples in that leaf.\\n---------new doc---------\\n1.11.1.2.4. Loss Functions#\\nThe following loss functions are supported and can be specified using\\nthe parameter loss:\\n\\n\\nRegression#\\n\\nSquared error ('squared_error'): The natural choice for regression\\ndue to its superior computational properties. The initial model is\\ngiven by the mean of the target values.\\nAbsolute error ('absolute_error'): A robust loss function for\\nregression. The initial model is given by the median of the\\ntarget values.\\nHuber ('huber'): Another robust loss function that combines\\nleast squares and least absolute deviation; use alpha to\\ncontrol the sensitivity with regards to outliers (see [Friedman2001] for\\nmore details).\\nQuantile ('quantile'): A loss function for quantile regression.\\nUse 0 < alpha < 1 to specify the quantile. This loss function\\ncan be used to create prediction intervals\\n(see Prediction Intervals for Gradient Boosting Regression).\\n\\n\\n\\n\\nClassification#\\n---------new doc---------\\n1.11.4.3. Weighted Average Probabilities (Soft Voting)#\\nIn contrast to majority voting (hard voting), soft voting\\nreturns the class label as argmax of the sum of predicted probabilities.\\nSpecific weights can be assigned to each classifier via the weights\\nparameter. When weights are provided, the predicted class probabilities\\nfor each classifier are collected, multiplied by the classifier weight,\\nand averaged. The final class label is then derived from the class label\\nwith the highest average probability.\\nTo illustrate this with a simple example, let’s assume we have 3\\nclassifiers and a 3-class classification problems where we assign\\nequal weights to all classifiers: w1=1, w2=1, w3=1.\\nThe weighted average probabilities for a sample would then be\\ncalculated as follows:\\n\\n\\nclassifier\\nclass 1\\nclass 2\\nclass 3\\n\\n\\n\\nclassifier 1\\nw1 * 0.2\\nw1 * 0.5\\nw1 * 0.3\\n\\nclassifier 2\\nw2 * 0.6\\nw2 * 0.3\\nw2 * 0.1\\n\\nclassifier 3\\nw3 * 0.3\\nw3 * 0.4\\nw3 * 0.3\\n\\nweighted average\\n0.37\\n0.4\\n0.23\\n---------new doc---------\\n...                                             metric='euclidean'))],\\n...     final_estimator=final_layer\\n... )\\n>>> multi_layer_regressor.fit(X_train, y_train)\\nStackingRegressor(...)\\n>>> print('R2 score: {:.2f}'\\n...       .format(multi_layer_regressor.score(X_test, y_test)))\\nR2 score: 0.53\\n---------new doc---------\\nFor more information about LabelBinarizer,\\nrefer to Transforming the prediction target (y).\\n---------new doc---------\\nOneVsRestClassifier also supports multilabel\\nclassification. To use this feature, feed the classifier an indicator matrix,\\nin which cell [i, j] indicates the presence of label j in sample i.\\n\\n\\n\\n\\nExamples\\n\\nMultilabel classification\\nPlot classification probability\\n---------new doc---------\\nThese objects take as input a scoring function that returns univariate scores\\nand p-values (or only scores for SelectKBest and\\nSelectPercentile):\\n\\nFor regression: r_regression, f_regression, mutual_info_regression\\nFor classification: chi2, f_classif, mutual_info_classif\\n\\nThe methods based on F-test estimate the degree of linear dependency between\\ntwo random variables. On the other hand, mutual information methods can capture\\nany kind of statistical dependency, but being nonparametric, they require more\\nsamples for accurate estimation. Note that the \\\\(\\\\chi^2\\\\)-test should only be\\napplied to non-negative features, such as frequencies.\\n\\nFeature selection with sparse data\\nIf you use sparse data (i.e. data represented as sparse matrices),\\nchi2, mutual_info_regression, mutual_info_classif\\nwill deal with the data without making it dense.\\n\\n\\nWarning\\nBeware not to use a regression scoring function with a classification\\nproblem, you will get useless results.\\n\\n\\nNote\\nThe SelectPercentile and SelectKBest support unsupervised\\nfeature selection as well. One needs to provide a score_func where y=None.\\nThe score_func should use internally X to compute the scores.\\n\\nExamples\\n\\nUnivariate Feature Selection\\nComparison of F-test and mutual information\\n---------new doc---------\\nNote\\nStrictly proper scoring rules for probabilistic predictions like\\nsklearn.metrics.brier_score_loss and\\nsklearn.metrics.log_loss assess calibration (reliability) and\\ndiscriminative power (resolution) of a model, as well as the randomness of the data\\n(uncertainty) at the same time. This follows from the well-known Brier score\\ndecomposition of Murphy [1]. As it is not clear which term dominates, the score is\\nof limited use for assessing calibration alone (unless one computes each term of\\nthe decomposition). A lower Brier loss, for instance, does not necessarily\\nmean a better calibrated model, it could also mean a worse calibrated model with much\\nmore discriminatory power, e.g. using many more features.\\n---------new doc---------\\n1.16.1. Calibration curves#\\nCalibration curves, also referred to as reliability diagrams (Wilks 1995 [2]),\\ncompare how well the probabilistic predictions of a binary classifier are calibrated.\\nIt plots the frequency of the positive label (to be more precise, an estimation of the\\nconditional event probability \\\\(P(Y=1|\\\\text{predict_proba})\\\\)) on the y-axis\\nagainst the predicted probability predict_proba of a model on the x-axis.\\nThe tricky part is to get values for the y-axis.\\nIn scikit-learn, this is accomplished by binning the predictions such that the x-axis\\nrepresents the average predicted probability in each bin.\\nThe y-axis is then the fraction of positives given the predictions of that bin, i.e.\\nthe proportion of samples whose class is the positive class (in each bin).\\nThe top calibration curve plot is created with\\nCalibrationDisplay.from_estimator, which uses calibration_curve to\\ncalculate the per bin average predicted probabilities and fraction of positives.\\nCalibrationDisplay.from_estimator\\ntakes as input a fitted classifier, which is used to calculate the predicted\\nprobabilities. The classifier thus must have predict_proba method. For\\nthe few classifiers that do not have a predict_proba method, it is\\npossible to use CalibratedClassifierCV to calibrate the classifier\\noutputs to probabilities.\\nThe bottom histogram gives some insight into the behavior of each classifier\\nby showing the number of samples in each predicted probability bin.\\n---------new doc---------\\nNote\\nImpact on ranking metrics like AUC\\nIt is generally expected that calibration does not affect ranking metrics such as\\nROC-AUC. However, these metrics might differ after calibration when using\\nmethod=\\\"isotonic\\\" since isotonic regression introduces ties in the predicted\\nprobabilities. This can be seen as within the uncertainty of the model predictions.\\nIn case, you strictly want to keep the ranking and thus AUC scores, use\\nmethod=\\\"sigmoid\\\" which is a strictly monotonic transformation and thus keeps\\nthe ranking.\\n\\n\\n\\n1.16.3.3. Multiclass support#\\nBoth isotonic and sigmoid regressors only\\nsupport 1-dimensional data (e.g., binary classification output) but are\\nextended for multiclass classification if the base_estimator supports\\nmulticlass predictions. For multiclass predictions,\\nCalibratedClassifierCV calibrates for\\neach class separately in a OneVsRestClassifier fashion [5]. When\\npredicting\\nprobabilities, the calibrated probabilities for each class\\nare predicted separately. As those probabilities do not necessarily sum to\\none, a postprocessing is performed to normalize them.\\nExamples\\n\\nProbability Calibration curves\\nProbability Calibration for 3-class classification\\nProbability calibration of classifiers\\nComparison of Calibration of Classifiers\\n\\nReferences\\n\\n\\n[1]\\nAllan H. Murphy (1973).\\n“A New Vector Partition of the Probability Score”\\nJournal of Applied Meteorology and Climatology\\n\\n\\n[2]\\nOn the combination of forecast probabilities for\\nconsecutive precipitation periods.\\nWea. Forecasting, 5, 640–650., Wilks, D. S., 1990a\\n---------new doc---------\\nThe Rand index does not ensure to obtain a value close to 0.0 for a\\nrandom labelling. The adjusted Rand index corrects for chance and\\nwill give such a baseline.\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n0.24...\\n\\n\\nAs with all clustering metrics, one can permute 0 and 1 in the predicted\\nlabels, rename 2 to 3, and get the same score:\\n>>> labels_pred = [1, 1, 0, 0, 3, 3]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n0.66...\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n0.24...\\n\\n\\nFurthermore, both rand_score adjusted_rand_score are\\nsymmetric: swapping the argument does not change the scores. They can\\nthus be used as consensus measures:\\n>>> metrics.rand_score(labels_pred, labels_true)\\n0.66...\\n>>> metrics.adjusted_rand_score(labels_pred, labels_true)\\n0.24...\\n\\n\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n1.0\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n1.0\\n---------new doc---------\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n1.0\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n1.0\\n\\n\\nPoorly agreeing labels (e.g. independent labelings) have lower scores,\\nand for the adjusted Rand index the score will be negative or close to\\nzero. However, for the unadjusted Rand index the score, while lower,\\nwill not necessarily be close to zero.:\\n>>> labels_true = [0, 0, 0, 0, 0, 0, 1, 1]\\n>>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n0.39...\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n-0.07...\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nAdvantages:\\n\\nInterpretability: The unadjusted Rand index is proportional to the\\nnumber of sample pairs whose labels are the same in both labels_pred and\\nlabels_true, or are different in both.\\nRandom (uniform) label assignments have an adjusted Rand index score close\\nto 0.0 for any value of n_clusters and n_samples (which is not the\\ncase for the unadjusted Rand index or the V-measure for instance).\\nBounded range: Lower values indicate different labelings, similar\\nclusterings have a high (adjusted or unadjusted) Rand index, 1.0 is the\\nperfect match score. The score range is [0, 1] for the unadjusted Rand index\\nand [-0.5, 1] for the adjusted Rand index.\\nNo assumption is made on the cluster structure: The (adjusted or\\nunadjusted) Rand index can be used to compare all kinds of clustering\\nalgorithms, and can be used to compare clustering algorithms such as k-means\\nwhich assumes isotropic blob shapes with results of spectral clustering\\nalgorithms which can find cluster with “folded” shapes.\\n\\n\\n\\nDrawbacks:\\n\\nContrary to inertia, the (adjusted or unadjusted) Rand index requires\\nknowledge of the ground truth classes which is almost never available in\\npractice or requires manual assignment by human annotators (as in the\\nsupervised learning setting).\\nHowever (adjusted or unadjusted) Rand index can also be useful in a purely\\nunsupervised setting as a building block for a Consensus Index that can be\\nused for clustering model selection (TODO).\\n---------new doc---------\\nContrary to inertia, the (adjusted or unadjusted) Rand index requires\\nknowledge of the ground truth classes which is almost never available in\\npractice or requires manual assignment by human annotators (as in the\\nsupervised learning setting).\\nHowever (adjusted or unadjusted) Rand index can also be useful in a purely\\nunsupervised setting as a building block for a Consensus Index that can be\\nused for clustering model selection (TODO).\\n\\nThe unadjusted Rand index is often close to 1.0 even if the clusterings\\nthemselves differ significantly. This can be understood when interpreting\\nthe Rand index as the accuracy of element pair labeling resulting from the\\nclusterings: In practice there often is a majority of element pairs that are\\nassigned the different pair label under both the predicted and the\\nground truth clustering resulting in a high proportion of pair labels that\\nagree, which leads subsequently to a high score.\\n\\n\\nExamples\\n\\nAdjustment for chance in clustering performance evaluation:\\nAnalysis of the impact of the dataset size on the value of\\nclustering measures for random assignments.\\n\\n\\n\\nMathematical formulation#\\nIf C is a ground truth class assignment and K the clustering, let us define\\n\\\\(a\\\\) and \\\\(b\\\\) as:\\n\\n\\\\(a\\\\), the number of pairs of elements that are in the same set in C and\\nin the same set in K\\n\\\\(b\\\\), the number of pairs of elements that are in different sets in C and\\nin different sets in K\\n\\nThe unadjusted Rand index is then given by:\\n---------new doc---------\\n\\\\(a\\\\), the number of pairs of elements that are in the same set in C and\\nin the same set in K\\n\\\\(b\\\\), the number of pairs of elements that are in different sets in C and\\nin different sets in K\\n\\nThe unadjusted Rand index is then given by:\\n\\n\\\\[\\\\text{RI} = \\\\frac{a + b}{C_2^{n_{samples}}}\\\\]\\nwhere \\\\(C_2^{n_{samples}}\\\\) is the total number of possible pairs in the\\ndataset. It does not matter if the calculation is performed on ordered pairs or\\nunordered pairs as long as the calculation is performed consistently.\\nHowever, the Rand index does not guarantee that random label assignments will\\nget a value close to zero (esp. if the number of clusters is in the same order\\nof magnitude as the number of samples).\\nTo counter this effect we can discount the expected RI \\\\(E[\\\\text{RI}]\\\\) of\\nrandom labelings by defining the adjusted Rand index as follows:\\n\\n\\\\[\\\\text{ARI} = \\\\frac{\\\\text{RI} - E[\\\\text{RI}]}{\\\\max(\\\\text{RI}) - E[\\\\text{RI}]}\\\\]\\n\\n\\n\\nReferences#\\n\\nComparing Partitions L. Hubert and P.\\nArabie, Journal of Classification 1985\\nProperties of the Hubert-Arabie adjusted Rand index D. Steinley, Psychological\\nMethods 2004\\nWikipedia entry for the Rand index\\nMinimum adjusted Rand index for two clusterings of a given size, 2022, J. E. Chacón and A. I. Rastrojo\\n---------new doc---------\\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get\\nthe same score:\\n>>> labels_pred = [1, 1, 0, 0, 3, 3]\\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \\n0.22504...\\n\\n\\nAll, mutual_info_score, adjusted_mutual_info_score and\\nnormalized_mutual_info_score are symmetric: swapping the argument does\\nnot change the score. Thus they can be used as a consensus measure:\\n>>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  \\n0.22504...\\n\\n\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \\n1.0\\n\\n>>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  \\n1.0\\n\\n\\nThis is not true for mutual_info_score, which is therefore harder to judge:\\n>>> metrics.mutual_info_score(labels_true, labels_pred)  \\n0.69...\\n\\n\\nBad (e.g. independent labelings) have non-positive scores:\\n>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \\n-0.10526...\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nAdvantages:\\n\\nRandom (uniform) label assignments have a AMI score close to 0.0 for any\\nvalue of n_clusters and n_samples (which is not the case for raw\\nMutual Information or the V-measure for instance).\\nUpper bound  of 1:  Values close to zero indicate two label assignments\\nthat are largely independent, while values close to one indicate significant\\nagreement. Further, an AMI of exactly 1 indicates that the two label\\nassignments are equal (with or without permutation).\\n\\n\\n\\nDrawbacks:\\n\\nContrary to inertia, MI-based measures require the knowledge of the ground\\ntruth classes while almost never available in practice or requires manual\\nassignment by human annotators (as in the supervised learning setting).\\nHowever MI-based measures can also be useful in purely unsupervised setting\\nas a building block for a Consensus Index that can be used for clustering\\nmodel selection.\\n\\nNMI and MI are not adjusted against chance.\\n\\n\\nExamples\\n\\nAdjustment for chance in clustering performance evaluation: Analysis\\nof the impact of the dataset size on the value of clustering measures for random\\nassignments. This example also includes the Adjusted Rand Index.\\n\\n\\n\\nMathematical formulation#\\nAssume two label assignments (of the same N objects), \\\\(U\\\\) and \\\\(V\\\\).\\nTheir entropy is the amount of uncertainty for a partition set, defined by:\\n\\n\\\\[H(U) = - \\\\sum_{i=1}^{|U|}P(i)\\\\log(P(i))\\\\]\\nwhere \\\\(P(i) = |U_i| / N\\\\) is the probability that an object picked at\\nrandom from \\\\(U\\\\) falls into class \\\\(U_i\\\\). Likewise for \\\\(V\\\\):\\n---------new doc---------\\n\\\\[H(U) = - \\\\sum_{i=1}^{|U|}P(i)\\\\log(P(i))\\\\]\\nwhere \\\\(P(i) = |U_i| / N\\\\) is the probability that an object picked at\\nrandom from \\\\(U\\\\) falls into class \\\\(U_i\\\\). Likewise for \\\\(V\\\\):\\n\\n\\\\[H(V) = - \\\\sum_{j=1}^{|V|}P'(j)\\\\log(P'(j))\\\\]\\nWith \\\\(P'(j) = |V_j| / N\\\\). The mutual information (MI) between \\\\(U\\\\)\\nand \\\\(V\\\\) is calculated by:\\n\\n\\\\[\\\\text{MI}(U, V) = \\\\sum_{i=1}^{|U|}\\\\sum_{j=1}^{|V|}P(i, j)\\\\log\\\\left(\\\\frac{P(i,j)}{P(i)P'(j)}\\\\right)\\\\]\\nwhere \\\\(P(i, j) = |U_i \\\\cap V_j| / N\\\\) is the probability that an object\\npicked at random falls into both classes \\\\(U_i\\\\) and \\\\(V_j\\\\).\\nIt also can be expressed in set cardinality formulation:\\n\\n\\\\[\\\\text{MI}(U, V) = \\\\sum_{i=1}^{|U|} \\\\sum_{j=1}^{|V|} \\\\frac{|U_i \\\\cap V_j|}{N}\\\\log\\\\left(\\\\frac{N|U_i \\\\cap V_j|}{|U_i||V_j|}\\\\right)\\\\]\\nThe normalized mutual information is defined as\\n---------new doc---------\\nhomogeneity: each cluster contains only members of a single class.\\ncompleteness: all members of a given class are assigned to the same\\ncluster.\\n\\nWe can turn those concept as scores homogeneity_score and\\ncompleteness_score. Both are bounded below by 0.0 and above by\\n1.0 (higher is better):\\n>>> from sklearn import metrics\\n>>> labels_true = [0, 0, 0, 1, 1, 1]\\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\\n\\n>>> metrics.homogeneity_score(labels_true, labels_pred)\\n0.66...\\n\\n>>> metrics.completeness_score(labels_true, labels_pred)\\n0.42...\\n\\n\\nTheir harmonic mean called V-measure is computed by\\nv_measure_score:\\n>>> metrics.v_measure_score(labels_true, labels_pred)\\n0.51...\\n\\n\\nThis function’s formula is as follows:\\n\\n\\\\[v = \\\\frac{(1 + \\\\beta) \\\\times \\\\text{homogeneity} \\\\times \\\\text{completeness}}{(\\\\beta \\\\times \\\\text{homogeneity} + \\\\text{completeness})}\\\\]\\nbeta defaults to a value of 1.0, but for using a value less than 1 for beta:\\n>>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6)\\n0.54...\\n\\n\\nmore weight will be attributed to homogeneity, and using a value greater than 1:\\n>>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)\\n0.48...\\n---------new doc---------\\nmore weight will be attributed to homogeneity, and using a value greater than 1:\\n>>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)\\n0.48...\\n\\n\\nmore weight will be attributed to completeness.\\nThe V-measure is actually equivalent to the mutual information (NMI)\\ndiscussed above, with the aggregation function being the arithmetic mean [B2011].\\nHomogeneity, completeness and V-measure can be computed at once using\\nhomogeneity_completeness_v_measure as follows:\\n>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\\n(0.66..., 0.42..., 0.51...)\\n\\n\\nThe following clustering assignment is slightly better, since it is\\nhomogeneous but not complete:\\n>>> labels_pred = [0, 0, 0, 1, 2, 2]\\n>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\\n(1.0, 0.68..., 0.81...)\\n\\n\\n\\nNote\\nv_measure_score is symmetric: it can be used to evaluate\\nthe agreement of two independent assignments on the same dataset.\\nThis is not the case for completeness_score and\\nhomogeneity_score: both are bound by the relationship:\\nhomogeneity_score(a, b) == completeness_score(b, a)\\n\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nNote\\nv_measure_score is symmetric: it can be used to evaluate\\nthe agreement of two independent assignments on the same dataset.\\nThis is not the case for completeness_score and\\nhomogeneity_score: both are bound by the relationship:\\nhomogeneity_score(a, b) == completeness_score(b, a)\\n\\n\\n\\n\\nAdvantages:\\n\\nBounded scores: 0.0 is as bad as it can be, 1.0 is a perfect score.\\nIntuitive interpretation: clustering with bad V-measure can be\\nqualitatively analyzed in terms of homogeneity and completeness to\\nbetter feel what ‘kind’ of mistakes is done by the assignment.\\nNo assumption is made on the cluster structure: can be used to compare\\nclustering algorithms such as k-means which assumes isotropic blob shapes\\nwith results of spectral clustering algorithms which can find cluster with\\n“folded” shapes.\\n\\n\\n\\nDrawbacks:\\n\\nThe previously introduced metrics are not normalized with regards to\\nrandom labeling: this means that depending on the number of samples,\\nclusters and ground truth classes, a completely random labeling will not\\nalways yield the same values for homogeneity, completeness and hence\\nv-measure. In particular random labeling won’t yield zero scores\\nespecially when the number of clusters is large.\\nThis problem can safely be ignored when the number of samples is more than a\\nthousand and the number of clusters is less than 10. For smaller sample\\nsizes or larger number of clusters it is safer to use an adjusted index such\\nas the Adjusted Rand Index (ARI).\\n\\n\\n\\n\\n\\n\\n\\nThese metrics require the knowledge of the ground truth classes while\\nalmost never available in practice or requires manual assignment by human\\nannotators (as in the supervised learning setting).\\n\\n\\nExamples\\n---------new doc---------\\nTP (True Positive): The number of pairs of points that are clustered together\\nboth in the true labels and in the predicted labels.\\nFP (False Positive): The number of pairs of points that are clustered together\\nin the predicted labels but not in the true labels.\\nFN (False Negative): The number of pairs of points that are clustered together\\nin the true labels but not in the predicted labels.\\n\\nThe score ranges from 0 to 1. A high value indicates a good similarity\\nbetween two clusters.\\n>>> from sklearn import metrics\\n>>> labels_true = [0, 0, 0, 1, 1, 1]\\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\\n\\n\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.47140...\\n\\n\\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get\\nthe same score:\\n>>> labels_pred = [1, 1, 0, 0, 3, 3]\\n\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.47140...\\n\\n\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n1.0\\n\\n\\nBad (e.g. independent labelings) have zero scores:\\n>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.0\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nBad (e.g. independent labelings) have zero scores:\\n>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.0\\n\\n\\n\\nAdvantages:\\n\\nRandom (uniform) label assignments have a FMI score close to 0.0 for any\\nvalue of n_clusters and n_samples (which is not the case for raw\\nMutual Information or the V-measure for instance).\\nUpper-bounded at 1:  Values close to zero indicate two label assignments\\nthat are largely independent, while values close to one indicate significant\\nagreement. Further, values of exactly 0 indicate purely independent\\nlabel assignments and a FMI of exactly 1 indicates that the two label\\nassignments are equal (with or without permutation).\\nNo assumption is made on the cluster structure: can be used to compare\\nclustering algorithms such as k-means which assumes isotropic blob shapes\\nwith results of spectral clustering algorithms which can find cluster with\\n“folded” shapes.\\n\\n\\n\\nDrawbacks:\\n\\nContrary to inertia, FMI-based measures require the knowledge of the\\nground truth classes while almost never available in practice or requires\\nmanual assignment by human annotators (as in the supervised learning\\nsetting).\\n\\n\\n\\n\\nReferences#\\n---------new doc---------\\nDrawbacks:\\n\\nContrary to inertia, FMI-based measures require the knowledge of the\\nground truth classes while almost never available in practice or requires\\nmanual assignment by human annotators (as in the supervised learning\\nsetting).\\n\\n\\n\\n\\nReferences#\\n\\nE. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two\\nhierarchical clusterings”. Journal of the American Statistical Association.\\nhttps://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008\\nWikipedia entry for the Fowlkes-Mallows Index\\n\\n\\n\\n\\n2.3.11.5. Silhouette Coefficient#\\nIf the ground truth labels are not known, evaluation must be performed using\\nthe model itself. The Silhouette Coefficient\\n(sklearn.metrics.silhouette_score)\\nis an example of such an evaluation, where a\\nhigher Silhouette Coefficient score relates to a model with better defined\\nclusters. The Silhouette Coefficient is defined for each sample and is composed\\nof two scores:\\n\\na: The mean distance between a sample and all other points in the same\\nclass.\\nb: The mean distance between a sample and all other points in the next\\nnearest cluster.\\n\\nThe Silhouette Coefficient s for a single sample is then given as:\\n---------new doc---------\\nThe mean score and the standard deviation are hence given by:\\n>>> print(\\\"%0.2f accuracy with a standard deviation of %0.2f\\\" % (scores.mean(), scores.std()))\\n0.98 accuracy with a standard deviation of 0.02\\n\\n\\nBy default, the score computed at each CV iteration is the score\\nmethod of the estimator. It is possible to change this by using the\\nscoring parameter:\\n>>> from sklearn import metrics\\n>>> scores = cross_val_score(\\n...     clf, X, y, cv=5, scoring='f1_macro')\\n>>> scores\\narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])\\n---------new doc---------\\nSee Pipelines and composite estimators.\\n\\n\\n3.1.1.1. The cross_validate function and multiple metric evaluation#\\nThe cross_validate function differs from cross_val_score in\\ntwo ways:\\n\\nIt allows specifying multiple metrics for evaluation.\\nIt returns a dict containing fit-times, score-times\\n(and optionally training scores, fitted estimators, train-test split indices)\\nin addition to the test score.\\n---------new doc---------\\nFor single metric evaluation, where the scoring parameter is a string,\\ncallable or None, the keys will be - ['test_score', 'fit_time', 'score_time']\\nAnd for multiple metric evaluation, the return value is a dict with the\\nfollowing keys -\\n['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']\\nreturn_train_score is set to False by default to save computation time.\\nTo evaluate the scores on the training set as well you need to set it to\\nTrue. You may also retain the estimator fitted on each training set by\\nsetting return_estimator=True. Similarly, you may set\\nreturn_indices=True to retain the training and testing indices used to split\\nthe dataset into train and test sets for each cv split.\\nThe multiple metrics can be specified either as a list, tuple or set of\\npredefined scorer names:\\n>>> from sklearn.model_selection import cross_validate\\n>>> from sklearn.metrics import recall_score\\n>>> scoring = ['precision_macro', 'recall_macro']\\n>>> clf = svm.SVC(kernel='linear', C=1, random_state=0)\\n>>> scores = cross_validate(clf, X, y, scoring=scoring)\\n>>> sorted(scores.keys())\\n['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']\\n>>> scores['test_recall_macro']\\narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])\\n---------new doc---------\\nOr as a dict mapping scorer name to a predefined or custom scoring function:\\n>>> from sklearn.metrics import make_scorer\\n>>> scoring = {'prec_macro': 'precision_macro',\\n...            'rec_macro': make_scorer(recall_score, average='macro')}\\n>>> scores = cross_validate(clf, X, y, scoring=scoring,\\n...                         cv=5, return_train_score=True)\\n>>> sorted(scores.keys())\\n['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',\\n 'train_prec_macro', 'train_rec_macro']\\n>>> scores['train_rec_macro']\\narray([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])\\n\\n\\nHere is an example of cross_validate using a single metric:\\n>>> scores = cross_validate(clf, X, y,\\n...                         scoring='precision_macro', cv=5,\\n...                         return_estimator=True)\\n>>> sorted(scores.keys())\\n['estimator', 'fit_time', 'score_time', 'test_score']\\n---------new doc---------\\n3.1.5. Permutation test score#\\npermutation_test_score offers another way\\nto evaluate the performance of classifiers. It provides a permutation-based\\np-value, which represents how likely an observed performance of the\\nclassifier would be obtained by chance. The null hypothesis in this test is\\nthat the classifier fails to leverage any statistical dependency between the\\nfeatures and the labels to make correct predictions on left out data.\\npermutation_test_score generates a null\\ndistribution by calculating n_permutations different permutations of the\\ndata. In each permutation the labels are randomly shuffled, thereby removing\\nany dependency between the features and the labels. The p-value output\\nis the fraction of permutations for which the average cross-validation score\\nobtained by the model is better than the cross-validation score obtained by\\nthe model using the original data. For reliable results n_permutations\\nshould typically be larger than 100 and cv between 3-10 folds.\\nA low p-value provides evidence that the dataset contains real dependency\\nbetween features and labels and the classifier was able to utilize this\\nto obtain good results. A high p-value could be due to a lack of dependency\\nbetween features and labels (there is no difference in feature values between\\nthe classes) or because the classifier was not able to use the dependency in\\nthe data. In the latter case, using a more appropriate classifier that\\nis able to utilize the structure in the data, would result in a lower\\np-value.\\nCross-validation provides information about how well a classifier generalizes,\\nspecifically the range of expected errors of the classifier. However, a\\nclassifier trained on a high dimensional dataset with no structure may still\\nperform better than expected on cross-validation, just by chance.\\n---------new doc---------\\n3.2.4.2. Specifying multiple metrics for evaluation#\\nGridSearchCV and RandomizedSearchCV allow specifying\\nmultiple metrics for the scoring parameter.\\nMultimetric scoring can either be specified as a list of strings of predefined\\nscores names or a dict mapping the scorer name to the scorer function and/or\\nthe predefined scorer name(s). See Using multiple metric evaluation for more details.\\nWhen specifying multiple metrics, the refit parameter must be set to the\\nmetric (string) for which the best_params_ will be found and used to build\\nthe best_estimator_ on the whole dataset. If the search should not be\\nrefit, set refit=False. Leaving refit to the default value None will\\nresult in an error when using multiple metrics.\\nSee Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV\\nfor an example usage.\\nHalvingRandomSearchCV and HalvingGridSearchCV do not support\\nmultimetric scoring.\\n---------new doc---------\\nWhile these hard-coded rules might at first seem reasonable as default behavior, they\\nare most certainly not ideal for most use cases. Let’s illustrate with an example.\\nConsider a scenario where a predictive model is being deployed to assist\\nphysicians in detecting tumors. In this setting, physicians will most likely be\\ninterested in identifying all patients with cancer and not missing anyone with cancer so\\nthat they can provide them with the right treatment. In other words, physicians\\nprioritize achieving a high recall rate. This emphasis on recall comes, of course, with\\nthe trade-off of potentially more false-positive predictions, reducing the precision of\\nthe model. That is a risk physicians are willing to take because the cost of a missed\\ncancer is much higher than the cost of further diagnostic tests. Consequently, when it\\ncomes to deciding whether to classify a patient as having cancer or not, it may be more\\nbeneficial to classify them as positive for cancer when the conditional probability\\nestimate is much lower than 0.5.\\n---------new doc---------\\nNote\\nIt is important to notice that these metrics come with default parameters, notably\\nthe label of the class of interest (i.e. pos_label). Thus, if this label is not\\nthe right one for your application, you need to define a scorer and pass the right\\npos_label (and additional parameters) using the\\nmake_scorer. Refer to Callable scorers to get\\ninformation to define your own scoring function. For instance, we show how to pass\\nthe information to the scorer that the label of interest is 0 when maximizing the\\nf1_score:\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.model_selection import TunedThresholdClassifierCV\\n>>> from sklearn.metrics import make_scorer, f1_score\\n>>> X, y = make_classification(\\n...   n_samples=1_000, weights=[0.1, 0.9], random_state=0)\\n>>> pos_label = 0\\n>>> scorer = make_scorer(f1_score, pos_label=pos_label)\\n>>> base_model = LogisticRegression()\\n>>> model = TunedThresholdClassifierCV(base_model, scoring=scorer)\\n>>> scorer(model.fit(X, y), X, y)\\n0.88...\\n>>> # compare it with the internal score found by cross-validation\\n>>> model.best_score_\\n0.86...\\n---------new doc---------\\n3.3.1.4. Examples#\\n\\nSee the example entitled\\nPost-hoc tuning the cut-off point of decision function,\\nto get insights on the post-tuning of the decision threshold.\\nSee the example entitled\\nPost-tuning the decision threshold for cost-sensitive learning,\\nto learn about cost-sensitive learning and decision threshold tuning.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.2. Tuning the hyper-parameters of an estimator\\n\\n\\n\\n\\nnext\\n3.4. Metrics and scoring: quantifying the quality of predictions\\n\\n\\n --- \\n\\n\\nand Developer\\nGuide for\\nmore details.\\n\\n\\n\\n\\nUsing custom scorers in functions where n_jobs > 1#\\nWhile defining the custom scoring function alongside the calling function\\nshould work out of the box with the default joblib backend (loky),\\nimporting it from another module will be a more robust approach and work\\nindependently of the joblib backend.\\nFor example, to use n_jobs greater than 1 in the example below,\\ncustom_scoring_function function is saved in a user-created module\\n(custom_scorer_module.py) and imported:\\n>>> from custom_scorer_module import custom_scoring_function \\n>>> cross_val_score(model,\\n...  X_train,\\n...  y_train,\\n...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),\\n...  cv=5,\\n...  n_jobs=-1) \\n\\n\\n\\n\\n\\n\\n3.4.3.3. Using multiple metric evaluation#\\nScikit-learn also permits evaluation of multiple metrics in GridSearchCV,\\nRandomizedSearchCV and cross_validate.\\nThere are three ways to specify multiple scoring metrics for the scoring\\nparameter:\\n---------new doc---------\\n3.4.3.3. Using multiple metric evaluation#\\nScikit-learn also permits evaluation of multiple metrics in GridSearchCV,\\nRandomizedSearchCV and cross_validate.\\nThere are three ways to specify multiple scoring metrics for the scoring\\nparameter:\\n\\nAs an iterable of string metrics:\\n>>> scoring = ['accuracy', 'precision']\\n\\n\\n\\nAs a dict mapping the scorer name to the scoring function:\\n>>> from sklearn.metrics import accuracy_score\\n>>> from sklearn.metrics import make_scorer\\n>>> scoring = {'accuracy': make_scorer(accuracy_score),\\n...            'prec': 'precision'}\\n\\n\\nNote that the dict values can either be scorer functions or one of the\\npredefined metric strings.\\n---------new doc---------\\nNote that the dict values can either be scorer functions or one of the\\npredefined metric strings.\\n\\nAs a callable that returns a dictionary of scores:\\n>>> from sklearn.model_selection import cross_validate\\n>>> from sklearn.metrics import confusion_matrix\\n>>> # A sample toy binary classification dataset\\n>>> X, y = datasets.make_classification(n_classes=2, random_state=0)\\n>>> svm = LinearSVC(random_state=0)\\n>>> def confusion_matrix_scorer(clf, X, y):\\n...      y_pred = clf.predict(X)\\n...      cm = confusion_matrix(y, y_pred)\\n...      return {'tn': cm[0, 0], 'fp': cm[0, 1],\\n...              'fn': cm[1, 0], 'tp': cm[1, 1]}\\n>>> cv_results = cross_validate(svm, X, y, cv=5,\\n...                             scoring=confusion_matrix_scorer)\\n>>> # Getting the test set true positive scores\\n>>> print(cv_results['test_tp'])\\n[10  9  8  7  8]\\n>>> # Getting the test set false negative scores\\n>>> print(cv_results['test_fn'])\\n[0 1 2 3 2]\\n---------new doc---------\\n3.4.4. Classification metrics#\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions to measure classification performance.\\nSome metrics might require probability estimates of the positive class,\\nconfidence values, or binary decisions values.\\nMost implementations allow each sample to provide a weighted contribution\\nto the overall score, through the sample_weight parameter.\\nSome of these are restricted to the binary classification case:\\n\\n\\nprecision_recall_curve(y_true[, y_score, ...])\\nCompute precision-recall pairs for different probability thresholds.\\n\\nroc_curve(y_true, y_score, *[, pos_label, ...])\\nCompute Receiver operating characteristic (ROC).\\n\\nclass_likelihood_ratios(y_true, y_pred, *[, ...])\\nCompute binary classification positive and negative likelihood ratios.\\n\\ndet_curve(y_true, y_score[, pos_label, ...])\\nCompute error rates for different probability thresholds.\\n\\n\\n\\n\\nOthers also work in the multiclass case:\\n\\n\\nbalanced_accuracy_score(y_true, y_pred, *[, ...])\\nCompute the balanced accuracy.\\n\\ncohen_kappa_score(y1, y2, *[, labels, ...])\\nCompute Cohen's kappa: a statistic that measures inter-annotator agreement.\\n\\nconfusion_matrix(y_true, y_pred, *[, ...])\\nCompute confusion matrix to evaluate the accuracy of a classification.\\n---------new doc---------\\ncohen_kappa_score(y1, y2, *[, labels, ...])\\nCompute Cohen's kappa: a statistic that measures inter-annotator agreement.\\n\\nconfusion_matrix(y_true, y_pred, *[, ...])\\nCompute confusion matrix to evaluate the accuracy of a classification.\\n\\nhinge_loss(y_true, pred_decision, *[, ...])\\nAverage hinge loss (non-regularized).\\n\\nmatthews_corrcoef(y_true, y_pred, *[, ...])\\nCompute the Matthews correlation coefficient (MCC).\\n\\nroc_auc_score(y_true, y_score, *[, average, ...])\\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\\n\\ntop_k_accuracy_score(y_true, y_score, *[, ...])\\nTop-k Accuracy classification score.\\n\\n\\n\\n\\nSome also work in the multilabel case:\\n\\n\\naccuracy_score(y_true, y_pred, *[, ...])\\nAccuracy classification score.\\n\\nclassification_report(y_true, y_pred, *[, ...])\\nBuild a text report showing the main classification metrics.\\n\\nf1_score(y_true, y_pred, *[, labels, ...])\\nCompute the F1 score, also known as balanced F-score or F-measure.\\n\\nfbeta_score(y_true, y_pred, *, beta[, ...])\\nCompute the F-beta score.\\n---------new doc---------\\nf1_score(y_true, y_pred, *[, labels, ...])\\nCompute the F1 score, also known as balanced F-score or F-measure.\\n\\nfbeta_score(y_true, y_pred, *, beta[, ...])\\nCompute the F-beta score.\\n\\nhamming_loss(y_true, y_pred, *[, sample_weight])\\nCompute the average Hamming loss.\\n\\njaccard_score(y_true, y_pred, *[, labels, ...])\\nJaccard similarity coefficient score.\\n\\nlog_loss(y_true, y_pred, *[, normalize, ...])\\nLog loss, aka logistic loss or cross-entropy loss.\\n\\nmultilabel_confusion_matrix(y_true, y_pred, *)\\nCompute a confusion matrix for each class or sample.\\n\\nprecision_recall_fscore_support(y_true, ...)\\nCompute precision, recall, F-measure and support for each class.\\n\\nprecision_score(y_true, y_pred, *[, labels, ...])\\nCompute the precision.\\n\\nrecall_score(y_true, y_pred, *[, labels, ...])\\nCompute the recall.\\n\\nroc_auc_score(y_true, y_score, *[, average, ...])\\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\\n---------new doc---------\\nrecall_score(y_true, y_pred, *[, labels, ...])\\nCompute the recall.\\n\\nroc_auc_score(y_true, y_score, *[, average, ...])\\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\\n\\nzero_one_loss(y_true, y_pred, *[, ...])\\nZero-one classification loss.\\n\\nd2_log_loss_score(y_true, y_pred, *[, ...])\\n\\\\(D^2\\\\) score function, fraction of log loss explained.\\n\\n\\n\\n\\nAnd some work with binary and multilabel (but not multiclass) problems:\\n\\n\\naverage_precision_score(y_true, y_score, *)\\nCompute average precision (AP) from prediction scores.\\n\\n\\n\\n\\nIn the following sub-sections, we will describe each of those functions,\\npreceded by some notes on common API and metric definition.\\n---------new doc---------\\nAnd some work with binary and multilabel (but not multiclass) problems:\\n\\n\\naverage_precision_score(y_true, y_score, *)\\nCompute average precision (AP) from prediction scores.\\n\\n\\n\\n\\nIn the following sub-sections, we will describe each of those functions,\\npreceded by some notes on common API and metric definition.\\n\\n3.4.4.1. From binary to multiclass and multilabel#\\nSome metrics are essentially defined for binary classification tasks (e.g.\\nf1_score, roc_auc_score). In these cases, by default\\nonly the positive label is evaluated, assuming by default that the positive\\nclass is labelled 1 (though this may be configurable through the\\npos_label parameter).\\nIn extending a binary metric to multiclass or multilabel problems, the data\\nis treated as a collection of binary problems, one for each class.\\nThere are then a number of ways to average binary metric calculations across\\nthe set of classes, each of which may be useful in some scenario.\\nWhere available, you should select among these using the average parameter.\\n---------new doc---------\\n\\\"macro\\\" simply calculates the mean of the binary metrics,\\ngiving equal weight to each class.  In problems where infrequent classes\\nare nonetheless important, macro-averaging may be a means of highlighting\\ntheir performance. On the other hand, the assumption that all classes are\\nequally important is often untrue, such that macro-averaging will\\nover-emphasize the typically low performance on an infrequent class.\\n\\\"weighted\\\" accounts for class imbalance by computing the average of\\nbinary metrics in which each class’s score is weighted by its presence in the\\ntrue data sample.\\n\\\"micro\\\" gives each sample-class pair an equal contribution to the overall\\nmetric (except as a result of sample-weight). Rather than summing the\\nmetric per class, this sums the dividends and divisors that make up the\\nper-class metrics to calculate an overall quotient.\\nMicro-averaging may be preferred in multilabel settings, including\\nmulticlass classification where a majority class is to be ignored.\\n\\\"samples\\\" applies only to multilabel problems. It does not calculate a\\nper-class measure, instead calculating the metric over the true and predicted\\nclasses for each sample in the evaluation data, and returning their\\n(sample_weight-weighted) average.\\nSelecting average=None will return an array with the score for each\\nclass.\\n\\nWhile multiclass data is provided to the metric, like binary targets, as an\\narray of class labels, multilabel data is specified as an indicator matrix,\\nin which cell [i, j] has value 1 if sample i has label j and value\\n0 otherwise.\\n---------new doc---------\\nWhile multiclass data is provided to the metric, like binary targets, as an\\narray of class labels, multilabel data is specified as an indicator matrix,\\nin which cell [i, j] has value 1 if sample i has label j and value\\n0 otherwise.\\n\\n\\n3.4.4.2. Accuracy score#\\nThe accuracy_score function computes the\\naccuracy, either the fraction\\n(default) or the count (normalize=False) of correct predictions.\\nIn multilabel classification, the function returns the subset accuracy. If\\nthe entire set of predicted labels for a sample strictly match with the true\\nset of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of\\nthe \\\\(i\\\\)-th sample and \\\\(y_i\\\\) is the corresponding true value,\\nthen the fraction of correct predictions over \\\\(n_\\\\text{samples}\\\\) is\\ndefined as\\n\\n\\\\[\\\\texttt{accuracy}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} 1(\\\\hat{y}_i = y_i)\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function.\\n>>> import numpy as np\\n>>> from sklearn.metrics import accuracy_score\\n>>> y_pred = [0, 2, 1, 3]\\n>>> y_true = [0, 1, 2, 3]\\n>>> accuracy_score(y_true, y_pred)\\n0.5\\n>>> accuracy_score(y_true, y_pred, normalize=False)\\n2.0\\n---------new doc---------\\nIn the multilabel case with binary label indicators:\\n>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\\n0.5\\n\\n\\nExamples\\n\\nSee Test with permutations the significance of a classification score\\nfor an example of accuracy score usage using permutations of\\nthe dataset.\\n\\n\\n\\n3.4.4.3. Top-k accuracy score#\\nThe top_k_accuracy_score function is a generalization of\\naccuracy_score. The difference is that a prediction is considered\\ncorrect as long as the true label is associated with one of the k highest\\npredicted scores. accuracy_score is the special case of k = 1.\\nThe function covers the binary and multiclass classification cases but not the\\nmultilabel case.\\nIf \\\\(\\\\hat{f}_{i,j}\\\\) is the predicted class for the \\\\(i\\\\)-th sample\\ncorresponding to the \\\\(j\\\\)-th largest predicted score and \\\\(y_i\\\\) is the\\ncorresponding true value, then the fraction of correct predictions over\\n\\\\(n_\\\\text{samples}\\\\) is defined as\\n---------new doc---------\\n\\\\[\\\\texttt{top-k accuracy}(y, \\\\hat{f}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} \\\\sum_{j=1}^{k} 1(\\\\hat{f}_{i,j} = y_i)\\\\]\\nwhere \\\\(k\\\\) is the number of guesses allowed and \\\\(1(x)\\\\) is the\\nindicator function.\\n>>> import numpy as np\\n>>> from sklearn.metrics import top_k_accuracy_score\\n>>> y_true = np.array([0, 1, 2, 2])\\n>>> y_score = np.array([[0.5, 0.2, 0.2],\\n...                     [0.3, 0.4, 0.2],\\n...                     [0.2, 0.4, 0.3],\\n...                     [0.7, 0.2, 0.1]])\\n>>> top_k_accuracy_score(y_true, y_score, k=2)\\n0.75\\n>>> # Not normalizing gives the number of \\\"correctly\\\" classified samples\\n>>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\\n3\\n---------new doc---------\\n3.4.4.4. Balanced accuracy score#\\nThe balanced_accuracy_score function computes the balanced accuracy, which avoids inflated\\nperformance estimates on imbalanced datasets. It is the macro-average of recall\\nscores per class or, equivalently, raw accuracy where each sample is weighted\\naccording to the inverse prevalence of its true class.\\nThus for balanced datasets, the score is equal to accuracy.\\nIn the binary case, balanced accuracy is equal to the arithmetic mean of\\nsensitivity\\n(true positive rate) and specificity (true negative\\nrate), or the area under the ROC curve with binary predictions rather than\\nscores:\\n\\n\\\\[\\\\texttt{balanced-accuracy} = \\\\frac{1}{2}\\\\left( \\\\frac{TP}{TP + FN} + \\\\frac{TN}{TN + FP}\\\\right )\\\\]\\nIf the classifier performs equally well on either class, this term reduces to\\nthe conventional accuracy (i.e., the number of correct predictions divided by\\nthe total number of predictions).\\nIn contrast, if the conventional accuracy is above chance only because the\\nclassifier takes advantage of an imbalanced test set, then the balanced\\naccuracy, as appropriate, will drop to \\\\(\\\\frac{1}{n\\\\_classes}\\\\).\\nThe score ranges from 0 to 1, or when adjusted=True is used, it rescaled to\\nthe range \\\\(\\\\frac{1}{1 - n\\\\_classes}\\\\) to 1, inclusive, with\\nperformance at random scoring 0.\\nIf \\\\(y_i\\\\) is the true value of the \\\\(i\\\\)-th sample, and \\\\(w_i\\\\)\\nis the corresponding sample weight, then we adjust the sample weight to:\\n---------new doc---------\\n\\\\[\\\\hat{w}_i = \\\\frac{w_i}{\\\\sum_j{1(y_j = y_i) w_j}}\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function.\\nGiven predicted \\\\(\\\\hat{y}_i\\\\) for sample \\\\(i\\\\), balanced accuracy is\\ndefined as:\\n\\n\\\\[\\\\texttt{balanced-accuracy}(y, \\\\hat{y}, w) = \\\\frac{1}{\\\\sum{\\\\hat{w}_i}} \\\\sum_i 1(\\\\hat{y}_i = y_i) \\\\hat{w}_i\\\\]\\nWith adjusted=True, balanced accuracy reports the relative increase from\\n\\\\(\\\\texttt{balanced-accuracy}(y, \\\\mathbf{0}, w) =\\n\\\\frac{1}{n\\\\_classes}\\\\).  In the binary case, this is also known as\\n*Youden’s J statistic*,\\nor informedness.\\n\\nNote\\nThe multiclass definition here seems the most reasonable extension of the\\nmetric used in binary classification, though there is no certain consensus\\nin the literature:\\n\\nOur definition: [Mosley2013], [Kelleher2015] and [Guyon2015], where\\n[Guyon2015] adopt the adjusted version to ensure that random predictions\\nhave a score of \\\\(0\\\\) and perfect predictions have a score of \\\\(1\\\\)..\\nClass balanced accuracy as described in [Mosley2013]: the minimum between the precision\\nand the recall for each class is computed. Those values are then averaged over the total\\nnumber of classes to get the balanced accuracy.\\nBalanced Accuracy as described in [Urbanowicz2015]: the average of sensitivity and specificity\\nis computed for each class and then averaged over total number of classes.\\n\\n\\nReferences\\n---------new doc---------\\n[Urbanowicz2015]\\nUrbanowicz R.J.,  Moore, J.H. ExSTraCS 2.0: description\\nand evaluation of a scalable learning classifier\\nsystem, Evol. Intel. (2015) 8: 89.\\n\\n\\n\\n\\n3.4.4.5. Cohen’s kappa#\\nThe function cohen_kappa_score computes Cohen’s kappa statistic.\\nThis measure is intended to compare labelings by different human annotators,\\nnot a classifier versus a ground truth.\\nThe kappa score is a number between -1 and 1.\\nScores above .8 are generally considered good agreement;\\nzero or lower means no agreement (practically random labels).\\nKappa scores can be computed for binary or multiclass problems,\\nbut not for multilabel problems (except by manually computing a per-label score)\\nand not for more than two annotators.\\n>>> from sklearn.metrics import cohen_kappa_score\\n>>> labeling1 = [2, 0, 2, 2, 0, 1]\\n>>> labeling2 = [0, 0, 2, 2, 0, 2]\\n>>> cohen_kappa_score(labeling1, labeling2)\\n0.4285714285714286\\n---------new doc---------\\n3.4.4.6. Confusion matrix#\\nThe confusion_matrix function evaluates\\nclassification accuracy by computing the confusion matrix with each row corresponding\\nto the true class (Wikipedia and other references may use different convention\\nfor axes).\\nBy definition, entry \\\\(i, j\\\\) in a confusion matrix is\\nthe number of observations actually in group \\\\(i\\\\), but\\npredicted to be in group \\\\(j\\\\). Here is an example:\\n>>> from sklearn.metrics import confusion_matrix\\n>>> y_true = [2, 0, 2, 2, 0, 1]\\n>>> y_pred = [0, 0, 2, 2, 0, 2]\\n>>> confusion_matrix(y_true, y_pred)\\narray([[2, 0, 0],\\n       [0, 0, 1],\\n       [1, 0, 2]])\\n\\n\\nConfusionMatrixDisplay can be used to visually represent a confusion\\nmatrix as shown in the\\nConfusion matrix\\nexample, which creates the following figure:\\n\\n\\nThe parameter normalize allows to report ratios instead of counts. The\\nconfusion matrix can be normalized in 3 different ways: 'pred', 'true',\\nand 'all' which will divide the counts by the sum of each columns, rows, or\\nthe entire matrix, respectively.\\n>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]\\n>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\\n>>> confusion_matrix(y_true, y_pred, normalize='all')\\narray([[0.25 , 0.125],\\n       [0.25 , 0.375]])\\n---------new doc---------\\nFor binary problems, we can get counts of true negatives, false positives,\\nfalse negatives and true positives as follows:\\n>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]\\n>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\\n>>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\\n>>> tn, fp, fn, tp\\n(2, 1, 2, 3)\\n\\n\\nExamples\\n\\nSee Confusion matrix\\nfor an example of using a confusion matrix to evaluate classifier output\\nquality.\\nSee Recognizing hand-written digits\\nfor an example of using a confusion matrix to classify\\nhand-written digits.\\nSee Classification of text documents using sparse features\\nfor an example of using a confusion matrix to classify text\\ndocuments.\\n\\n\\n\\n3.4.4.7. Classification report#\\nThe classification_report function builds a text report showing the\\nmain classification metrics. Here is a small example with custom target_names\\nand inferred labels:\\n>>> from sklearn.metrics import classification_report\\n>>> y_true = [0, 1, 2, 2, 0]\\n>>> y_pred = [0, 0, 2, 1, 0]\\n>>> target_names = ['class 0', 'class 1', 'class 2']\\n>>> print(classification_report(y_true, y_pred, target_names=target_names))\\n              precision    recall  f1-score   support\\n---------new doc---------\\n\\\\[L_{Hamming}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples} * n_\\\\text{labels}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} \\\\sum_{j=0}^{n_\\\\text{labels} - 1} 1(\\\\hat{y}_{i,j} \\\\not= y_{i,j})\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function.\\nThe equation above does not hold true in the case of multiclass classification.\\nPlease refer to the note below for more information.\\n>>> from sklearn.metrics import hamming_loss\\n>>> y_pred = [1, 2, 3, 4]\\n>>> y_true = [2, 2, 3, 4]\\n>>> hamming_loss(y_true, y_pred)\\n0.25\\n\\n\\nIn the multilabel case with binary label indicators:\\n>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\\n0.75\\n\\n\\n\\nNote\\nIn multiclass classification, the Hamming loss corresponds to the Hamming\\ndistance between y_true and y_pred which is similar to the\\nZero one loss function.  However, while zero-one loss penalizes\\nprediction sets that do not strictly match true sets, the Hamming loss\\npenalizes individual labels.  Thus the Hamming loss, upper bounded by the zero-one\\nloss, is always between zero and one, inclusive; and predicting a proper subset\\nor superset of the true labels will give a Hamming loss between\\nzero and one, exclusive.\\n---------new doc---------\\n3.4.4.9. Precision, recall and F-measures#\\nIntuitively, precision is the ability\\nof the classifier not to label as positive a sample that is negative, and\\nrecall is the\\nability of the classifier to find all the positive samples.\\nThe  F-measure\\n(\\\\(F_\\\\beta\\\\) and \\\\(F_1\\\\) measures) can be interpreted as a weighted\\nharmonic mean of the precision and recall. A\\n\\\\(F_\\\\beta\\\\) measure reaches its best value at 1 and its worst score at 0.\\nWith \\\\(\\\\beta = 1\\\\),  \\\\(F_\\\\beta\\\\) and\\n\\\\(F_1\\\\)  are equivalent, and the recall and the precision are equally important.\\nThe precision_recall_curve computes a precision-recall curve\\nfrom the ground truth label and a score given by the classifier\\nby varying a decision threshold.\\nThe average_precision_score function computes the\\naverage precision\\n(AP) from prediction scores. The value is between 0 and 1 and higher is better.\\nAP is defined as\\n---------new doc---------\\n\\\\[\\\\text{AP} = \\\\sum_n (R_n - R_{n-1}) P_n\\\\]\\nwhere \\\\(P_n\\\\) and \\\\(R_n\\\\) are the precision and recall at the\\nnth threshold. With random predictions, the AP is the fraction of positive\\nsamples.\\nReferences [Manning2008] and [Everingham2010] present alternative variants of\\nAP that interpolate the precision-recall curve. Currently,\\naverage_precision_score does not implement any interpolated variant.\\nReferences [Davis2006] and [Flach2015] describe why a linear interpolation of\\npoints on the precision-recall curve provides an overly-optimistic measure of\\nclassifier performance. This linear interpolation is used when computing area\\nunder the curve with the trapezoidal rule in auc.\\nSeveral functions allow you to analyze the precision, recall and F-measures\\nscore:\\n\\n\\naverage_precision_score(y_true, y_score, *)\\nCompute average precision (AP) from prediction scores.\\n\\nf1_score(y_true, y_pred, *[, labels, ...])\\nCompute the F1 score, also known as balanced F-score or F-measure.\\n\\nfbeta_score(y_true, y_pred, *, beta[, ...])\\nCompute the F-beta score.\\n\\nprecision_recall_curve(y_true[, y_score, ...])\\nCompute precision-recall pairs for different probability thresholds.\\n\\nprecision_recall_fscore_support(y_true, ...)\\nCompute precision, recall, F-measure and support for each class.\\n---------new doc---------\\nprecision_recall_curve(y_true[, y_score, ...])\\nCompute precision-recall pairs for different probability thresholds.\\n\\nprecision_recall_fscore_support(y_true, ...)\\nCompute precision, recall, F-measure and support for each class.\\n\\nprecision_score(y_true, y_pred, *[, labels, ...])\\nCompute the precision.\\n\\nrecall_score(y_true, y_pred, *[, labels, ...])\\nCompute the recall.\\n\\n\\n\\n\\nNote that the precision_recall_curve function is restricted to the\\nbinary case. The average_precision_score function supports multiclass\\nand multilabel formats by computing each class score in a One-vs-the-rest (OvR)\\nfashion and averaging them or not depending of its average argument value.\\nThe PrecisionRecallDisplay.from_estimator and\\nPrecisionRecallDisplay.from_predictions functions will plot the\\nprecision-recall curve as follows.\\n\\n\\nExamples\\n\\nSee Custom refit strategy of a grid search with cross-validation\\nfor an example of precision_score and recall_score usage\\nto estimate parameters using grid search with nested cross-validation.\\nSee Precision-Recall\\nfor an example of precision_recall_curve usage to evaluate\\nclassifier output quality.\\n\\nReferences\\n\\n\\n[Manning2008]\\nC.D. Manning, P. Raghavan, H. Schütze, Introduction to Information Retrieval,\\n2008.\\n---------new doc---------\\nReferences\\n\\n\\n[Manning2008]\\nC.D. Manning, P. Raghavan, H. Schütze, Introduction to Information Retrieval,\\n2008.\\n\\n\\n[Everingham2010]\\nM. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman,\\nThe Pascal Visual Object Classes (VOC) Challenge,\\nIJCV 2010.\\n\\n\\n[Davis2006]\\nJ. Davis, M. Goadrich, The Relationship Between Precision-Recall and ROC Curves,\\nICML 2006.\\n\\n\\n[Flach2015]\\nP.A. Flach, M. Kull, Precision-Recall-Gain Curves: PR Analysis Done Right,\\nNIPS 2015.\\n\\n\\n\\n3.4.4.9.1. Binary classification#\\nIn a binary classification task, the terms ‘’positive’’ and ‘’negative’’ refer\\nto the classifier’s prediction, and the terms ‘’true’’ and ‘’false’’ refer to\\nwhether that prediction corresponds to the external judgment (sometimes known\\nas the ‘’observation’’). Given these definitions, we can formulate the\\nfollowing table:\\n\\n\\n\\nActual class (observation)\\n\\nPredicted class\\n(expectation)\\ntp (true positive)\\nCorrect result\\nfp (false positive)\\nUnexpected result\\n\\nfn (false negative)\\nMissing result\\ntn (true negative)\\nCorrect absence of result\\n\\n\\n\\n\\nIn this context, we can define the notions of precision and recall:\\n\\n\\\\[\\\\text{precision} = \\\\frac{\\\\text{tp}}{\\\\text{tp} + \\\\text{fp}},\\\\]\\n---------new doc---------\\nPredicted class\\n(expectation)\\ntp (true positive)\\nCorrect result\\nfp (false positive)\\nUnexpected result\\n\\nfn (false negative)\\nMissing result\\ntn (true negative)\\nCorrect absence of result\\n\\n\\n\\n\\nIn this context, we can define the notions of precision and recall:\\n\\n\\\\[\\\\text{precision} = \\\\frac{\\\\text{tp}}{\\\\text{tp} + \\\\text{fp}},\\\\]\\n\\n\\\\[\\\\text{recall} = \\\\frac{\\\\text{tp}}{\\\\text{tp} + \\\\text{fn}},\\\\]\\n(Sometimes recall is also called ‘’sensitivity’’)\\nF-measure is the weighted harmonic mean of precision and recall, with precision’s\\ncontribution to the mean weighted by some parameter \\\\(\\\\beta\\\\):\\n\\n\\\\[F_\\\\beta = (1 + \\\\beta^2) \\\\frac{\\\\text{precision} \\\\times \\\\text{recall}}{\\\\beta^2 \\\\text{precision} + \\\\text{recall}}\\\\]\\nTo avoid division by zero when precision and recall are zero, Scikit-Learn calculates F-measure with this\\notherwise-equivalent formula:\\n---------new doc---------\\n\\\\[F_\\\\beta = \\\\frac{(1 + \\\\beta^2) \\\\text{tp}}{(1 + \\\\beta^2) \\\\text{tp} + \\\\text{fp} + \\\\beta^2 \\\\text{fn}}\\\\]\\nNote that this formula is still undefined when there are no true positives, false\\npositives, or false negatives. By default, F-1 for a set of exclusively true negatives\\nis calculated as 0, however this behavior can be changed using the zero_division\\nparameter.\\nHere are some small examples in binary classification:\\n>>> from sklearn import metrics\\n>>> y_pred = [0, 1, 0, 0]\\n>>> y_true = [0, 1, 0, 1]\\n>>> metrics.precision_score(y_true, y_pred)\\n1.0\\n>>> metrics.recall_score(y_true, y_pred)\\n0.5\\n>>> metrics.f1_score(y_true, y_pred)\\n0.66...\\n>>> metrics.fbeta_score(y_true, y_pred, beta=0.5)\\n0.83...\\n>>> metrics.fbeta_score(y_true, y_pred, beta=1)\\n0.66...\\n>>> metrics.fbeta_score(y_true, y_pred, beta=2)\\n0.55...\\n>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)\\n(array([0.66..., 1.        ]), array([1. , 0.5]), array([0.71..., 0.83...]), array([2, 2]))\\n---------new doc---------\\n>>> import numpy as np\\n>>> from sklearn.metrics import precision_recall_curve\\n>>> from sklearn.metrics import average_precision_score\\n>>> y_true = np.array([0, 0, 1, 1])\\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\\n>>> precision, recall, threshold = precision_recall_curve(y_true, y_scores)\\n>>> precision\\narray([0.5       , 0.66..., 0.5       , 1.        , 1.        ])\\n>>> recall\\narray([1. , 1. , 0.5, 0.5, 0. ])\\n>>> threshold\\narray([0.1 , 0.35, 0.4 , 0.8 ])\\n>>> average_precision_score(y_true, y_scores)\\n0.83...\\n\\n\\n\\n\\n3.4.4.9.2. Multiclass and multilabel classification#\\nIn a multiclass and multilabel classification task, the notions of precision,\\nrecall, and F-measures can be applied to each label independently.\\nThere are a few ways to combine results across labels,\\nspecified by the average argument to the\\naverage_precision_score, f1_score,\\nfbeta_score, precision_recall_fscore_support,\\nprecision_score and recall_score functions, as described\\nabove.\\nNote the following behaviors when averaging:\\n---------new doc---------\\nIf all labels are included, “micro”-averaging in a multiclass setting will produce\\nprecision, recall and \\\\(F\\\\) that are all identical to accuracy.\\n“weighted” averaging may produce a F-score that is not between precision and recall.\\n“macro” averaging for F-measures is calculated as the arithmetic mean over\\nper-label/class F-measures, not the harmonic mean over the arithmetic precision and\\nrecall means. Both calculations can be seen in the literature but are not equivalent,\\nsee [OB2019] for details.\\n\\nTo make this more explicit, consider the following notation:\\n---------new doc---------\\nTo make this more explicit, consider the following notation:\\n\\n\\\\(y\\\\) the set of true \\\\((sample, label)\\\\) pairs\\n\\\\(\\\\hat{y}\\\\) the set of predicted \\\\((sample, label)\\\\) pairs\\n\\\\(L\\\\) the set of labels\\n\\\\(S\\\\) the set of samples\\n\\\\(y_s\\\\) the subset of \\\\(y\\\\) with sample \\\\(s\\\\),\\ni.e. \\\\(y_s := \\\\left\\\\{(s', l) \\\\in y | s' = s\\\\right\\\\}\\\\)\\n\\\\(y_l\\\\) the subset of \\\\(y\\\\) with label \\\\(l\\\\)\\nsimilarly, \\\\(\\\\hat{y}_s\\\\) and \\\\(\\\\hat{y}_l\\\\) are subsets of\\n\\\\(\\\\hat{y}\\\\)\\n\\\\(P(A, B) := \\\\frac{\\\\left| A \\\\cap B \\\\right|}{\\\\left|B\\\\right|}\\\\) for some\\nsets \\\\(A\\\\) and \\\\(B\\\\)\\n\\\\(R(A, B) := \\\\frac{\\\\left| A \\\\cap B \\\\right|}{\\\\left|A\\\\right|}\\\\)\\n(Conventions vary on handling \\\\(A = \\\\emptyset\\\\); this implementation uses\\n\\\\(R(A, B):=0\\\\), and similar for \\\\(P\\\\).)\\n\\\\(F_\\\\beta(A, B) := \\\\left(1 + \\\\beta^2\\\\right) \\\\frac{P(A, B) \\\\times R(A, B)}{\\\\beta^2 P(A, B) + R(A, B)}\\\\)\\n\\nThen the metrics are defined as:\\n\\n\\naverage\\nPrecision\\nRecall\\nF_beta\\n---------new doc---------\\nNone\\n\\\\(\\\\langle P(y_l, \\\\hat{y}_l) | l \\\\in L \\\\rangle\\\\)\\n\\\\(\\\\langle R(y_l, \\\\hat{y}_l) | l \\\\in L \\\\rangle\\\\)\\n\\\\(\\\\langle F_\\\\beta(y_l, \\\\hat{y}_l) | l \\\\in L \\\\rangle\\\\)\\n\\n\\n\\n\\n>>> from sklearn import metrics\\n>>> y_true = [0, 1, 2, 0, 1, 2]\\n>>> y_pred = [0, 2, 1, 0, 0, 1]\\n>>> metrics.precision_score(y_true, y_pred, average='macro')\\n0.22...\\n>>> metrics.recall_score(y_true, y_pred, average='micro')\\n0.33...\\n>>> metrics.f1_score(y_true, y_pred, average='weighted')\\n0.26...\\n>>> metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)\\n0.23...\\n>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)\\n(array([0.66..., 0.        , 0.        ]), array([1., 0., 0.]), array([0.71..., 0.        , 0.        ]), array([2, 2, 2]...))\\n---------new doc---------\\nFor multiclass classification with a “negative class”, it is possible to exclude some labels:\\n>>> metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')\\n... # excluding 0, no labels were correctly recalled\\n0.0\\n\\n\\nSimilarly, labels not present in the data sample may be accounted for in macro-averaging.\\n>>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')\\n0.166...\\n\\n\\nReferences\\n\\n\\n[OB2019]\\nOpitz, J., & Burst, S. (2019). “Macro f1 and macro f1.”\\n\\n\\n\\n\\n\\n3.4.4.10. Jaccard similarity coefficient score#\\nThe jaccard_score function computes the average of Jaccard similarity\\ncoefficients, also called the\\nJaccard index, between pairs of label sets.\\nThe Jaccard similarity coefficient with a ground truth label set \\\\(y\\\\) and\\npredicted label set \\\\(\\\\hat{y}\\\\), is defined as\\n---------new doc---------\\n3.4.4.10. Jaccard similarity coefficient score#\\nThe jaccard_score function computes the average of Jaccard similarity\\ncoefficients, also called the\\nJaccard index, between pairs of label sets.\\nThe Jaccard similarity coefficient with a ground truth label set \\\\(y\\\\) and\\npredicted label set \\\\(\\\\hat{y}\\\\), is defined as\\n\\n\\\\[J(y, \\\\hat{y}) = \\\\frac{|y \\\\cap \\\\hat{y}|}{|y \\\\cup \\\\hat{y}|}.\\\\]\\nThe jaccard_score (like precision_recall_fscore_support) applies\\nnatively to binary targets. By computing it set-wise it can be extended to apply\\nto multilabel and multiclass through the use of average (see\\nabove).\\nIn the binary case:\\n>>> import numpy as np\\n>>> from sklearn.metrics import jaccard_score\\n>>> y_true = np.array([[0, 1, 1],\\n...                    [1, 1, 0]])\\n>>> y_pred = np.array([[1, 1, 1],\\n...                    [1, 0, 0]])\\n>>> jaccard_score(y_true[0], y_pred[0])\\n0.6666...\\n\\n\\nIn the 2D comparison case (e.g. image similarity):\\n>>> jaccard_score(y_true, y_pred, average=\\\"micro\\\")\\n0.6\\n---------new doc---------\\nIn the 2D comparison case (e.g. image similarity):\\n>>> jaccard_score(y_true, y_pred, average=\\\"micro\\\")\\n0.6\\n\\n\\nIn the multilabel case with binary label indicators:\\n>>> jaccard_score(y_true, y_pred, average='samples')\\n0.5833...\\n>>> jaccard_score(y_true, y_pred, average='macro')\\n0.6666...\\n>>> jaccard_score(y_true, y_pred, average=None)\\narray([0.5, 0.5, 1. ])\\n\\n\\nMulticlass problems are binarized and treated like the corresponding\\nmultilabel problem:\\n>>> y_pred = [0, 2, 1, 2]\\n>>> y_true = [0, 1, 2, 2]\\n>>> jaccard_score(y_true, y_pred, average=None)\\narray([1. , 0. , 0.33...])\\n>>> jaccard_score(y_true, y_pred, average='macro')\\n0.44...\\n>>> jaccard_score(y_true, y_pred, average='micro')\\n0.33...\\n---------new doc---------\\nThe first [.9, .1] in y_pred denotes 90% probability that the first\\nsample has label 0.  The log loss is non-negative.\\n\\n\\n3.4.4.13. Matthews correlation coefficient#\\nThe matthews_corrcoef function computes the\\nMatthew’s correlation coefficient (MCC)\\nfor binary classes.  Quoting Wikipedia:\\n\\n“The Matthews correlation coefficient is used in machine learning as a\\nmeasure of the quality of binary (two-class) classifications. It takes\\ninto account true and false positives and negatives and is generally\\nregarded as a balanced measure which can be used even if the classes are\\nof very different sizes. The MCC is in essence a correlation coefficient\\nvalue between -1 and +1. A coefficient of +1 represents a perfect\\nprediction, 0 an average random prediction and -1 an inverse prediction.\\nThe statistic is also known as the phi coefficient.”\\n\\nIn the binary (two-class) case, \\\\(tp\\\\), \\\\(tn\\\\), \\\\(fp\\\\) and\\n\\\\(fn\\\\) are respectively the number of true positives, true negatives, false\\npositives and false negatives, the MCC is defined as\\n\\n\\\\[MCC = \\\\frac{tp \\\\times tn - fp \\\\times fn}{\\\\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.\\\\]\\nIn the multiclass case, the Matthews correlation coefficient can be defined in terms of a\\nconfusion_matrix \\\\(C\\\\) for \\\\(K\\\\) classes.  To simplify the\\ndefinition consider the following intermediate variables:\\n---------new doc---------\\n3.4.4.14. Multi-label confusion matrix#\\nThe multilabel_confusion_matrix function computes class-wise (default)\\nor sample-wise (samplewise=True) multilabel confusion matrix to evaluate\\nthe accuracy of a classification. multilabel_confusion_matrix also treats\\nmulticlass data as if it were multilabel, as this is a transformation commonly\\napplied to evaluate multiclass problems with binary classification metrics\\n(such as precision, recall, etc.).\\nWhen calculating class-wise multilabel confusion matrix \\\\(C\\\\), the\\ncount of true negatives for class \\\\(i\\\\) is \\\\(C_{i,0,0}\\\\), false\\nnegatives is \\\\(C_{i,1,0}\\\\), true positives is \\\\(C_{i,1,1}\\\\)\\nand false positives is \\\\(C_{i,0,1}\\\\).\\nHere is an example demonstrating the use of the\\nmultilabel_confusion_matrix function with\\nmultilabel indicator matrix input:\\n>>> import numpy as np\\n>>> from sklearn.metrics import multilabel_confusion_matrix\\n>>> y_true = np.array([[1, 0, 1],\\n...                    [0, 1, 0]])\\n>>> y_pred = np.array([[1, 0, 0],\\n...                    [0, 1, 1]])\\n>>> multilabel_confusion_matrix(y_true, y_pred)\\narray([[[1, 0],\\n        [0, 1]],\\n---------new doc---------\\n[[5, 0],\\n        [1, 0]],\\n\\n       [[2, 1],\\n        [1, 2]]])\\n\\n\\nHere are some examples demonstrating the use of the\\nmultilabel_confusion_matrix function to calculate recall\\n(or sensitivity), specificity, fall out and miss rate for each class in a\\nproblem with multilabel indicator matrix input.\\nCalculating\\nrecall\\n(also called the true positive rate or the sensitivity) for each class:\\n>>> y_true = np.array([[0, 0, 1],\\n...                    [0, 1, 0],\\n...                    [1, 1, 0]])\\n>>> y_pred = np.array([[0, 1, 0],\\n...                    [0, 0, 1],\\n...                    [1, 1, 0]])\\n>>> mcm = multilabel_confusion_matrix(y_true, y_pred)\\n>>> tn = mcm[:, 0, 0]\\n>>> tp = mcm[:, 1, 1]\\n>>> fn = mcm[:, 1, 0]\\n>>> fp = mcm[:, 0, 1]\\n>>> tp / (tp + fn)\\narray([1. , 0.5, 0. ])\\n---------new doc---------\\nCalculating\\nspecificity\\n(also called the true negative rate) for each class:\\n>>> tn / (tn + fp)\\narray([1. , 0. , 0.5])\\n\\n\\nCalculating fall out\\n(also called the false positive rate) for each class:\\n>>> fp / (fp + tn)\\narray([0. , 1. , 0.5])\\n\\n\\nCalculating miss rate\\n(also called the false negative rate) for each class:\\n>>> fn / (fn + tp)\\narray([0. , 0.5, 1. ])\\n\\n\\n\\n\\n3.4.4.15. Receiver operating characteristic (ROC)#\\nThe function roc_curve computes the\\nreceiver operating characteristic curve, or ROC curve.\\nQuoting Wikipedia :\\n\\n“A receiver operating characteristic (ROC), or simply ROC curve, is a\\ngraphical plot which illustrates the performance of a binary classifier\\nsystem as its discrimination threshold is varied. It is created by plotting\\nthe fraction of true positives out of the positives (TPR = true positive\\nrate) vs. the fraction of false positives out of the negatives (FPR = false\\npositive rate), at various threshold settings. TPR is also known as\\nsensitivity, and FPR is one minus the specificity or true negative rate.”\\n---------new doc---------\\nThis function requires the true binary value and the target scores, which can\\neither be probability estimates of the positive class, confidence values, or\\nbinary decisions. Here is a small example of how to use the roc_curve\\nfunction:\\n>>> import numpy as np\\n>>> from sklearn.metrics import roc_curve\\n>>> y = np.array([1, 1, 2, 2])\\n>>> scores = np.array([0.1, 0.4, 0.35, 0.8])\\n>>> fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\\n>>> fpr\\narray([0. , 0. , 0.5, 0.5, 1. ])\\n>>> tpr\\narray([0. , 0.5, 0.5, 1. , 1. ])\\n>>> thresholds\\narray([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\\n\\n\\nCompared to metrics such as the subset accuracy, the Hamming loss, or the\\nF1 score, ROC doesn’t require optimizing a threshold for each label.\\nThe roc_auc_score function, denoted by ROC-AUC or AUROC, computes the\\narea under the ROC curve. By doing so, the curve information is summarized in\\none number.\\nThe following figure shows the ROC curve and ROC-AUC score for a classifier\\naimed to distinguish the virginica flower from the rest of the species in the\\nIris plants dataset:\\n\\n\\nFor more information see the Wikipedia article on AUC.\\n---------new doc---------\\nFor more information see the Wikipedia article on AUC.\\n\\n3.4.4.15.1. Binary case#\\nIn the binary case, you can either provide the probability estimates, using\\nthe classifier.predict_proba() method, or the non-thresholded decision values\\ngiven by the classifier.decision_function() method. In the case of providing\\nthe probability estimates, the probability of the class with the\\n“greater label” should be provided. The “greater label” corresponds to\\nclassifier.classes_[1] and thus classifier.predict_proba(X)[:, 1].\\nTherefore, the y_score parameter is of size (n_samples,).\\n>>> from sklearn.datasets import load_breast_cancer\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.metrics import roc_auc_score\\n>>> X, y = load_breast_cancer(return_X_y=True)\\n>>> clf = LogisticRegression(solver=\\\"liblinear\\\").fit(X, y)\\n>>> clf.classes_\\narray([0, 1])\\n\\n\\nWe can use the probability estimates corresponding to clf.classes_[1].\\n>>> y_score = clf.predict_proba(X)[:, 1]\\n>>> roc_auc_score(y, y_score)\\n0.99...\\n\\n\\nOtherwise, we can use the non-thresholded decision values\\n>>> roc_auc_score(y, clf.decision_function(X))\\n0.99...\\n---------new doc---------\\nWe can use the probability estimates corresponding to clf.classes_[1].\\n>>> y_score = clf.predict_proba(X)[:, 1]\\n>>> roc_auc_score(y, y_score)\\n0.99...\\n\\n\\nOtherwise, we can use the non-thresholded decision values\\n>>> roc_auc_score(y, clf.decision_function(X))\\n0.99...\\n\\n\\n\\n\\n3.4.4.15.2. Multi-class case#\\nThe roc_auc_score function can also be used in multi-class\\nclassification. Two averaging strategies are currently supported: the\\none-vs-one algorithm computes the average of the pairwise ROC AUC scores, and\\nthe one-vs-rest algorithm computes the average of the ROC AUC scores for each\\nclass against all other classes. In both cases, the predicted labels are\\nprovided in an array with values from 0 to n_classes, and the scores\\ncorrespond to the probability estimates that a sample belongs to a particular\\nclass. The OvO and OvR algorithms support weighting uniformly\\n(average='macro') and by prevalence (average='weighted').\\n\\n\\nOne-vs-one Algorithm#\\nComputes the average AUC of all possible pairwise\\ncombinations of classes. [HT2001] defines a multiclass AUC metric weighted\\nuniformly:\\n---------new doc---------\\nOne-vs-rest Algorithm#\\nComputes the AUC of each class against the rest\\n[PD2000]. The algorithm is functionally the same as the multilabel case. To\\nenable this algorithm set the keyword argument multiclass to 'ovr'.\\nAdditionally to 'macro' [F2006] and 'weighted' [F2001] averaging, OvR\\nsupports 'micro' averaging.\\nIn applications where a high false positive rate is not tolerable the parameter\\nmax_fpr of roc_auc_score can be used to summarize the ROC curve up\\nto the given limit.\\nThe following figure shows the micro-averaged ROC curve and its corresponding\\nROC-AUC score for a classifier aimed to distinguish the different species in\\nthe Iris plants dataset:\\n---------new doc---------\\n3.4.4.15.3. Multi-label case#\\nIn multi-label classification, the roc_auc_score function is\\nextended by averaging over the labels as above. In this case,\\nyou should provide a y_score of shape (n_samples, n_classes). Thus, when\\nusing the probability estimates, one needs to select the probability of the\\nclass with the greater label for each output.\\n>>> from sklearn.datasets import make_multilabel_classification\\n>>> from sklearn.multioutput import MultiOutputClassifier\\n>>> X, y = make_multilabel_classification(random_state=0)\\n>>> inner_clf = LogisticRegression(solver=\\\"liblinear\\\", random_state=0)\\n>>> clf = MultiOutputClassifier(inner_clf).fit(X, y)\\n>>> y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\\n>>> roc_auc_score(y, y_score, average=None)\\narray([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\\n\\n\\nAnd the decision values do not require such processing.\\n>>> from sklearn.linear_model import RidgeClassifierCV\\n>>> clf = RidgeClassifierCV().fit(X, y)\\n>>> y_score = clf.decision_function(X)\\n>>> roc_auc_score(y, y_score, average=None)\\narray([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\\n\\n\\nExamples\\n---------new doc---------\\n[F2006]\\nFawcett, T., 2006. An introduction to ROC analysis.\\nPattern Recognition Letters, 27(8), pp. 861-874.\\n\\n\\n[F2001]\\nFawcett, T., 2001. Using rule sets to maximize\\nROC performance\\nIn Data Mining, 2001.\\nProceedings IEEE International Conference, pp. 131-138.\\n\\n\\n\\n\\n\\n3.4.4.16. Detection error tradeoff (DET)#\\nThe function det_curve computes the\\ndetection error tradeoff curve (DET) curve [WikipediaDET2017].\\nQuoting Wikipedia:\\n\\n“A detection error tradeoff (DET) graph is a graphical plot of error rates\\nfor binary classification systems, plotting false reject rate vs. false\\naccept rate. The x- and y-axes are scaled non-linearly by their standard\\nnormal deviates (or just by logarithmic transformation), yielding tradeoff\\ncurves that are more linear than ROC curves, and use most of the image area\\nto highlight the differences of importance in the critical operating region.”\\n\\nDET curves are a variation of receiver operating characteristic (ROC) curves\\nwhere False Negative Rate is plotted on the y-axis instead of True Positive\\nRate.\\nDET curves are commonly plotted in normal deviate scale by transformation with\\n\\\\(\\\\phi^{-1}\\\\) (with \\\\(\\\\phi\\\\) being the cumulative distribution\\nfunction).\\nThe resulting performance curves explicitly visualize the tradeoff of error\\ntypes for given classification algorithms.\\nSee [Martin1997] for examples and further motivation.\\nThis figure compares the ROC and DET curves of two example classifiers on the\\nsame classification task:\\n\\n\\n\\n\\nProperties#\\n---------new doc---------\\nProperties#\\n\\nDET curves form a linear curve in normal deviate scale if the detection\\nscores are normally (or close-to normally) distributed.\\nIt was shown by [Navratil2007] that the reverse is not necessarily true and\\neven more general distributions are able to produce linear DET curves.\\nThe normal deviate scale transformation spreads out the points such that a\\ncomparatively larger space of plot is occupied.\\nTherefore curves with similar classification performance might be easier to\\ndistinguish on a DET plot.\\nWith False Negative Rate being “inverse” to True Positive Rate the point\\nof perfection for DET curves is the origin (in contrast to the top left\\ncorner for ROC curves).\\n\\n\\n\\n\\nApplications and limitations#\\nDET curves are intuitive to read and hence allow quick visual assessment of a\\nclassifier’s performance.\\nAdditionally DET curves can be consulted for threshold analysis and operating\\npoint selection.\\nThis is particularly helpful if a comparison of error types is required.\\nOn the other hand DET curves do not provide their metric as a single number.\\nTherefore for either automated evaluation or comparison to other\\nclassification tasks metrics like the derived area under ROC curve might be\\nbetter suited.\\n\\nExamples\\n\\nSee Detection error tradeoff (DET) curve\\nfor an example comparison between receiver operating characteristic (ROC)\\ncurves and Detection error tradeoff (DET) curves.\\n\\nReferences\\n\\n\\n[WikipediaDET2017]\\nWikipedia contributors. Detection error tradeoff.\\nWikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.\\nAvailable at: https://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054.\\nAccessed February 19, 2018.\\n---------new doc---------\\nReferences\\n\\n\\n[WikipediaDET2017]\\nWikipedia contributors. Detection error tradeoff.\\nWikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.\\nAvailable at: https://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054.\\nAccessed February 19, 2018.\\n\\n\\n[Martin1997]\\nA. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki,\\nThe DET Curve in Assessment of Detection Task Performance, NIST 1997.\\n\\n\\n[Navratil2007]\\nJ. Navractil and D. Klusacek,\\n“On Linear DETs”,\\n2007 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing - ICASSP ‘07, Honolulu,\\nHI, 2007, pp. IV-229-IV-232.\\n---------new doc---------\\n[Navratil2007]\\nJ. Navractil and D. Klusacek,\\n“On Linear DETs”,\\n2007 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing - ICASSP ‘07, Honolulu,\\nHI, 2007, pp. IV-229-IV-232.\\n\\n\\n\\n\\n3.4.4.17. Zero one loss#\\nThe zero_one_loss function computes the sum or the average of the 0-1\\nclassification loss (\\\\(L_{0-1}\\\\)) over \\\\(n_{\\\\text{samples}}\\\\). By\\ndefault, the function normalizes over the sample. To get the sum of the\\n\\\\(L_{0-1}\\\\), set normalize to False.\\nIn multilabel classification, the zero_one_loss scores a subset as\\none if its labels strictly match the predictions, and as a zero if there\\nare any errors.  By default, the function returns the percentage of imperfectly\\npredicted subsets.  To get the count of such subsets instead, set\\nnormalize to False\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of\\nthe \\\\(i\\\\)-th sample and \\\\(y_i\\\\) is the corresponding true value,\\nthen the 0-1 loss \\\\(L_{0-1}\\\\) is defined as:\\n---------new doc---------\\n\\\\[L_{0-1}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} 1(\\\\hat{y}_i \\\\not= y_i)\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function. The zero one\\nloss can also be computed as \\\\(zero-one loss = 1 - accuracy\\\\).\\n>>> from sklearn.metrics import zero_one_loss\\n>>> y_pred = [1, 2, 3, 4]\\n>>> y_true = [2, 2, 3, 4]\\n>>> zero_one_loss(y_true, y_pred)\\n0.25\\n>>> zero_one_loss(y_true, y_pred, normalize=False)\\n1.0\\n\\n\\nIn the multilabel case with binary label indicators, where the first label\\nset [0,1] has an error:\\n>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\\n0.5\\n\\n>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)),  normalize=False)\\n1.0\\n\\n\\nExamples\\n\\nSee Recursive feature elimination with cross-validation\\nfor an example of zero one loss usage to perform recursive feature\\nelimination with cross-validation.\\n\\n\\n\\n3.4.4.18. Brier score loss#\\nThe brier_score_loss function computes the\\nBrier score\\nfor binary classes [Brier1950]. Quoting Wikipedia:\\n---------new doc---------\\nExamples\\n\\nSee Recursive feature elimination with cross-validation\\nfor an example of zero one loss usage to perform recursive feature\\nelimination with cross-validation.\\n\\n\\n\\n3.4.4.18. Brier score loss#\\nThe brier_score_loss function computes the\\nBrier score\\nfor binary classes [Brier1950]. Quoting Wikipedia:\\n\\n“The Brier score is a proper score function that measures the accuracy of\\nprobabilistic predictions. It is applicable to tasks in which predictions\\nmust assign probabilities to a set of mutually exclusive discrete outcomes.”\\n\\nThis function returns the mean squared error of the actual outcome\\n\\\\(y \\\\in \\\\{0,1\\\\}\\\\) and the predicted probability estimate\\n\\\\(p = \\\\operatorname{Pr}(y = 1)\\\\) (predict_proba) as outputted by:\\n---------new doc---------\\nThis function returns the mean squared error of the actual outcome\\n\\\\(y \\\\in \\\\{0,1\\\\}\\\\) and the predicted probability estimate\\n\\\\(p = \\\\operatorname{Pr}(y = 1)\\\\) (predict_proba) as outputted by:\\n\\n\\\\[BS = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}} - 1}(y_i - p_i)^2\\\\]\\nThe Brier score loss is also between 0 to 1 and the lower the value (the mean\\nsquare difference is smaller), the more accurate the prediction is.\\nHere is a small example of usage of this function:\\n>>> import numpy as np\\n>>> from sklearn.metrics import brier_score_loss\\n>>> y_true = np.array([0, 1, 1, 0])\\n>>> y_true_categorical = np.array([\\\"spam\\\", \\\"ham\\\", \\\"ham\\\", \\\"spam\\\"])\\n>>> y_prob = np.array([0.1, 0.9, 0.8, 0.4])\\n>>> y_pred = np.array([0, 1, 1, 0])\\n>>> brier_score_loss(y_true, y_prob)\\n0.055\\n>>> brier_score_loss(y_true, 1 - y_prob, pos_label=0)\\n0.055\\n>>> brier_score_loss(y_true_categorical, y_prob, pos_label=\\\"ham\\\")\\n0.055\\n>>> brier_score_loss(y_true, y_prob > 0.5)\\n0.0\\n---------new doc---------\\nThe Brier score can be used to assess how well a classifier is calibrated.\\nHowever, a lower Brier score loss does not always mean a better calibration.\\nThis is because, by analogy with the bias-variance decomposition of the mean\\nsquared error, the Brier score loss can be decomposed as the sum of calibration\\nloss and refinement loss [Bella2012]. Calibration loss is defined as the mean\\nsquared deviation from empirical probabilities derived from the slope of ROC\\nsegments. Refinement loss can be defined as the expected optimal loss as\\nmeasured by the area under the optimal cost curve. Refinement loss can change\\nindependently from calibration loss, thus a lower Brier score loss does not\\nnecessarily mean a better calibrated model. “Only when refinement loss remains\\nthe same does a lower Brier score loss always mean better calibration”\\n[Bella2012], [Flach2008].\\nExamples\\n\\nSee Probability calibration of classifiers\\nfor an example of Brier score loss usage to perform probability\\ncalibration of classifiers.\\n\\nReferences\\n\\n\\n[Brier1950]\\nG. Brier, Verification of forecasts expressed in terms of probability,\\nMonthly weather review 78.1 (1950)\\n\\n\\n[Bella2012]\\n(1,2)\\nBella, Ferri, Hernández-Orallo, and Ramírez-Quintana\\n“Calibration of Machine Learning Models”\\nin Khosrow-Pour, M. “Machine learning: concepts, methodologies, tools\\nand applications.” Hershey, PA: Information Science Reference (2012).\\n---------new doc---------\\n[Bella2012]\\n(1,2)\\nBella, Ferri, Hernández-Orallo, and Ramírez-Quintana\\n“Calibration of Machine Learning Models”\\nin Khosrow-Pour, M. “Machine learning: concepts, methodologies, tools\\nand applications.” Hershey, PA: Information Science Reference (2012).\\n\\n\\n[Flach2008]\\nFlach, Peter, and Edson Matsubara. “On classification, ranking,\\nand probability estimation.”\\nDagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum fr Informatik (2008).\\n\\n\\n\\n\\n3.4.4.19. Class likelihood ratios#\\nThe class_likelihood_ratios function computes the positive and negative\\nlikelihood ratios\\n\\\\(LR_\\\\pm\\\\) for binary classes, which can be interpreted as the ratio of\\npost-test to pre-test odds as explained below. As a consequence, this metric is\\ninvariant w.r.t. the class prevalence (the number of samples in the positive\\nclass divided by the total number of samples) and can be extrapolated between\\npopulations regardless of any possible class imbalance.\\nThe \\\\(LR_\\\\pm\\\\) metrics are therefore very useful in settings where the data\\navailable to learn and evaluate a classifier is a study population with nearly\\nbalanced classes, such as a case-control study, while the target application,\\ni.e. the general population, has very low prevalence.\\nThe positive likelihood ratio \\\\(LR_+\\\\) is the probability of a classifier to\\ncorrectly predict that a sample belongs to the positive class divided by the\\nprobability of predicting the positive class for a sample belonging to the\\nnegative class:\\n---------new doc---------\\n\\\\[LR_+ = \\\\frac{\\\\text{PR}(P+|T+)}{\\\\text{PR}(P+|T-)}.\\\\]\\nThe notation here refers to predicted (\\\\(P\\\\)) or true (\\\\(T\\\\)) label and\\nthe sign \\\\(+\\\\) and \\\\(-\\\\) refer to the positive and negative class,\\nrespectively, e.g. \\\\(P+\\\\) stands for “predicted positive”.\\nAnalogously, the negative likelihood ratio \\\\(LR_-\\\\) is the probability of a\\nsample of the positive class being classified as belonging to the negative class\\ndivided by the probability of a sample of the negative class being correctly\\nclassified:\\n\\n\\\\[LR_- = \\\\frac{\\\\text{PR}(P-|T+)}{\\\\text{PR}(P-|T-)}.\\\\]\\nFor classifiers above chance \\\\(LR_+\\\\) above 1 higher is better, while\\n\\\\(LR_-\\\\) ranges from 0 to 1 and lower is better.\\nValues of \\\\(LR_\\\\pm\\\\approx 1\\\\) correspond to chance level.\\nNotice that probabilities differ from counts, for instance\\n\\\\(\\\\operatorname{PR}(P+|T+)\\\\) is not equal to the number of true positive\\ncounts tp (see the wikipedia page for\\nthe actual formulas).\\nExamples\\n\\nClass Likelihood Ratios to measure classification performance\\n\\n\\n\\nInterpretation across varying prevalence#\\nBoth class likelihood ratios are interpretable in terms of an odds ratio\\n(pre-test and post-tests):\\n\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times \\\\text{pre-test odds}.\\\\]\\nOdds are in general related to probabilities via\\n---------new doc---------\\nClass Likelihood Ratios to measure classification performance\\n\\n\\n\\nInterpretation across varying prevalence#\\nBoth class likelihood ratios are interpretable in terms of an odds ratio\\n(pre-test and post-tests):\\n\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times \\\\text{pre-test odds}.\\\\]\\nOdds are in general related to probabilities via\\n\\n\\\\[\\\\text{odds} = \\\\frac{\\\\text{probability}}{1 - \\\\text{probability}},\\\\]\\nor equivalently\\n\\n\\\\[\\\\text{probability} = \\\\frac{\\\\text{odds}}{1 + \\\\text{odds}}.\\\\]\\nOn a given population, the pre-test probability is given by the prevalence. By\\nconverting odds to probabilities, the likelihood ratios can be translated into a\\nprobability of truly belonging to either class before and after a classifier\\nprediction:\\n\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times\\n\\\\frac{\\\\text{pre-test probability}}{1 - \\\\text{pre-test probability}},\\\\]\\n\\n\\\\[\\\\text{post-test probability} = \\\\frac{\\\\text{post-test odds}}{1 + \\\\text{post-test odds}}.\\\\]\\n---------new doc---------\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times\\n\\\\frac{\\\\text{pre-test probability}}{1 - \\\\text{pre-test probability}},\\\\]\\n\\n\\\\[\\\\text{post-test probability} = \\\\frac{\\\\text{post-test odds}}{1 + \\\\text{post-test odds}}.\\\\]\\n\\n\\n\\nMathematical divergences#\\nThe positive likelihood ratio is undefined when \\\\(fp = 0\\\\), which can be\\ninterpreted as the classifier perfectly identifying positive cases. If \\\\(fp\\n= 0\\\\) and additionally \\\\(tp = 0\\\\), this leads to a zero/zero division. This\\nhappens, for instance, when using a DummyClassifier that always predicts the\\nnegative class and therefore the interpretation as a perfect classifier is lost.\\nThe negative likelihood ratio is undefined when \\\\(tn = 0\\\\). Such divergence\\nis invalid, as \\\\(LR_- > 1\\\\) would indicate an increase in the odds of a\\nsample belonging to the positive class after being classified as negative, as if\\nthe act of classifying caused the positive condition. This includes the case of\\na DummyClassifier that always predicts the positive class (i.e. when\\n\\\\(tn=fn=0\\\\)).\\nBoth class likelihood ratios are undefined when \\\\(tp=fn=0\\\\), which means\\nthat no samples of the positive class were present in the testing set. This can\\nalso happen when cross-validating highly imbalanced data.\\nIn all the previous cases the class_likelihood_ratios function raises by\\ndefault an appropriate warning message and returns nan to avoid pollution when\\naveraging over cross-validation folds.\\nFor a worked-out demonstration of the class_likelihood_ratios function,\\nsee the example below.\\n\\n\\n\\nReferences#\\n---------new doc---------\\nReferences#\\n\\nWikipedia entry for Likelihood ratios in diagnostic testing\\nBrenner, H., & Gefeller, O. (1997).\\nVariation of sensitivity, specificity, likelihood ratios and predictive\\nvalues with disease prevalence.\\nStatistics in medicine, 16(9), 981-991.\\n\\n\\n\\n\\n3.4.4.20. D² score for classification#\\nThe D² score computes the fraction of deviance explained.\\nIt is a generalization of R², where the squared error is generalized and replaced\\nby a classification deviance of choice \\\\(\\\\text{dev}(y, \\\\hat{y})\\\\)\\n(e.g., Log loss). D² is a form of a skill score.\\nIt is calculated as\\n\\n\\\\[D^2(y, \\\\hat{y}) = 1 - \\\\frac{\\\\text{dev}(y, \\\\hat{y})}{\\\\text{dev}(y, y_{\\\\text{null}})} \\\\,.\\\\]\\nWhere \\\\(y_{\\\\text{null}}\\\\) is the optimal prediction of an intercept-only model\\n(e.g., the per-class proportion of y_true in the case of the Log loss).\\nLike R², the best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). A constant model that always predicts\\n\\\\(y_{\\\\text{null}}\\\\), disregarding the input features, would get a D² score\\nof 0.0.\\n\\n\\nD2 log loss score#\\nThe d2_log_loss_score function implements the special case\\nof D² with the log loss, see Log loss, i.e.:\\n---------new doc---------\\n\\\\[\\\\text{dev}(y, \\\\hat{y}) = \\\\text{log_loss}(y, \\\\hat{y}).\\\\]\\nHere are some usage examples of the d2_log_loss_score function:\\n>>> from sklearn.metrics import d2_log_loss_score\\n>>> y_true = [1, 1, 2, 3]\\n>>> y_pred = [\\n...    [0.5, 0.25, 0.25],\\n...    [0.5, 0.25, 0.25],\\n...    [0.5, 0.25, 0.25],\\n...    [0.5, 0.25, 0.25],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n0.0\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [\\n...     [0.98, 0.01, 0.01],\\n...     [0.01, 0.98, 0.01],\\n...     [0.01, 0.01, 0.98],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n0.981...\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [\\n...     [0.1, 0.6, 0.3],\\n...     [0.1, 0.6, 0.3],\\n...     [0.4, 0.5, 0.1],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n---------new doc---------\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [\\n...     [0.1, 0.6, 0.3],\\n...     [0.1, 0.6, 0.3],\\n...     [0.4, 0.5, 0.1],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n-0.552...\\n---------new doc---------\\n3.4.5. Multilabel ranking metrics#\\nIn multilabel learning, each sample can have any number of ground truth labels\\nassociated with it. The goal is to give high scores and better rank to\\nthe ground truth labels.\\n\\n3.4.5.1. Coverage error#\\nThe coverage_error function computes the average number of labels that\\nhave to be included in the final prediction such that all true labels\\nare predicted. This is useful if you want to know how many top-scored-labels\\nyou have to predict in average without missing any true one. The best value\\nof this metrics is thus the average number of true labels.\\n\\nNote\\nOur implementation’s score is 1 greater than the one given in Tsoumakas\\net al., 2010. This extends it to handle the degenerate case in which an\\ninstance has 0 true labels.\\n\\nFormally, given a binary indicator matrix of the ground truth labels\\n\\\\(y \\\\in \\\\left\\\\{0, 1\\\\right\\\\}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\) and the\\nscore associated with each label\\n\\\\(\\\\hat{f} \\\\in \\\\mathbb{R}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\),\\nthe coverage is defined as\\n---------new doc---------\\n\\\\[coverage(y, \\\\hat{f}) = \\\\frac{1}{n_{\\\\text{samples}}}\\n  \\\\sum_{i=0}^{n_{\\\\text{samples}} - 1} \\\\max_{j:y_{ij} = 1} \\\\text{rank}_{ij}\\\\]\\nwith \\\\(\\\\text{rank}_{ij} = \\\\left|\\\\left\\\\{k: \\\\hat{f}_{ik} \\\\geq \\\\hat{f}_{ij} \\\\right\\\\}\\\\right|\\\\).\\nGiven the rank definition, ties in y_scores are broken by giving the\\nmaximal rank that would have been assigned to all tied values.\\nHere is a small example of usage of this function:\\n>>> import numpy as np\\n>>> from sklearn.metrics import coverage_error\\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\\n>>> coverage_error(y_true, y_score)\\n2.5\\n---------new doc---------\\n3.4.5.2. Label ranking average precision#\\nThe label_ranking_average_precision_score function\\nimplements label ranking average precision (LRAP). This metric is linked to\\nthe average_precision_score function, but is based on the notion of\\nlabel ranking instead of precision and recall.\\nLabel ranking average precision (LRAP) averages over the samples the answer to\\nthe following question: for each ground truth label, what fraction of\\nhigher-ranked labels were true labels? This performance measure will be higher\\nif you are able to give better rank to the labels associated with each sample.\\nThe obtained score is always strictly greater than 0, and the best value is 1.\\nIf there is exactly one relevant label per sample, label ranking average\\nprecision is equivalent to the mean\\nreciprocal rank.\\nFormally, given a binary indicator matrix of the ground truth labels\\n\\\\(y \\\\in \\\\left\\\\{0, 1\\\\right\\\\}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\)\\nand the score associated with each label\\n\\\\(\\\\hat{f} \\\\in \\\\mathbb{R}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\),\\nthe average precision is defined as\\n---------new doc---------\\n3.4.5.3. Ranking loss#\\nThe label_ranking_loss function computes the ranking loss which\\naverages over the samples the number of label pairs that are incorrectly\\nordered, i.e. true labels have a lower score than false labels, weighted by\\nthe inverse of the number of ordered pairs of false and true labels.\\nThe lowest achievable ranking loss is zero.\\nFormally, given a binary indicator matrix of the ground truth labels\\n\\\\(y \\\\in \\\\left\\\\{0, 1\\\\right\\\\}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\) and the\\nscore associated with each label\\n\\\\(\\\\hat{f} \\\\in \\\\mathbb{R}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\),\\nthe ranking loss is defined as\\n---------new doc---------\\nCompared with the ranking loss, NDCG can take into account relevance scores,\\nrather than a ground-truth ranking. So if the ground-truth consists only of an\\nordering, the ranking loss should be preferred; if the ground-truth consists of\\nactual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very\\nrelevant), NDCG can be used.\\nFor one sample, given the vector of continuous ground-truth values for each\\ntarget \\\\(y \\\\in \\\\mathbb{R}^{M}\\\\), where \\\\(M\\\\) is the number of outputs, and\\nthe prediction \\\\(\\\\hat{y}\\\\), which induces the ranking function \\\\(f\\\\), the\\nDCG score is\\n---------new doc---------\\n3.4.6. Regression metrics#\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions to measure regression performance. Some of those have been enhanced\\nto handle the multioutput case: mean_squared_error,\\nmean_absolute_error, r2_score,\\nexplained_variance_score, mean_pinball_loss, d2_pinball_score\\nand d2_absolute_error_score.\\nThese functions have a multioutput keyword argument which specifies the\\nway the scores or losses for each individual target should be averaged. The\\ndefault is 'uniform_average', which specifies a uniformly weighted mean\\nover outputs. If an ndarray of shape (n_outputs,) is passed, then its\\nentries are interpreted as weights and an according weighted average is\\nreturned. If multioutput is 'raw_values', then all unaltered\\nindividual scores or losses will be returned in an array of shape\\n(n_outputs,).\\nThe r2_score and explained_variance_score accept an additional\\nvalue 'variance_weighted' for the multioutput parameter. This option\\nleads to a weighting of each individual score by the variance of the\\ncorresponding target variable. This setting quantifies the globally captured\\nunscaled variance. If the target variables are of different scale, then this\\nscore puts more importance on explaining the higher variance variables.\\n---------new doc---------\\n3.4.6.1. R² score, the coefficient of determination#\\nThe r2_score function computes the coefficient of\\ndetermination,\\nusually denoted as \\\\(R^2\\\\).\\nIt represents the proportion of variance (of y) that has been explained by the\\nindependent variables in the model. It provides an indication of goodness of\\nfit and therefore a measure of how well unseen samples are likely to be\\npredicted by the model, through the proportion of explained variance.\\nAs such variance is dataset dependent, \\\\(R^2\\\\) may not be meaningfully comparable\\nacross different datasets. Best possible score is 1.0 and it can be negative\\n(because the model can be arbitrarily worse). A constant model that always\\npredicts the expected (average) value of y, disregarding the input features,\\nwould get an \\\\(R^2\\\\) score of 0.0.\\nNote: when the prediction residuals have zero mean, the \\\\(R^2\\\\) score and\\nthe Explained variance score are identical.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample\\nand \\\\(y_i\\\\) is the corresponding true value for total \\\\(n\\\\) samples,\\nthe estimated \\\\(R^2\\\\) is defined as:\\n---------new doc---------\\n\\\\[R^2(y, \\\\hat{y}) = 1 - \\\\frac{\\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2}{\\\\sum_{i=1}^{n} (y_i - \\\\bar{y})^2}\\\\]\\nwhere \\\\(\\\\bar{y} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} y_i\\\\) and \\\\(\\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2 = \\\\sum_{i=1}^{n} \\\\epsilon_i^2\\\\).\\nNote that r2_score calculates unadjusted \\\\(R^2\\\\) without correcting for\\nbias in sample variance of y.\\nIn the particular case where the true target is constant, the \\\\(R^2\\\\) score is\\nnot finite: it is either NaN (perfect predictions) or -Inf (imperfect\\npredictions). Such non-finite scores may prevent correct model optimization\\nsuch as grid-search cross-validation to be performed correctly. For this reason\\nthe default behaviour of r2_score is to replace them with 1.0 (perfect\\npredictions) or 0.0 (imperfect predictions). If force_finite\\nis set to False, this score falls back on the original \\\\(R^2\\\\) definition.\\nHere is a small example of usage of the r2_score function:\\n>>> from sklearn.metrics import r2_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> r2_score(y_true, y_pred)\\n0.948...\\n---------new doc---------\\nHere is a small example of usage of the r2_score function:\\n>>> from sklearn.metrics import r2_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> r2_score(y_true, y_pred)\\n0.948...\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> r2_score(y_true, y_pred, multioutput='variance_weighted')\\n0.938...\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> r2_score(y_true, y_pred, multioutput='uniform_average')\\n0.936...\\n>>> r2_score(y_true, y_pred, multioutput='raw_values')\\narray([0.965..., 0.908...])\\n>>> r2_score(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.925...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n>>> r2_score(y_true, y_pred)\\n1.0\\n>>> r2_score(y_true, y_pred, force_finite=False)\\nnan\\n>>> y_true = [-2, -2, -2]\\n---------new doc---------\\n0.925...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n>>> r2_score(y_true, y_pred)\\n1.0\\n>>> r2_score(y_true, y_pred, force_finite=False)\\nnan\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2 + 1e-8]\\n>>> r2_score(y_true, y_pred)\\n0.0\\n>>> r2_score(y_true, y_pred, force_finite=False)\\n-inf\\n---------new doc---------\\n\\\\[\\\\text{MAE}(y, \\\\hat{y}) = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1} \\\\left| y_i - \\\\hat{y}_i \\\\right|.\\\\]\\nHere is a small example of usage of the mean_absolute_error function:\\n>>> from sklearn.metrics import mean_absolute_error\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> mean_absolute_error(y_true, y_pred)\\n0.5\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> mean_absolute_error(y_true, y_pred)\\n0.75\\n>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\\narray([0.5, 1. ])\\n>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.85...\\n---------new doc---------\\n3.4.6.3. Mean squared error#\\nThe mean_squared_error function computes mean squared\\nerror, a risk\\nmetric corresponding to the expected value of the squared (quadratic) error or\\nloss.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample,\\nand \\\\(y_i\\\\) is the corresponding true value, then the mean squared error\\n(MSE) estimated over \\\\(n_{\\\\text{samples}}\\\\) is defined as\\n\\n\\\\[\\\\text{MSE}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples} - 1} (y_i - \\\\hat{y}_i)^2.\\\\]\\nHere is a small example of usage of the mean_squared_error\\nfunction:\\n>>> from sklearn.metrics import mean_squared_error\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> mean_squared_error(y_true, y_pred)\\n0.375\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> mean_squared_error(y_true, y_pred)\\n0.7083...\\n\\n\\nExamples\\n\\nSee Gradient Boosting regression\\nfor an example of mean squared error usage to evaluate gradient boosting regression.\\n---------new doc---------\\nThe root mean squared logarithmic error (RMSLE) is available through the\\nroot_mean_squared_log_error function.\\n\\n\\n3.4.6.5. Mean absolute percentage error#\\nThe mean_absolute_percentage_error (MAPE), also known as mean absolute\\npercentage deviation (MAPD), is an evaluation metric for regression problems.\\nThe idea of this metric is to be sensitive to relative errors. It is for example\\nnot changed by a global scaling of the target variable.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample\\nand \\\\(y_i\\\\) is the corresponding true value, then the mean absolute percentage\\nerror (MAPE) estimated over \\\\(n_{\\\\text{samples}}\\\\) is defined as\\n---------new doc---------\\n\\\\[\\\\text{MAPE}(y, \\\\hat{y}) = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1} \\\\frac{{}\\\\left| y_i - \\\\hat{y}_i \\\\right|}{\\\\max(\\\\epsilon, \\\\left| y_i \\\\right|)}\\\\]\\nwhere \\\\(\\\\epsilon\\\\) is an arbitrary small yet strictly positive number to\\navoid undefined results when y is zero.\\nThe mean_absolute_percentage_error function supports multioutput.\\nHere is a small example of usage of the mean_absolute_percentage_error\\nfunction:\\n>>> from sklearn.metrics import mean_absolute_percentage_error\\n>>> y_true = [1, 10, 1e6]\\n>>> y_pred = [0.9, 15, 1.2e6]\\n>>> mean_absolute_percentage_error(y_true, y_pred)\\n0.2666...\\n\\n\\nIn above example, if we had used mean_absolute_error, it would have ignored\\nthe small magnitude values and only reflected the error in prediction of highest\\nmagnitude value. But that problem is resolved in case of MAPE because it calculates\\nrelative percentage error with respect to actual output.\\n---------new doc---------\\nIn above example, if we had used mean_absolute_error, it would have ignored\\nthe small magnitude values and only reflected the error in prediction of highest\\nmagnitude value. But that problem is resolved in case of MAPE because it calculates\\nrelative percentage error with respect to actual output.\\n\\nNote\\nThe MAPE formula here does not represent the common “percentage” definition: the\\npercentage in the range [0, 100] is converted to a relative value in the range [0,\\n1] by dividing by 100. Thus, an error of 200% corresponds to a relative error of 2.\\nThe motivation here is to have a range of values that is more consistent with other\\nerror metrics in scikit-learn, such as accuracy_score.\\nTo obtain the mean absolute percentage error as per the Wikipedia formula,\\nmultiply the mean_absolute_percentage_error computed here by 100.\\n\\n\\n\\nReferences#\\n\\nWikipedia entry for Mean Absolute Percentage Error\\n\\n\\n\\n\\n3.4.6.6. Median absolute error#\\nThe median_absolute_error is particularly interesting because it is\\nrobust to outliers. The loss is calculated by taking the median of all absolute\\ndifferences between the target and the prediction.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample\\nand \\\\(y_i\\\\) is the corresponding true value, then the median absolute error\\n(MedAE) estimated over \\\\(n_{\\\\text{samples}}\\\\) is defined as\\n---------new doc---------\\n\\\\[\\\\text{MedAE}(y, \\\\hat{y}) = \\\\text{median}(\\\\mid y_1 - \\\\hat{y}_1 \\\\mid, \\\\ldots, \\\\mid y_n - \\\\hat{y}_n \\\\mid).\\\\]\\nThe median_absolute_error does not support multioutput.\\nHere is a small example of usage of the median_absolute_error\\nfunction:\\n>>> from sklearn.metrics import median_absolute_error\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> median_absolute_error(y_true, y_pred)\\n0.5\\n\\n\\n\\n\\n3.4.6.7. Max error#\\nThe max_error function computes the maximum residual error , a metric\\nthat captures the worst case error between the predicted value and\\nthe true value. In a perfectly fitted single output regression\\nmodel, max_error would be 0 on the training set and though this\\nwould be highly unlikely in the real world, this metric shows the\\nextent of error that the model had when it was fitted.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample,\\nand \\\\(y_i\\\\) is the corresponding true value, then the max error is\\ndefined as\\n---------new doc---------\\n\\\\[\\\\text{Max Error}(y, \\\\hat{y}) = \\\\max(| y_i - \\\\hat{y}_i |)\\\\]\\nHere is a small example of usage of the max_error function:\\n>>> from sklearn.metrics import max_error\\n>>> y_true = [3, 2, 7, 1]\\n>>> y_pred = [9, 2, 7, 1]\\n>>> max_error(y_true, y_pred)\\n6\\n\\n\\nThe max_error does not support multioutput.\\n\\n\\n3.4.6.8. Explained variance score#\\nThe explained_variance_score computes the explained variance\\nregression score.\\nIf \\\\(\\\\hat{y}\\\\) is the estimated target output, \\\\(y\\\\) the corresponding\\n(correct) target output, and \\\\(Var\\\\) is Variance, the square of the standard deviation,\\nthen the explained variance is estimated as follow:\\n\\n\\\\[explained\\\\_{}variance(y, \\\\hat{y}) = 1 - \\\\frac{Var\\\\{ y - \\\\hat{y}\\\\}}{Var\\\\{y\\\\}}\\\\]\\nThe best possible score is 1.0, lower values are worse.\\n\\nLink to R² score, the coefficient of determination\\nThe difference between the explained variance score and the R² score, the coefficient of determination\\nis that the explained variance score does not account for\\nsystematic offset in the prediction. For this reason, the\\nR² score, the coefficient of determination should be preferred in general.\\n---------new doc---------\\nIn the particular case where the true target is constant, the Explained\\nVariance score is not finite: it is either NaN (perfect predictions) or\\n-Inf (imperfect predictions). Such non-finite scores may prevent correct\\nmodel optimization such as grid-search cross-validation to be performed\\ncorrectly. For this reason the default behaviour of\\nexplained_variance_score is to replace them with 1.0 (perfect\\npredictions) or 0.0 (imperfect predictions). You can set the force_finite\\nparameter to False to prevent this fix from happening and fallback on the\\noriginal Explained Variance score.\\nHere is a small example of usage of the explained_variance_score\\nfunction:\\n>>> from sklearn.metrics import explained_variance_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> explained_variance_score(y_true, y_pred)\\n0.957...\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> explained_variance_score(y_true, y_pred, multioutput='raw_values')\\narray([0.967..., 1.        ])\\n>>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.990...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n---------new doc---------\\narray([0.967..., 1.        ])\\n>>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.990...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n>>> explained_variance_score(y_true, y_pred)\\n1.0\\n>>> explained_variance_score(y_true, y_pred, force_finite=False)\\nnan\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2 + 1e-8]\\n>>> explained_variance_score(y_true, y_pred)\\n0.0\\n>>> explained_variance_score(y_true, y_pred, force_finite=False)\\n-inf\\n---------new doc---------\\n3.4.6.9. Mean Poisson, Gamma, and Tweedie deviances#\\nThe mean_tweedie_deviance function computes the mean Tweedie\\ndeviance error\\nwith a power parameter (\\\\(p\\\\)). This is a metric that elicits\\npredicted expectation values of regression targets.\\nFollowing special cases exist,\\n\\nwhen power=0 it is equivalent to mean_squared_error.\\nwhen power=1 it is equivalent to mean_poisson_deviance.\\nwhen power=2 it is equivalent to mean_gamma_deviance.\\n\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample,\\nand \\\\(y_i\\\\) is the corresponding true value, then the mean Tweedie\\ndeviance error (D) for power \\\\(p\\\\), estimated over \\\\(n_{\\\\text{samples}}\\\\)\\nis defined as\\n---------new doc---------\\n\\\\[\\\\begin{split}\\\\text{D}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}}\\n\\\\sum_{i=0}^{n_\\\\text{samples} - 1}\\n\\\\begin{cases}\\n(y_i-\\\\hat{y}_i)^2, & \\\\text{for }p=0\\\\text{ (Normal)}\\\\\\\\\\n2(y_i \\\\log(y_i/\\\\hat{y}_i) + \\\\hat{y}_i - y_i),  & \\\\text{for }p=1\\\\text{ (Poisson)}\\\\\\\\\\n2(\\\\log(\\\\hat{y}_i/y_i) + y_i/\\\\hat{y}_i - 1),  & \\\\text{for }p=2\\\\text{ (Gamma)}\\\\\\\\\\n2\\\\left(\\\\frac{\\\\max(y_i,0)^{2-p}}{(1-p)(2-p)}-\\n\\\\frac{y_i\\\\,\\\\hat{y}_i^{1-p}}{1-p}+\\\\frac{\\\\hat{y}_i^{2-p}}{2-p}\\\\right),\\n& \\\\text{otherwise}\\n\\\\end{cases}\\\\end{split}\\\\]\\nTweedie deviance is a homogeneous function of degree 2-power.\\nThus, Gamma distribution with power=2 means that simultaneously scaling\\ny_true and y_pred has no effect on the deviance. For Poisson\\ndistribution power=1 the deviance scales linearly, and for Normal\\ndistribution (power=0), quadratically.  In general, the higher\\npower the less weight is given to extreme deviations between true\\nand predicted targets.\\n---------new doc---------\\nTweedie deviance is a homogeneous function of degree 2-power.\\nThus, Gamma distribution with power=2 means that simultaneously scaling\\ny_true and y_pred has no effect on the deviance. For Poisson\\ndistribution power=1 the deviance scales linearly, and for Normal\\ndistribution (power=0), quadratically.  In general, the higher\\npower the less weight is given to extreme deviations between true\\nand predicted targets.\\nFor instance, let’s compare the two predictions 1.5 and 150 that are both\\n50% larger than their corresponding true value.\\nThe mean squared error (power=0) is very sensitive to the\\nprediction difference of the second point,:\\n>>> from sklearn.metrics import mean_tweedie_deviance\\n>>> mean_tweedie_deviance([1.0], [1.5], power=0)\\n0.25\\n>>> mean_tweedie_deviance([100.], [150.], power=0)\\n2500.0\\n---------new doc---------\\nIf we increase power to 1,:\\n>>> mean_tweedie_deviance([1.0], [1.5], power=1)\\n0.18...\\n>>> mean_tweedie_deviance([100.], [150.], power=1)\\n18.9...\\n\\n\\nthe difference in errors decreases. Finally, by setting, power=2:\\n>>> mean_tweedie_deviance([1.0], [1.5], power=2)\\n0.14...\\n>>> mean_tweedie_deviance([100.], [150.], power=2)\\n0.14...\\n\\n\\nwe would get identical errors. The deviance when power=2 is thus only\\nsensitive to relative errors.\\n\\n\\n3.4.6.10. Pinball loss#\\nThe mean_pinball_loss function is used to evaluate the predictive\\nperformance of quantile regression models.\\n---------new doc---------\\n3.4.6.10. Pinball loss#\\nThe mean_pinball_loss function is used to evaluate the predictive\\nperformance of quantile regression models.\\n\\n\\\\[\\\\text{pinball}(y, \\\\hat{y}) = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1}  \\\\alpha \\\\max(y_i - \\\\hat{y}_i, 0) + (1 - \\\\alpha) \\\\max(\\\\hat{y}_i - y_i, 0)\\\\]\\nThe value of pinball loss is equivalent to half of mean_absolute_error when the quantile\\nparameter alpha is set to 0.5.\\nHere is a small example of usage of the mean_pinball_loss function:\\n>>> from sklearn.metrics import mean_pinball_loss\\n>>> y_true = [1, 2, 3]\\n>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\\n0.03...\\n>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\\n0.3...\\n>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\\n0.3...\\n>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\\n0.03...\\n>>> mean_pinball_loss(y_true, y_true, alpha=0.1)\\n0.0\\n>>> mean_pinball_loss(y_true, y_true, alpha=0.9)\\n0.0\\n---------new doc---------\\nIt is possible to build a scorer object with a specific choice of alpha:\\n>>> from sklearn.metrics import make_scorer\\n>>> mean_pinball_loss_95p = make_scorer(mean_pinball_loss, alpha=0.95)\\n\\n\\nSuch a scorer can be used to evaluate the generalization performance of a\\nquantile regressor via cross-validation:\\n>>> from sklearn.datasets import make_regression\\n>>> from sklearn.model_selection import cross_val_score\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>>\\n>>> X, y = make_regression(n_samples=100, random_state=0)\\n>>> estimator = GradientBoostingRegressor(\\n...     loss=\\\"quantile\\\",\\n...     alpha=0.95,\\n...     random_state=0,\\n... )\\n>>> cross_val_score(estimator, X, y, cv=5, scoring=mean_pinball_loss_95p)\\narray([13.6..., 9.7..., 23.3..., 9.5..., 10.4...])\\n\\n\\nIt is also possible to build scorer objects for hyper-parameter tuning. The\\nsign of the loss must be switched to ensure that greater means better as\\nexplained in the example linked below.\\nExamples\\n\\nSee Prediction Intervals for Gradient Boosting Regression\\nfor an example of using the pinball loss to evaluate and tune the\\nhyper-parameters of quantile regression models on data with non-symmetric\\nnoise and outliers.\\n---------new doc---------\\nIt is also possible to build scorer objects for hyper-parameter tuning. The\\nsign of the loss must be switched to ensure that greater means better as\\nexplained in the example linked below.\\nExamples\\n\\nSee Prediction Intervals for Gradient Boosting Regression\\nfor an example of using the pinball loss to evaluate and tune the\\nhyper-parameters of quantile regression models on data with non-symmetric\\nnoise and outliers.\\n\\n\\n\\n3.4.6.11. D² score#\\nThe D² score computes the fraction of deviance explained.\\nIt is a generalization of R², where the squared error is generalized and replaced\\nby a deviance of choice \\\\(\\\\text{dev}(y, \\\\hat{y})\\\\)\\n(e.g., Tweedie, pinball or mean absolute error). D² is a form of a skill score.\\nIt is calculated as\\n\\n\\\\[D^2(y, \\\\hat{y}) = 1 - \\\\frac{\\\\text{dev}(y, \\\\hat{y})}{\\\\text{dev}(y, y_{\\\\text{null}})} \\\\,.\\\\]\\nWhere \\\\(y_{\\\\text{null}}\\\\) is the optimal prediction of an intercept-only model\\n(e.g., the mean of y_true for the Tweedie case, the median for absolute\\nerror and the alpha-quantile for pinball loss).\\nLike R², the best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). A constant model that always predicts\\n\\\\(y_{\\\\text{null}}\\\\), disregarding the input features, would get a D² score\\nof 0.0.\\n---------new doc---------\\nD² Tweedie score#\\nThe d2_tweedie_score function implements the special case of D²\\nwhere \\\\(\\\\text{dev}(y, \\\\hat{y})\\\\) is the Tweedie deviance, see Mean Poisson, Gamma, and Tweedie deviances.\\nIt is also known as D² Tweedie and is related to McFadden’s likelihood ratio index.\\nThe argument power defines the Tweedie power as for\\nmean_tweedie_deviance. Note that for power=0,\\nd2_tweedie_score equals r2_score (for single targets).\\nA scorer object with a specific choice of power can be built by:\\n>>> from sklearn.metrics import d2_tweedie_score, make_scorer\\n>>> d2_tweedie_score_15 = make_scorer(d2_tweedie_score, power=1.5)\\n\\n\\n\\n\\n\\nD² pinball score#\\nThe d2_pinball_score function implements the special case\\nof D² with the pinball loss, see Pinball loss, i.e.:\\n---------new doc---------\\nD² pinball score#\\nThe d2_pinball_score function implements the special case\\nof D² with the pinball loss, see Pinball loss, i.e.:\\n\\n\\\\[\\\\text{dev}(y, \\\\hat{y}) = \\\\text{pinball}(y, \\\\hat{y}).\\\\]\\nThe argument alpha defines the slope of the pinball loss as for\\nmean_pinball_loss (Pinball loss). It determines the\\nquantile level alpha for which the pinball loss and also D²\\nare optimal. Note that for alpha=0.5 (the default) d2_pinball_score\\nequals d2_absolute_error_score.\\nA scorer object with a specific choice of alpha can be built by:\\n>>> from sklearn.metrics import d2_pinball_score, make_scorer\\n>>> d2_pinball_score_08 = make_scorer(d2_pinball_score, alpha=0.8)\\n\\n\\n\\n\\n\\nD² absolute error score#\\nThe d2_absolute_error_score function implements the special case of\\nthe Mean absolute error:\\n---------new doc---------\\nD² absolute error score#\\nThe d2_absolute_error_score function implements the special case of\\nthe Mean absolute error:\\n\\n\\\\[\\\\text{dev}(y, \\\\hat{y}) = \\\\text{MAE}(y, \\\\hat{y}).\\\\]\\nHere are some usage examples of the d2_absolute_error_score function:\\n>>> from sklearn.metrics import d2_absolute_error_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> d2_absolute_error_score(y_true, y_pred)\\n0.764...\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [1, 2, 3]\\n>>> d2_absolute_error_score(y_true, y_pred)\\n1.0\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [2, 2, 2]\\n>>> d2_absolute_error_score(y_true, y_pred)\\n0.0\\n\\n\\n\\n\\n\\n3.4.6.12. Visual evaluation of regression models#\\nAmong methods to assess the quality of regression models, scikit-learn provides\\nthe PredictionErrorDisplay class. It allows to\\nvisually inspect the prediction errors of a model in two different manners.\\n---------new doc---------\\nThe plot on the left shows the actual values vs predicted values. For a\\nnoise-free regression task aiming to predict the (conditional) expectation of\\ny, a perfect regression model would display data points on the diagonal\\ndefined by predicted equal to actual values. The further away from this optimal\\nline, the larger the error of the model. In a more realistic setting with\\nirreducible noise, that is, when not all the variations of y can be explained\\nby features in X, then the best model would lead to a cloud of points densely\\narranged around the diagonal.\\nNote that the above only holds when the predicted values is the expected value\\nof y given X. This is typically the case for regression models that\\nminimize the mean squared error objective function or more generally the\\nmean Tweedie deviance for any value of its\\n“power” parameter.\\nWhen plotting the predictions of an estimator that predicts a quantile\\nof y given X, e.g. QuantileRegressor\\nor any other model minimizing the pinball loss, a\\nfraction of the points are either expected to lie above or below the diagonal\\ndepending on the estimated quantile level.\\nAll in all, while intuitive to read, this plot does not really inform us on\\nwhat to do to obtain a better model.\\nThe right-hand side plot shows the residuals (i.e. the difference between the\\nactual and the predicted values) vs. the predicted values.\\nThis plot makes it easier to visualize if the residuals follow and\\nhomoscedastic or heteroschedastic\\ndistribution.\\nIn particular, if the true distribution of y|X is Poisson or Gamma\\ndistributed, it is expected that the variance of the residuals of the optimal\\nmodel would grow with the predicted value of E[y|X] (either linearly for\\n---------new doc---------\\nactual and the predicted values) vs. the predicted values.\\nThis plot makes it easier to visualize if the residuals follow and\\nhomoscedastic or heteroschedastic\\ndistribution.\\nIn particular, if the true distribution of y|X is Poisson or Gamma\\ndistributed, it is expected that the variance of the residuals of the optimal\\nmodel would grow with the predicted value of E[y|X] (either linearly for\\nPoisson or quadratically for Gamma).\\nWhen fitting a linear least squares regression model (see\\nLinearRegression and\\nRidge), we can use this plot to check\\nif some of the model assumptions\\nare met, in particular that the residuals should be uncorrelated, their\\nexpected value should be null and that their variance should be constant\\n(homoschedasticity).\\nIf this is not the case, and in particular if the residuals plot show some\\nbanana-shaped structure, this is a hint that the model is likely mis-specified\\nand that non-linear feature engineering or switching to a non-linear regression\\nmodel might be useful.\\nRefer to the example below to see a model evaluation that makes use of this\\ndisplay.\\nExamples\\n---------new doc---------\\nExample of permutation feature importance using multiple scorers#\\nIn the example below we use a list of metrics, but more input formats are\\npossible, as documented in Using multiple metric evaluation.\\n>>> scoring = ['r2', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error']\\n>>> r_multi = permutation_importance(\\n...     model, X_val, y_val, n_repeats=30, random_state=0, scoring=scoring)\\n...\\n>>> for metric in r_multi:\\n...     print(f\\\"{metric}\\\")\\n...     r = r_multi[metric]\\n...     for i in r.importances_mean.argsort()[::-1]:\\n...         if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\\n...             print(f\\\"    {diabetes.feature_names[i]:<8}\\\"\\n...                   f\\\"{r.importances_mean[i]:.3f}\\\"\\n...                   f\\\" +/- {r.importances_std[i]:.3f}\\\")\\n...\\nr2\\n    s5      0.204 +/- 0.050\\n    bmi     0.176 +/- 0.048\\n    bp      0.088 +/- 0.033\\n---------new doc---------\\n...                   f\\\" +/- {r.importances_std[i]:.3f}\\\")\\n...\\nr2\\n    s5      0.204 +/- 0.050\\n    bmi     0.176 +/- 0.048\\n    bp      0.088 +/- 0.033\\n    sex     0.056 +/- 0.023\\nneg_mean_absolute_percentage_error\\n    s5      0.081 +/- 0.020\\n    bmi     0.064 +/- 0.015\\n    bp      0.029 +/- 0.010\\nneg_mean_squared_error\\n    s5      1013.866 +/- 246.445\\n    bmi     872.726 +/- 240.298\\n    bp      438.663 +/- 163.022\\n    sex     277.376 +/- 115.123\\n---------new doc---------\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: 0.61\\n>>> raw_target_regr = LinearRegression().fit(X_train, y_train)\\n>>> print('R2 score: {0:.2f}'.format(raw_target_regr.score(X_test, y_test)))\\nR2 score: 0.59\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"PDF of a random variable Y following Poisson, Tweedie (power=1.5) and Gamma\\ndistributions with different mean values (\\\\(\\\\mu\\\\)). Observe the point\\nmass at \\\\(Y=0\\\\) for the Poisson distribution and the Tweedie (power=1.5)\\ndistribution, but not for the Gamma distribution which has a strictly\\npositive target domain.#\\n\\n\\nThe Bernoulli distribution is a discrete probability distribution modelling a\\nBernoulli trial - an event that has only two mutually exclusive outcomes.\\nThe Categorical distribution is a generalization of the Bernoulli distribution\\nfor a categorical random variable. While a random variable in a Bernoulli\\ndistribution has two possible outcomes, a Categorical random variable can take\\non one of K possible categories, with the probability of each category\\nspecified separately.\\nThe choice of the distribution depends on the problem at hand:\\n\\nIf the target values \\\\(y\\\\) are counts (non-negative integer valued) or\\nrelative frequencies (non-negative), you might use a Poisson distribution\\nwith a log-link.\\nIf the target values are positive valued and skewed, you might try a Gamma\\ndistribution with a log-link.\\nIf the target values seem to be heavier tailed than a Gamma distribution, you\\nmight try an Inverse Gaussian distribution (or even higher variance powers of\\nthe Tweedie family).\\nIf the target values \\\\(y\\\\) are probabilities, you can use the Bernoulli\\ndistribution. The Bernoulli distribution with a logit link can be used for\\nbinary classification. The Categorical distribution with a softmax link can be\\nused for multiclass classification.\\n\\n\\n\\nExamples of use cases#\\n---------new doc---------\\nExamples of use cases#\\n\\nAgriculture / weather modeling:  number of rain events per year (Poisson),\\namount of rainfall per event (Gamma), total rainfall per year (Tweedie /\\nCompound Poisson Gamma).\\nRisk modeling / insurance policy pricing:  number of claim events /\\npolicyholder per year (Poisson), cost per event (Gamma), total cost per\\npolicyholder per year (Tweedie / Compound Poisson Gamma).\\nCredit Default: probability that a loan can’t be paid back (Bernoulli).\\nFraud Detection: probability that a financial transaction like a cash transfer\\nis a fraudulent transaction (Bernoulli).\\nPredictive maintenance: number of production interruption events per year\\n(Poisson), duration of interruption (Gamma), total interruption time per year\\n(Tweedie / Compound Poisson Gamma).\\nMedical Drug Testing: probability of curing a patient in a set of trials or\\nprobability that a patient will experience side effects (Bernoulli).\\nNews Classification: classification of news articles into three categories\\nnamely Business News, Politics and Entertainment news (Categorical).\\n\\n\\nReferences\\n\\n\\n[10]\\nMcCullagh, Peter; Nelder, John (1989). Generalized Linear Models,\\nSecond Edition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.\\n\\n\\n[11]\\nJørgensen, B. (1992). The theory of exponential dispersion models\\nand analysis of deviance. Monografias de matemática, no. 51.  See also\\nExponential dispersion model.\\n---------new doc---------\\nExamples\\n\\nPoisson regression and non-normal loss\\nTweedie regression on insurance claims\\n\\n\\n\\nPractical considerations#\\nThe feature matrix X should be standardized before fitting. This ensures\\nthat the penalty treats features equally.\\nSince the linear predictor \\\\(Xw\\\\) can be negative and Poisson,\\nGamma and Inverse Gaussian distributions don’t support negative values, it\\nis necessary to apply an inverse link function that guarantees the\\nnon-negativeness. For example with link='log', the inverse link function\\nbecomes \\\\(h(Xw)=\\\\exp(Xw)\\\\).\\nIf you want to model a relative frequency, i.e. counts per exposure (time,\\nvolume, …) you can do so by using a Poisson distribution and passing\\n\\\\(y=\\\\frac{\\\\mathrm{counts}}{\\\\mathrm{exposure}}\\\\) as target values\\ntogether with \\\\(\\\\mathrm{exposure}\\\\) as sample weights. For a concrete\\nexample see e.g.\\nTweedie regression on insurance claims.\\nWhen performing cross-validation for the power parameter of\\nTweedieRegressor, it is advisable to specify an explicit scoring function,\\nbecause the default scorer TweedieRegressor.score is a function of\\npower itself.\\n---------new doc---------\\nPeter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale\\nestimates, p. 172.\\n\\n\\nThe HuberRegressor differs from using SGDRegressor with loss set to huber\\nin the following ways.\\n\\nHuberRegressor is scaling invariant. Once epsilon is set, scaling X and y\\ndown or up by different values would produce the same robustness to outliers as before.\\nas compared to SGDRegressor where epsilon has to be set again when X and y are\\nscaled.\\nHuberRegressor should be more efficient to use on data with small number of\\nsamples while SGDRegressor needs a number of passes on the training data to\\nproduce the same robustness.\\n\\nNote that this estimator is different from the R implementation of Robust\\nRegression  because the R\\nimplementation does a weighted least squares implementation with weights given to each\\nsample on the basis of how much the residual is greater than a certain threshold.\\n\\n\\n\\n1.1.17. Quantile Regression#\\nQuantile regression estimates the median or other quantiles of \\\\(y\\\\)\\nconditional on \\\\(X\\\\), while ordinary least squares (OLS) estimates the\\nconditional mean.\\nQuantile regression may be useful if one is interested in predicting an\\ninterval instead of point prediction. Sometimes, prediction intervals are\\ncalculated based on the assumption that prediction error is distributed\\nnormally with zero mean and constant variance. Quantile regression provides\\nsensible prediction intervals even for errors with non-constant (but\\npredictable) variance or non-normal distribution.\\n---------new doc---------\\nBased on minimizing the pinball loss, conditional quantiles can also be\\nestimated by models other than linear models. For example,\\nGradientBoostingRegressor can predict conditional\\nquantiles if its parameter loss is set to \\\"quantile\\\" and parameter\\nalpha is set to the quantile that should be predicted. See the example in\\nPrediction Intervals for Gradient Boosting Regression.\\nMost implementations of quantile regression are based on linear programming\\nproblem. The current implementation is based on\\nscipy.optimize.linprog.\\nExamples\\n\\nQuantile regression\\n\\n\\n\\nMathematical details#\\nAs a linear model, the QuantileRegressor gives linear predictions\\n\\\\(\\\\hat{y}(w, X) = Xw\\\\) for the \\\\(q\\\\)-th quantile, \\\\(q \\\\in (0, 1)\\\\).\\nThe weights or coefficients \\\\(w\\\\) are then found by the following\\nminimization problem:\\n\\n\\\\[\\\\min_{w} {\\\\frac{1}{n_{\\\\text{samples}}}\\n\\\\sum_i PB_q(y_i - X_i w) + \\\\alpha ||w||_1}.\\\\]\\nThis consists of the pinball loss (also known as linear loss),\\nsee also mean_pinball_loss,\\n---------new doc---------\\n... stratify=y, test_size=0.7, random_state=42)\\n>>> nca = NeighborhoodComponentsAnalysis(random_state=42)\\n>>> knn = KNeighborsClassifier(n_neighbors=3)\\n>>> nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\\n>>> nca_pipe.fit(X_train, y_train)\\nPipeline(...)\\n>>> print(nca_pipe.score(X_test, y_test))\\n0.96190476...\\n---------new doc---------\\nReferences#\\n\\nRennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\\nTackling the poor assumptions of naive bayes text classifiers.\\nIn ICML (Vol. 3, pp. 616-623).\\n\\n\\n\\n\\n1.9.4. Bernoulli Naive Bayes#\\nBernoulliNB implements the naive Bayes training and classification\\nalgorithms for data that is distributed according to multivariate Bernoulli\\ndistributions; i.e., there may be multiple features but each one is assumed\\nto be a binary-valued (Bernoulli, boolean) variable.\\nTherefore, this class requires samples to be represented as binary-valued\\nfeature vectors; if handed any other kind of data, a BernoulliNB instance\\nmay binarize its input (depending on the binarize parameter).\\nThe decision rule for Bernoulli naive Bayes is based on\\n\\n\\\\[P(x_i \\\\mid y) = P(x_i = 1 \\\\mid y) x_i + (1 - P(x_i = 1 \\\\mid y)) (1 - x_i)\\\\]\\nwhich differs from multinomial NB’s rule\\nin that it explicitly penalizes the non-occurrence of a feature \\\\(i\\\\)\\nthat is an indicator for class \\\\(y\\\\),\\nwhere the multinomial variant would simply ignore a non-occurring feature.\\nIn the case of text classification, word occurrence vectors (rather than word\\ncount vectors) may be used to train and use this classifier. BernoulliNB\\nmight perform better on some datasets, especially those with shorter documents.\\nIt is advisable to evaluate both models, if time permits.\\n\\n\\nReferences#\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> X_train, X_test = X[:2000], X[2000:]\\n>>> y_train, y_test = y[:2000], y[2000:]\\n\\n>>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.8965\\n\\n\\nAvailable losses for regression are:\\n\\n‘squared_error’, which is the default loss;\\n‘absolute_error’, which is less sensitive to outliers than the squared error;\\n‘gamma’, which is well suited to model strictly positive outcomes;\\n‘poisson’, which is well suited to model counts and frequencies;\\n‘quantile’, which allows for estimating a conditional quantile that can later\\nbe used to obtain prediction intervals.\\n---------new doc---------\\nNote\\nFor some losses, e.g. 'absolute_error' where the gradients\\nare \\\\(\\\\pm 1\\\\), the values predicted by a fitted \\\\(h_m\\\\) are not\\naccurate enough: the tree can only output integer values. As a result, the\\nleaves values of the tree \\\\(h_m\\\\) are modified once the tree is\\nfitted, such that the leaves values minimize the loss \\\\(L_m\\\\). The\\nupdate is loss-dependent: for the absolute error loss, the value of\\na leaf is updated to the median of the samples in that leaf.\\n---------new doc---------\\n1.11.1.2.4. Loss Functions#\\nThe following loss functions are supported and can be specified using\\nthe parameter loss:\\n\\n\\nRegression#\\n\\nSquared error ('squared_error'): The natural choice for regression\\ndue to its superior computational properties. The initial model is\\ngiven by the mean of the target values.\\nAbsolute error ('absolute_error'): A robust loss function for\\nregression. The initial model is given by the median of the\\ntarget values.\\nHuber ('huber'): Another robust loss function that combines\\nleast squares and least absolute deviation; use alpha to\\ncontrol the sensitivity with regards to outliers (see [Friedman2001] for\\nmore details).\\nQuantile ('quantile'): A loss function for quantile regression.\\nUse 0 < alpha < 1 to specify the quantile. This loss function\\ncan be used to create prediction intervals\\n(see Prediction Intervals for Gradient Boosting Regression).\\n\\n\\n\\n\\nClassification#\\n---------new doc---------\\n1.11.4.3. Weighted Average Probabilities (Soft Voting)#\\nIn contrast to majority voting (hard voting), soft voting\\nreturns the class label as argmax of the sum of predicted probabilities.\\nSpecific weights can be assigned to each classifier via the weights\\nparameter. When weights are provided, the predicted class probabilities\\nfor each classifier are collected, multiplied by the classifier weight,\\nand averaged. The final class label is then derived from the class label\\nwith the highest average probability.\\nTo illustrate this with a simple example, let’s assume we have 3\\nclassifiers and a 3-class classification problems where we assign\\nequal weights to all classifiers: w1=1, w2=1, w3=1.\\nThe weighted average probabilities for a sample would then be\\ncalculated as follows:\\n\\n\\nclassifier\\nclass 1\\nclass 2\\nclass 3\\n\\n\\n\\nclassifier 1\\nw1 * 0.2\\nw1 * 0.5\\nw1 * 0.3\\n\\nclassifier 2\\nw2 * 0.6\\nw2 * 0.3\\nw2 * 0.1\\n\\nclassifier 3\\nw3 * 0.3\\nw3 * 0.4\\nw3 * 0.3\\n\\nweighted average\\n0.37\\n0.4\\n0.23\\n---------new doc---------\\n...                                             metric='euclidean'))],\\n...     final_estimator=final_layer\\n... )\\n>>> multi_layer_regressor.fit(X_train, y_train)\\nStackingRegressor(...)\\n>>> print('R2 score: {:.2f}'\\n...       .format(multi_layer_regressor.score(X_test, y_test)))\\nR2 score: 0.53\\n---------new doc---------\\nFor more information about LabelBinarizer,\\nrefer to Transforming the prediction target (y).\\n---------new doc---------\\nOneVsRestClassifier also supports multilabel\\nclassification. To use this feature, feed the classifier an indicator matrix,\\nin which cell [i, j] indicates the presence of label j in sample i.\\n\\n\\n\\n\\nExamples\\n\\nMultilabel classification\\nPlot classification probability\\n---------new doc---------\\nThese objects take as input a scoring function that returns univariate scores\\nand p-values (or only scores for SelectKBest and\\nSelectPercentile):\\n\\nFor regression: r_regression, f_regression, mutual_info_regression\\nFor classification: chi2, f_classif, mutual_info_classif\\n\\nThe methods based on F-test estimate the degree of linear dependency between\\ntwo random variables. On the other hand, mutual information methods can capture\\nany kind of statistical dependency, but being nonparametric, they require more\\nsamples for accurate estimation. Note that the \\\\(\\\\chi^2\\\\)-test should only be\\napplied to non-negative features, such as frequencies.\\n\\nFeature selection with sparse data\\nIf you use sparse data (i.e. data represented as sparse matrices),\\nchi2, mutual_info_regression, mutual_info_classif\\nwill deal with the data without making it dense.\\n\\n\\nWarning\\nBeware not to use a regression scoring function with a classification\\nproblem, you will get useless results.\\n\\n\\nNote\\nThe SelectPercentile and SelectKBest support unsupervised\\nfeature selection as well. One needs to provide a score_func where y=None.\\nThe score_func should use internally X to compute the scores.\\n\\nExamples\\n\\nUnivariate Feature Selection\\nComparison of F-test and mutual information\\n---------new doc---------\\nNote\\nStrictly proper scoring rules for probabilistic predictions like\\nsklearn.metrics.brier_score_loss and\\nsklearn.metrics.log_loss assess calibration (reliability) and\\ndiscriminative power (resolution) of a model, as well as the randomness of the data\\n(uncertainty) at the same time. This follows from the well-known Brier score\\ndecomposition of Murphy [1]. As it is not clear which term dominates, the score is\\nof limited use for assessing calibration alone (unless one computes each term of\\nthe decomposition). A lower Brier loss, for instance, does not necessarily\\nmean a better calibrated model, it could also mean a worse calibrated model with much\\nmore discriminatory power, e.g. using many more features.\\n---------new doc---------\\n1.16.1. Calibration curves#\\nCalibration curves, also referred to as reliability diagrams (Wilks 1995 [2]),\\ncompare how well the probabilistic predictions of a binary classifier are calibrated.\\nIt plots the frequency of the positive label (to be more precise, an estimation of the\\nconditional event probability \\\\(P(Y=1|\\\\text{predict_proba})\\\\)) on the y-axis\\nagainst the predicted probability predict_proba of a model on the x-axis.\\nThe tricky part is to get values for the y-axis.\\nIn scikit-learn, this is accomplished by binning the predictions such that the x-axis\\nrepresents the average predicted probability in each bin.\\nThe y-axis is then the fraction of positives given the predictions of that bin, i.e.\\nthe proportion of samples whose class is the positive class (in each bin).\\nThe top calibration curve plot is created with\\nCalibrationDisplay.from_estimator, which uses calibration_curve to\\ncalculate the per bin average predicted probabilities and fraction of positives.\\nCalibrationDisplay.from_estimator\\ntakes as input a fitted classifier, which is used to calculate the predicted\\nprobabilities. The classifier thus must have predict_proba method. For\\nthe few classifiers that do not have a predict_proba method, it is\\npossible to use CalibratedClassifierCV to calibrate the classifier\\noutputs to probabilities.\\nThe bottom histogram gives some insight into the behavior of each classifier\\nby showing the number of samples in each predicted probability bin.\\n---------new doc---------\\nNote\\nImpact on ranking metrics like AUC\\nIt is generally expected that calibration does not affect ranking metrics such as\\nROC-AUC. However, these metrics might differ after calibration when using\\nmethod=\\\"isotonic\\\" since isotonic regression introduces ties in the predicted\\nprobabilities. This can be seen as within the uncertainty of the model predictions.\\nIn case, you strictly want to keep the ranking and thus AUC scores, use\\nmethod=\\\"sigmoid\\\" which is a strictly monotonic transformation and thus keeps\\nthe ranking.\\n\\n\\n\\n1.16.3.3. Multiclass support#\\nBoth isotonic and sigmoid regressors only\\nsupport 1-dimensional data (e.g., binary classification output) but are\\nextended for multiclass classification if the base_estimator supports\\nmulticlass predictions. For multiclass predictions,\\nCalibratedClassifierCV calibrates for\\neach class separately in a OneVsRestClassifier fashion [5]. When\\npredicting\\nprobabilities, the calibrated probabilities for each class\\nare predicted separately. As those probabilities do not necessarily sum to\\none, a postprocessing is performed to normalize them.\\nExamples\\n\\nProbability Calibration curves\\nProbability Calibration for 3-class classification\\nProbability calibration of classifiers\\nComparison of Calibration of Classifiers\\n\\nReferences\\n\\n\\n[1]\\nAllan H. Murphy (1973).\\n“A New Vector Partition of the Probability Score”\\nJournal of Applied Meteorology and Climatology\\n\\n\\n[2]\\nOn the combination of forecast probabilities for\\nconsecutive precipitation periods.\\nWea. Forecasting, 5, 640–650., Wilks, D. S., 1990a\\n---------new doc---------\\nThe Rand index does not ensure to obtain a value close to 0.0 for a\\nrandom labelling. The adjusted Rand index corrects for chance and\\nwill give such a baseline.\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n0.24...\\n\\n\\nAs with all clustering metrics, one can permute 0 and 1 in the predicted\\nlabels, rename 2 to 3, and get the same score:\\n>>> labels_pred = [1, 1, 0, 0, 3, 3]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n0.66...\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n0.24...\\n\\n\\nFurthermore, both rand_score adjusted_rand_score are\\nsymmetric: swapping the argument does not change the scores. They can\\nthus be used as consensus measures:\\n>>> metrics.rand_score(labels_pred, labels_true)\\n0.66...\\n>>> metrics.adjusted_rand_score(labels_pred, labels_true)\\n0.24...\\n\\n\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n1.0\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n1.0\\n---------new doc---------\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n1.0\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n1.0\\n\\n\\nPoorly agreeing labels (e.g. independent labelings) have lower scores,\\nand for the adjusted Rand index the score will be negative or close to\\nzero. However, for the unadjusted Rand index the score, while lower,\\nwill not necessarily be close to zero.:\\n>>> labels_true = [0, 0, 0, 0, 0, 0, 1, 1]\\n>>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n0.39...\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n-0.07...\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nAdvantages:\\n\\nInterpretability: The unadjusted Rand index is proportional to the\\nnumber of sample pairs whose labels are the same in both labels_pred and\\nlabels_true, or are different in both.\\nRandom (uniform) label assignments have an adjusted Rand index score close\\nto 0.0 for any value of n_clusters and n_samples (which is not the\\ncase for the unadjusted Rand index or the V-measure for instance).\\nBounded range: Lower values indicate different labelings, similar\\nclusterings have a high (adjusted or unadjusted) Rand index, 1.0 is the\\nperfect match score. The score range is [0, 1] for the unadjusted Rand index\\nand [-0.5, 1] for the adjusted Rand index.\\nNo assumption is made on the cluster structure: The (adjusted or\\nunadjusted) Rand index can be used to compare all kinds of clustering\\nalgorithms, and can be used to compare clustering algorithms such as k-means\\nwhich assumes isotropic blob shapes with results of spectral clustering\\nalgorithms which can find cluster with “folded” shapes.\\n\\n\\n\\nDrawbacks:\\n\\nContrary to inertia, the (adjusted or unadjusted) Rand index requires\\nknowledge of the ground truth classes which is almost never available in\\npractice or requires manual assignment by human annotators (as in the\\nsupervised learning setting).\\nHowever (adjusted or unadjusted) Rand index can also be useful in a purely\\nunsupervised setting as a building block for a Consensus Index that can be\\nused for clustering model selection (TODO).\\n---------new doc---------\\nContrary to inertia, the (adjusted or unadjusted) Rand index requires\\nknowledge of the ground truth classes which is almost never available in\\npractice or requires manual assignment by human annotators (as in the\\nsupervised learning setting).\\nHowever (adjusted or unadjusted) Rand index can also be useful in a purely\\nunsupervised setting as a building block for a Consensus Index that can be\\nused for clustering model selection (TODO).\\n\\nThe unadjusted Rand index is often close to 1.0 even if the clusterings\\nthemselves differ significantly. This can be understood when interpreting\\nthe Rand index as the accuracy of element pair labeling resulting from the\\nclusterings: In practice there often is a majority of element pairs that are\\nassigned the different pair label under both the predicted and the\\nground truth clustering resulting in a high proportion of pair labels that\\nagree, which leads subsequently to a high score.\\n\\n\\nExamples\\n\\nAdjustment for chance in clustering performance evaluation:\\nAnalysis of the impact of the dataset size on the value of\\nclustering measures for random assignments.\\n\\n\\n\\nMathematical formulation#\\nIf C is a ground truth class assignment and K the clustering, let us define\\n\\\\(a\\\\) and \\\\(b\\\\) as:\\n\\n\\\\(a\\\\), the number of pairs of elements that are in the same set in C and\\nin the same set in K\\n\\\\(b\\\\), the number of pairs of elements that are in different sets in C and\\nin different sets in K\\n\\nThe unadjusted Rand index is then given by:\\n---------new doc---------\\n\\\\(a\\\\), the number of pairs of elements that are in the same set in C and\\nin the same set in K\\n\\\\(b\\\\), the number of pairs of elements that are in different sets in C and\\nin different sets in K\\n\\nThe unadjusted Rand index is then given by:\\n\\n\\\\[\\\\text{RI} = \\\\frac{a + b}{C_2^{n_{samples}}}\\\\]\\nwhere \\\\(C_2^{n_{samples}}\\\\) is the total number of possible pairs in the\\ndataset. It does not matter if the calculation is performed on ordered pairs or\\nunordered pairs as long as the calculation is performed consistently.\\nHowever, the Rand index does not guarantee that random label assignments will\\nget a value close to zero (esp. if the number of clusters is in the same order\\nof magnitude as the number of samples).\\nTo counter this effect we can discount the expected RI \\\\(E[\\\\text{RI}]\\\\) of\\nrandom labelings by defining the adjusted Rand index as follows:\\n\\n\\\\[\\\\text{ARI} = \\\\frac{\\\\text{RI} - E[\\\\text{RI}]}{\\\\max(\\\\text{RI}) - E[\\\\text{RI}]}\\\\]\\n\\n\\n\\nReferences#\\n\\nComparing Partitions L. Hubert and P.\\nArabie, Journal of Classification 1985\\nProperties of the Hubert-Arabie adjusted Rand index D. Steinley, Psychological\\nMethods 2004\\nWikipedia entry for the Rand index\\nMinimum adjusted Rand index for two clusterings of a given size, 2022, J. E. Chacón and A. I. Rastrojo\\n---------new doc---------\\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get\\nthe same score:\\n>>> labels_pred = [1, 1, 0, 0, 3, 3]\\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \\n0.22504...\\n\\n\\nAll, mutual_info_score, adjusted_mutual_info_score and\\nnormalized_mutual_info_score are symmetric: swapping the argument does\\nnot change the score. Thus they can be used as a consensus measure:\\n>>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  \\n0.22504...\\n\\n\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \\n1.0\\n\\n>>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  \\n1.0\\n\\n\\nThis is not true for mutual_info_score, which is therefore harder to judge:\\n>>> metrics.mutual_info_score(labels_true, labels_pred)  \\n0.69...\\n\\n\\nBad (e.g. independent labelings) have non-positive scores:\\n>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \\n-0.10526...\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nAdvantages:\\n\\nRandom (uniform) label assignments have a AMI score close to 0.0 for any\\nvalue of n_clusters and n_samples (which is not the case for raw\\nMutual Information or the V-measure for instance).\\nUpper bound  of 1:  Values close to zero indicate two label assignments\\nthat are largely independent, while values close to one indicate significant\\nagreement. Further, an AMI of exactly 1 indicates that the two label\\nassignments are equal (with or without permutation).\\n\\n\\n\\nDrawbacks:\\n\\nContrary to inertia, MI-based measures require the knowledge of the ground\\ntruth classes while almost never available in practice or requires manual\\nassignment by human annotators (as in the supervised learning setting).\\nHowever MI-based measures can also be useful in purely unsupervised setting\\nas a building block for a Consensus Index that can be used for clustering\\nmodel selection.\\n\\nNMI and MI are not adjusted against chance.\\n\\n\\nExamples\\n\\nAdjustment for chance in clustering performance evaluation: Analysis\\nof the impact of the dataset size on the value of clustering measures for random\\nassignments. This example also includes the Adjusted Rand Index.\\n\\n\\n\\nMathematical formulation#\\nAssume two label assignments (of the same N objects), \\\\(U\\\\) and \\\\(V\\\\).\\nTheir entropy is the amount of uncertainty for a partition set, defined by:\\n\\n\\\\[H(U) = - \\\\sum_{i=1}^{|U|}P(i)\\\\log(P(i))\\\\]\\nwhere \\\\(P(i) = |U_i| / N\\\\) is the probability that an object picked at\\nrandom from \\\\(U\\\\) falls into class \\\\(U_i\\\\). Likewise for \\\\(V\\\\):\\n---------new doc---------\\n\\\\[H(U) = - \\\\sum_{i=1}^{|U|}P(i)\\\\log(P(i))\\\\]\\nwhere \\\\(P(i) = |U_i| / N\\\\) is the probability that an object picked at\\nrandom from \\\\(U\\\\) falls into class \\\\(U_i\\\\). Likewise for \\\\(V\\\\):\\n\\n\\\\[H(V) = - \\\\sum_{j=1}^{|V|}P'(j)\\\\log(P'(j))\\\\]\\nWith \\\\(P'(j) = |V_j| / N\\\\). The mutual information (MI) between \\\\(U\\\\)\\nand \\\\(V\\\\) is calculated by:\\n\\n\\\\[\\\\text{MI}(U, V) = \\\\sum_{i=1}^{|U|}\\\\sum_{j=1}^{|V|}P(i, j)\\\\log\\\\left(\\\\frac{P(i,j)}{P(i)P'(j)}\\\\right)\\\\]\\nwhere \\\\(P(i, j) = |U_i \\\\cap V_j| / N\\\\) is the probability that an object\\npicked at random falls into both classes \\\\(U_i\\\\) and \\\\(V_j\\\\).\\nIt also can be expressed in set cardinality formulation:\\n\\n\\\\[\\\\text{MI}(U, V) = \\\\sum_{i=1}^{|U|} \\\\sum_{j=1}^{|V|} \\\\frac{|U_i \\\\cap V_j|}{N}\\\\log\\\\left(\\\\frac{N|U_i \\\\cap V_j|}{|U_i||V_j|}\\\\right)\\\\]\\nThe normalized mutual information is defined as\\n---------new doc---------\\nhomogeneity: each cluster contains only members of a single class.\\ncompleteness: all members of a given class are assigned to the same\\ncluster.\\n\\nWe can turn those concept as scores homogeneity_score and\\ncompleteness_score. Both are bounded below by 0.0 and above by\\n1.0 (higher is better):\\n>>> from sklearn import metrics\\n>>> labels_true = [0, 0, 0, 1, 1, 1]\\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\\n\\n>>> metrics.homogeneity_score(labels_true, labels_pred)\\n0.66...\\n\\n>>> metrics.completeness_score(labels_true, labels_pred)\\n0.42...\\n\\n\\nTheir harmonic mean called V-measure is computed by\\nv_measure_score:\\n>>> metrics.v_measure_score(labels_true, labels_pred)\\n0.51...\\n\\n\\nThis function’s formula is as follows:\\n\\n\\\\[v = \\\\frac{(1 + \\\\beta) \\\\times \\\\text{homogeneity} \\\\times \\\\text{completeness}}{(\\\\beta \\\\times \\\\text{homogeneity} + \\\\text{completeness})}\\\\]\\nbeta defaults to a value of 1.0, but for using a value less than 1 for beta:\\n>>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6)\\n0.54...\\n\\n\\nmore weight will be attributed to homogeneity, and using a value greater than 1:\\n>>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)\\n0.48...\\n---------new doc---------\\nmore weight will be attributed to homogeneity, and using a value greater than 1:\\n>>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)\\n0.48...\\n\\n\\nmore weight will be attributed to completeness.\\nThe V-measure is actually equivalent to the mutual information (NMI)\\ndiscussed above, with the aggregation function being the arithmetic mean [B2011].\\nHomogeneity, completeness and V-measure can be computed at once using\\nhomogeneity_completeness_v_measure as follows:\\n>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\\n(0.66..., 0.42..., 0.51...)\\n\\n\\nThe following clustering assignment is slightly better, since it is\\nhomogeneous but not complete:\\n>>> labels_pred = [0, 0, 0, 1, 2, 2]\\n>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\\n(1.0, 0.68..., 0.81...)\\n\\n\\n\\nNote\\nv_measure_score is symmetric: it can be used to evaluate\\nthe agreement of two independent assignments on the same dataset.\\nThis is not the case for completeness_score and\\nhomogeneity_score: both are bound by the relationship:\\nhomogeneity_score(a, b) == completeness_score(b, a)\\n\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nNote\\nv_measure_score is symmetric: it can be used to evaluate\\nthe agreement of two independent assignments on the same dataset.\\nThis is not the case for completeness_score and\\nhomogeneity_score: both are bound by the relationship:\\nhomogeneity_score(a, b) == completeness_score(b, a)\\n\\n\\n\\n\\nAdvantages:\\n\\nBounded scores: 0.0 is as bad as it can be, 1.0 is a perfect score.\\nIntuitive interpretation: clustering with bad V-measure can be\\nqualitatively analyzed in terms of homogeneity and completeness to\\nbetter feel what ‘kind’ of mistakes is done by the assignment.\\nNo assumption is made on the cluster structure: can be used to compare\\nclustering algorithms such as k-means which assumes isotropic blob shapes\\nwith results of spectral clustering algorithms which can find cluster with\\n“folded” shapes.\\n\\n\\n\\nDrawbacks:\\n\\nThe previously introduced metrics are not normalized with regards to\\nrandom labeling: this means that depending on the number of samples,\\nclusters and ground truth classes, a completely random labeling will not\\nalways yield the same values for homogeneity, completeness and hence\\nv-measure. In particular random labeling won’t yield zero scores\\nespecially when the number of clusters is large.\\nThis problem can safely be ignored when the number of samples is more than a\\nthousand and the number of clusters is less than 10. For smaller sample\\nsizes or larger number of clusters it is safer to use an adjusted index such\\nas the Adjusted Rand Index (ARI).\\n\\n\\n\\n\\n\\n\\n\\nThese metrics require the knowledge of the ground truth classes while\\nalmost never available in practice or requires manual assignment by human\\nannotators (as in the supervised learning setting).\\n\\n\\nExamples\\n---------new doc---------\\nTP (True Positive): The number of pairs of points that are clustered together\\nboth in the true labels and in the predicted labels.\\nFP (False Positive): The number of pairs of points that are clustered together\\nin the predicted labels but not in the true labels.\\nFN (False Negative): The number of pairs of points that are clustered together\\nin the true labels but not in the predicted labels.\\n\\nThe score ranges from 0 to 1. A high value indicates a good similarity\\nbetween two clusters.\\n>>> from sklearn import metrics\\n>>> labels_true = [0, 0, 0, 1, 1, 1]\\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\\n\\n\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.47140...\\n\\n\\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get\\nthe same score:\\n>>> labels_pred = [1, 1, 0, 0, 3, 3]\\n\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.47140...\\n\\n\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n1.0\\n\\n\\nBad (e.g. independent labelings) have zero scores:\\n>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.0\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nBad (e.g. independent labelings) have zero scores:\\n>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.0\\n\\n\\n\\nAdvantages:\\n\\nRandom (uniform) label assignments have a FMI score close to 0.0 for any\\nvalue of n_clusters and n_samples (which is not the case for raw\\nMutual Information or the V-measure for instance).\\nUpper-bounded at 1:  Values close to zero indicate two label assignments\\nthat are largely independent, while values close to one indicate significant\\nagreement. Further, values of exactly 0 indicate purely independent\\nlabel assignments and a FMI of exactly 1 indicates that the two label\\nassignments are equal (with or without permutation).\\nNo assumption is made on the cluster structure: can be used to compare\\nclustering algorithms such as k-means which assumes isotropic blob shapes\\nwith results of spectral clustering algorithms which can find cluster with\\n“folded” shapes.\\n\\n\\n\\nDrawbacks:\\n\\nContrary to inertia, FMI-based measures require the knowledge of the\\nground truth classes while almost never available in practice or requires\\nmanual assignment by human annotators (as in the supervised learning\\nsetting).\\n\\n\\n\\n\\nReferences#\\n---------new doc---------\\nDrawbacks:\\n\\nContrary to inertia, FMI-based measures require the knowledge of the\\nground truth classes while almost never available in practice or requires\\nmanual assignment by human annotators (as in the supervised learning\\nsetting).\\n\\n\\n\\n\\nReferences#\\n\\nE. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two\\nhierarchical clusterings”. Journal of the American Statistical Association.\\nhttps://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008\\nWikipedia entry for the Fowlkes-Mallows Index\\n\\n\\n\\n\\n2.3.11.5. Silhouette Coefficient#\\nIf the ground truth labels are not known, evaluation must be performed using\\nthe model itself. The Silhouette Coefficient\\n(sklearn.metrics.silhouette_score)\\nis an example of such an evaluation, where a\\nhigher Silhouette Coefficient score relates to a model with better defined\\nclusters. The Silhouette Coefficient is defined for each sample and is composed\\nof two scores:\\n\\na: The mean distance between a sample and all other points in the same\\nclass.\\nb: The mean distance between a sample and all other points in the next\\nnearest cluster.\\n\\nThe Silhouette Coefficient s for a single sample is then given as:\\n---------new doc---------\\nThe mean score and the standard deviation are hence given by:\\n>>> print(\\\"%0.2f accuracy with a standard deviation of %0.2f\\\" % (scores.mean(), scores.std()))\\n0.98 accuracy with a standard deviation of 0.02\\n\\n\\nBy default, the score computed at each CV iteration is the score\\nmethod of the estimator. It is possible to change this by using the\\nscoring parameter:\\n>>> from sklearn import metrics\\n>>> scores = cross_val_score(\\n...     clf, X, y, cv=5, scoring='f1_macro')\\n>>> scores\\narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])\\n---------new doc---------\\nSee Pipelines and composite estimators.\\n\\n\\n3.1.1.1. The cross_validate function and multiple metric evaluation#\\nThe cross_validate function differs from cross_val_score in\\ntwo ways:\\n\\nIt allows specifying multiple metrics for evaluation.\\nIt returns a dict containing fit-times, score-times\\n(and optionally training scores, fitted estimators, train-test split indices)\\nin addition to the test score.\\n---------new doc---------\\nFor single metric evaluation, where the scoring parameter is a string,\\ncallable or None, the keys will be - ['test_score', 'fit_time', 'score_time']\\nAnd for multiple metric evaluation, the return value is a dict with the\\nfollowing keys -\\n['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']\\nreturn_train_score is set to False by default to save computation time.\\nTo evaluate the scores on the training set as well you need to set it to\\nTrue. You may also retain the estimator fitted on each training set by\\nsetting return_estimator=True. Similarly, you may set\\nreturn_indices=True to retain the training and testing indices used to split\\nthe dataset into train and test sets for each cv split.\\nThe multiple metrics can be specified either as a list, tuple or set of\\npredefined scorer names:\\n>>> from sklearn.model_selection import cross_validate\\n>>> from sklearn.metrics import recall_score\\n>>> scoring = ['precision_macro', 'recall_macro']\\n>>> clf = svm.SVC(kernel='linear', C=1, random_state=0)\\n>>> scores = cross_validate(clf, X, y, scoring=scoring)\\n>>> sorted(scores.keys())\\n['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']\\n>>> scores['test_recall_macro']\\narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])\\n---------new doc---------\\nOr as a dict mapping scorer name to a predefined or custom scoring function:\\n>>> from sklearn.metrics import make_scorer\\n>>> scoring = {'prec_macro': 'precision_macro',\\n...            'rec_macro': make_scorer(recall_score, average='macro')}\\n>>> scores = cross_validate(clf, X, y, scoring=scoring,\\n...                         cv=5, return_train_score=True)\\n>>> sorted(scores.keys())\\n['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',\\n 'train_prec_macro', 'train_rec_macro']\\n>>> scores['train_rec_macro']\\narray([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])\\n\\n\\nHere is an example of cross_validate using a single metric:\\n>>> scores = cross_validate(clf, X, y,\\n...                         scoring='precision_macro', cv=5,\\n...                         return_estimator=True)\\n>>> sorted(scores.keys())\\n['estimator', 'fit_time', 'score_time', 'test_score']\\n---------new doc---------\\n3.1.5. Permutation test score#\\npermutation_test_score offers another way\\nto evaluate the performance of classifiers. It provides a permutation-based\\np-value, which represents how likely an observed performance of the\\nclassifier would be obtained by chance. The null hypothesis in this test is\\nthat the classifier fails to leverage any statistical dependency between the\\nfeatures and the labels to make correct predictions on left out data.\\npermutation_test_score generates a null\\ndistribution by calculating n_permutations different permutations of the\\ndata. In each permutation the labels are randomly shuffled, thereby removing\\nany dependency between the features and the labels. The p-value output\\nis the fraction of permutations for which the average cross-validation score\\nobtained by the model is better than the cross-validation score obtained by\\nthe model using the original data. For reliable results n_permutations\\nshould typically be larger than 100 and cv between 3-10 folds.\\nA low p-value provides evidence that the dataset contains real dependency\\nbetween features and labels and the classifier was able to utilize this\\nto obtain good results. A high p-value could be due to a lack of dependency\\nbetween features and labels (there is no difference in feature values between\\nthe classes) or because the classifier was not able to use the dependency in\\nthe data. In the latter case, using a more appropriate classifier that\\nis able to utilize the structure in the data, would result in a lower\\np-value.\\nCross-validation provides information about how well a classifier generalizes,\\nspecifically the range of expected errors of the classifier. However, a\\nclassifier trained on a high dimensional dataset with no structure may still\\nperform better than expected on cross-validation, just by chance.\\n---------new doc---------\\n3.2.4.2. Specifying multiple metrics for evaluation#\\nGridSearchCV and RandomizedSearchCV allow specifying\\nmultiple metrics for the scoring parameter.\\nMultimetric scoring can either be specified as a list of strings of predefined\\nscores names or a dict mapping the scorer name to the scorer function and/or\\nthe predefined scorer name(s). See Using multiple metric evaluation for more details.\\nWhen specifying multiple metrics, the refit parameter must be set to the\\nmetric (string) for which the best_params_ will be found and used to build\\nthe best_estimator_ on the whole dataset. If the search should not be\\nrefit, set refit=False. Leaving refit to the default value None will\\nresult in an error when using multiple metrics.\\nSee Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV\\nfor an example usage.\\nHalvingRandomSearchCV and HalvingGridSearchCV do not support\\nmultimetric scoring.\\n---------new doc---------\\nWhile these hard-coded rules might at first seem reasonable as default behavior, they\\nare most certainly not ideal for most use cases. Let’s illustrate with an example.\\nConsider a scenario where a predictive model is being deployed to assist\\nphysicians in detecting tumors. In this setting, physicians will most likely be\\ninterested in identifying all patients with cancer and not missing anyone with cancer so\\nthat they can provide them with the right treatment. In other words, physicians\\nprioritize achieving a high recall rate. This emphasis on recall comes, of course, with\\nthe trade-off of potentially more false-positive predictions, reducing the precision of\\nthe model. That is a risk physicians are willing to take because the cost of a missed\\ncancer is much higher than the cost of further diagnostic tests. Consequently, when it\\ncomes to deciding whether to classify a patient as having cancer or not, it may be more\\nbeneficial to classify them as positive for cancer when the conditional probability\\nestimate is much lower than 0.5.\\n---------new doc---------\\nNote\\nIt is important to notice that these metrics come with default parameters, notably\\nthe label of the class of interest (i.e. pos_label). Thus, if this label is not\\nthe right one for your application, you need to define a scorer and pass the right\\npos_label (and additional parameters) using the\\nmake_scorer. Refer to Callable scorers to get\\ninformation to define your own scoring function. For instance, we show how to pass\\nthe information to the scorer that the label of interest is 0 when maximizing the\\nf1_score:\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.model_selection import TunedThresholdClassifierCV\\n>>> from sklearn.metrics import make_scorer, f1_score\\n>>> X, y = make_classification(\\n...   n_samples=1_000, weights=[0.1, 0.9], random_state=0)\\n>>> pos_label = 0\\n>>> scorer = make_scorer(f1_score, pos_label=pos_label)\\n>>> base_model = LogisticRegression()\\n>>> model = TunedThresholdClassifierCV(base_model, scoring=scorer)\\n>>> scorer(model.fit(X, y), X, y)\\n0.88...\\n>>> # compare it with the internal score found by cross-validation\\n>>> model.best_score_\\n0.86...\\n---------new doc---------\\n3.3.1.4. Examples#\\n\\nSee the example entitled\\nPost-hoc tuning the cut-off point of decision function,\\nto get insights on the post-tuning of the decision threshold.\\nSee the example entitled\\nPost-tuning the decision threshold for cost-sensitive learning,\\nto learn about cost-sensitive learning and decision threshold tuning.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.2. Tuning the hyper-parameters of an estimator\\n\\n\\n\\n\\nnext\\n3.4. Metrics and scoring: quantifying the quality of predictions\\n\\n\\n --- \\n\\n\\nand Developer\\nGuide for\\nmore details.\\n\\n\\n\\n\\nUsing custom scorers in functions where n_jobs > 1#\\nWhile defining the custom scoring function alongside the calling function\\nshould work out of the box with the default joblib backend (loky),\\nimporting it from another module will be a more robust approach and work\\nindependently of the joblib backend.\\nFor example, to use n_jobs greater than 1 in the example below,\\ncustom_scoring_function function is saved in a user-created module\\n(custom_scorer_module.py) and imported:\\n>>> from custom_scorer_module import custom_scoring_function \\n>>> cross_val_score(model,\\n...  X_train,\\n...  y_train,\\n...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),\\n...  cv=5,\\n...  n_jobs=-1) \\n\\n\\n\\n\\n\\n\\n3.4.3.3. Using multiple metric evaluation#\\nScikit-learn also permits evaluation of multiple metrics in GridSearchCV,\\nRandomizedSearchCV and cross_validate.\\nThere are three ways to specify multiple scoring metrics for the scoring\\nparameter:\\n---------new doc---------\\n3.4.3.3. Using multiple metric evaluation#\\nScikit-learn also permits evaluation of multiple metrics in GridSearchCV,\\nRandomizedSearchCV and cross_validate.\\nThere are three ways to specify multiple scoring metrics for the scoring\\nparameter:\\n\\nAs an iterable of string metrics:\\n>>> scoring = ['accuracy', 'precision']\\n\\n\\n\\nAs a dict mapping the scorer name to the scoring function:\\n>>> from sklearn.metrics import accuracy_score\\n>>> from sklearn.metrics import make_scorer\\n>>> scoring = {'accuracy': make_scorer(accuracy_score),\\n...            'prec': 'precision'}\\n\\n\\nNote that the dict values can either be scorer functions or one of the\\npredefined metric strings.\\n---------new doc---------\\nNote that the dict values can either be scorer functions or one of the\\npredefined metric strings.\\n\\nAs a callable that returns a dictionary of scores:\\n>>> from sklearn.model_selection import cross_validate\\n>>> from sklearn.metrics import confusion_matrix\\n>>> # A sample toy binary classification dataset\\n>>> X, y = datasets.make_classification(n_classes=2, random_state=0)\\n>>> svm = LinearSVC(random_state=0)\\n>>> def confusion_matrix_scorer(clf, X, y):\\n...      y_pred = clf.predict(X)\\n...      cm = confusion_matrix(y, y_pred)\\n...      return {'tn': cm[0, 0], 'fp': cm[0, 1],\\n...              'fn': cm[1, 0], 'tp': cm[1, 1]}\\n>>> cv_results = cross_validate(svm, X, y, cv=5,\\n...                             scoring=confusion_matrix_scorer)\\n>>> # Getting the test set true positive scores\\n>>> print(cv_results['test_tp'])\\n[10  9  8  7  8]\\n>>> # Getting the test set false negative scores\\n>>> print(cv_results['test_fn'])\\n[0 1 2 3 2]\\n---------new doc---------\\n3.4.4. Classification metrics#\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions to measure classification performance.\\nSome metrics might require probability estimates of the positive class,\\nconfidence values, or binary decisions values.\\nMost implementations allow each sample to provide a weighted contribution\\nto the overall score, through the sample_weight parameter.\\nSome of these are restricted to the binary classification case:\\n\\n\\nprecision_recall_curve(y_true[, y_score, ...])\\nCompute precision-recall pairs for different probability thresholds.\\n\\nroc_curve(y_true, y_score, *[, pos_label, ...])\\nCompute Receiver operating characteristic (ROC).\\n\\nclass_likelihood_ratios(y_true, y_pred, *[, ...])\\nCompute binary classification positive and negative likelihood ratios.\\n\\ndet_curve(y_true, y_score[, pos_label, ...])\\nCompute error rates for different probability thresholds.\\n\\n\\n\\n\\nOthers also work in the multiclass case:\\n\\n\\nbalanced_accuracy_score(y_true, y_pred, *[, ...])\\nCompute the balanced accuracy.\\n\\ncohen_kappa_score(y1, y2, *[, labels, ...])\\nCompute Cohen's kappa: a statistic that measures inter-annotator agreement.\\n\\nconfusion_matrix(y_true, y_pred, *[, ...])\\nCompute confusion matrix to evaluate the accuracy of a classification.\\n---------new doc---------\\ncohen_kappa_score(y1, y2, *[, labels, ...])\\nCompute Cohen's kappa: a statistic that measures inter-annotator agreement.\\n\\nconfusion_matrix(y_true, y_pred, *[, ...])\\nCompute confusion matrix to evaluate the accuracy of a classification.\\n\\nhinge_loss(y_true, pred_decision, *[, ...])\\nAverage hinge loss (non-regularized).\\n\\nmatthews_corrcoef(y_true, y_pred, *[, ...])\\nCompute the Matthews correlation coefficient (MCC).\\n\\nroc_auc_score(y_true, y_score, *[, average, ...])\\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\\n\\ntop_k_accuracy_score(y_true, y_score, *[, ...])\\nTop-k Accuracy classification score.\\n\\n\\n\\n\\nSome also work in the multilabel case:\\n\\n\\naccuracy_score(y_true, y_pred, *[, ...])\\nAccuracy classification score.\\n\\nclassification_report(y_true, y_pred, *[, ...])\\nBuild a text report showing the main classification metrics.\\n\\nf1_score(y_true, y_pred, *[, labels, ...])\\nCompute the F1 score, also known as balanced F-score or F-measure.\\n\\nfbeta_score(y_true, y_pred, *, beta[, ...])\\nCompute the F-beta score.\\n---------new doc---------\\nf1_score(y_true, y_pred, *[, labels, ...])\\nCompute the F1 score, also known as balanced F-score or F-measure.\\n\\nfbeta_score(y_true, y_pred, *, beta[, ...])\\nCompute the F-beta score.\\n\\nhamming_loss(y_true, y_pred, *[, sample_weight])\\nCompute the average Hamming loss.\\n\\njaccard_score(y_true, y_pred, *[, labels, ...])\\nJaccard similarity coefficient score.\\n\\nlog_loss(y_true, y_pred, *[, normalize, ...])\\nLog loss, aka logistic loss or cross-entropy loss.\\n\\nmultilabel_confusion_matrix(y_true, y_pred, *)\\nCompute a confusion matrix for each class or sample.\\n\\nprecision_recall_fscore_support(y_true, ...)\\nCompute precision, recall, F-measure and support for each class.\\n\\nprecision_score(y_true, y_pred, *[, labels, ...])\\nCompute the precision.\\n\\nrecall_score(y_true, y_pred, *[, labels, ...])\\nCompute the recall.\\n\\nroc_auc_score(y_true, y_score, *[, average, ...])\\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\\n---------new doc---------\\nrecall_score(y_true, y_pred, *[, labels, ...])\\nCompute the recall.\\n\\nroc_auc_score(y_true, y_score, *[, average, ...])\\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\\n\\nzero_one_loss(y_true, y_pred, *[, ...])\\nZero-one classification loss.\\n\\nd2_log_loss_score(y_true, y_pred, *[, ...])\\n\\\\(D^2\\\\) score function, fraction of log loss explained.\\n\\n\\n\\n\\nAnd some work with binary and multilabel (but not multiclass) problems:\\n\\n\\naverage_precision_score(y_true, y_score, *)\\nCompute average precision (AP) from prediction scores.\\n\\n\\n\\n\\nIn the following sub-sections, we will describe each of those functions,\\npreceded by some notes on common API and metric definition.\\n---------new doc---------\\nAnd some work with binary and multilabel (but not multiclass) problems:\\n\\n\\naverage_precision_score(y_true, y_score, *)\\nCompute average precision (AP) from prediction scores.\\n\\n\\n\\n\\nIn the following sub-sections, we will describe each of those functions,\\npreceded by some notes on common API and metric definition.\\n\\n3.4.4.1. From binary to multiclass and multilabel#\\nSome metrics are essentially defined for binary classification tasks (e.g.\\nf1_score, roc_auc_score). In these cases, by default\\nonly the positive label is evaluated, assuming by default that the positive\\nclass is labelled 1 (though this may be configurable through the\\npos_label parameter).\\nIn extending a binary metric to multiclass or multilabel problems, the data\\nis treated as a collection of binary problems, one for each class.\\nThere are then a number of ways to average binary metric calculations across\\nthe set of classes, each of which may be useful in some scenario.\\nWhere available, you should select among these using the average parameter.\\n---------new doc---------\\n\\\"macro\\\" simply calculates the mean of the binary metrics,\\ngiving equal weight to each class.  In problems where infrequent classes\\nare nonetheless important, macro-averaging may be a means of highlighting\\ntheir performance. On the other hand, the assumption that all classes are\\nequally important is often untrue, such that macro-averaging will\\nover-emphasize the typically low performance on an infrequent class.\\n\\\"weighted\\\" accounts for class imbalance by computing the average of\\nbinary metrics in which each class’s score is weighted by its presence in the\\ntrue data sample.\\n\\\"micro\\\" gives each sample-class pair an equal contribution to the overall\\nmetric (except as a result of sample-weight). Rather than summing the\\nmetric per class, this sums the dividends and divisors that make up the\\nper-class metrics to calculate an overall quotient.\\nMicro-averaging may be preferred in multilabel settings, including\\nmulticlass classification where a majority class is to be ignored.\\n\\\"samples\\\" applies only to multilabel problems. It does not calculate a\\nper-class measure, instead calculating the metric over the true and predicted\\nclasses for each sample in the evaluation data, and returning their\\n(sample_weight-weighted) average.\\nSelecting average=None will return an array with the score for each\\nclass.\\n\\nWhile multiclass data is provided to the metric, like binary targets, as an\\narray of class labels, multilabel data is specified as an indicator matrix,\\nin which cell [i, j] has value 1 if sample i has label j and value\\n0 otherwise.\\n---------new doc---------\\nWhile multiclass data is provided to the metric, like binary targets, as an\\narray of class labels, multilabel data is specified as an indicator matrix,\\nin which cell [i, j] has value 1 if sample i has label j and value\\n0 otherwise.\\n\\n\\n3.4.4.2. Accuracy score#\\nThe accuracy_score function computes the\\naccuracy, either the fraction\\n(default) or the count (normalize=False) of correct predictions.\\nIn multilabel classification, the function returns the subset accuracy. If\\nthe entire set of predicted labels for a sample strictly match with the true\\nset of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of\\nthe \\\\(i\\\\)-th sample and \\\\(y_i\\\\) is the corresponding true value,\\nthen the fraction of correct predictions over \\\\(n_\\\\text{samples}\\\\) is\\ndefined as\\n\\n\\\\[\\\\texttt{accuracy}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} 1(\\\\hat{y}_i = y_i)\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function.\\n>>> import numpy as np\\n>>> from sklearn.metrics import accuracy_score\\n>>> y_pred = [0, 2, 1, 3]\\n>>> y_true = [0, 1, 2, 3]\\n>>> accuracy_score(y_true, y_pred)\\n0.5\\n>>> accuracy_score(y_true, y_pred, normalize=False)\\n2.0\\n---------new doc---------\\nIn the multilabel case with binary label indicators:\\n>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\\n0.5\\n\\n\\nExamples\\n\\nSee Test with permutations the significance of a classification score\\nfor an example of accuracy score usage using permutations of\\nthe dataset.\\n\\n\\n\\n3.4.4.3. Top-k accuracy score#\\nThe top_k_accuracy_score function is a generalization of\\naccuracy_score. The difference is that a prediction is considered\\ncorrect as long as the true label is associated with one of the k highest\\npredicted scores. accuracy_score is the special case of k = 1.\\nThe function covers the binary and multiclass classification cases but not the\\nmultilabel case.\\nIf \\\\(\\\\hat{f}_{i,j}\\\\) is the predicted class for the \\\\(i\\\\)-th sample\\ncorresponding to the \\\\(j\\\\)-th largest predicted score and \\\\(y_i\\\\) is the\\ncorresponding true value, then the fraction of correct predictions over\\n\\\\(n_\\\\text{samples}\\\\) is defined as\\n---------new doc---------\\n\\\\[\\\\texttt{top-k accuracy}(y, \\\\hat{f}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} \\\\sum_{j=1}^{k} 1(\\\\hat{f}_{i,j} = y_i)\\\\]\\nwhere \\\\(k\\\\) is the number of guesses allowed and \\\\(1(x)\\\\) is the\\nindicator function.\\n>>> import numpy as np\\n>>> from sklearn.metrics import top_k_accuracy_score\\n>>> y_true = np.array([0, 1, 2, 2])\\n>>> y_score = np.array([[0.5, 0.2, 0.2],\\n...                     [0.3, 0.4, 0.2],\\n...                     [0.2, 0.4, 0.3],\\n...                     [0.7, 0.2, 0.1]])\\n>>> top_k_accuracy_score(y_true, y_score, k=2)\\n0.75\\n>>> # Not normalizing gives the number of \\\"correctly\\\" classified samples\\n>>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\\n3\\n---------new doc---------\\n3.4.4.4. Balanced accuracy score#\\nThe balanced_accuracy_score function computes the balanced accuracy, which avoids inflated\\nperformance estimates on imbalanced datasets. It is the macro-average of recall\\nscores per class or, equivalently, raw accuracy where each sample is weighted\\naccording to the inverse prevalence of its true class.\\nThus for balanced datasets, the score is equal to accuracy.\\nIn the binary case, balanced accuracy is equal to the arithmetic mean of\\nsensitivity\\n(true positive rate) and specificity (true negative\\nrate), or the area under the ROC curve with binary predictions rather than\\nscores:\\n\\n\\\\[\\\\texttt{balanced-accuracy} = \\\\frac{1}{2}\\\\left( \\\\frac{TP}{TP + FN} + \\\\frac{TN}{TN + FP}\\\\right )\\\\]\\nIf the classifier performs equally well on either class, this term reduces to\\nthe conventional accuracy (i.e., the number of correct predictions divided by\\nthe total number of predictions).\\nIn contrast, if the conventional accuracy is above chance only because the\\nclassifier takes advantage of an imbalanced test set, then the balanced\\naccuracy, as appropriate, will drop to \\\\(\\\\frac{1}{n\\\\_classes}\\\\).\\nThe score ranges from 0 to 1, or when adjusted=True is used, it rescaled to\\nthe range \\\\(\\\\frac{1}{1 - n\\\\_classes}\\\\) to 1, inclusive, with\\nperformance at random scoring 0.\\nIf \\\\(y_i\\\\) is the true value of the \\\\(i\\\\)-th sample, and \\\\(w_i\\\\)\\nis the corresponding sample weight, then we adjust the sample weight to:\\n---------new doc---------\\n\\\\[\\\\hat{w}_i = \\\\frac{w_i}{\\\\sum_j{1(y_j = y_i) w_j}}\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function.\\nGiven predicted \\\\(\\\\hat{y}_i\\\\) for sample \\\\(i\\\\), balanced accuracy is\\ndefined as:\\n\\n\\\\[\\\\texttt{balanced-accuracy}(y, \\\\hat{y}, w) = \\\\frac{1}{\\\\sum{\\\\hat{w}_i}} \\\\sum_i 1(\\\\hat{y}_i = y_i) \\\\hat{w}_i\\\\]\\nWith adjusted=True, balanced accuracy reports the relative increase from\\n\\\\(\\\\texttt{balanced-accuracy}(y, \\\\mathbf{0}, w) =\\n\\\\frac{1}{n\\\\_classes}\\\\).  In the binary case, this is also known as\\n*Youden’s J statistic*,\\nor informedness.\\n\\nNote\\nThe multiclass definition here seems the most reasonable extension of the\\nmetric used in binary classification, though there is no certain consensus\\nin the literature:\\n\\nOur definition: [Mosley2013], [Kelleher2015] and [Guyon2015], where\\n[Guyon2015] adopt the adjusted version to ensure that random predictions\\nhave a score of \\\\(0\\\\) and perfect predictions have a score of \\\\(1\\\\)..\\nClass balanced accuracy as described in [Mosley2013]: the minimum between the precision\\nand the recall for each class is computed. Those values are then averaged over the total\\nnumber of classes to get the balanced accuracy.\\nBalanced Accuracy as described in [Urbanowicz2015]: the average of sensitivity and specificity\\nis computed for each class and then averaged over total number of classes.\\n\\n\\nReferences\\n---------new doc---------\\n[Urbanowicz2015]\\nUrbanowicz R.J.,  Moore, J.H. ExSTraCS 2.0: description\\nand evaluation of a scalable learning classifier\\nsystem, Evol. Intel. (2015) 8: 89.\\n\\n\\n\\n\\n3.4.4.5. Cohen’s kappa#\\nThe function cohen_kappa_score computes Cohen’s kappa statistic.\\nThis measure is intended to compare labelings by different human annotators,\\nnot a classifier versus a ground truth.\\nThe kappa score is a number between -1 and 1.\\nScores above .8 are generally considered good agreement;\\nzero or lower means no agreement (practically random labels).\\nKappa scores can be computed for binary or multiclass problems,\\nbut not for multilabel problems (except by manually computing a per-label score)\\nand not for more than two annotators.\\n>>> from sklearn.metrics import cohen_kappa_score\\n>>> labeling1 = [2, 0, 2, 2, 0, 1]\\n>>> labeling2 = [0, 0, 2, 2, 0, 2]\\n>>> cohen_kappa_score(labeling1, labeling2)\\n0.4285714285714286\\n---------new doc---------\\n3.4.4.6. Confusion matrix#\\nThe confusion_matrix function evaluates\\nclassification accuracy by computing the confusion matrix with each row corresponding\\nto the true class (Wikipedia and other references may use different convention\\nfor axes).\\nBy definition, entry \\\\(i, j\\\\) in a confusion matrix is\\nthe number of observations actually in group \\\\(i\\\\), but\\npredicted to be in group \\\\(j\\\\). Here is an example:\\n>>> from sklearn.metrics import confusion_matrix\\n>>> y_true = [2, 0, 2, 2, 0, 1]\\n>>> y_pred = [0, 0, 2, 2, 0, 2]\\n>>> confusion_matrix(y_true, y_pred)\\narray([[2, 0, 0],\\n       [0, 0, 1],\\n       [1, 0, 2]])\\n\\n\\nConfusionMatrixDisplay can be used to visually represent a confusion\\nmatrix as shown in the\\nConfusion matrix\\nexample, which creates the following figure:\\n\\n\\nThe parameter normalize allows to report ratios instead of counts. The\\nconfusion matrix can be normalized in 3 different ways: 'pred', 'true',\\nand 'all' which will divide the counts by the sum of each columns, rows, or\\nthe entire matrix, respectively.\\n>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]\\n>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\\n>>> confusion_matrix(y_true, y_pred, normalize='all')\\narray([[0.25 , 0.125],\\n       [0.25 , 0.375]])\\n---------new doc---------\\nFor binary problems, we can get counts of true negatives, false positives,\\nfalse negatives and true positives as follows:\\n>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]\\n>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\\n>>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\\n>>> tn, fp, fn, tp\\n(2, 1, 2, 3)\\n\\n\\nExamples\\n\\nSee Confusion matrix\\nfor an example of using a confusion matrix to evaluate classifier output\\nquality.\\nSee Recognizing hand-written digits\\nfor an example of using a confusion matrix to classify\\nhand-written digits.\\nSee Classification of text documents using sparse features\\nfor an example of using a confusion matrix to classify text\\ndocuments.\\n\\n\\n\\n3.4.4.7. Classification report#\\nThe classification_report function builds a text report showing the\\nmain classification metrics. Here is a small example with custom target_names\\nand inferred labels:\\n>>> from sklearn.metrics import classification_report\\n>>> y_true = [0, 1, 2, 2, 0]\\n>>> y_pred = [0, 0, 2, 1, 0]\\n>>> target_names = ['class 0', 'class 1', 'class 2']\\n>>> print(classification_report(y_true, y_pred, target_names=target_names))\\n              precision    recall  f1-score   support\\n---------new doc---------\\n\\\\[L_{Hamming}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples} * n_\\\\text{labels}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} \\\\sum_{j=0}^{n_\\\\text{labels} - 1} 1(\\\\hat{y}_{i,j} \\\\not= y_{i,j})\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function.\\nThe equation above does not hold true in the case of multiclass classification.\\nPlease refer to the note below for more information.\\n>>> from sklearn.metrics import hamming_loss\\n>>> y_pred = [1, 2, 3, 4]\\n>>> y_true = [2, 2, 3, 4]\\n>>> hamming_loss(y_true, y_pred)\\n0.25\\n\\n\\nIn the multilabel case with binary label indicators:\\n>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\\n0.75\\n\\n\\n\\nNote\\nIn multiclass classification, the Hamming loss corresponds to the Hamming\\ndistance between y_true and y_pred which is similar to the\\nZero one loss function.  However, while zero-one loss penalizes\\nprediction sets that do not strictly match true sets, the Hamming loss\\npenalizes individual labels.  Thus the Hamming loss, upper bounded by the zero-one\\nloss, is always between zero and one, inclusive; and predicting a proper subset\\nor superset of the true labels will give a Hamming loss between\\nzero and one, exclusive.\\n---------new doc---------\\n3.4.4.9. Precision, recall and F-measures#\\nIntuitively, precision is the ability\\nof the classifier not to label as positive a sample that is negative, and\\nrecall is the\\nability of the classifier to find all the positive samples.\\nThe  F-measure\\n(\\\\(F_\\\\beta\\\\) and \\\\(F_1\\\\) measures) can be interpreted as a weighted\\nharmonic mean of the precision and recall. A\\n\\\\(F_\\\\beta\\\\) measure reaches its best value at 1 and its worst score at 0.\\nWith \\\\(\\\\beta = 1\\\\),  \\\\(F_\\\\beta\\\\) and\\n\\\\(F_1\\\\)  are equivalent, and the recall and the precision are equally important.\\nThe precision_recall_curve computes a precision-recall curve\\nfrom the ground truth label and a score given by the classifier\\nby varying a decision threshold.\\nThe average_precision_score function computes the\\naverage precision\\n(AP) from prediction scores. The value is between 0 and 1 and higher is better.\\nAP is defined as\\n---------new doc---------\\n\\\\[\\\\text{AP} = \\\\sum_n (R_n - R_{n-1}) P_n\\\\]\\nwhere \\\\(P_n\\\\) and \\\\(R_n\\\\) are the precision and recall at the\\nnth threshold. With random predictions, the AP is the fraction of positive\\nsamples.\\nReferences [Manning2008] and [Everingham2010] present alternative variants of\\nAP that interpolate the precision-recall curve. Currently,\\naverage_precision_score does not implement any interpolated variant.\\nReferences [Davis2006] and [Flach2015] describe why a linear interpolation of\\npoints on the precision-recall curve provides an overly-optimistic measure of\\nclassifier performance. This linear interpolation is used when computing area\\nunder the curve with the trapezoidal rule in auc.\\nSeveral functions allow you to analyze the precision, recall and F-measures\\nscore:\\n\\n\\naverage_precision_score(y_true, y_score, *)\\nCompute average precision (AP) from prediction scores.\\n\\nf1_score(y_true, y_pred, *[, labels, ...])\\nCompute the F1 score, also known as balanced F-score or F-measure.\\n\\nfbeta_score(y_true, y_pred, *, beta[, ...])\\nCompute the F-beta score.\\n\\nprecision_recall_curve(y_true[, y_score, ...])\\nCompute precision-recall pairs for different probability thresholds.\\n\\nprecision_recall_fscore_support(y_true, ...)\\nCompute precision, recall, F-measure and support for each class.\\n---------new doc---------\\nprecision_recall_curve(y_true[, y_score, ...])\\nCompute precision-recall pairs for different probability thresholds.\\n\\nprecision_recall_fscore_support(y_true, ...)\\nCompute precision, recall, F-measure and support for each class.\\n\\nprecision_score(y_true, y_pred, *[, labels, ...])\\nCompute the precision.\\n\\nrecall_score(y_true, y_pred, *[, labels, ...])\\nCompute the recall.\\n\\n\\n\\n\\nNote that the precision_recall_curve function is restricted to the\\nbinary case. The average_precision_score function supports multiclass\\nand multilabel formats by computing each class score in a One-vs-the-rest (OvR)\\nfashion and averaging them or not depending of its average argument value.\\nThe PrecisionRecallDisplay.from_estimator and\\nPrecisionRecallDisplay.from_predictions functions will plot the\\nprecision-recall curve as follows.\\n\\n\\nExamples\\n\\nSee Custom refit strategy of a grid search with cross-validation\\nfor an example of precision_score and recall_score usage\\nto estimate parameters using grid search with nested cross-validation.\\nSee Precision-Recall\\nfor an example of precision_recall_curve usage to evaluate\\nclassifier output quality.\\n\\nReferences\\n\\n\\n[Manning2008]\\nC.D. Manning, P. Raghavan, H. Schütze, Introduction to Information Retrieval,\\n2008.\\n---------new doc---------\\nReferences\\n\\n\\n[Manning2008]\\nC.D. Manning, P. Raghavan, H. Schütze, Introduction to Information Retrieval,\\n2008.\\n\\n\\n[Everingham2010]\\nM. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman,\\nThe Pascal Visual Object Classes (VOC) Challenge,\\nIJCV 2010.\\n\\n\\n[Davis2006]\\nJ. Davis, M. Goadrich, The Relationship Between Precision-Recall and ROC Curves,\\nICML 2006.\\n\\n\\n[Flach2015]\\nP.A. Flach, M. Kull, Precision-Recall-Gain Curves: PR Analysis Done Right,\\nNIPS 2015.\\n\\n\\n\\n3.4.4.9.1. Binary classification#\\nIn a binary classification task, the terms ‘’positive’’ and ‘’negative’’ refer\\nto the classifier’s prediction, and the terms ‘’true’’ and ‘’false’’ refer to\\nwhether that prediction corresponds to the external judgment (sometimes known\\nas the ‘’observation’’). Given these definitions, we can formulate the\\nfollowing table:\\n\\n\\n\\nActual class (observation)\\n\\nPredicted class\\n(expectation)\\ntp (true positive)\\nCorrect result\\nfp (false positive)\\nUnexpected result\\n\\nfn (false negative)\\nMissing result\\ntn (true negative)\\nCorrect absence of result\\n\\n\\n\\n\\nIn this context, we can define the notions of precision and recall:\\n\\n\\\\[\\\\text{precision} = \\\\frac{\\\\text{tp}}{\\\\text{tp} + \\\\text{fp}},\\\\]\\n---------new doc---------\\nPredicted class\\n(expectation)\\ntp (true positive)\\nCorrect result\\nfp (false positive)\\nUnexpected result\\n\\nfn (false negative)\\nMissing result\\ntn (true negative)\\nCorrect absence of result\\n\\n\\n\\n\\nIn this context, we can define the notions of precision and recall:\\n\\n\\\\[\\\\text{precision} = \\\\frac{\\\\text{tp}}{\\\\text{tp} + \\\\text{fp}},\\\\]\\n\\n\\\\[\\\\text{recall} = \\\\frac{\\\\text{tp}}{\\\\text{tp} + \\\\text{fn}},\\\\]\\n(Sometimes recall is also called ‘’sensitivity’’)\\nF-measure is the weighted harmonic mean of precision and recall, with precision’s\\ncontribution to the mean weighted by some parameter \\\\(\\\\beta\\\\):\\n\\n\\\\[F_\\\\beta = (1 + \\\\beta^2) \\\\frac{\\\\text{precision} \\\\times \\\\text{recall}}{\\\\beta^2 \\\\text{precision} + \\\\text{recall}}\\\\]\\nTo avoid division by zero when precision and recall are zero, Scikit-Learn calculates F-measure with this\\notherwise-equivalent formula:\\n---------new doc---------\\n\\\\[F_\\\\beta = \\\\frac{(1 + \\\\beta^2) \\\\text{tp}}{(1 + \\\\beta^2) \\\\text{tp} + \\\\text{fp} + \\\\beta^2 \\\\text{fn}}\\\\]\\nNote that this formula is still undefined when there are no true positives, false\\npositives, or false negatives. By default, F-1 for a set of exclusively true negatives\\nis calculated as 0, however this behavior can be changed using the zero_division\\nparameter.\\nHere are some small examples in binary classification:\\n>>> from sklearn import metrics\\n>>> y_pred = [0, 1, 0, 0]\\n>>> y_true = [0, 1, 0, 1]\\n>>> metrics.precision_score(y_true, y_pred)\\n1.0\\n>>> metrics.recall_score(y_true, y_pred)\\n0.5\\n>>> metrics.f1_score(y_true, y_pred)\\n0.66...\\n>>> metrics.fbeta_score(y_true, y_pred, beta=0.5)\\n0.83...\\n>>> metrics.fbeta_score(y_true, y_pred, beta=1)\\n0.66...\\n>>> metrics.fbeta_score(y_true, y_pred, beta=2)\\n0.55...\\n>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)\\n(array([0.66..., 1.        ]), array([1. , 0.5]), array([0.71..., 0.83...]), array([2, 2]))\\n---------new doc---------\\n>>> import numpy as np\\n>>> from sklearn.metrics import precision_recall_curve\\n>>> from sklearn.metrics import average_precision_score\\n>>> y_true = np.array([0, 0, 1, 1])\\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\\n>>> precision, recall, threshold = precision_recall_curve(y_true, y_scores)\\n>>> precision\\narray([0.5       , 0.66..., 0.5       , 1.        , 1.        ])\\n>>> recall\\narray([1. , 1. , 0.5, 0.5, 0. ])\\n>>> threshold\\narray([0.1 , 0.35, 0.4 , 0.8 ])\\n>>> average_precision_score(y_true, y_scores)\\n0.83...\\n\\n\\n\\n\\n3.4.4.9.2. Multiclass and multilabel classification#\\nIn a multiclass and multilabel classification task, the notions of precision,\\nrecall, and F-measures can be applied to each label independently.\\nThere are a few ways to combine results across labels,\\nspecified by the average argument to the\\naverage_precision_score, f1_score,\\nfbeta_score, precision_recall_fscore_support,\\nprecision_score and recall_score functions, as described\\nabove.\\nNote the following behaviors when averaging:\\n---------new doc---------\\nIf all labels are included, “micro”-averaging in a multiclass setting will produce\\nprecision, recall and \\\\(F\\\\) that are all identical to accuracy.\\n“weighted” averaging may produce a F-score that is not between precision and recall.\\n“macro” averaging for F-measures is calculated as the arithmetic mean over\\nper-label/class F-measures, not the harmonic mean over the arithmetic precision and\\nrecall means. Both calculations can be seen in the literature but are not equivalent,\\nsee [OB2019] for details.\\n\\nTo make this more explicit, consider the following notation:\\n---------new doc---------\\nTo make this more explicit, consider the following notation:\\n\\n\\\\(y\\\\) the set of true \\\\((sample, label)\\\\) pairs\\n\\\\(\\\\hat{y}\\\\) the set of predicted \\\\((sample, label)\\\\) pairs\\n\\\\(L\\\\) the set of labels\\n\\\\(S\\\\) the set of samples\\n\\\\(y_s\\\\) the subset of \\\\(y\\\\) with sample \\\\(s\\\\),\\ni.e. \\\\(y_s := \\\\left\\\\{(s', l) \\\\in y | s' = s\\\\right\\\\}\\\\)\\n\\\\(y_l\\\\) the subset of \\\\(y\\\\) with label \\\\(l\\\\)\\nsimilarly, \\\\(\\\\hat{y}_s\\\\) and \\\\(\\\\hat{y}_l\\\\) are subsets of\\n\\\\(\\\\hat{y}\\\\)\\n\\\\(P(A, B) := \\\\frac{\\\\left| A \\\\cap B \\\\right|}{\\\\left|B\\\\right|}\\\\) for some\\nsets \\\\(A\\\\) and \\\\(B\\\\)\\n\\\\(R(A, B) := \\\\frac{\\\\left| A \\\\cap B \\\\right|}{\\\\left|A\\\\right|}\\\\)\\n(Conventions vary on handling \\\\(A = \\\\emptyset\\\\); this implementation uses\\n\\\\(R(A, B):=0\\\\), and similar for \\\\(P\\\\).)\\n\\\\(F_\\\\beta(A, B) := \\\\left(1 + \\\\beta^2\\\\right) \\\\frac{P(A, B) \\\\times R(A, B)}{\\\\beta^2 P(A, B) + R(A, B)}\\\\)\\n\\nThen the metrics are defined as:\\n\\n\\naverage\\nPrecision\\nRecall\\nF_beta\\n---------new doc---------\\nNone\\n\\\\(\\\\langle P(y_l, \\\\hat{y}_l) | l \\\\in L \\\\rangle\\\\)\\n\\\\(\\\\langle R(y_l, \\\\hat{y}_l) | l \\\\in L \\\\rangle\\\\)\\n\\\\(\\\\langle F_\\\\beta(y_l, \\\\hat{y}_l) | l \\\\in L \\\\rangle\\\\)\\n\\n\\n\\n\\n>>> from sklearn import metrics\\n>>> y_true = [0, 1, 2, 0, 1, 2]\\n>>> y_pred = [0, 2, 1, 0, 0, 1]\\n>>> metrics.precision_score(y_true, y_pred, average='macro')\\n0.22...\\n>>> metrics.recall_score(y_true, y_pred, average='micro')\\n0.33...\\n>>> metrics.f1_score(y_true, y_pred, average='weighted')\\n0.26...\\n>>> metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)\\n0.23...\\n>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)\\n(array([0.66..., 0.        , 0.        ]), array([1., 0., 0.]), array([0.71..., 0.        , 0.        ]), array([2, 2, 2]...))\\n---------new doc---------\\nFor multiclass classification with a “negative class”, it is possible to exclude some labels:\\n>>> metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')\\n... # excluding 0, no labels were correctly recalled\\n0.0\\n\\n\\nSimilarly, labels not present in the data sample may be accounted for in macro-averaging.\\n>>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')\\n0.166...\\n\\n\\nReferences\\n\\n\\n[OB2019]\\nOpitz, J., & Burst, S. (2019). “Macro f1 and macro f1.”\\n\\n\\n\\n\\n\\n3.4.4.10. Jaccard similarity coefficient score#\\nThe jaccard_score function computes the average of Jaccard similarity\\ncoefficients, also called the\\nJaccard index, between pairs of label sets.\\nThe Jaccard similarity coefficient with a ground truth label set \\\\(y\\\\) and\\npredicted label set \\\\(\\\\hat{y}\\\\), is defined as\\n---------new doc---------\\n3.4.4.10. Jaccard similarity coefficient score#\\nThe jaccard_score function computes the average of Jaccard similarity\\ncoefficients, also called the\\nJaccard index, between pairs of label sets.\\nThe Jaccard similarity coefficient with a ground truth label set \\\\(y\\\\) and\\npredicted label set \\\\(\\\\hat{y}\\\\), is defined as\\n\\n\\\\[J(y, \\\\hat{y}) = \\\\frac{|y \\\\cap \\\\hat{y}|}{|y \\\\cup \\\\hat{y}|}.\\\\]\\nThe jaccard_score (like precision_recall_fscore_support) applies\\nnatively to binary targets. By computing it set-wise it can be extended to apply\\nto multilabel and multiclass through the use of average (see\\nabove).\\nIn the binary case:\\n>>> import numpy as np\\n>>> from sklearn.metrics import jaccard_score\\n>>> y_true = np.array([[0, 1, 1],\\n...                    [1, 1, 0]])\\n>>> y_pred = np.array([[1, 1, 1],\\n...                    [1, 0, 0]])\\n>>> jaccard_score(y_true[0], y_pred[0])\\n0.6666...\\n\\n\\nIn the 2D comparison case (e.g. image similarity):\\n>>> jaccard_score(y_true, y_pred, average=\\\"micro\\\")\\n0.6\\n---------new doc---------\\nIn the 2D comparison case (e.g. image similarity):\\n>>> jaccard_score(y_true, y_pred, average=\\\"micro\\\")\\n0.6\\n\\n\\nIn the multilabel case with binary label indicators:\\n>>> jaccard_score(y_true, y_pred, average='samples')\\n0.5833...\\n>>> jaccard_score(y_true, y_pred, average='macro')\\n0.6666...\\n>>> jaccard_score(y_true, y_pred, average=None)\\narray([0.5, 0.5, 1. ])\\n\\n\\nMulticlass problems are binarized and treated like the corresponding\\nmultilabel problem:\\n>>> y_pred = [0, 2, 1, 2]\\n>>> y_true = [0, 1, 2, 2]\\n>>> jaccard_score(y_true, y_pred, average=None)\\narray([1. , 0. , 0.33...])\\n>>> jaccard_score(y_true, y_pred, average='macro')\\n0.44...\\n>>> jaccard_score(y_true, y_pred, average='micro')\\n0.33...\\n---------new doc---------\\nThe first [.9, .1] in y_pred denotes 90% probability that the first\\nsample has label 0.  The log loss is non-negative.\\n\\n\\n3.4.4.13. Matthews correlation coefficient#\\nThe matthews_corrcoef function computes the\\nMatthew’s correlation coefficient (MCC)\\nfor binary classes.  Quoting Wikipedia:\\n\\n“The Matthews correlation coefficient is used in machine learning as a\\nmeasure of the quality of binary (two-class) classifications. It takes\\ninto account true and false positives and negatives and is generally\\nregarded as a balanced measure which can be used even if the classes are\\nof very different sizes. The MCC is in essence a correlation coefficient\\nvalue between -1 and +1. A coefficient of +1 represents a perfect\\nprediction, 0 an average random prediction and -1 an inverse prediction.\\nThe statistic is also known as the phi coefficient.”\\n\\nIn the binary (two-class) case, \\\\(tp\\\\), \\\\(tn\\\\), \\\\(fp\\\\) and\\n\\\\(fn\\\\) are respectively the number of true positives, true negatives, false\\npositives and false negatives, the MCC is defined as\\n\\n\\\\[MCC = \\\\frac{tp \\\\times tn - fp \\\\times fn}{\\\\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.\\\\]\\nIn the multiclass case, the Matthews correlation coefficient can be defined in terms of a\\nconfusion_matrix \\\\(C\\\\) for \\\\(K\\\\) classes.  To simplify the\\ndefinition consider the following intermediate variables:\\n---------new doc---------\\n3.4.4.14. Multi-label confusion matrix#\\nThe multilabel_confusion_matrix function computes class-wise (default)\\nor sample-wise (samplewise=True) multilabel confusion matrix to evaluate\\nthe accuracy of a classification. multilabel_confusion_matrix also treats\\nmulticlass data as if it were multilabel, as this is a transformation commonly\\napplied to evaluate multiclass problems with binary classification metrics\\n(such as precision, recall, etc.).\\nWhen calculating class-wise multilabel confusion matrix \\\\(C\\\\), the\\ncount of true negatives for class \\\\(i\\\\) is \\\\(C_{i,0,0}\\\\), false\\nnegatives is \\\\(C_{i,1,0}\\\\), true positives is \\\\(C_{i,1,1}\\\\)\\nand false positives is \\\\(C_{i,0,1}\\\\).\\nHere is an example demonstrating the use of the\\nmultilabel_confusion_matrix function with\\nmultilabel indicator matrix input:\\n>>> import numpy as np\\n>>> from sklearn.metrics import multilabel_confusion_matrix\\n>>> y_true = np.array([[1, 0, 1],\\n...                    [0, 1, 0]])\\n>>> y_pred = np.array([[1, 0, 0],\\n...                    [0, 1, 1]])\\n>>> multilabel_confusion_matrix(y_true, y_pred)\\narray([[[1, 0],\\n        [0, 1]],\\n---------new doc---------\\n[[5, 0],\\n        [1, 0]],\\n\\n       [[2, 1],\\n        [1, 2]]])\\n\\n\\nHere are some examples demonstrating the use of the\\nmultilabel_confusion_matrix function to calculate recall\\n(or sensitivity), specificity, fall out and miss rate for each class in a\\nproblem with multilabel indicator matrix input.\\nCalculating\\nrecall\\n(also called the true positive rate or the sensitivity) for each class:\\n>>> y_true = np.array([[0, 0, 1],\\n...                    [0, 1, 0],\\n...                    [1, 1, 0]])\\n>>> y_pred = np.array([[0, 1, 0],\\n...                    [0, 0, 1],\\n...                    [1, 1, 0]])\\n>>> mcm = multilabel_confusion_matrix(y_true, y_pred)\\n>>> tn = mcm[:, 0, 0]\\n>>> tp = mcm[:, 1, 1]\\n>>> fn = mcm[:, 1, 0]\\n>>> fp = mcm[:, 0, 1]\\n>>> tp / (tp + fn)\\narray([1. , 0.5, 0. ])\\n---------new doc---------\\nCalculating\\nspecificity\\n(also called the true negative rate) for each class:\\n>>> tn / (tn + fp)\\narray([1. , 0. , 0.5])\\n\\n\\nCalculating fall out\\n(also called the false positive rate) for each class:\\n>>> fp / (fp + tn)\\narray([0. , 1. , 0.5])\\n\\n\\nCalculating miss rate\\n(also called the false negative rate) for each class:\\n>>> fn / (fn + tp)\\narray([0. , 0.5, 1. ])\\n\\n\\n\\n\\n3.4.4.15. Receiver operating characteristic (ROC)#\\nThe function roc_curve computes the\\nreceiver operating characteristic curve, or ROC curve.\\nQuoting Wikipedia :\\n\\n“A receiver operating characteristic (ROC), or simply ROC curve, is a\\ngraphical plot which illustrates the performance of a binary classifier\\nsystem as its discrimination threshold is varied. It is created by plotting\\nthe fraction of true positives out of the positives (TPR = true positive\\nrate) vs. the fraction of false positives out of the negatives (FPR = false\\npositive rate), at various threshold settings. TPR is also known as\\nsensitivity, and FPR is one minus the specificity or true negative rate.”\\n---------new doc---------\\nThis function requires the true binary value and the target scores, which can\\neither be probability estimates of the positive class, confidence values, or\\nbinary decisions. Here is a small example of how to use the roc_curve\\nfunction:\\n>>> import numpy as np\\n>>> from sklearn.metrics import roc_curve\\n>>> y = np.array([1, 1, 2, 2])\\n>>> scores = np.array([0.1, 0.4, 0.35, 0.8])\\n>>> fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\\n>>> fpr\\narray([0. , 0. , 0.5, 0.5, 1. ])\\n>>> tpr\\narray([0. , 0.5, 0.5, 1. , 1. ])\\n>>> thresholds\\narray([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\\n\\n\\nCompared to metrics such as the subset accuracy, the Hamming loss, or the\\nF1 score, ROC doesn’t require optimizing a threshold for each label.\\nThe roc_auc_score function, denoted by ROC-AUC or AUROC, computes the\\narea under the ROC curve. By doing so, the curve information is summarized in\\none number.\\nThe following figure shows the ROC curve and ROC-AUC score for a classifier\\naimed to distinguish the virginica flower from the rest of the species in the\\nIris plants dataset:\\n\\n\\nFor more information see the Wikipedia article on AUC.\\n---------new doc---------\\nFor more information see the Wikipedia article on AUC.\\n\\n3.4.4.15.1. Binary case#\\nIn the binary case, you can either provide the probability estimates, using\\nthe classifier.predict_proba() method, or the non-thresholded decision values\\ngiven by the classifier.decision_function() method. In the case of providing\\nthe probability estimates, the probability of the class with the\\n“greater label” should be provided. The “greater label” corresponds to\\nclassifier.classes_[1] and thus classifier.predict_proba(X)[:, 1].\\nTherefore, the y_score parameter is of size (n_samples,).\\n>>> from sklearn.datasets import load_breast_cancer\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.metrics import roc_auc_score\\n>>> X, y = load_breast_cancer(return_X_y=True)\\n>>> clf = LogisticRegression(solver=\\\"liblinear\\\").fit(X, y)\\n>>> clf.classes_\\narray([0, 1])\\n\\n\\nWe can use the probability estimates corresponding to clf.classes_[1].\\n>>> y_score = clf.predict_proba(X)[:, 1]\\n>>> roc_auc_score(y, y_score)\\n0.99...\\n\\n\\nOtherwise, we can use the non-thresholded decision values\\n>>> roc_auc_score(y, clf.decision_function(X))\\n0.99...\\n---------new doc---------\\nWe can use the probability estimates corresponding to clf.classes_[1].\\n>>> y_score = clf.predict_proba(X)[:, 1]\\n>>> roc_auc_score(y, y_score)\\n0.99...\\n\\n\\nOtherwise, we can use the non-thresholded decision values\\n>>> roc_auc_score(y, clf.decision_function(X))\\n0.99...\\n\\n\\n\\n\\n3.4.4.15.2. Multi-class case#\\nThe roc_auc_score function can also be used in multi-class\\nclassification. Two averaging strategies are currently supported: the\\none-vs-one algorithm computes the average of the pairwise ROC AUC scores, and\\nthe one-vs-rest algorithm computes the average of the ROC AUC scores for each\\nclass against all other classes. In both cases, the predicted labels are\\nprovided in an array with values from 0 to n_classes, and the scores\\ncorrespond to the probability estimates that a sample belongs to a particular\\nclass. The OvO and OvR algorithms support weighting uniformly\\n(average='macro') and by prevalence (average='weighted').\\n\\n\\nOne-vs-one Algorithm#\\nComputes the average AUC of all possible pairwise\\ncombinations of classes. [HT2001] defines a multiclass AUC metric weighted\\nuniformly:\\n---------new doc---------\\nOne-vs-rest Algorithm#\\nComputes the AUC of each class against the rest\\n[PD2000]. The algorithm is functionally the same as the multilabel case. To\\nenable this algorithm set the keyword argument multiclass to 'ovr'.\\nAdditionally to 'macro' [F2006] and 'weighted' [F2001] averaging, OvR\\nsupports 'micro' averaging.\\nIn applications where a high false positive rate is not tolerable the parameter\\nmax_fpr of roc_auc_score can be used to summarize the ROC curve up\\nto the given limit.\\nThe following figure shows the micro-averaged ROC curve and its corresponding\\nROC-AUC score for a classifier aimed to distinguish the different species in\\nthe Iris plants dataset:\\n---------new doc---------\\n3.4.4.15.3. Multi-label case#\\nIn multi-label classification, the roc_auc_score function is\\nextended by averaging over the labels as above. In this case,\\nyou should provide a y_score of shape (n_samples, n_classes). Thus, when\\nusing the probability estimates, one needs to select the probability of the\\nclass with the greater label for each output.\\n>>> from sklearn.datasets import make_multilabel_classification\\n>>> from sklearn.multioutput import MultiOutputClassifier\\n>>> X, y = make_multilabel_classification(random_state=0)\\n>>> inner_clf = LogisticRegression(solver=\\\"liblinear\\\", random_state=0)\\n>>> clf = MultiOutputClassifier(inner_clf).fit(X, y)\\n>>> y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\\n>>> roc_auc_score(y, y_score, average=None)\\narray([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\\n\\n\\nAnd the decision values do not require such processing.\\n>>> from sklearn.linear_model import RidgeClassifierCV\\n>>> clf = RidgeClassifierCV().fit(X, y)\\n>>> y_score = clf.decision_function(X)\\n>>> roc_auc_score(y, y_score, average=None)\\narray([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\\n\\n\\nExamples\\n---------new doc---------\\n[F2006]\\nFawcett, T., 2006. An introduction to ROC analysis.\\nPattern Recognition Letters, 27(8), pp. 861-874.\\n\\n\\n[F2001]\\nFawcett, T., 2001. Using rule sets to maximize\\nROC performance\\nIn Data Mining, 2001.\\nProceedings IEEE International Conference, pp. 131-138.\\n\\n\\n\\n\\n\\n3.4.4.16. Detection error tradeoff (DET)#\\nThe function det_curve computes the\\ndetection error tradeoff curve (DET) curve [WikipediaDET2017].\\nQuoting Wikipedia:\\n\\n“A detection error tradeoff (DET) graph is a graphical plot of error rates\\nfor binary classification systems, plotting false reject rate vs. false\\naccept rate. The x- and y-axes are scaled non-linearly by their standard\\nnormal deviates (or just by logarithmic transformation), yielding tradeoff\\ncurves that are more linear than ROC curves, and use most of the image area\\nto highlight the differences of importance in the critical operating region.”\\n\\nDET curves are a variation of receiver operating characteristic (ROC) curves\\nwhere False Negative Rate is plotted on the y-axis instead of True Positive\\nRate.\\nDET curves are commonly plotted in normal deviate scale by transformation with\\n\\\\(\\\\phi^{-1}\\\\) (with \\\\(\\\\phi\\\\) being the cumulative distribution\\nfunction).\\nThe resulting performance curves explicitly visualize the tradeoff of error\\ntypes for given classification algorithms.\\nSee [Martin1997] for examples and further motivation.\\nThis figure compares the ROC and DET curves of two example classifiers on the\\nsame classification task:\\n\\n\\n\\n\\nProperties#\\n---------new doc---------\\nProperties#\\n\\nDET curves form a linear curve in normal deviate scale if the detection\\nscores are normally (or close-to normally) distributed.\\nIt was shown by [Navratil2007] that the reverse is not necessarily true and\\neven more general distributions are able to produce linear DET curves.\\nThe normal deviate scale transformation spreads out the points such that a\\ncomparatively larger space of plot is occupied.\\nTherefore curves with similar classification performance might be easier to\\ndistinguish on a DET plot.\\nWith False Negative Rate being “inverse” to True Positive Rate the point\\nof perfection for DET curves is the origin (in contrast to the top left\\ncorner for ROC curves).\\n\\n\\n\\n\\nApplications and limitations#\\nDET curves are intuitive to read and hence allow quick visual assessment of a\\nclassifier’s performance.\\nAdditionally DET curves can be consulted for threshold analysis and operating\\npoint selection.\\nThis is particularly helpful if a comparison of error types is required.\\nOn the other hand DET curves do not provide their metric as a single number.\\nTherefore for either automated evaluation or comparison to other\\nclassification tasks metrics like the derived area under ROC curve might be\\nbetter suited.\\n\\nExamples\\n\\nSee Detection error tradeoff (DET) curve\\nfor an example comparison between receiver operating characteristic (ROC)\\ncurves and Detection error tradeoff (DET) curves.\\n\\nReferences\\n\\n\\n[WikipediaDET2017]\\nWikipedia contributors. Detection error tradeoff.\\nWikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.\\nAvailable at: https://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054.\\nAccessed February 19, 2018.\\n---------new doc---------\\nReferences\\n\\n\\n[WikipediaDET2017]\\nWikipedia contributors. Detection error tradeoff.\\nWikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.\\nAvailable at: https://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054.\\nAccessed February 19, 2018.\\n\\n\\n[Martin1997]\\nA. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki,\\nThe DET Curve in Assessment of Detection Task Performance, NIST 1997.\\n\\n\\n[Navratil2007]\\nJ. Navractil and D. Klusacek,\\n“On Linear DETs”,\\n2007 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing - ICASSP ‘07, Honolulu,\\nHI, 2007, pp. IV-229-IV-232.\\n---------new doc---------\\n[Navratil2007]\\nJ. Navractil and D. Klusacek,\\n“On Linear DETs”,\\n2007 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing - ICASSP ‘07, Honolulu,\\nHI, 2007, pp. IV-229-IV-232.\\n\\n\\n\\n\\n3.4.4.17. Zero one loss#\\nThe zero_one_loss function computes the sum or the average of the 0-1\\nclassification loss (\\\\(L_{0-1}\\\\)) over \\\\(n_{\\\\text{samples}}\\\\). By\\ndefault, the function normalizes over the sample. To get the sum of the\\n\\\\(L_{0-1}\\\\), set normalize to False.\\nIn multilabel classification, the zero_one_loss scores a subset as\\none if its labels strictly match the predictions, and as a zero if there\\nare any errors.  By default, the function returns the percentage of imperfectly\\npredicted subsets.  To get the count of such subsets instead, set\\nnormalize to False\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of\\nthe \\\\(i\\\\)-th sample and \\\\(y_i\\\\) is the corresponding true value,\\nthen the 0-1 loss \\\\(L_{0-1}\\\\) is defined as:\\n---------new doc---------\\n\\\\[L_{0-1}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} 1(\\\\hat{y}_i \\\\not= y_i)\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function. The zero one\\nloss can also be computed as \\\\(zero-one loss = 1 - accuracy\\\\).\\n>>> from sklearn.metrics import zero_one_loss\\n>>> y_pred = [1, 2, 3, 4]\\n>>> y_true = [2, 2, 3, 4]\\n>>> zero_one_loss(y_true, y_pred)\\n0.25\\n>>> zero_one_loss(y_true, y_pred, normalize=False)\\n1.0\\n\\n\\nIn the multilabel case with binary label indicators, where the first label\\nset [0,1] has an error:\\n>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\\n0.5\\n\\n>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)),  normalize=False)\\n1.0\\n\\n\\nExamples\\n\\nSee Recursive feature elimination with cross-validation\\nfor an example of zero one loss usage to perform recursive feature\\nelimination with cross-validation.\\n\\n\\n\\n3.4.4.18. Brier score loss#\\nThe brier_score_loss function computes the\\nBrier score\\nfor binary classes [Brier1950]. Quoting Wikipedia:\\n---------new doc---------\\nExamples\\n\\nSee Recursive feature elimination with cross-validation\\nfor an example of zero one loss usage to perform recursive feature\\nelimination with cross-validation.\\n\\n\\n\\n3.4.4.18. Brier score loss#\\nThe brier_score_loss function computes the\\nBrier score\\nfor binary classes [Brier1950]. Quoting Wikipedia:\\n\\n“The Brier score is a proper score function that measures the accuracy of\\nprobabilistic predictions. It is applicable to tasks in which predictions\\nmust assign probabilities to a set of mutually exclusive discrete outcomes.”\\n\\nThis function returns the mean squared error of the actual outcome\\n\\\\(y \\\\in \\\\{0,1\\\\}\\\\) and the predicted probability estimate\\n\\\\(p = \\\\operatorname{Pr}(y = 1)\\\\) (predict_proba) as outputted by:\\n---------new doc---------\\nThis function returns the mean squared error of the actual outcome\\n\\\\(y \\\\in \\\\{0,1\\\\}\\\\) and the predicted probability estimate\\n\\\\(p = \\\\operatorname{Pr}(y = 1)\\\\) (predict_proba) as outputted by:\\n\\n\\\\[BS = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}} - 1}(y_i - p_i)^2\\\\]\\nThe Brier score loss is also between 0 to 1 and the lower the value (the mean\\nsquare difference is smaller), the more accurate the prediction is.\\nHere is a small example of usage of this function:\\n>>> import numpy as np\\n>>> from sklearn.metrics import brier_score_loss\\n>>> y_true = np.array([0, 1, 1, 0])\\n>>> y_true_categorical = np.array([\\\"spam\\\", \\\"ham\\\", \\\"ham\\\", \\\"spam\\\"])\\n>>> y_prob = np.array([0.1, 0.9, 0.8, 0.4])\\n>>> y_pred = np.array([0, 1, 1, 0])\\n>>> brier_score_loss(y_true, y_prob)\\n0.055\\n>>> brier_score_loss(y_true, 1 - y_prob, pos_label=0)\\n0.055\\n>>> brier_score_loss(y_true_categorical, y_prob, pos_label=\\\"ham\\\")\\n0.055\\n>>> brier_score_loss(y_true, y_prob > 0.5)\\n0.0\\n---------new doc---------\\nThe Brier score can be used to assess how well a classifier is calibrated.\\nHowever, a lower Brier score loss does not always mean a better calibration.\\nThis is because, by analogy with the bias-variance decomposition of the mean\\nsquared error, the Brier score loss can be decomposed as the sum of calibration\\nloss and refinement loss [Bella2012]. Calibration loss is defined as the mean\\nsquared deviation from empirical probabilities derived from the slope of ROC\\nsegments. Refinement loss can be defined as the expected optimal loss as\\nmeasured by the area under the optimal cost curve. Refinement loss can change\\nindependently from calibration loss, thus a lower Brier score loss does not\\nnecessarily mean a better calibrated model. “Only when refinement loss remains\\nthe same does a lower Brier score loss always mean better calibration”\\n[Bella2012], [Flach2008].\\nExamples\\n\\nSee Probability calibration of classifiers\\nfor an example of Brier score loss usage to perform probability\\ncalibration of classifiers.\\n\\nReferences\\n\\n\\n[Brier1950]\\nG. Brier, Verification of forecasts expressed in terms of probability,\\nMonthly weather review 78.1 (1950)\\n\\n\\n[Bella2012]\\n(1,2)\\nBella, Ferri, Hernández-Orallo, and Ramírez-Quintana\\n“Calibration of Machine Learning Models”\\nin Khosrow-Pour, M. “Machine learning: concepts, methodologies, tools\\nand applications.” Hershey, PA: Information Science Reference (2012).\\n---------new doc---------\\n[Bella2012]\\n(1,2)\\nBella, Ferri, Hernández-Orallo, and Ramírez-Quintana\\n“Calibration of Machine Learning Models”\\nin Khosrow-Pour, M. “Machine learning: concepts, methodologies, tools\\nand applications.” Hershey, PA: Information Science Reference (2012).\\n\\n\\n[Flach2008]\\nFlach, Peter, and Edson Matsubara. “On classification, ranking,\\nand probability estimation.”\\nDagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum fr Informatik (2008).\\n\\n\\n\\n\\n3.4.4.19. Class likelihood ratios#\\nThe class_likelihood_ratios function computes the positive and negative\\nlikelihood ratios\\n\\\\(LR_\\\\pm\\\\) for binary classes, which can be interpreted as the ratio of\\npost-test to pre-test odds as explained below. As a consequence, this metric is\\ninvariant w.r.t. the class prevalence (the number of samples in the positive\\nclass divided by the total number of samples) and can be extrapolated between\\npopulations regardless of any possible class imbalance.\\nThe \\\\(LR_\\\\pm\\\\) metrics are therefore very useful in settings where the data\\navailable to learn and evaluate a classifier is a study population with nearly\\nbalanced classes, such as a case-control study, while the target application,\\ni.e. the general population, has very low prevalence.\\nThe positive likelihood ratio \\\\(LR_+\\\\) is the probability of a classifier to\\ncorrectly predict that a sample belongs to the positive class divided by the\\nprobability of predicting the positive class for a sample belonging to the\\nnegative class:\\n---------new doc---------\\n\\\\[LR_+ = \\\\frac{\\\\text{PR}(P+|T+)}{\\\\text{PR}(P+|T-)}.\\\\]\\nThe notation here refers to predicted (\\\\(P\\\\)) or true (\\\\(T\\\\)) label and\\nthe sign \\\\(+\\\\) and \\\\(-\\\\) refer to the positive and negative class,\\nrespectively, e.g. \\\\(P+\\\\) stands for “predicted positive”.\\nAnalogously, the negative likelihood ratio \\\\(LR_-\\\\) is the probability of a\\nsample of the positive class being classified as belonging to the negative class\\ndivided by the probability of a sample of the negative class being correctly\\nclassified:\\n\\n\\\\[LR_- = \\\\frac{\\\\text{PR}(P-|T+)}{\\\\text{PR}(P-|T-)}.\\\\]\\nFor classifiers above chance \\\\(LR_+\\\\) above 1 higher is better, while\\n\\\\(LR_-\\\\) ranges from 0 to 1 and lower is better.\\nValues of \\\\(LR_\\\\pm\\\\approx 1\\\\) correspond to chance level.\\nNotice that probabilities differ from counts, for instance\\n\\\\(\\\\operatorname{PR}(P+|T+)\\\\) is not equal to the number of true positive\\ncounts tp (see the wikipedia page for\\nthe actual formulas).\\nExamples\\n\\nClass Likelihood Ratios to measure classification performance\\n\\n\\n\\nInterpretation across varying prevalence#\\nBoth class likelihood ratios are interpretable in terms of an odds ratio\\n(pre-test and post-tests):\\n\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times \\\\text{pre-test odds}.\\\\]\\nOdds are in general related to probabilities via\\n---------new doc---------\\nClass Likelihood Ratios to measure classification performance\\n\\n\\n\\nInterpretation across varying prevalence#\\nBoth class likelihood ratios are interpretable in terms of an odds ratio\\n(pre-test and post-tests):\\n\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times \\\\text{pre-test odds}.\\\\]\\nOdds are in general related to probabilities via\\n\\n\\\\[\\\\text{odds} = \\\\frac{\\\\text{probability}}{1 - \\\\text{probability}},\\\\]\\nor equivalently\\n\\n\\\\[\\\\text{probability} = \\\\frac{\\\\text{odds}}{1 + \\\\text{odds}}.\\\\]\\nOn a given population, the pre-test probability is given by the prevalence. By\\nconverting odds to probabilities, the likelihood ratios can be translated into a\\nprobability of truly belonging to either class before and after a classifier\\nprediction:\\n\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times\\n\\\\frac{\\\\text{pre-test probability}}{1 - \\\\text{pre-test probability}},\\\\]\\n\\n\\\\[\\\\text{post-test probability} = \\\\frac{\\\\text{post-test odds}}{1 + \\\\text{post-test odds}}.\\\\]\\n---------new doc---------\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times\\n\\\\frac{\\\\text{pre-test probability}}{1 - \\\\text{pre-test probability}},\\\\]\\n\\n\\\\[\\\\text{post-test probability} = \\\\frac{\\\\text{post-test odds}}{1 + \\\\text{post-test odds}}.\\\\]\\n\\n\\n\\nMathematical divergences#\\nThe positive likelihood ratio is undefined when \\\\(fp = 0\\\\), which can be\\ninterpreted as the classifier perfectly identifying positive cases. If \\\\(fp\\n= 0\\\\) and additionally \\\\(tp = 0\\\\), this leads to a zero/zero division. This\\nhappens, for instance, when using a DummyClassifier that always predicts the\\nnegative class and therefore the interpretation as a perfect classifier is lost.\\nThe negative likelihood ratio is undefined when \\\\(tn = 0\\\\). Such divergence\\nis invalid, as \\\\(LR_- > 1\\\\) would indicate an increase in the odds of a\\nsample belonging to the positive class after being classified as negative, as if\\nthe act of classifying caused the positive condition. This includes the case of\\na DummyClassifier that always predicts the positive class (i.e. when\\n\\\\(tn=fn=0\\\\)).\\nBoth class likelihood ratios are undefined when \\\\(tp=fn=0\\\\), which means\\nthat no samples of the positive class were present in the testing set. This can\\nalso happen when cross-validating highly imbalanced data.\\nIn all the previous cases the class_likelihood_ratios function raises by\\ndefault an appropriate warning message and returns nan to avoid pollution when\\naveraging over cross-validation folds.\\nFor a worked-out demonstration of the class_likelihood_ratios function,\\nsee the example below.\\n\\n\\n\\nReferences#\\n---------new doc---------\\nReferences#\\n\\nWikipedia entry for Likelihood ratios in diagnostic testing\\nBrenner, H., & Gefeller, O. (1997).\\nVariation of sensitivity, specificity, likelihood ratios and predictive\\nvalues with disease prevalence.\\nStatistics in medicine, 16(9), 981-991.\\n\\n\\n\\n\\n3.4.4.20. D² score for classification#\\nThe D² score computes the fraction of deviance explained.\\nIt is a generalization of R², where the squared error is generalized and replaced\\nby a classification deviance of choice \\\\(\\\\text{dev}(y, \\\\hat{y})\\\\)\\n(e.g., Log loss). D² is a form of a skill score.\\nIt is calculated as\\n\\n\\\\[D^2(y, \\\\hat{y}) = 1 - \\\\frac{\\\\text{dev}(y, \\\\hat{y})}{\\\\text{dev}(y, y_{\\\\text{null}})} \\\\,.\\\\]\\nWhere \\\\(y_{\\\\text{null}}\\\\) is the optimal prediction of an intercept-only model\\n(e.g., the per-class proportion of y_true in the case of the Log loss).\\nLike R², the best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). A constant model that always predicts\\n\\\\(y_{\\\\text{null}}\\\\), disregarding the input features, would get a D² score\\nof 0.0.\\n\\n\\nD2 log loss score#\\nThe d2_log_loss_score function implements the special case\\nof D² with the log loss, see Log loss, i.e.:\\n---------new doc---------\\n\\\\[\\\\text{dev}(y, \\\\hat{y}) = \\\\text{log_loss}(y, \\\\hat{y}).\\\\]\\nHere are some usage examples of the d2_log_loss_score function:\\n>>> from sklearn.metrics import d2_log_loss_score\\n>>> y_true = [1, 1, 2, 3]\\n>>> y_pred = [\\n...    [0.5, 0.25, 0.25],\\n...    [0.5, 0.25, 0.25],\\n...    [0.5, 0.25, 0.25],\\n...    [0.5, 0.25, 0.25],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n0.0\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [\\n...     [0.98, 0.01, 0.01],\\n...     [0.01, 0.98, 0.01],\\n...     [0.01, 0.01, 0.98],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n0.981...\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [\\n...     [0.1, 0.6, 0.3],\\n...     [0.1, 0.6, 0.3],\\n...     [0.4, 0.5, 0.1],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n---------new doc---------\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [\\n...     [0.1, 0.6, 0.3],\\n...     [0.1, 0.6, 0.3],\\n...     [0.4, 0.5, 0.1],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n-0.552...\\n---------new doc---------\\n3.4.5. Multilabel ranking metrics#\\nIn multilabel learning, each sample can have any number of ground truth labels\\nassociated with it. The goal is to give high scores and better rank to\\nthe ground truth labels.\\n\\n3.4.5.1. Coverage error#\\nThe coverage_error function computes the average number of labels that\\nhave to be included in the final prediction such that all true labels\\nare predicted. This is useful if you want to know how many top-scored-labels\\nyou have to predict in average without missing any true one. The best value\\nof this metrics is thus the average number of true labels.\\n\\nNote\\nOur implementation’s score is 1 greater than the one given in Tsoumakas\\net al., 2010. This extends it to handle the degenerate case in which an\\ninstance has 0 true labels.\\n\\nFormally, given a binary indicator matrix of the ground truth labels\\n\\\\(y \\\\in \\\\left\\\\{0, 1\\\\right\\\\}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\) and the\\nscore associated with each label\\n\\\\(\\\\hat{f} \\\\in \\\\mathbb{R}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\),\\nthe coverage is defined as\\n---------new doc---------\\n\\\\[coverage(y, \\\\hat{f}) = \\\\frac{1}{n_{\\\\text{samples}}}\\n  \\\\sum_{i=0}^{n_{\\\\text{samples}} - 1} \\\\max_{j:y_{ij} = 1} \\\\text{rank}_{ij}\\\\]\\nwith \\\\(\\\\text{rank}_{ij} = \\\\left|\\\\left\\\\{k: \\\\hat{f}_{ik} \\\\geq \\\\hat{f}_{ij} \\\\right\\\\}\\\\right|\\\\).\\nGiven the rank definition, ties in y_scores are broken by giving the\\nmaximal rank that would have been assigned to all tied values.\\nHere is a small example of usage of this function:\\n>>> import numpy as np\\n>>> from sklearn.metrics import coverage_error\\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\\n>>> coverage_error(y_true, y_score)\\n2.5\\n---------new doc---------\\n3.4.5.2. Label ranking average precision#\\nThe label_ranking_average_precision_score function\\nimplements label ranking average precision (LRAP). This metric is linked to\\nthe average_precision_score function, but is based on the notion of\\nlabel ranking instead of precision and recall.\\nLabel ranking average precision (LRAP) averages over the samples the answer to\\nthe following question: for each ground truth label, what fraction of\\nhigher-ranked labels were true labels? This performance measure will be higher\\nif you are able to give better rank to the labels associated with each sample.\\nThe obtained score is always strictly greater than 0, and the best value is 1.\\nIf there is exactly one relevant label per sample, label ranking average\\nprecision is equivalent to the mean\\nreciprocal rank.\\nFormally, given a binary indicator matrix of the ground truth labels\\n\\\\(y \\\\in \\\\left\\\\{0, 1\\\\right\\\\}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\)\\nand the score associated with each label\\n\\\\(\\\\hat{f} \\\\in \\\\mathbb{R}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\),\\nthe average precision is defined as\\n---------new doc---------\\n3.4.5.3. Ranking loss#\\nThe label_ranking_loss function computes the ranking loss which\\naverages over the samples the number of label pairs that are incorrectly\\nordered, i.e. true labels have a lower score than false labels, weighted by\\nthe inverse of the number of ordered pairs of false and true labels.\\nThe lowest achievable ranking loss is zero.\\nFormally, given a binary indicator matrix of the ground truth labels\\n\\\\(y \\\\in \\\\left\\\\{0, 1\\\\right\\\\}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\) and the\\nscore associated with each label\\n\\\\(\\\\hat{f} \\\\in \\\\mathbb{R}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\),\\nthe ranking loss is defined as\\n---------new doc---------\\nCompared with the ranking loss, NDCG can take into account relevance scores,\\nrather than a ground-truth ranking. So if the ground-truth consists only of an\\nordering, the ranking loss should be preferred; if the ground-truth consists of\\nactual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very\\nrelevant), NDCG can be used.\\nFor one sample, given the vector of continuous ground-truth values for each\\ntarget \\\\(y \\\\in \\\\mathbb{R}^{M}\\\\), where \\\\(M\\\\) is the number of outputs, and\\nthe prediction \\\\(\\\\hat{y}\\\\), which induces the ranking function \\\\(f\\\\), the\\nDCG score is\\n---------new doc---------\\n3.4.6. Regression metrics#\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions to measure regression performance. Some of those have been enhanced\\nto handle the multioutput case: mean_squared_error,\\nmean_absolute_error, r2_score,\\nexplained_variance_score, mean_pinball_loss, d2_pinball_score\\nand d2_absolute_error_score.\\nThese functions have a multioutput keyword argument which specifies the\\nway the scores or losses for each individual target should be averaged. The\\ndefault is 'uniform_average', which specifies a uniformly weighted mean\\nover outputs. If an ndarray of shape (n_outputs,) is passed, then its\\nentries are interpreted as weights and an according weighted average is\\nreturned. If multioutput is 'raw_values', then all unaltered\\nindividual scores or losses will be returned in an array of shape\\n(n_outputs,).\\nThe r2_score and explained_variance_score accept an additional\\nvalue 'variance_weighted' for the multioutput parameter. This option\\nleads to a weighting of each individual score by the variance of the\\ncorresponding target variable. This setting quantifies the globally captured\\nunscaled variance. If the target variables are of different scale, then this\\nscore puts more importance on explaining the higher variance variables.\\n---------new doc---------\\n3.4.6.1. R² score, the coefficient of determination#\\nThe r2_score function computes the coefficient of\\ndetermination,\\nusually denoted as \\\\(R^2\\\\).\\nIt represents the proportion of variance (of y) that has been explained by the\\nindependent variables in the model. It provides an indication of goodness of\\nfit and therefore a measure of how well unseen samples are likely to be\\npredicted by the model, through the proportion of explained variance.\\nAs such variance is dataset dependent, \\\\(R^2\\\\) may not be meaningfully comparable\\nacross different datasets. Best possible score is 1.0 and it can be negative\\n(because the model can be arbitrarily worse). A constant model that always\\npredicts the expected (average) value of y, disregarding the input features,\\nwould get an \\\\(R^2\\\\) score of 0.0.\\nNote: when the prediction residuals have zero mean, the \\\\(R^2\\\\) score and\\nthe Explained variance score are identical.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample\\nand \\\\(y_i\\\\) is the corresponding true value for total \\\\(n\\\\) samples,\\nthe estimated \\\\(R^2\\\\) is defined as:\\n---------new doc---------\\n\\\\[R^2(y, \\\\hat{y}) = 1 - \\\\frac{\\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2}{\\\\sum_{i=1}^{n} (y_i - \\\\bar{y})^2}\\\\]\\nwhere \\\\(\\\\bar{y} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} y_i\\\\) and \\\\(\\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2 = \\\\sum_{i=1}^{n} \\\\epsilon_i^2\\\\).\\nNote that r2_score calculates unadjusted \\\\(R^2\\\\) without correcting for\\nbias in sample variance of y.\\nIn the particular case where the true target is constant, the \\\\(R^2\\\\) score is\\nnot finite: it is either NaN (perfect predictions) or -Inf (imperfect\\npredictions). Such non-finite scores may prevent correct model optimization\\nsuch as grid-search cross-validation to be performed correctly. For this reason\\nthe default behaviour of r2_score is to replace them with 1.0 (perfect\\npredictions) or 0.0 (imperfect predictions). If force_finite\\nis set to False, this score falls back on the original \\\\(R^2\\\\) definition.\\nHere is a small example of usage of the r2_score function:\\n>>> from sklearn.metrics import r2_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> r2_score(y_true, y_pred)\\n0.948...\\n---------new doc---------\\nHere is a small example of usage of the r2_score function:\\n>>> from sklearn.metrics import r2_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> r2_score(y_true, y_pred)\\n0.948...\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> r2_score(y_true, y_pred, multioutput='variance_weighted')\\n0.938...\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> r2_score(y_true, y_pred, multioutput='uniform_average')\\n0.936...\\n>>> r2_score(y_true, y_pred, multioutput='raw_values')\\narray([0.965..., 0.908...])\\n>>> r2_score(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.925...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n>>> r2_score(y_true, y_pred)\\n1.0\\n>>> r2_score(y_true, y_pred, force_finite=False)\\nnan\\n>>> y_true = [-2, -2, -2]\\n---------new doc---------\\n0.925...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n>>> r2_score(y_true, y_pred)\\n1.0\\n>>> r2_score(y_true, y_pred, force_finite=False)\\nnan\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2 + 1e-8]\\n>>> r2_score(y_true, y_pred)\\n0.0\\n>>> r2_score(y_true, y_pred, force_finite=False)\\n-inf\\n---------new doc---------\\n\\\\[\\\\text{MAE}(y, \\\\hat{y}) = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1} \\\\left| y_i - \\\\hat{y}_i \\\\right|.\\\\]\\nHere is a small example of usage of the mean_absolute_error function:\\n>>> from sklearn.metrics import mean_absolute_error\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> mean_absolute_error(y_true, y_pred)\\n0.5\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> mean_absolute_error(y_true, y_pred)\\n0.75\\n>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\\narray([0.5, 1. ])\\n>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.85...\\n---------new doc---------\\n3.4.6.3. Mean squared error#\\nThe mean_squared_error function computes mean squared\\nerror, a risk\\nmetric corresponding to the expected value of the squared (quadratic) error or\\nloss.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample,\\nand \\\\(y_i\\\\) is the corresponding true value, then the mean squared error\\n(MSE) estimated over \\\\(n_{\\\\text{samples}}\\\\) is defined as\\n\\n\\\\[\\\\text{MSE}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples} - 1} (y_i - \\\\hat{y}_i)^2.\\\\]\\nHere is a small example of usage of the mean_squared_error\\nfunction:\\n>>> from sklearn.metrics import mean_squared_error\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> mean_squared_error(y_true, y_pred)\\n0.375\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> mean_squared_error(y_true, y_pred)\\n0.7083...\\n\\n\\nExamples\\n\\nSee Gradient Boosting regression\\nfor an example of mean squared error usage to evaluate gradient boosting regression.\\n---------new doc---------\\nThe root mean squared logarithmic error (RMSLE) is available through the\\nroot_mean_squared_log_error function.\\n\\n\\n3.4.6.5. Mean absolute percentage error#\\nThe mean_absolute_percentage_error (MAPE), also known as mean absolute\\npercentage deviation (MAPD), is an evaluation metric for regression problems.\\nThe idea of this metric is to be sensitive to relative errors. It is for example\\nnot changed by a global scaling of the target variable.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample\\nand \\\\(y_i\\\\) is the corresponding true value, then the mean absolute percentage\\nerror (MAPE) estimated over \\\\(n_{\\\\text{samples}}\\\\) is defined as\\n---------new doc---------\\n\\\\[\\\\text{MAPE}(y, \\\\hat{y}) = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1} \\\\frac{{}\\\\left| y_i - \\\\hat{y}_i \\\\right|}{\\\\max(\\\\epsilon, \\\\left| y_i \\\\right|)}\\\\]\\nwhere \\\\(\\\\epsilon\\\\) is an arbitrary small yet strictly positive number to\\navoid undefined results when y is zero.\\nThe mean_absolute_percentage_error function supports multioutput.\\nHere is a small example of usage of the mean_absolute_percentage_error\\nfunction:\\n>>> from sklearn.metrics import mean_absolute_percentage_error\\n>>> y_true = [1, 10, 1e6]\\n>>> y_pred = [0.9, 15, 1.2e6]\\n>>> mean_absolute_percentage_error(y_true, y_pred)\\n0.2666...\\n\\n\\nIn above example, if we had used mean_absolute_error, it would have ignored\\nthe small magnitude values and only reflected the error in prediction of highest\\nmagnitude value. But that problem is resolved in case of MAPE because it calculates\\nrelative percentage error with respect to actual output.\\n---------new doc---------\\nIn above example, if we had used mean_absolute_error, it would have ignored\\nthe small magnitude values and only reflected the error in prediction of highest\\nmagnitude value. But that problem is resolved in case of MAPE because it calculates\\nrelative percentage error with respect to actual output.\\n\\nNote\\nThe MAPE formula here does not represent the common “percentage” definition: the\\npercentage in the range [0, 100] is converted to a relative value in the range [0,\\n1] by dividing by 100. Thus, an error of 200% corresponds to a relative error of 2.\\nThe motivation here is to have a range of values that is more consistent with other\\nerror metrics in scikit-learn, such as accuracy_score.\\nTo obtain the mean absolute percentage error as per the Wikipedia formula,\\nmultiply the mean_absolute_percentage_error computed here by 100.\\n\\n\\n\\nReferences#\\n\\nWikipedia entry for Mean Absolute Percentage Error\\n\\n\\n\\n\\n3.4.6.6. Median absolute error#\\nThe median_absolute_error is particularly interesting because it is\\nrobust to outliers. The loss is calculated by taking the median of all absolute\\ndifferences between the target and the prediction.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample\\nand \\\\(y_i\\\\) is the corresponding true value, then the median absolute error\\n(MedAE) estimated over \\\\(n_{\\\\text{samples}}\\\\) is defined as\\n---------new doc---------\\n\\\\[\\\\text{MedAE}(y, \\\\hat{y}) = \\\\text{median}(\\\\mid y_1 - \\\\hat{y}_1 \\\\mid, \\\\ldots, \\\\mid y_n - \\\\hat{y}_n \\\\mid).\\\\]\\nThe median_absolute_error does not support multioutput.\\nHere is a small example of usage of the median_absolute_error\\nfunction:\\n>>> from sklearn.metrics import median_absolute_error\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> median_absolute_error(y_true, y_pred)\\n0.5\\n\\n\\n\\n\\n3.4.6.7. Max error#\\nThe max_error function computes the maximum residual error , a metric\\nthat captures the worst case error between the predicted value and\\nthe true value. In a perfectly fitted single output regression\\nmodel, max_error would be 0 on the training set and though this\\nwould be highly unlikely in the real world, this metric shows the\\nextent of error that the model had when it was fitted.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample,\\nand \\\\(y_i\\\\) is the corresponding true value, then the max error is\\ndefined as\\n---------new doc---------\\n\\\\[\\\\text{Max Error}(y, \\\\hat{y}) = \\\\max(| y_i - \\\\hat{y}_i |)\\\\]\\nHere is a small example of usage of the max_error function:\\n>>> from sklearn.metrics import max_error\\n>>> y_true = [3, 2, 7, 1]\\n>>> y_pred = [9, 2, 7, 1]\\n>>> max_error(y_true, y_pred)\\n6\\n\\n\\nThe max_error does not support multioutput.\\n\\n\\n3.4.6.8. Explained variance score#\\nThe explained_variance_score computes the explained variance\\nregression score.\\nIf \\\\(\\\\hat{y}\\\\) is the estimated target output, \\\\(y\\\\) the corresponding\\n(correct) target output, and \\\\(Var\\\\) is Variance, the square of the standard deviation,\\nthen the explained variance is estimated as follow:\\n\\n\\\\[explained\\\\_{}variance(y, \\\\hat{y}) = 1 - \\\\frac{Var\\\\{ y - \\\\hat{y}\\\\}}{Var\\\\{y\\\\}}\\\\]\\nThe best possible score is 1.0, lower values are worse.\\n\\nLink to R² score, the coefficient of determination\\nThe difference between the explained variance score and the R² score, the coefficient of determination\\nis that the explained variance score does not account for\\nsystematic offset in the prediction. For this reason, the\\nR² score, the coefficient of determination should be preferred in general.\\n---------new doc---------\\nIn the particular case where the true target is constant, the Explained\\nVariance score is not finite: it is either NaN (perfect predictions) or\\n-Inf (imperfect predictions). Such non-finite scores may prevent correct\\nmodel optimization such as grid-search cross-validation to be performed\\ncorrectly. For this reason the default behaviour of\\nexplained_variance_score is to replace them with 1.0 (perfect\\npredictions) or 0.0 (imperfect predictions). You can set the force_finite\\nparameter to False to prevent this fix from happening and fallback on the\\noriginal Explained Variance score.\\nHere is a small example of usage of the explained_variance_score\\nfunction:\\n>>> from sklearn.metrics import explained_variance_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> explained_variance_score(y_true, y_pred)\\n0.957...\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> explained_variance_score(y_true, y_pred, multioutput='raw_values')\\narray([0.967..., 1.        ])\\n>>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.990...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n---------new doc---------\\narray([0.967..., 1.        ])\\n>>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.990...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n>>> explained_variance_score(y_true, y_pred)\\n1.0\\n>>> explained_variance_score(y_true, y_pred, force_finite=False)\\nnan\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2 + 1e-8]\\n>>> explained_variance_score(y_true, y_pred)\\n0.0\\n>>> explained_variance_score(y_true, y_pred, force_finite=False)\\n-inf\\n---------new doc---------\\n3.4.6.9. Mean Poisson, Gamma, and Tweedie deviances#\\nThe mean_tweedie_deviance function computes the mean Tweedie\\ndeviance error\\nwith a power parameter (\\\\(p\\\\)). This is a metric that elicits\\npredicted expectation values of regression targets.\\nFollowing special cases exist,\\n\\nwhen power=0 it is equivalent to mean_squared_error.\\nwhen power=1 it is equivalent to mean_poisson_deviance.\\nwhen power=2 it is equivalent to mean_gamma_deviance.\\n\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample,\\nand \\\\(y_i\\\\) is the corresponding true value, then the mean Tweedie\\ndeviance error (D) for power \\\\(p\\\\), estimated over \\\\(n_{\\\\text{samples}}\\\\)\\nis defined as\\n---------new doc---------\\n\\\\[\\\\begin{split}\\\\text{D}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}}\\n\\\\sum_{i=0}^{n_\\\\text{samples} - 1}\\n\\\\begin{cases}\\n(y_i-\\\\hat{y}_i)^2, & \\\\text{for }p=0\\\\text{ (Normal)}\\\\\\\\\\n2(y_i \\\\log(y_i/\\\\hat{y}_i) + \\\\hat{y}_i - y_i),  & \\\\text{for }p=1\\\\text{ (Poisson)}\\\\\\\\\\n2(\\\\log(\\\\hat{y}_i/y_i) + y_i/\\\\hat{y}_i - 1),  & \\\\text{for }p=2\\\\text{ (Gamma)}\\\\\\\\\\n2\\\\left(\\\\frac{\\\\max(y_i,0)^{2-p}}{(1-p)(2-p)}-\\n\\\\frac{y_i\\\\,\\\\hat{y}_i^{1-p}}{1-p}+\\\\frac{\\\\hat{y}_i^{2-p}}{2-p}\\\\right),\\n& \\\\text{otherwise}\\n\\\\end{cases}\\\\end{split}\\\\]\\nTweedie deviance is a homogeneous function of degree 2-power.\\nThus, Gamma distribution with power=2 means that simultaneously scaling\\ny_true and y_pred has no effect on the deviance. For Poisson\\ndistribution power=1 the deviance scales linearly, and for Normal\\ndistribution (power=0), quadratically.  In general, the higher\\npower the less weight is given to extreme deviations between true\\nand predicted targets.\\n---------new doc---------\\nTweedie deviance is a homogeneous function of degree 2-power.\\nThus, Gamma distribution with power=2 means that simultaneously scaling\\ny_true and y_pred has no effect on the deviance. For Poisson\\ndistribution power=1 the deviance scales linearly, and for Normal\\ndistribution (power=0), quadratically.  In general, the higher\\npower the less weight is given to extreme deviations between true\\nand predicted targets.\\nFor instance, let’s compare the two predictions 1.5 and 150 that are both\\n50% larger than their corresponding true value.\\nThe mean squared error (power=0) is very sensitive to the\\nprediction difference of the second point,:\\n>>> from sklearn.metrics import mean_tweedie_deviance\\n>>> mean_tweedie_deviance([1.0], [1.5], power=0)\\n0.25\\n>>> mean_tweedie_deviance([100.], [150.], power=0)\\n2500.0\\n---------new doc---------\\nIf we increase power to 1,:\\n>>> mean_tweedie_deviance([1.0], [1.5], power=1)\\n0.18...\\n>>> mean_tweedie_deviance([100.], [150.], power=1)\\n18.9...\\n\\n\\nthe difference in errors decreases. Finally, by setting, power=2:\\n>>> mean_tweedie_deviance([1.0], [1.5], power=2)\\n0.14...\\n>>> mean_tweedie_deviance([100.], [150.], power=2)\\n0.14...\\n\\n\\nwe would get identical errors. The deviance when power=2 is thus only\\nsensitive to relative errors.\\n\\n\\n3.4.6.10. Pinball loss#\\nThe mean_pinball_loss function is used to evaluate the predictive\\nperformance of quantile regression models.\\n---------new doc---------\\n3.4.6.10. Pinball loss#\\nThe mean_pinball_loss function is used to evaluate the predictive\\nperformance of quantile regression models.\\n\\n\\\\[\\\\text{pinball}(y, \\\\hat{y}) = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1}  \\\\alpha \\\\max(y_i - \\\\hat{y}_i, 0) + (1 - \\\\alpha) \\\\max(\\\\hat{y}_i - y_i, 0)\\\\]\\nThe value of pinball loss is equivalent to half of mean_absolute_error when the quantile\\nparameter alpha is set to 0.5.\\nHere is a small example of usage of the mean_pinball_loss function:\\n>>> from sklearn.metrics import mean_pinball_loss\\n>>> y_true = [1, 2, 3]\\n>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\\n0.03...\\n>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\\n0.3...\\n>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\\n0.3...\\n>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\\n0.03...\\n>>> mean_pinball_loss(y_true, y_true, alpha=0.1)\\n0.0\\n>>> mean_pinball_loss(y_true, y_true, alpha=0.9)\\n0.0\\n---------new doc---------\\nIt is possible to build a scorer object with a specific choice of alpha:\\n>>> from sklearn.metrics import make_scorer\\n>>> mean_pinball_loss_95p = make_scorer(mean_pinball_loss, alpha=0.95)\\n\\n\\nSuch a scorer can be used to evaluate the generalization performance of a\\nquantile regressor via cross-validation:\\n>>> from sklearn.datasets import make_regression\\n>>> from sklearn.model_selection import cross_val_score\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>>\\n>>> X, y = make_regression(n_samples=100, random_state=0)\\n>>> estimator = GradientBoostingRegressor(\\n...     loss=\\\"quantile\\\",\\n...     alpha=0.95,\\n...     random_state=0,\\n... )\\n>>> cross_val_score(estimator, X, y, cv=5, scoring=mean_pinball_loss_95p)\\narray([13.6..., 9.7..., 23.3..., 9.5..., 10.4...])\\n\\n\\nIt is also possible to build scorer objects for hyper-parameter tuning. The\\nsign of the loss must be switched to ensure that greater means better as\\nexplained in the example linked below.\\nExamples\\n\\nSee Prediction Intervals for Gradient Boosting Regression\\nfor an example of using the pinball loss to evaluate and tune the\\nhyper-parameters of quantile regression models on data with non-symmetric\\nnoise and outliers.\\n---------new doc---------\\nIt is also possible to build scorer objects for hyper-parameter tuning. The\\nsign of the loss must be switched to ensure that greater means better as\\nexplained in the example linked below.\\nExamples\\n\\nSee Prediction Intervals for Gradient Boosting Regression\\nfor an example of using the pinball loss to evaluate and tune the\\nhyper-parameters of quantile regression models on data with non-symmetric\\nnoise and outliers.\\n\\n\\n\\n3.4.6.11. D² score#\\nThe D² score computes the fraction of deviance explained.\\nIt is a generalization of R², where the squared error is generalized and replaced\\nby a deviance of choice \\\\(\\\\text{dev}(y, \\\\hat{y})\\\\)\\n(e.g., Tweedie, pinball or mean absolute error). D² is a form of a skill score.\\nIt is calculated as\\n\\n\\\\[D^2(y, \\\\hat{y}) = 1 - \\\\frac{\\\\text{dev}(y, \\\\hat{y})}{\\\\text{dev}(y, y_{\\\\text{null}})} \\\\,.\\\\]\\nWhere \\\\(y_{\\\\text{null}}\\\\) is the optimal prediction of an intercept-only model\\n(e.g., the mean of y_true for the Tweedie case, the median for absolute\\nerror and the alpha-quantile for pinball loss).\\nLike R², the best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). A constant model that always predicts\\n\\\\(y_{\\\\text{null}}\\\\), disregarding the input features, would get a D² score\\nof 0.0.\\n---------new doc---------\\nD² Tweedie score#\\nThe d2_tweedie_score function implements the special case of D²\\nwhere \\\\(\\\\text{dev}(y, \\\\hat{y})\\\\) is the Tweedie deviance, see Mean Poisson, Gamma, and Tweedie deviances.\\nIt is also known as D² Tweedie and is related to McFadden’s likelihood ratio index.\\nThe argument power defines the Tweedie power as for\\nmean_tweedie_deviance. Note that for power=0,\\nd2_tweedie_score equals r2_score (for single targets).\\nA scorer object with a specific choice of power can be built by:\\n>>> from sklearn.metrics import d2_tweedie_score, make_scorer\\n>>> d2_tweedie_score_15 = make_scorer(d2_tweedie_score, power=1.5)\\n\\n\\n\\n\\n\\nD² pinball score#\\nThe d2_pinball_score function implements the special case\\nof D² with the pinball loss, see Pinball loss, i.e.:\\n---------new doc---------\\nD² pinball score#\\nThe d2_pinball_score function implements the special case\\nof D² with the pinball loss, see Pinball loss, i.e.:\\n\\n\\\\[\\\\text{dev}(y, \\\\hat{y}) = \\\\text{pinball}(y, \\\\hat{y}).\\\\]\\nThe argument alpha defines the slope of the pinball loss as for\\nmean_pinball_loss (Pinball loss). It determines the\\nquantile level alpha for which the pinball loss and also D²\\nare optimal. Note that for alpha=0.5 (the default) d2_pinball_score\\nequals d2_absolute_error_score.\\nA scorer object with a specific choice of alpha can be built by:\\n>>> from sklearn.metrics import d2_pinball_score, make_scorer\\n>>> d2_pinball_score_08 = make_scorer(d2_pinball_score, alpha=0.8)\\n\\n\\n\\n\\n\\nD² absolute error score#\\nThe d2_absolute_error_score function implements the special case of\\nthe Mean absolute error:\\n---------new doc---------\\nD² absolute error score#\\nThe d2_absolute_error_score function implements the special case of\\nthe Mean absolute error:\\n\\n\\\\[\\\\text{dev}(y, \\\\hat{y}) = \\\\text{MAE}(y, \\\\hat{y}).\\\\]\\nHere are some usage examples of the d2_absolute_error_score function:\\n>>> from sklearn.metrics import d2_absolute_error_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> d2_absolute_error_score(y_true, y_pred)\\n0.764...\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [1, 2, 3]\\n>>> d2_absolute_error_score(y_true, y_pred)\\n1.0\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [2, 2, 2]\\n>>> d2_absolute_error_score(y_true, y_pred)\\n0.0\\n\\n\\n\\n\\n\\n3.4.6.12. Visual evaluation of regression models#\\nAmong methods to assess the quality of regression models, scikit-learn provides\\nthe PredictionErrorDisplay class. It allows to\\nvisually inspect the prediction errors of a model in two different manners.\\n---------new doc---------\\nThe plot on the left shows the actual values vs predicted values. For a\\nnoise-free regression task aiming to predict the (conditional) expectation of\\ny, a perfect regression model would display data points on the diagonal\\ndefined by predicted equal to actual values. The further away from this optimal\\nline, the larger the error of the model. In a more realistic setting with\\nirreducible noise, that is, when not all the variations of y can be explained\\nby features in X, then the best model would lead to a cloud of points densely\\narranged around the diagonal.\\nNote that the above only holds when the predicted values is the expected value\\nof y given X. This is typically the case for regression models that\\nminimize the mean squared error objective function or more generally the\\nmean Tweedie deviance for any value of its\\n“power” parameter.\\nWhen plotting the predictions of an estimator that predicts a quantile\\nof y given X, e.g. QuantileRegressor\\nor any other model minimizing the pinball loss, a\\nfraction of the points are either expected to lie above or below the diagonal\\ndepending on the estimated quantile level.\\nAll in all, while intuitive to read, this plot does not really inform us on\\nwhat to do to obtain a better model.\\nThe right-hand side plot shows the residuals (i.e. the difference between the\\nactual and the predicted values) vs. the predicted values.\\nThis plot makes it easier to visualize if the residuals follow and\\nhomoscedastic or heteroschedastic\\ndistribution.\\nIn particular, if the true distribution of y|X is Poisson or Gamma\\ndistributed, it is expected that the variance of the residuals of the optimal\\nmodel would grow with the predicted value of E[y|X] (either linearly for\\n---------new doc---------\\nactual and the predicted values) vs. the predicted values.\\nThis plot makes it easier to visualize if the residuals follow and\\nhomoscedastic or heteroschedastic\\ndistribution.\\nIn particular, if the true distribution of y|X is Poisson or Gamma\\ndistributed, it is expected that the variance of the residuals of the optimal\\nmodel would grow with the predicted value of E[y|X] (either linearly for\\nPoisson or quadratically for Gamma).\\nWhen fitting a linear least squares regression model (see\\nLinearRegression and\\nRidge), we can use this plot to check\\nif some of the model assumptions\\nare met, in particular that the residuals should be uncorrelated, their\\nexpected value should be null and that their variance should be constant\\n(homoschedasticity).\\nIf this is not the case, and in particular if the residuals plot show some\\nbanana-shaped structure, this is a hint that the model is likely mis-specified\\nand that non-linear feature engineering or switching to a non-linear regression\\nmodel might be useful.\\nRefer to the example below to see a model evaluation that makes use of this\\ndisplay.\\nExamples\\n---------new doc---------\\nExample of permutation feature importance using multiple scorers#\\nIn the example below we use a list of metrics, but more input formats are\\npossible, as documented in Using multiple metric evaluation.\\n>>> scoring = ['r2', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error']\\n>>> r_multi = permutation_importance(\\n...     model, X_val, y_val, n_repeats=30, random_state=0, scoring=scoring)\\n...\\n>>> for metric in r_multi:\\n...     print(f\\\"{metric}\\\")\\n...     r = r_multi[metric]\\n...     for i in r.importances_mean.argsort()[::-1]:\\n...         if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\\n...             print(f\\\"    {diabetes.feature_names[i]:<8}\\\"\\n...                   f\\\"{r.importances_mean[i]:.3f}\\\"\\n...                   f\\\" +/- {r.importances_std[i]:.3f}\\\")\\n...\\nr2\\n    s5      0.204 +/- 0.050\\n    bmi     0.176 +/- 0.048\\n    bp      0.088 +/- 0.033\\n---------new doc---------\\n...                   f\\\" +/- {r.importances_std[i]:.3f}\\\")\\n...\\nr2\\n    s5      0.204 +/- 0.050\\n    bmi     0.176 +/- 0.048\\n    bp      0.088 +/- 0.033\\n    sex     0.056 +/- 0.023\\nneg_mean_absolute_percentage_error\\n    s5      0.081 +/- 0.020\\n    bmi     0.064 +/- 0.015\\n    bp      0.029 +/- 0.010\\nneg_mean_squared_error\\n    s5      1013.866 +/- 246.445\\n    bmi     872.726 +/- 240.298\\n    bp      438.663 +/- 163.022\\n    sex     277.376 +/- 115.123\\n---------new doc---------\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: 0.61\\n>>> raw_target_regr = LinearRegression().fit(X_train, y_train)\\n>>> print('R2 score: {0:.2f}'.format(raw_target_regr.score(X_test, y_test)))\\nR2 score: 0.59\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Your job is to give a detailed summary of the following documents:\\n\\nDocuments:\\nPDF of a random variable Y following Poisson, Tweedie (power=1.5) and Gamma\\ndistributions with different mean values (\\\\(\\\\mu\\\\)). Observe the point\\nmass at \\\\(Y=0\\\\) for the Poisson distribution and the Tweedie (power=1.5)\\ndistribution, but not for the Gamma distribution which has a strictly\\npositive target domain.#\\n\\n\\nThe Bernoulli distribution is a discrete probability distribution modelling a\\nBernoulli trial - an event that has only two mutually exclusive outcomes.\\nThe Categorical distribution is a generalization of the Bernoulli distribution\\nfor a categorical random variable. While a random variable in a Bernoulli\\ndistribution has two possible outcomes, a Categorical random variable can take\\non one of K possible categories, with the probability of each category\\nspecified separately.\\nThe choice of the distribution depends on the problem at hand:\\n\\nIf the target values \\\\(y\\\\) are counts (non-negative integer valued) or\\nrelative frequencies (non-negative), you might use a Poisson distribution\\nwith a log-link.\\nIf the target values are positive valued and skewed, you might try a Gamma\\ndistribution with a log-link.\\nIf the target values seem to be heavier tailed than a Gamma distribution, you\\nmight try an Inverse Gaussian distribution (or even higher variance powers of\\nthe Tweedie family).\\nIf the target values \\\\(y\\\\) are probabilities, you can use the Bernoulli\\ndistribution. The Bernoulli distribution with a logit link can be used for\\nbinary classification. The Categorical distribution with a softmax link can be\\nused for multiclass classification.\\n\\n\\n\\nExamples of use cases#\\n---------new doc---------\\nExamples of use cases#\\n\\nAgriculture / weather modeling:  number of rain events per year (Poisson),\\namount of rainfall per event (Gamma), total rainfall per year (Tweedie /\\nCompound Poisson Gamma).\\nRisk modeling / insurance policy pricing:  number of claim events /\\npolicyholder per year (Poisson), cost per event (Gamma), total cost per\\npolicyholder per year (Tweedie / Compound Poisson Gamma).\\nCredit Default: probability that a loan can’t be paid back (Bernoulli).\\nFraud Detection: probability that a financial transaction like a cash transfer\\nis a fraudulent transaction (Bernoulli).\\nPredictive maintenance: number of production interruption events per year\\n(Poisson), duration of interruption (Gamma), total interruption time per year\\n(Tweedie / Compound Poisson Gamma).\\nMedical Drug Testing: probability of curing a patient in a set of trials or\\nprobability that a patient will experience side effects (Bernoulli).\\nNews Classification: classification of news articles into three categories\\nnamely Business News, Politics and Entertainment news (Categorical).\\n\\n\\nReferences\\n\\n\\n[10]\\nMcCullagh, Peter; Nelder, John (1989). Generalized Linear Models,\\nSecond Edition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.\\n\\n\\n[11]\\nJørgensen, B. (1992). The theory of exponential dispersion models\\nand analysis of deviance. Monografias de matemática, no. 51.  See also\\nExponential dispersion model.\\n---------new doc---------\\nExamples\\n\\nPoisson regression and non-normal loss\\nTweedie regression on insurance claims\\n\\n\\n\\nPractical considerations#\\nThe feature matrix X should be standardized before fitting. This ensures\\nthat the penalty treats features equally.\\nSince the linear predictor \\\\(Xw\\\\) can be negative and Poisson,\\nGamma and Inverse Gaussian distributions don’t support negative values, it\\nis necessary to apply an inverse link function that guarantees the\\nnon-negativeness. For example with link='log', the inverse link function\\nbecomes \\\\(h(Xw)=\\\\exp(Xw)\\\\).\\nIf you want to model a relative frequency, i.e. counts per exposure (time,\\nvolume, …) you can do so by using a Poisson distribution and passing\\n\\\\(y=\\\\frac{\\\\mathrm{counts}}{\\\\mathrm{exposure}}\\\\) as target values\\ntogether with \\\\(\\\\mathrm{exposure}\\\\) as sample weights. For a concrete\\nexample see e.g.\\nTweedie regression on insurance claims.\\nWhen performing cross-validation for the power parameter of\\nTweedieRegressor, it is advisable to specify an explicit scoring function,\\nbecause the default scorer TweedieRegressor.score is a function of\\npower itself.\\n---------new doc---------\\nPeter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale\\nestimates, p. 172.\\n\\n\\nThe HuberRegressor differs from using SGDRegressor with loss set to huber\\nin the following ways.\\n\\nHuberRegressor is scaling invariant. Once epsilon is set, scaling X and y\\ndown or up by different values would produce the same robustness to outliers as before.\\nas compared to SGDRegressor where epsilon has to be set again when X and y are\\nscaled.\\nHuberRegressor should be more efficient to use on data with small number of\\nsamples while SGDRegressor needs a number of passes on the training data to\\nproduce the same robustness.\\n\\nNote that this estimator is different from the R implementation of Robust\\nRegression  because the R\\nimplementation does a weighted least squares implementation with weights given to each\\nsample on the basis of how much the residual is greater than a certain threshold.\\n\\n\\n\\n1.1.17. Quantile Regression#\\nQuantile regression estimates the median or other quantiles of \\\\(y\\\\)\\nconditional on \\\\(X\\\\), while ordinary least squares (OLS) estimates the\\nconditional mean.\\nQuantile regression may be useful if one is interested in predicting an\\ninterval instead of point prediction. Sometimes, prediction intervals are\\ncalculated based on the assumption that prediction error is distributed\\nnormally with zero mean and constant variance. Quantile regression provides\\nsensible prediction intervals even for errors with non-constant (but\\npredictable) variance or non-normal distribution.\\n---------new doc---------\\nBased on minimizing the pinball loss, conditional quantiles can also be\\nestimated by models other than linear models. For example,\\nGradientBoostingRegressor can predict conditional\\nquantiles if its parameter loss is set to \\\"quantile\\\" and parameter\\nalpha is set to the quantile that should be predicted. See the example in\\nPrediction Intervals for Gradient Boosting Regression.\\nMost implementations of quantile regression are based on linear programming\\nproblem. The current implementation is based on\\nscipy.optimize.linprog.\\nExamples\\n\\nQuantile regression\\n\\n\\n\\nMathematical details#\\nAs a linear model, the QuantileRegressor gives linear predictions\\n\\\\(\\\\hat{y}(w, X) = Xw\\\\) for the \\\\(q\\\\)-th quantile, \\\\(q \\\\in (0, 1)\\\\).\\nThe weights or coefficients \\\\(w\\\\) are then found by the following\\nminimization problem:\\n\\n\\\\[\\\\min_{w} {\\\\frac{1}{n_{\\\\text{samples}}}\\n\\\\sum_i PB_q(y_i - X_i w) + \\\\alpha ||w||_1}.\\\\]\\nThis consists of the pinball loss (also known as linear loss),\\nsee also mean_pinball_loss,\\n---------new doc---------\\n... stratify=y, test_size=0.7, random_state=42)\\n>>> nca = NeighborhoodComponentsAnalysis(random_state=42)\\n>>> knn = KNeighborsClassifier(n_neighbors=3)\\n>>> nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\\n>>> nca_pipe.fit(X_train, y_train)\\nPipeline(...)\\n>>> print(nca_pipe.score(X_test, y_test))\\n0.96190476...\\n---------new doc---------\\nReferences#\\n\\nRennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\\nTackling the poor assumptions of naive bayes text classifiers.\\nIn ICML (Vol. 3, pp. 616-623).\\n\\n\\n\\n\\n1.9.4. Bernoulli Naive Bayes#\\nBernoulliNB implements the naive Bayes training and classification\\nalgorithms for data that is distributed according to multivariate Bernoulli\\ndistributions; i.e., there may be multiple features but each one is assumed\\nto be a binary-valued (Bernoulli, boolean) variable.\\nTherefore, this class requires samples to be represented as binary-valued\\nfeature vectors; if handed any other kind of data, a BernoulliNB instance\\nmay binarize its input (depending on the binarize parameter).\\nThe decision rule for Bernoulli naive Bayes is based on\\n\\n\\\\[P(x_i \\\\mid y) = P(x_i = 1 \\\\mid y) x_i + (1 - P(x_i = 1 \\\\mid y)) (1 - x_i)\\\\]\\nwhich differs from multinomial NB’s rule\\nin that it explicitly penalizes the non-occurrence of a feature \\\\(i\\\\)\\nthat is an indicator for class \\\\(y\\\\),\\nwhere the multinomial variant would simply ignore a non-occurring feature.\\nIn the case of text classification, word occurrence vectors (rather than word\\ncount vectors) may be used to train and use this classifier. BernoulliNB\\nmight perform better on some datasets, especially those with shorter documents.\\nIt is advisable to evaluate both models, if time permits.\\n\\n\\nReferences#\\n---------new doc---------\\n>>> X, y = make_hastie_10_2(random_state=0)\\n>>> X_train, X_test = X[:2000], X[2000:]\\n>>> y_train, y_test = y[:2000], y[2000:]\\n\\n>>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\\n>>> clf.score(X_test, y_test)\\n0.8965\\n\\n\\nAvailable losses for regression are:\\n\\n‘squared_error’, which is the default loss;\\n‘absolute_error’, which is less sensitive to outliers than the squared error;\\n‘gamma’, which is well suited to model strictly positive outcomes;\\n‘poisson’, which is well suited to model counts and frequencies;\\n‘quantile’, which allows for estimating a conditional quantile that can later\\nbe used to obtain prediction intervals.\\n---------new doc---------\\nNote\\nFor some losses, e.g. 'absolute_error' where the gradients\\nare \\\\(\\\\pm 1\\\\), the values predicted by a fitted \\\\(h_m\\\\) are not\\naccurate enough: the tree can only output integer values. As a result, the\\nleaves values of the tree \\\\(h_m\\\\) are modified once the tree is\\nfitted, such that the leaves values minimize the loss \\\\(L_m\\\\). The\\nupdate is loss-dependent: for the absolute error loss, the value of\\na leaf is updated to the median of the samples in that leaf.\\n---------new doc---------\\n1.11.1.2.4. Loss Functions#\\nThe following loss functions are supported and can be specified using\\nthe parameter loss:\\n\\n\\nRegression#\\n\\nSquared error ('squared_error'): The natural choice for regression\\ndue to its superior computational properties. The initial model is\\ngiven by the mean of the target values.\\nAbsolute error ('absolute_error'): A robust loss function for\\nregression. The initial model is given by the median of the\\ntarget values.\\nHuber ('huber'): Another robust loss function that combines\\nleast squares and least absolute deviation; use alpha to\\ncontrol the sensitivity with regards to outliers (see [Friedman2001] for\\nmore details).\\nQuantile ('quantile'): A loss function for quantile regression.\\nUse 0 < alpha < 1 to specify the quantile. This loss function\\ncan be used to create prediction intervals\\n(see Prediction Intervals for Gradient Boosting Regression).\\n\\n\\n\\n\\nClassification#\\n---------new doc---------\\n1.11.4.3. Weighted Average Probabilities (Soft Voting)#\\nIn contrast to majority voting (hard voting), soft voting\\nreturns the class label as argmax of the sum of predicted probabilities.\\nSpecific weights can be assigned to each classifier via the weights\\nparameter. When weights are provided, the predicted class probabilities\\nfor each classifier are collected, multiplied by the classifier weight,\\nand averaged. The final class label is then derived from the class label\\nwith the highest average probability.\\nTo illustrate this with a simple example, let’s assume we have 3\\nclassifiers and a 3-class classification problems where we assign\\nequal weights to all classifiers: w1=1, w2=1, w3=1.\\nThe weighted average probabilities for a sample would then be\\ncalculated as follows:\\n\\n\\nclassifier\\nclass 1\\nclass 2\\nclass 3\\n\\n\\n\\nclassifier 1\\nw1 * 0.2\\nw1 * 0.5\\nw1 * 0.3\\n\\nclassifier 2\\nw2 * 0.6\\nw2 * 0.3\\nw2 * 0.1\\n\\nclassifier 3\\nw3 * 0.3\\nw3 * 0.4\\nw3 * 0.3\\n\\nweighted average\\n0.37\\n0.4\\n0.23\\n---------new doc---------\\n...                                             metric='euclidean'))],\\n...     final_estimator=final_layer\\n... )\\n>>> multi_layer_regressor.fit(X_train, y_train)\\nStackingRegressor(...)\\n>>> print('R2 score: {:.2f}'\\n...       .format(multi_layer_regressor.score(X_test, y_test)))\\nR2 score: 0.53\\n---------new doc---------\\nFor more information about LabelBinarizer,\\nrefer to Transforming the prediction target (y).\\n---------new doc---------\\nOneVsRestClassifier also supports multilabel\\nclassification. To use this feature, feed the classifier an indicator matrix,\\nin which cell [i, j] indicates the presence of label j in sample i.\\n\\n\\n\\n\\nExamples\\n\\nMultilabel classification\\nPlot classification probability\\n---------new doc---------\\nThese objects take as input a scoring function that returns univariate scores\\nand p-values (or only scores for SelectKBest and\\nSelectPercentile):\\n\\nFor regression: r_regression, f_regression, mutual_info_regression\\nFor classification: chi2, f_classif, mutual_info_classif\\n\\nThe methods based on F-test estimate the degree of linear dependency between\\ntwo random variables. On the other hand, mutual information methods can capture\\nany kind of statistical dependency, but being nonparametric, they require more\\nsamples for accurate estimation. Note that the \\\\(\\\\chi^2\\\\)-test should only be\\napplied to non-negative features, such as frequencies.\\n\\nFeature selection with sparse data\\nIf you use sparse data (i.e. data represented as sparse matrices),\\nchi2, mutual_info_regression, mutual_info_classif\\nwill deal with the data without making it dense.\\n\\n\\nWarning\\nBeware not to use a regression scoring function with a classification\\nproblem, you will get useless results.\\n\\n\\nNote\\nThe SelectPercentile and SelectKBest support unsupervised\\nfeature selection as well. One needs to provide a score_func where y=None.\\nThe score_func should use internally X to compute the scores.\\n\\nExamples\\n\\nUnivariate Feature Selection\\nComparison of F-test and mutual information\\n---------new doc---------\\nNote\\nStrictly proper scoring rules for probabilistic predictions like\\nsklearn.metrics.brier_score_loss and\\nsklearn.metrics.log_loss assess calibration (reliability) and\\ndiscriminative power (resolution) of a model, as well as the randomness of the data\\n(uncertainty) at the same time. This follows from the well-known Brier score\\ndecomposition of Murphy [1]. As it is not clear which term dominates, the score is\\nof limited use for assessing calibration alone (unless one computes each term of\\nthe decomposition). A lower Brier loss, for instance, does not necessarily\\nmean a better calibrated model, it could also mean a worse calibrated model with much\\nmore discriminatory power, e.g. using many more features.\\n---------new doc---------\\n1.16.1. Calibration curves#\\nCalibration curves, also referred to as reliability diagrams (Wilks 1995 [2]),\\ncompare how well the probabilistic predictions of a binary classifier are calibrated.\\nIt plots the frequency of the positive label (to be more precise, an estimation of the\\nconditional event probability \\\\(P(Y=1|\\\\text{predict_proba})\\\\)) on the y-axis\\nagainst the predicted probability predict_proba of a model on the x-axis.\\nThe tricky part is to get values for the y-axis.\\nIn scikit-learn, this is accomplished by binning the predictions such that the x-axis\\nrepresents the average predicted probability in each bin.\\nThe y-axis is then the fraction of positives given the predictions of that bin, i.e.\\nthe proportion of samples whose class is the positive class (in each bin).\\nThe top calibration curve plot is created with\\nCalibrationDisplay.from_estimator, which uses calibration_curve to\\ncalculate the per bin average predicted probabilities and fraction of positives.\\nCalibrationDisplay.from_estimator\\ntakes as input a fitted classifier, which is used to calculate the predicted\\nprobabilities. The classifier thus must have predict_proba method. For\\nthe few classifiers that do not have a predict_proba method, it is\\npossible to use CalibratedClassifierCV to calibrate the classifier\\noutputs to probabilities.\\nThe bottom histogram gives some insight into the behavior of each classifier\\nby showing the number of samples in each predicted probability bin.\\n---------new doc---------\\nNote\\nImpact on ranking metrics like AUC\\nIt is generally expected that calibration does not affect ranking metrics such as\\nROC-AUC. However, these metrics might differ after calibration when using\\nmethod=\\\"isotonic\\\" since isotonic regression introduces ties in the predicted\\nprobabilities. This can be seen as within the uncertainty of the model predictions.\\nIn case, you strictly want to keep the ranking and thus AUC scores, use\\nmethod=\\\"sigmoid\\\" which is a strictly monotonic transformation and thus keeps\\nthe ranking.\\n\\n\\n\\n1.16.3.3. Multiclass support#\\nBoth isotonic and sigmoid regressors only\\nsupport 1-dimensional data (e.g., binary classification output) but are\\nextended for multiclass classification if the base_estimator supports\\nmulticlass predictions. For multiclass predictions,\\nCalibratedClassifierCV calibrates for\\neach class separately in a OneVsRestClassifier fashion [5]. When\\npredicting\\nprobabilities, the calibrated probabilities for each class\\nare predicted separately. As those probabilities do not necessarily sum to\\none, a postprocessing is performed to normalize them.\\nExamples\\n\\nProbability Calibration curves\\nProbability Calibration for 3-class classification\\nProbability calibration of classifiers\\nComparison of Calibration of Classifiers\\n\\nReferences\\n\\n\\n[1]\\nAllan H. Murphy (1973).\\n“A New Vector Partition of the Probability Score”\\nJournal of Applied Meteorology and Climatology\\n\\n\\n[2]\\nOn the combination of forecast probabilities for\\nconsecutive precipitation periods.\\nWea. Forecasting, 5, 640–650., Wilks, D. S., 1990a\\n---------new doc---------\\nThe Rand index does not ensure to obtain a value close to 0.0 for a\\nrandom labelling. The adjusted Rand index corrects for chance and\\nwill give such a baseline.\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n0.24...\\n\\n\\nAs with all clustering metrics, one can permute 0 and 1 in the predicted\\nlabels, rename 2 to 3, and get the same score:\\n>>> labels_pred = [1, 1, 0, 0, 3, 3]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n0.66...\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n0.24...\\n\\n\\nFurthermore, both rand_score adjusted_rand_score are\\nsymmetric: swapping the argument does not change the scores. They can\\nthus be used as consensus measures:\\n>>> metrics.rand_score(labels_pred, labels_true)\\n0.66...\\n>>> metrics.adjusted_rand_score(labels_pred, labels_true)\\n0.24...\\n\\n\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n1.0\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n1.0\\n---------new doc---------\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n1.0\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n1.0\\n\\n\\nPoorly agreeing labels (e.g. independent labelings) have lower scores,\\nand for the adjusted Rand index the score will be negative or close to\\nzero. However, for the unadjusted Rand index the score, while lower,\\nwill not necessarily be close to zero.:\\n>>> labels_true = [0, 0, 0, 0, 0, 0, 1, 1]\\n>>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]\\n>>> metrics.rand_score(labels_true, labels_pred)\\n0.39...\\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\\n-0.07...\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nAdvantages:\\n\\nInterpretability: The unadjusted Rand index is proportional to the\\nnumber of sample pairs whose labels are the same in both labels_pred and\\nlabels_true, or are different in both.\\nRandom (uniform) label assignments have an adjusted Rand index score close\\nto 0.0 for any value of n_clusters and n_samples (which is not the\\ncase for the unadjusted Rand index or the V-measure for instance).\\nBounded range: Lower values indicate different labelings, similar\\nclusterings have a high (adjusted or unadjusted) Rand index, 1.0 is the\\nperfect match score. The score range is [0, 1] for the unadjusted Rand index\\nand [-0.5, 1] for the adjusted Rand index.\\nNo assumption is made on the cluster structure: The (adjusted or\\nunadjusted) Rand index can be used to compare all kinds of clustering\\nalgorithms, and can be used to compare clustering algorithms such as k-means\\nwhich assumes isotropic blob shapes with results of spectral clustering\\nalgorithms which can find cluster with “folded” shapes.\\n\\n\\n\\nDrawbacks:\\n\\nContrary to inertia, the (adjusted or unadjusted) Rand index requires\\nknowledge of the ground truth classes which is almost never available in\\npractice or requires manual assignment by human annotators (as in the\\nsupervised learning setting).\\nHowever (adjusted or unadjusted) Rand index can also be useful in a purely\\nunsupervised setting as a building block for a Consensus Index that can be\\nused for clustering model selection (TODO).\\n---------new doc---------\\nContrary to inertia, the (adjusted or unadjusted) Rand index requires\\nknowledge of the ground truth classes which is almost never available in\\npractice or requires manual assignment by human annotators (as in the\\nsupervised learning setting).\\nHowever (adjusted or unadjusted) Rand index can also be useful in a purely\\nunsupervised setting as a building block for a Consensus Index that can be\\nused for clustering model selection (TODO).\\n\\nThe unadjusted Rand index is often close to 1.0 even if the clusterings\\nthemselves differ significantly. This can be understood when interpreting\\nthe Rand index as the accuracy of element pair labeling resulting from the\\nclusterings: In practice there often is a majority of element pairs that are\\nassigned the different pair label under both the predicted and the\\nground truth clustering resulting in a high proportion of pair labels that\\nagree, which leads subsequently to a high score.\\n\\n\\nExamples\\n\\nAdjustment for chance in clustering performance evaluation:\\nAnalysis of the impact of the dataset size on the value of\\nclustering measures for random assignments.\\n\\n\\n\\nMathematical formulation#\\nIf C is a ground truth class assignment and K the clustering, let us define\\n\\\\(a\\\\) and \\\\(b\\\\) as:\\n\\n\\\\(a\\\\), the number of pairs of elements that are in the same set in C and\\nin the same set in K\\n\\\\(b\\\\), the number of pairs of elements that are in different sets in C and\\nin different sets in K\\n\\nThe unadjusted Rand index is then given by:\\n---------new doc---------\\n\\\\(a\\\\), the number of pairs of elements that are in the same set in C and\\nin the same set in K\\n\\\\(b\\\\), the number of pairs of elements that are in different sets in C and\\nin different sets in K\\n\\nThe unadjusted Rand index is then given by:\\n\\n\\\\[\\\\text{RI} = \\\\frac{a + b}{C_2^{n_{samples}}}\\\\]\\nwhere \\\\(C_2^{n_{samples}}\\\\) is the total number of possible pairs in the\\ndataset. It does not matter if the calculation is performed on ordered pairs or\\nunordered pairs as long as the calculation is performed consistently.\\nHowever, the Rand index does not guarantee that random label assignments will\\nget a value close to zero (esp. if the number of clusters is in the same order\\nof magnitude as the number of samples).\\nTo counter this effect we can discount the expected RI \\\\(E[\\\\text{RI}]\\\\) of\\nrandom labelings by defining the adjusted Rand index as follows:\\n\\n\\\\[\\\\text{ARI} = \\\\frac{\\\\text{RI} - E[\\\\text{RI}]}{\\\\max(\\\\text{RI}) - E[\\\\text{RI}]}\\\\]\\n\\n\\n\\nReferences#\\n\\nComparing Partitions L. Hubert and P.\\nArabie, Journal of Classification 1985\\nProperties of the Hubert-Arabie adjusted Rand index D. Steinley, Psychological\\nMethods 2004\\nWikipedia entry for the Rand index\\nMinimum adjusted Rand index for two clusterings of a given size, 2022, J. E. Chacón and A. I. Rastrojo\\n---------new doc---------\\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get\\nthe same score:\\n>>> labels_pred = [1, 1, 0, 0, 3, 3]\\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \\n0.22504...\\n\\n\\nAll, mutual_info_score, adjusted_mutual_info_score and\\nnormalized_mutual_info_score are symmetric: swapping the argument does\\nnot change the score. Thus they can be used as a consensus measure:\\n>>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  \\n0.22504...\\n\\n\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \\n1.0\\n\\n>>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  \\n1.0\\n\\n\\nThis is not true for mutual_info_score, which is therefore harder to judge:\\n>>> metrics.mutual_info_score(labels_true, labels_pred)  \\n0.69...\\n\\n\\nBad (e.g. independent labelings) have non-positive scores:\\n>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \\n-0.10526...\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nAdvantages:\\n\\nRandom (uniform) label assignments have a AMI score close to 0.0 for any\\nvalue of n_clusters and n_samples (which is not the case for raw\\nMutual Information or the V-measure for instance).\\nUpper bound  of 1:  Values close to zero indicate two label assignments\\nthat are largely independent, while values close to one indicate significant\\nagreement. Further, an AMI of exactly 1 indicates that the two label\\nassignments are equal (with or without permutation).\\n\\n\\n\\nDrawbacks:\\n\\nContrary to inertia, MI-based measures require the knowledge of the ground\\ntruth classes while almost never available in practice or requires manual\\nassignment by human annotators (as in the supervised learning setting).\\nHowever MI-based measures can also be useful in purely unsupervised setting\\nas a building block for a Consensus Index that can be used for clustering\\nmodel selection.\\n\\nNMI and MI are not adjusted against chance.\\n\\n\\nExamples\\n\\nAdjustment for chance in clustering performance evaluation: Analysis\\nof the impact of the dataset size on the value of clustering measures for random\\nassignments. This example also includes the Adjusted Rand Index.\\n\\n\\n\\nMathematical formulation#\\nAssume two label assignments (of the same N objects), \\\\(U\\\\) and \\\\(V\\\\).\\nTheir entropy is the amount of uncertainty for a partition set, defined by:\\n\\n\\\\[H(U) = - \\\\sum_{i=1}^{|U|}P(i)\\\\log(P(i))\\\\]\\nwhere \\\\(P(i) = |U_i| / N\\\\) is the probability that an object picked at\\nrandom from \\\\(U\\\\) falls into class \\\\(U_i\\\\). Likewise for \\\\(V\\\\):\\n---------new doc---------\\n\\\\[H(U) = - \\\\sum_{i=1}^{|U|}P(i)\\\\log(P(i))\\\\]\\nwhere \\\\(P(i) = |U_i| / N\\\\) is the probability that an object picked at\\nrandom from \\\\(U\\\\) falls into class \\\\(U_i\\\\). Likewise for \\\\(V\\\\):\\n\\n\\\\[H(V) = - \\\\sum_{j=1}^{|V|}P'(j)\\\\log(P'(j))\\\\]\\nWith \\\\(P'(j) = |V_j| / N\\\\). The mutual information (MI) between \\\\(U\\\\)\\nand \\\\(V\\\\) is calculated by:\\n\\n\\\\[\\\\text{MI}(U, V) = \\\\sum_{i=1}^{|U|}\\\\sum_{j=1}^{|V|}P(i, j)\\\\log\\\\left(\\\\frac{P(i,j)}{P(i)P'(j)}\\\\right)\\\\]\\nwhere \\\\(P(i, j) = |U_i \\\\cap V_j| / N\\\\) is the probability that an object\\npicked at random falls into both classes \\\\(U_i\\\\) and \\\\(V_j\\\\).\\nIt also can be expressed in set cardinality formulation:\\n\\n\\\\[\\\\text{MI}(U, V) = \\\\sum_{i=1}^{|U|} \\\\sum_{j=1}^{|V|} \\\\frac{|U_i \\\\cap V_j|}{N}\\\\log\\\\left(\\\\frac{N|U_i \\\\cap V_j|}{|U_i||V_j|}\\\\right)\\\\]\\nThe normalized mutual information is defined as\\n---------new doc---------\\nhomogeneity: each cluster contains only members of a single class.\\ncompleteness: all members of a given class are assigned to the same\\ncluster.\\n\\nWe can turn those concept as scores homogeneity_score and\\ncompleteness_score. Both are bounded below by 0.0 and above by\\n1.0 (higher is better):\\n>>> from sklearn import metrics\\n>>> labels_true = [0, 0, 0, 1, 1, 1]\\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\\n\\n>>> metrics.homogeneity_score(labels_true, labels_pred)\\n0.66...\\n\\n>>> metrics.completeness_score(labels_true, labels_pred)\\n0.42...\\n\\n\\nTheir harmonic mean called V-measure is computed by\\nv_measure_score:\\n>>> metrics.v_measure_score(labels_true, labels_pred)\\n0.51...\\n\\n\\nThis function’s formula is as follows:\\n\\n\\\\[v = \\\\frac{(1 + \\\\beta) \\\\times \\\\text{homogeneity} \\\\times \\\\text{completeness}}{(\\\\beta \\\\times \\\\text{homogeneity} + \\\\text{completeness})}\\\\]\\nbeta defaults to a value of 1.0, but for using a value less than 1 for beta:\\n>>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6)\\n0.54...\\n\\n\\nmore weight will be attributed to homogeneity, and using a value greater than 1:\\n>>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)\\n0.48...\\n---------new doc---------\\nmore weight will be attributed to homogeneity, and using a value greater than 1:\\n>>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)\\n0.48...\\n\\n\\nmore weight will be attributed to completeness.\\nThe V-measure is actually equivalent to the mutual information (NMI)\\ndiscussed above, with the aggregation function being the arithmetic mean [B2011].\\nHomogeneity, completeness and V-measure can be computed at once using\\nhomogeneity_completeness_v_measure as follows:\\n>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\\n(0.66..., 0.42..., 0.51...)\\n\\n\\nThe following clustering assignment is slightly better, since it is\\nhomogeneous but not complete:\\n>>> labels_pred = [0, 0, 0, 1, 2, 2]\\n>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\\n(1.0, 0.68..., 0.81...)\\n\\n\\n\\nNote\\nv_measure_score is symmetric: it can be used to evaluate\\nthe agreement of two independent assignments on the same dataset.\\nThis is not the case for completeness_score and\\nhomogeneity_score: both are bound by the relationship:\\nhomogeneity_score(a, b) == completeness_score(b, a)\\n\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nNote\\nv_measure_score is symmetric: it can be used to evaluate\\nthe agreement of two independent assignments on the same dataset.\\nThis is not the case for completeness_score and\\nhomogeneity_score: both are bound by the relationship:\\nhomogeneity_score(a, b) == completeness_score(b, a)\\n\\n\\n\\n\\nAdvantages:\\n\\nBounded scores: 0.0 is as bad as it can be, 1.0 is a perfect score.\\nIntuitive interpretation: clustering with bad V-measure can be\\nqualitatively analyzed in terms of homogeneity and completeness to\\nbetter feel what ‘kind’ of mistakes is done by the assignment.\\nNo assumption is made on the cluster structure: can be used to compare\\nclustering algorithms such as k-means which assumes isotropic blob shapes\\nwith results of spectral clustering algorithms which can find cluster with\\n“folded” shapes.\\n\\n\\n\\nDrawbacks:\\n\\nThe previously introduced metrics are not normalized with regards to\\nrandom labeling: this means that depending on the number of samples,\\nclusters and ground truth classes, a completely random labeling will not\\nalways yield the same values for homogeneity, completeness and hence\\nv-measure. In particular random labeling won’t yield zero scores\\nespecially when the number of clusters is large.\\nThis problem can safely be ignored when the number of samples is more than a\\nthousand and the number of clusters is less than 10. For smaller sample\\nsizes or larger number of clusters it is safer to use an adjusted index such\\nas the Adjusted Rand Index (ARI).\\n\\n\\n\\n\\n\\n\\n\\nThese metrics require the knowledge of the ground truth classes while\\nalmost never available in practice or requires manual assignment by human\\nannotators (as in the supervised learning setting).\\n\\n\\nExamples\\n---------new doc---------\\nTP (True Positive): The number of pairs of points that are clustered together\\nboth in the true labels and in the predicted labels.\\nFP (False Positive): The number of pairs of points that are clustered together\\nin the predicted labels but not in the true labels.\\nFN (False Negative): The number of pairs of points that are clustered together\\nin the true labels but not in the predicted labels.\\n\\nThe score ranges from 0 to 1. A high value indicates a good similarity\\nbetween two clusters.\\n>>> from sklearn import metrics\\n>>> labels_true = [0, 0, 0, 1, 1, 1]\\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\\n\\n\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.47140...\\n\\n\\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get\\nthe same score:\\n>>> labels_pred = [1, 1, 0, 0, 3, 3]\\n\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.47140...\\n\\n\\nPerfect labeling is scored 1.0:\\n>>> labels_pred = labels_true[:]\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n1.0\\n\\n\\nBad (e.g. independent labelings) have zero scores:\\n>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.0\\n\\n\\n\\nAdvantages:\\n---------new doc---------\\nBad (e.g. independent labelings) have zero scores:\\n>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.0\\n\\n\\n\\nAdvantages:\\n\\nRandom (uniform) label assignments have a FMI score close to 0.0 for any\\nvalue of n_clusters and n_samples (which is not the case for raw\\nMutual Information or the V-measure for instance).\\nUpper-bounded at 1:  Values close to zero indicate two label assignments\\nthat are largely independent, while values close to one indicate significant\\nagreement. Further, values of exactly 0 indicate purely independent\\nlabel assignments and a FMI of exactly 1 indicates that the two label\\nassignments are equal (with or without permutation).\\nNo assumption is made on the cluster structure: can be used to compare\\nclustering algorithms such as k-means which assumes isotropic blob shapes\\nwith results of spectral clustering algorithms which can find cluster with\\n“folded” shapes.\\n\\n\\n\\nDrawbacks:\\n\\nContrary to inertia, FMI-based measures require the knowledge of the\\nground truth classes while almost never available in practice or requires\\nmanual assignment by human annotators (as in the supervised learning\\nsetting).\\n\\n\\n\\n\\nReferences#\\n---------new doc---------\\nDrawbacks:\\n\\nContrary to inertia, FMI-based measures require the knowledge of the\\nground truth classes while almost never available in practice or requires\\nmanual assignment by human annotators (as in the supervised learning\\nsetting).\\n\\n\\n\\n\\nReferences#\\n\\nE. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two\\nhierarchical clusterings”. Journal of the American Statistical Association.\\nhttps://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008\\nWikipedia entry for the Fowlkes-Mallows Index\\n\\n\\n\\n\\n2.3.11.5. Silhouette Coefficient#\\nIf the ground truth labels are not known, evaluation must be performed using\\nthe model itself. The Silhouette Coefficient\\n(sklearn.metrics.silhouette_score)\\nis an example of such an evaluation, where a\\nhigher Silhouette Coefficient score relates to a model with better defined\\nclusters. The Silhouette Coefficient is defined for each sample and is composed\\nof two scores:\\n\\na: The mean distance between a sample and all other points in the same\\nclass.\\nb: The mean distance between a sample and all other points in the next\\nnearest cluster.\\n\\nThe Silhouette Coefficient s for a single sample is then given as:\\n---------new doc---------\\nThe mean score and the standard deviation are hence given by:\\n>>> print(\\\"%0.2f accuracy with a standard deviation of %0.2f\\\" % (scores.mean(), scores.std()))\\n0.98 accuracy with a standard deviation of 0.02\\n\\n\\nBy default, the score computed at each CV iteration is the score\\nmethod of the estimator. It is possible to change this by using the\\nscoring parameter:\\n>>> from sklearn import metrics\\n>>> scores = cross_val_score(\\n...     clf, X, y, cv=5, scoring='f1_macro')\\n>>> scores\\narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])\\n---------new doc---------\\nSee Pipelines and composite estimators.\\n\\n\\n3.1.1.1. The cross_validate function and multiple metric evaluation#\\nThe cross_validate function differs from cross_val_score in\\ntwo ways:\\n\\nIt allows specifying multiple metrics for evaluation.\\nIt returns a dict containing fit-times, score-times\\n(and optionally training scores, fitted estimators, train-test split indices)\\nin addition to the test score.\\n---------new doc---------\\nFor single metric evaluation, where the scoring parameter is a string,\\ncallable or None, the keys will be - ['test_score', 'fit_time', 'score_time']\\nAnd for multiple metric evaluation, the return value is a dict with the\\nfollowing keys -\\n['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']\\nreturn_train_score is set to False by default to save computation time.\\nTo evaluate the scores on the training set as well you need to set it to\\nTrue. You may also retain the estimator fitted on each training set by\\nsetting return_estimator=True. Similarly, you may set\\nreturn_indices=True to retain the training and testing indices used to split\\nthe dataset into train and test sets for each cv split.\\nThe multiple metrics can be specified either as a list, tuple or set of\\npredefined scorer names:\\n>>> from sklearn.model_selection import cross_validate\\n>>> from sklearn.metrics import recall_score\\n>>> scoring = ['precision_macro', 'recall_macro']\\n>>> clf = svm.SVC(kernel='linear', C=1, random_state=0)\\n>>> scores = cross_validate(clf, X, y, scoring=scoring)\\n>>> sorted(scores.keys())\\n['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']\\n>>> scores['test_recall_macro']\\narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])\\n---------new doc---------\\nOr as a dict mapping scorer name to a predefined or custom scoring function:\\n>>> from sklearn.metrics import make_scorer\\n>>> scoring = {'prec_macro': 'precision_macro',\\n...            'rec_macro': make_scorer(recall_score, average='macro')}\\n>>> scores = cross_validate(clf, X, y, scoring=scoring,\\n...                         cv=5, return_train_score=True)\\n>>> sorted(scores.keys())\\n['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',\\n 'train_prec_macro', 'train_rec_macro']\\n>>> scores['train_rec_macro']\\narray([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])\\n\\n\\nHere is an example of cross_validate using a single metric:\\n>>> scores = cross_validate(clf, X, y,\\n...                         scoring='precision_macro', cv=5,\\n...                         return_estimator=True)\\n>>> sorted(scores.keys())\\n['estimator', 'fit_time', 'score_time', 'test_score']\\n---------new doc---------\\n3.1.5. Permutation test score#\\npermutation_test_score offers another way\\nto evaluate the performance of classifiers. It provides a permutation-based\\np-value, which represents how likely an observed performance of the\\nclassifier would be obtained by chance. The null hypothesis in this test is\\nthat the classifier fails to leverage any statistical dependency between the\\nfeatures and the labels to make correct predictions on left out data.\\npermutation_test_score generates a null\\ndistribution by calculating n_permutations different permutations of the\\ndata. In each permutation the labels are randomly shuffled, thereby removing\\nany dependency between the features and the labels. The p-value output\\nis the fraction of permutations for which the average cross-validation score\\nobtained by the model is better than the cross-validation score obtained by\\nthe model using the original data. For reliable results n_permutations\\nshould typically be larger than 100 and cv between 3-10 folds.\\nA low p-value provides evidence that the dataset contains real dependency\\nbetween features and labels and the classifier was able to utilize this\\nto obtain good results. A high p-value could be due to a lack of dependency\\nbetween features and labels (there is no difference in feature values between\\nthe classes) or because the classifier was not able to use the dependency in\\nthe data. In the latter case, using a more appropriate classifier that\\nis able to utilize the structure in the data, would result in a lower\\np-value.\\nCross-validation provides information about how well a classifier generalizes,\\nspecifically the range of expected errors of the classifier. However, a\\nclassifier trained on a high dimensional dataset with no structure may still\\nperform better than expected on cross-validation, just by chance.\\n---------new doc---------\\n3.2.4.2. Specifying multiple metrics for evaluation#\\nGridSearchCV and RandomizedSearchCV allow specifying\\nmultiple metrics for the scoring parameter.\\nMultimetric scoring can either be specified as a list of strings of predefined\\nscores names or a dict mapping the scorer name to the scorer function and/or\\nthe predefined scorer name(s). See Using multiple metric evaluation for more details.\\nWhen specifying multiple metrics, the refit parameter must be set to the\\nmetric (string) for which the best_params_ will be found and used to build\\nthe best_estimator_ on the whole dataset. If the search should not be\\nrefit, set refit=False. Leaving refit to the default value None will\\nresult in an error when using multiple metrics.\\nSee Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV\\nfor an example usage.\\nHalvingRandomSearchCV and HalvingGridSearchCV do not support\\nmultimetric scoring.\\n---------new doc---------\\nWhile these hard-coded rules might at first seem reasonable as default behavior, they\\nare most certainly not ideal for most use cases. Let’s illustrate with an example.\\nConsider a scenario where a predictive model is being deployed to assist\\nphysicians in detecting tumors. In this setting, physicians will most likely be\\ninterested in identifying all patients with cancer and not missing anyone with cancer so\\nthat they can provide them with the right treatment. In other words, physicians\\nprioritize achieving a high recall rate. This emphasis on recall comes, of course, with\\nthe trade-off of potentially more false-positive predictions, reducing the precision of\\nthe model. That is a risk physicians are willing to take because the cost of a missed\\ncancer is much higher than the cost of further diagnostic tests. Consequently, when it\\ncomes to deciding whether to classify a patient as having cancer or not, it may be more\\nbeneficial to classify them as positive for cancer when the conditional probability\\nestimate is much lower than 0.5.\\n---------new doc---------\\nNote\\nIt is important to notice that these metrics come with default parameters, notably\\nthe label of the class of interest (i.e. pos_label). Thus, if this label is not\\nthe right one for your application, you need to define a scorer and pass the right\\npos_label (and additional parameters) using the\\nmake_scorer. Refer to Callable scorers to get\\ninformation to define your own scoring function. For instance, we show how to pass\\nthe information to the scorer that the label of interest is 0 when maximizing the\\nf1_score:\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.model_selection import TunedThresholdClassifierCV\\n>>> from sklearn.metrics import make_scorer, f1_score\\n>>> X, y = make_classification(\\n...   n_samples=1_000, weights=[0.1, 0.9], random_state=0)\\n>>> pos_label = 0\\n>>> scorer = make_scorer(f1_score, pos_label=pos_label)\\n>>> base_model = LogisticRegression()\\n>>> model = TunedThresholdClassifierCV(base_model, scoring=scorer)\\n>>> scorer(model.fit(X, y), X, y)\\n0.88...\\n>>> # compare it with the internal score found by cross-validation\\n>>> model.best_score_\\n0.86...\\n---------new doc---------\\n3.3.1.4. Examples#\\n\\nSee the example entitled\\nPost-hoc tuning the cut-off point of decision function,\\nto get insights on the post-tuning of the decision threshold.\\nSee the example entitled\\nPost-tuning the decision threshold for cost-sensitive learning,\\nto learn about cost-sensitive learning and decision threshold tuning.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\n3.2. Tuning the hyper-parameters of an estimator\\n\\n\\n\\n\\nnext\\n3.4. Metrics and scoring: quantifying the quality of predictions\\n\\n\\n --- \\n\\n\\nand Developer\\nGuide for\\nmore details.\\n\\n\\n\\n\\nUsing custom scorers in functions where n_jobs > 1#\\nWhile defining the custom scoring function alongside the calling function\\nshould work out of the box with the default joblib backend (loky),\\nimporting it from another module will be a more robust approach and work\\nindependently of the joblib backend.\\nFor example, to use n_jobs greater than 1 in the example below,\\ncustom_scoring_function function is saved in a user-created module\\n(custom_scorer_module.py) and imported:\\n>>> from custom_scorer_module import custom_scoring_function \\n>>> cross_val_score(model,\\n...  X_train,\\n...  y_train,\\n...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),\\n...  cv=5,\\n...  n_jobs=-1) \\n\\n\\n\\n\\n\\n\\n3.4.3.3. Using multiple metric evaluation#\\nScikit-learn also permits evaluation of multiple metrics in GridSearchCV,\\nRandomizedSearchCV and cross_validate.\\nThere are three ways to specify multiple scoring metrics for the scoring\\nparameter:\\n---------new doc---------\\n3.4.3.3. Using multiple metric evaluation#\\nScikit-learn also permits evaluation of multiple metrics in GridSearchCV,\\nRandomizedSearchCV and cross_validate.\\nThere are three ways to specify multiple scoring metrics for the scoring\\nparameter:\\n\\nAs an iterable of string metrics:\\n>>> scoring = ['accuracy', 'precision']\\n\\n\\n\\nAs a dict mapping the scorer name to the scoring function:\\n>>> from sklearn.metrics import accuracy_score\\n>>> from sklearn.metrics import make_scorer\\n>>> scoring = {'accuracy': make_scorer(accuracy_score),\\n...            'prec': 'precision'}\\n\\n\\nNote that the dict values can either be scorer functions or one of the\\npredefined metric strings.\\n---------new doc---------\\nNote that the dict values can either be scorer functions or one of the\\npredefined metric strings.\\n\\nAs a callable that returns a dictionary of scores:\\n>>> from sklearn.model_selection import cross_validate\\n>>> from sklearn.metrics import confusion_matrix\\n>>> # A sample toy binary classification dataset\\n>>> X, y = datasets.make_classification(n_classes=2, random_state=0)\\n>>> svm = LinearSVC(random_state=0)\\n>>> def confusion_matrix_scorer(clf, X, y):\\n...      y_pred = clf.predict(X)\\n...      cm = confusion_matrix(y, y_pred)\\n...      return {'tn': cm[0, 0], 'fp': cm[0, 1],\\n...              'fn': cm[1, 0], 'tp': cm[1, 1]}\\n>>> cv_results = cross_validate(svm, X, y, cv=5,\\n...                             scoring=confusion_matrix_scorer)\\n>>> # Getting the test set true positive scores\\n>>> print(cv_results['test_tp'])\\n[10  9  8  7  8]\\n>>> # Getting the test set false negative scores\\n>>> print(cv_results['test_fn'])\\n[0 1 2 3 2]\\n---------new doc---------\\n3.4.4. Classification metrics#\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions to measure classification performance.\\nSome metrics might require probability estimates of the positive class,\\nconfidence values, or binary decisions values.\\nMost implementations allow each sample to provide a weighted contribution\\nto the overall score, through the sample_weight parameter.\\nSome of these are restricted to the binary classification case:\\n\\n\\nprecision_recall_curve(y_true[, y_score, ...])\\nCompute precision-recall pairs for different probability thresholds.\\n\\nroc_curve(y_true, y_score, *[, pos_label, ...])\\nCompute Receiver operating characteristic (ROC).\\n\\nclass_likelihood_ratios(y_true, y_pred, *[, ...])\\nCompute binary classification positive and negative likelihood ratios.\\n\\ndet_curve(y_true, y_score[, pos_label, ...])\\nCompute error rates for different probability thresholds.\\n\\n\\n\\n\\nOthers also work in the multiclass case:\\n\\n\\nbalanced_accuracy_score(y_true, y_pred, *[, ...])\\nCompute the balanced accuracy.\\n\\ncohen_kappa_score(y1, y2, *[, labels, ...])\\nCompute Cohen's kappa: a statistic that measures inter-annotator agreement.\\n\\nconfusion_matrix(y_true, y_pred, *[, ...])\\nCompute confusion matrix to evaluate the accuracy of a classification.\\n---------new doc---------\\ncohen_kappa_score(y1, y2, *[, labels, ...])\\nCompute Cohen's kappa: a statistic that measures inter-annotator agreement.\\n\\nconfusion_matrix(y_true, y_pred, *[, ...])\\nCompute confusion matrix to evaluate the accuracy of a classification.\\n\\nhinge_loss(y_true, pred_decision, *[, ...])\\nAverage hinge loss (non-regularized).\\n\\nmatthews_corrcoef(y_true, y_pred, *[, ...])\\nCompute the Matthews correlation coefficient (MCC).\\n\\nroc_auc_score(y_true, y_score, *[, average, ...])\\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\\n\\ntop_k_accuracy_score(y_true, y_score, *[, ...])\\nTop-k Accuracy classification score.\\n\\n\\n\\n\\nSome also work in the multilabel case:\\n\\n\\naccuracy_score(y_true, y_pred, *[, ...])\\nAccuracy classification score.\\n\\nclassification_report(y_true, y_pred, *[, ...])\\nBuild a text report showing the main classification metrics.\\n\\nf1_score(y_true, y_pred, *[, labels, ...])\\nCompute the F1 score, also known as balanced F-score or F-measure.\\n\\nfbeta_score(y_true, y_pred, *, beta[, ...])\\nCompute the F-beta score.\\n---------new doc---------\\nf1_score(y_true, y_pred, *[, labels, ...])\\nCompute the F1 score, also known as balanced F-score or F-measure.\\n\\nfbeta_score(y_true, y_pred, *, beta[, ...])\\nCompute the F-beta score.\\n\\nhamming_loss(y_true, y_pred, *[, sample_weight])\\nCompute the average Hamming loss.\\n\\njaccard_score(y_true, y_pred, *[, labels, ...])\\nJaccard similarity coefficient score.\\n\\nlog_loss(y_true, y_pred, *[, normalize, ...])\\nLog loss, aka logistic loss or cross-entropy loss.\\n\\nmultilabel_confusion_matrix(y_true, y_pred, *)\\nCompute a confusion matrix for each class or sample.\\n\\nprecision_recall_fscore_support(y_true, ...)\\nCompute precision, recall, F-measure and support for each class.\\n\\nprecision_score(y_true, y_pred, *[, labels, ...])\\nCompute the precision.\\n\\nrecall_score(y_true, y_pred, *[, labels, ...])\\nCompute the recall.\\n\\nroc_auc_score(y_true, y_score, *[, average, ...])\\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\\n---------new doc---------\\nrecall_score(y_true, y_pred, *[, labels, ...])\\nCompute the recall.\\n\\nroc_auc_score(y_true, y_score, *[, average, ...])\\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\\n\\nzero_one_loss(y_true, y_pred, *[, ...])\\nZero-one classification loss.\\n\\nd2_log_loss_score(y_true, y_pred, *[, ...])\\n\\\\(D^2\\\\) score function, fraction of log loss explained.\\n\\n\\n\\n\\nAnd some work with binary and multilabel (but not multiclass) problems:\\n\\n\\naverage_precision_score(y_true, y_score, *)\\nCompute average precision (AP) from prediction scores.\\n\\n\\n\\n\\nIn the following sub-sections, we will describe each of those functions,\\npreceded by some notes on common API and metric definition.\\n---------new doc---------\\nAnd some work with binary and multilabel (but not multiclass) problems:\\n\\n\\naverage_precision_score(y_true, y_score, *)\\nCompute average precision (AP) from prediction scores.\\n\\n\\n\\n\\nIn the following sub-sections, we will describe each of those functions,\\npreceded by some notes on common API and metric definition.\\n\\n3.4.4.1. From binary to multiclass and multilabel#\\nSome metrics are essentially defined for binary classification tasks (e.g.\\nf1_score, roc_auc_score). In these cases, by default\\nonly the positive label is evaluated, assuming by default that the positive\\nclass is labelled 1 (though this may be configurable through the\\npos_label parameter).\\nIn extending a binary metric to multiclass or multilabel problems, the data\\nis treated as a collection of binary problems, one for each class.\\nThere are then a number of ways to average binary metric calculations across\\nthe set of classes, each of which may be useful in some scenario.\\nWhere available, you should select among these using the average parameter.\\n---------new doc---------\\n\\\"macro\\\" simply calculates the mean of the binary metrics,\\ngiving equal weight to each class.  In problems where infrequent classes\\nare nonetheless important, macro-averaging may be a means of highlighting\\ntheir performance. On the other hand, the assumption that all classes are\\nequally important is often untrue, such that macro-averaging will\\nover-emphasize the typically low performance on an infrequent class.\\n\\\"weighted\\\" accounts for class imbalance by computing the average of\\nbinary metrics in which each class’s score is weighted by its presence in the\\ntrue data sample.\\n\\\"micro\\\" gives each sample-class pair an equal contribution to the overall\\nmetric (except as a result of sample-weight). Rather than summing the\\nmetric per class, this sums the dividends and divisors that make up the\\nper-class metrics to calculate an overall quotient.\\nMicro-averaging may be preferred in multilabel settings, including\\nmulticlass classification where a majority class is to be ignored.\\n\\\"samples\\\" applies only to multilabel problems. It does not calculate a\\nper-class measure, instead calculating the metric over the true and predicted\\nclasses for each sample in the evaluation data, and returning their\\n(sample_weight-weighted) average.\\nSelecting average=None will return an array with the score for each\\nclass.\\n\\nWhile multiclass data is provided to the metric, like binary targets, as an\\narray of class labels, multilabel data is specified as an indicator matrix,\\nin which cell [i, j] has value 1 if sample i has label j and value\\n0 otherwise.\\n---------new doc---------\\nWhile multiclass data is provided to the metric, like binary targets, as an\\narray of class labels, multilabel data is specified as an indicator matrix,\\nin which cell [i, j] has value 1 if sample i has label j and value\\n0 otherwise.\\n\\n\\n3.4.4.2. Accuracy score#\\nThe accuracy_score function computes the\\naccuracy, either the fraction\\n(default) or the count (normalize=False) of correct predictions.\\nIn multilabel classification, the function returns the subset accuracy. If\\nthe entire set of predicted labels for a sample strictly match with the true\\nset of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of\\nthe \\\\(i\\\\)-th sample and \\\\(y_i\\\\) is the corresponding true value,\\nthen the fraction of correct predictions over \\\\(n_\\\\text{samples}\\\\) is\\ndefined as\\n\\n\\\\[\\\\texttt{accuracy}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} 1(\\\\hat{y}_i = y_i)\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function.\\n>>> import numpy as np\\n>>> from sklearn.metrics import accuracy_score\\n>>> y_pred = [0, 2, 1, 3]\\n>>> y_true = [0, 1, 2, 3]\\n>>> accuracy_score(y_true, y_pred)\\n0.5\\n>>> accuracy_score(y_true, y_pred, normalize=False)\\n2.0\\n---------new doc---------\\nIn the multilabel case with binary label indicators:\\n>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\\n0.5\\n\\n\\nExamples\\n\\nSee Test with permutations the significance of a classification score\\nfor an example of accuracy score usage using permutations of\\nthe dataset.\\n\\n\\n\\n3.4.4.3. Top-k accuracy score#\\nThe top_k_accuracy_score function is a generalization of\\naccuracy_score. The difference is that a prediction is considered\\ncorrect as long as the true label is associated with one of the k highest\\npredicted scores. accuracy_score is the special case of k = 1.\\nThe function covers the binary and multiclass classification cases but not the\\nmultilabel case.\\nIf \\\\(\\\\hat{f}_{i,j}\\\\) is the predicted class for the \\\\(i\\\\)-th sample\\ncorresponding to the \\\\(j\\\\)-th largest predicted score and \\\\(y_i\\\\) is the\\ncorresponding true value, then the fraction of correct predictions over\\n\\\\(n_\\\\text{samples}\\\\) is defined as\\n---------new doc---------\\n\\\\[\\\\texttt{top-k accuracy}(y, \\\\hat{f}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} \\\\sum_{j=1}^{k} 1(\\\\hat{f}_{i,j} = y_i)\\\\]\\nwhere \\\\(k\\\\) is the number of guesses allowed and \\\\(1(x)\\\\) is the\\nindicator function.\\n>>> import numpy as np\\n>>> from sklearn.metrics import top_k_accuracy_score\\n>>> y_true = np.array([0, 1, 2, 2])\\n>>> y_score = np.array([[0.5, 0.2, 0.2],\\n...                     [0.3, 0.4, 0.2],\\n...                     [0.2, 0.4, 0.3],\\n...                     [0.7, 0.2, 0.1]])\\n>>> top_k_accuracy_score(y_true, y_score, k=2)\\n0.75\\n>>> # Not normalizing gives the number of \\\"correctly\\\" classified samples\\n>>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\\n3\\n---------new doc---------\\n3.4.4.4. Balanced accuracy score#\\nThe balanced_accuracy_score function computes the balanced accuracy, which avoids inflated\\nperformance estimates on imbalanced datasets. It is the macro-average of recall\\nscores per class or, equivalently, raw accuracy where each sample is weighted\\naccording to the inverse prevalence of its true class.\\nThus for balanced datasets, the score is equal to accuracy.\\nIn the binary case, balanced accuracy is equal to the arithmetic mean of\\nsensitivity\\n(true positive rate) and specificity (true negative\\nrate), or the area under the ROC curve with binary predictions rather than\\nscores:\\n\\n\\\\[\\\\texttt{balanced-accuracy} = \\\\frac{1}{2}\\\\left( \\\\frac{TP}{TP + FN} + \\\\frac{TN}{TN + FP}\\\\right )\\\\]\\nIf the classifier performs equally well on either class, this term reduces to\\nthe conventional accuracy (i.e., the number of correct predictions divided by\\nthe total number of predictions).\\nIn contrast, if the conventional accuracy is above chance only because the\\nclassifier takes advantage of an imbalanced test set, then the balanced\\naccuracy, as appropriate, will drop to \\\\(\\\\frac{1}{n\\\\_classes}\\\\).\\nThe score ranges from 0 to 1, or when adjusted=True is used, it rescaled to\\nthe range \\\\(\\\\frac{1}{1 - n\\\\_classes}\\\\) to 1, inclusive, with\\nperformance at random scoring 0.\\nIf \\\\(y_i\\\\) is the true value of the \\\\(i\\\\)-th sample, and \\\\(w_i\\\\)\\nis the corresponding sample weight, then we adjust the sample weight to:\\n---------new doc---------\\n\\\\[\\\\hat{w}_i = \\\\frac{w_i}{\\\\sum_j{1(y_j = y_i) w_j}}\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function.\\nGiven predicted \\\\(\\\\hat{y}_i\\\\) for sample \\\\(i\\\\), balanced accuracy is\\ndefined as:\\n\\n\\\\[\\\\texttt{balanced-accuracy}(y, \\\\hat{y}, w) = \\\\frac{1}{\\\\sum{\\\\hat{w}_i}} \\\\sum_i 1(\\\\hat{y}_i = y_i) \\\\hat{w}_i\\\\]\\nWith adjusted=True, balanced accuracy reports the relative increase from\\n\\\\(\\\\texttt{balanced-accuracy}(y, \\\\mathbf{0}, w) =\\n\\\\frac{1}{n\\\\_classes}\\\\).  In the binary case, this is also known as\\n*Youden’s J statistic*,\\nor informedness.\\n\\nNote\\nThe multiclass definition here seems the most reasonable extension of the\\nmetric used in binary classification, though there is no certain consensus\\nin the literature:\\n\\nOur definition: [Mosley2013], [Kelleher2015] and [Guyon2015], where\\n[Guyon2015] adopt the adjusted version to ensure that random predictions\\nhave a score of \\\\(0\\\\) and perfect predictions have a score of \\\\(1\\\\)..\\nClass balanced accuracy as described in [Mosley2013]: the minimum between the precision\\nand the recall for each class is computed. Those values are then averaged over the total\\nnumber of classes to get the balanced accuracy.\\nBalanced Accuracy as described in [Urbanowicz2015]: the average of sensitivity and specificity\\nis computed for each class and then averaged over total number of classes.\\n\\n\\nReferences\\n---------new doc---------\\n[Urbanowicz2015]\\nUrbanowicz R.J.,  Moore, J.H. ExSTraCS 2.0: description\\nand evaluation of a scalable learning classifier\\nsystem, Evol. Intel. (2015) 8: 89.\\n\\n\\n\\n\\n3.4.4.5. Cohen’s kappa#\\nThe function cohen_kappa_score computes Cohen’s kappa statistic.\\nThis measure is intended to compare labelings by different human annotators,\\nnot a classifier versus a ground truth.\\nThe kappa score is a number between -1 and 1.\\nScores above .8 are generally considered good agreement;\\nzero or lower means no agreement (practically random labels).\\nKappa scores can be computed for binary or multiclass problems,\\nbut not for multilabel problems (except by manually computing a per-label score)\\nand not for more than two annotators.\\n>>> from sklearn.metrics import cohen_kappa_score\\n>>> labeling1 = [2, 0, 2, 2, 0, 1]\\n>>> labeling2 = [0, 0, 2, 2, 0, 2]\\n>>> cohen_kappa_score(labeling1, labeling2)\\n0.4285714285714286\\n---------new doc---------\\n3.4.4.6. Confusion matrix#\\nThe confusion_matrix function evaluates\\nclassification accuracy by computing the confusion matrix with each row corresponding\\nto the true class (Wikipedia and other references may use different convention\\nfor axes).\\nBy definition, entry \\\\(i, j\\\\) in a confusion matrix is\\nthe number of observations actually in group \\\\(i\\\\), but\\npredicted to be in group \\\\(j\\\\). Here is an example:\\n>>> from sklearn.metrics import confusion_matrix\\n>>> y_true = [2, 0, 2, 2, 0, 1]\\n>>> y_pred = [0, 0, 2, 2, 0, 2]\\n>>> confusion_matrix(y_true, y_pred)\\narray([[2, 0, 0],\\n       [0, 0, 1],\\n       [1, 0, 2]])\\n\\n\\nConfusionMatrixDisplay can be used to visually represent a confusion\\nmatrix as shown in the\\nConfusion matrix\\nexample, which creates the following figure:\\n\\n\\nThe parameter normalize allows to report ratios instead of counts. The\\nconfusion matrix can be normalized in 3 different ways: 'pred', 'true',\\nand 'all' which will divide the counts by the sum of each columns, rows, or\\nthe entire matrix, respectively.\\n>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]\\n>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\\n>>> confusion_matrix(y_true, y_pred, normalize='all')\\narray([[0.25 , 0.125],\\n       [0.25 , 0.375]])\\n---------new doc---------\\nFor binary problems, we can get counts of true negatives, false positives,\\nfalse negatives and true positives as follows:\\n>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]\\n>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\\n>>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\\n>>> tn, fp, fn, tp\\n(2, 1, 2, 3)\\n\\n\\nExamples\\n\\nSee Confusion matrix\\nfor an example of using a confusion matrix to evaluate classifier output\\nquality.\\nSee Recognizing hand-written digits\\nfor an example of using a confusion matrix to classify\\nhand-written digits.\\nSee Classification of text documents using sparse features\\nfor an example of using a confusion matrix to classify text\\ndocuments.\\n\\n\\n\\n3.4.4.7. Classification report#\\nThe classification_report function builds a text report showing the\\nmain classification metrics. Here is a small example with custom target_names\\nand inferred labels:\\n>>> from sklearn.metrics import classification_report\\n>>> y_true = [0, 1, 2, 2, 0]\\n>>> y_pred = [0, 0, 2, 1, 0]\\n>>> target_names = ['class 0', 'class 1', 'class 2']\\n>>> print(classification_report(y_true, y_pred, target_names=target_names))\\n              precision    recall  f1-score   support\\n---------new doc---------\\n\\\\[L_{Hamming}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples} * n_\\\\text{labels}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} \\\\sum_{j=0}^{n_\\\\text{labels} - 1} 1(\\\\hat{y}_{i,j} \\\\not= y_{i,j})\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function.\\nThe equation above does not hold true in the case of multiclass classification.\\nPlease refer to the note below for more information.\\n>>> from sklearn.metrics import hamming_loss\\n>>> y_pred = [1, 2, 3, 4]\\n>>> y_true = [2, 2, 3, 4]\\n>>> hamming_loss(y_true, y_pred)\\n0.25\\n\\n\\nIn the multilabel case with binary label indicators:\\n>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\\n0.75\\n\\n\\n\\nNote\\nIn multiclass classification, the Hamming loss corresponds to the Hamming\\ndistance between y_true and y_pred which is similar to the\\nZero one loss function.  However, while zero-one loss penalizes\\nprediction sets that do not strictly match true sets, the Hamming loss\\npenalizes individual labels.  Thus the Hamming loss, upper bounded by the zero-one\\nloss, is always between zero and one, inclusive; and predicting a proper subset\\nor superset of the true labels will give a Hamming loss between\\nzero and one, exclusive.\\n---------new doc---------\\n3.4.4.9. Precision, recall and F-measures#\\nIntuitively, precision is the ability\\nof the classifier not to label as positive a sample that is negative, and\\nrecall is the\\nability of the classifier to find all the positive samples.\\nThe  F-measure\\n(\\\\(F_\\\\beta\\\\) and \\\\(F_1\\\\) measures) can be interpreted as a weighted\\nharmonic mean of the precision and recall. A\\n\\\\(F_\\\\beta\\\\) measure reaches its best value at 1 and its worst score at 0.\\nWith \\\\(\\\\beta = 1\\\\),  \\\\(F_\\\\beta\\\\) and\\n\\\\(F_1\\\\)  are equivalent, and the recall and the precision are equally important.\\nThe precision_recall_curve computes a precision-recall curve\\nfrom the ground truth label and a score given by the classifier\\nby varying a decision threshold.\\nThe average_precision_score function computes the\\naverage precision\\n(AP) from prediction scores. The value is between 0 and 1 and higher is better.\\nAP is defined as\\n---------new doc---------\\n\\\\[\\\\text{AP} = \\\\sum_n (R_n - R_{n-1}) P_n\\\\]\\nwhere \\\\(P_n\\\\) and \\\\(R_n\\\\) are the precision and recall at the\\nnth threshold. With random predictions, the AP is the fraction of positive\\nsamples.\\nReferences [Manning2008] and [Everingham2010] present alternative variants of\\nAP that interpolate the precision-recall curve. Currently,\\naverage_precision_score does not implement any interpolated variant.\\nReferences [Davis2006] and [Flach2015] describe why a linear interpolation of\\npoints on the precision-recall curve provides an overly-optimistic measure of\\nclassifier performance. This linear interpolation is used when computing area\\nunder the curve with the trapezoidal rule in auc.\\nSeveral functions allow you to analyze the precision, recall and F-measures\\nscore:\\n\\n\\naverage_precision_score(y_true, y_score, *)\\nCompute average precision (AP) from prediction scores.\\n\\nf1_score(y_true, y_pred, *[, labels, ...])\\nCompute the F1 score, also known as balanced F-score or F-measure.\\n\\nfbeta_score(y_true, y_pred, *, beta[, ...])\\nCompute the F-beta score.\\n\\nprecision_recall_curve(y_true[, y_score, ...])\\nCompute precision-recall pairs for different probability thresholds.\\n\\nprecision_recall_fscore_support(y_true, ...)\\nCompute precision, recall, F-measure and support for each class.\\n---------new doc---------\\nprecision_recall_curve(y_true[, y_score, ...])\\nCompute precision-recall pairs for different probability thresholds.\\n\\nprecision_recall_fscore_support(y_true, ...)\\nCompute precision, recall, F-measure and support for each class.\\n\\nprecision_score(y_true, y_pred, *[, labels, ...])\\nCompute the precision.\\n\\nrecall_score(y_true, y_pred, *[, labels, ...])\\nCompute the recall.\\n\\n\\n\\n\\nNote that the precision_recall_curve function is restricted to the\\nbinary case. The average_precision_score function supports multiclass\\nand multilabel formats by computing each class score in a One-vs-the-rest (OvR)\\nfashion and averaging them or not depending of its average argument value.\\nThe PrecisionRecallDisplay.from_estimator and\\nPrecisionRecallDisplay.from_predictions functions will plot the\\nprecision-recall curve as follows.\\n\\n\\nExamples\\n\\nSee Custom refit strategy of a grid search with cross-validation\\nfor an example of precision_score and recall_score usage\\nto estimate parameters using grid search with nested cross-validation.\\nSee Precision-Recall\\nfor an example of precision_recall_curve usage to evaluate\\nclassifier output quality.\\n\\nReferences\\n\\n\\n[Manning2008]\\nC.D. Manning, P. Raghavan, H. Schütze, Introduction to Information Retrieval,\\n2008.\\n---------new doc---------\\nReferences\\n\\n\\n[Manning2008]\\nC.D. Manning, P. Raghavan, H. Schütze, Introduction to Information Retrieval,\\n2008.\\n\\n\\n[Everingham2010]\\nM. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman,\\nThe Pascal Visual Object Classes (VOC) Challenge,\\nIJCV 2010.\\n\\n\\n[Davis2006]\\nJ. Davis, M. Goadrich, The Relationship Between Precision-Recall and ROC Curves,\\nICML 2006.\\n\\n\\n[Flach2015]\\nP.A. Flach, M. Kull, Precision-Recall-Gain Curves: PR Analysis Done Right,\\nNIPS 2015.\\n\\n\\n\\n3.4.4.9.1. Binary classification#\\nIn a binary classification task, the terms ‘’positive’’ and ‘’negative’’ refer\\nto the classifier’s prediction, and the terms ‘’true’’ and ‘’false’’ refer to\\nwhether that prediction corresponds to the external judgment (sometimes known\\nas the ‘’observation’’). Given these definitions, we can formulate the\\nfollowing table:\\n\\n\\n\\nActual class (observation)\\n\\nPredicted class\\n(expectation)\\ntp (true positive)\\nCorrect result\\nfp (false positive)\\nUnexpected result\\n\\nfn (false negative)\\nMissing result\\ntn (true negative)\\nCorrect absence of result\\n\\n\\n\\n\\nIn this context, we can define the notions of precision and recall:\\n\\n\\\\[\\\\text{precision} = \\\\frac{\\\\text{tp}}{\\\\text{tp} + \\\\text{fp}},\\\\]\\n---------new doc---------\\nPredicted class\\n(expectation)\\ntp (true positive)\\nCorrect result\\nfp (false positive)\\nUnexpected result\\n\\nfn (false negative)\\nMissing result\\ntn (true negative)\\nCorrect absence of result\\n\\n\\n\\n\\nIn this context, we can define the notions of precision and recall:\\n\\n\\\\[\\\\text{precision} = \\\\frac{\\\\text{tp}}{\\\\text{tp} + \\\\text{fp}},\\\\]\\n\\n\\\\[\\\\text{recall} = \\\\frac{\\\\text{tp}}{\\\\text{tp} + \\\\text{fn}},\\\\]\\n(Sometimes recall is also called ‘’sensitivity’’)\\nF-measure is the weighted harmonic mean of precision and recall, with precision’s\\ncontribution to the mean weighted by some parameter \\\\(\\\\beta\\\\):\\n\\n\\\\[F_\\\\beta = (1 + \\\\beta^2) \\\\frac{\\\\text{precision} \\\\times \\\\text{recall}}{\\\\beta^2 \\\\text{precision} + \\\\text{recall}}\\\\]\\nTo avoid division by zero when precision and recall are zero, Scikit-Learn calculates F-measure with this\\notherwise-equivalent formula:\\n---------new doc---------\\n\\\\[F_\\\\beta = \\\\frac{(1 + \\\\beta^2) \\\\text{tp}}{(1 + \\\\beta^2) \\\\text{tp} + \\\\text{fp} + \\\\beta^2 \\\\text{fn}}\\\\]\\nNote that this formula is still undefined when there are no true positives, false\\npositives, or false negatives. By default, F-1 for a set of exclusively true negatives\\nis calculated as 0, however this behavior can be changed using the zero_division\\nparameter.\\nHere are some small examples in binary classification:\\n>>> from sklearn import metrics\\n>>> y_pred = [0, 1, 0, 0]\\n>>> y_true = [0, 1, 0, 1]\\n>>> metrics.precision_score(y_true, y_pred)\\n1.0\\n>>> metrics.recall_score(y_true, y_pred)\\n0.5\\n>>> metrics.f1_score(y_true, y_pred)\\n0.66...\\n>>> metrics.fbeta_score(y_true, y_pred, beta=0.5)\\n0.83...\\n>>> metrics.fbeta_score(y_true, y_pred, beta=1)\\n0.66...\\n>>> metrics.fbeta_score(y_true, y_pred, beta=2)\\n0.55...\\n>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)\\n(array([0.66..., 1.        ]), array([1. , 0.5]), array([0.71..., 0.83...]), array([2, 2]))\\n---------new doc---------\\n>>> import numpy as np\\n>>> from sklearn.metrics import precision_recall_curve\\n>>> from sklearn.metrics import average_precision_score\\n>>> y_true = np.array([0, 0, 1, 1])\\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\\n>>> precision, recall, threshold = precision_recall_curve(y_true, y_scores)\\n>>> precision\\narray([0.5       , 0.66..., 0.5       , 1.        , 1.        ])\\n>>> recall\\narray([1. , 1. , 0.5, 0.5, 0. ])\\n>>> threshold\\narray([0.1 , 0.35, 0.4 , 0.8 ])\\n>>> average_precision_score(y_true, y_scores)\\n0.83...\\n\\n\\n\\n\\n3.4.4.9.2. Multiclass and multilabel classification#\\nIn a multiclass and multilabel classification task, the notions of precision,\\nrecall, and F-measures can be applied to each label independently.\\nThere are a few ways to combine results across labels,\\nspecified by the average argument to the\\naverage_precision_score, f1_score,\\nfbeta_score, precision_recall_fscore_support,\\nprecision_score and recall_score functions, as described\\nabove.\\nNote the following behaviors when averaging:\\n---------new doc---------\\nIf all labels are included, “micro”-averaging in a multiclass setting will produce\\nprecision, recall and \\\\(F\\\\) that are all identical to accuracy.\\n“weighted” averaging may produce a F-score that is not between precision and recall.\\n“macro” averaging for F-measures is calculated as the arithmetic mean over\\nper-label/class F-measures, not the harmonic mean over the arithmetic precision and\\nrecall means. Both calculations can be seen in the literature but are not equivalent,\\nsee [OB2019] for details.\\n\\nTo make this more explicit, consider the following notation:\\n---------new doc---------\\nTo make this more explicit, consider the following notation:\\n\\n\\\\(y\\\\) the set of true \\\\((sample, label)\\\\) pairs\\n\\\\(\\\\hat{y}\\\\) the set of predicted \\\\((sample, label)\\\\) pairs\\n\\\\(L\\\\) the set of labels\\n\\\\(S\\\\) the set of samples\\n\\\\(y_s\\\\) the subset of \\\\(y\\\\) with sample \\\\(s\\\\),\\ni.e. \\\\(y_s := \\\\left\\\\{(s', l) \\\\in y | s' = s\\\\right\\\\}\\\\)\\n\\\\(y_l\\\\) the subset of \\\\(y\\\\) with label \\\\(l\\\\)\\nsimilarly, \\\\(\\\\hat{y}_s\\\\) and \\\\(\\\\hat{y}_l\\\\) are subsets of\\n\\\\(\\\\hat{y}\\\\)\\n\\\\(P(A, B) := \\\\frac{\\\\left| A \\\\cap B \\\\right|}{\\\\left|B\\\\right|}\\\\) for some\\nsets \\\\(A\\\\) and \\\\(B\\\\)\\n\\\\(R(A, B) := \\\\frac{\\\\left| A \\\\cap B \\\\right|}{\\\\left|A\\\\right|}\\\\)\\n(Conventions vary on handling \\\\(A = \\\\emptyset\\\\); this implementation uses\\n\\\\(R(A, B):=0\\\\), and similar for \\\\(P\\\\).)\\n\\\\(F_\\\\beta(A, B) := \\\\left(1 + \\\\beta^2\\\\right) \\\\frac{P(A, B) \\\\times R(A, B)}{\\\\beta^2 P(A, B) + R(A, B)}\\\\)\\n\\nThen the metrics are defined as:\\n\\n\\naverage\\nPrecision\\nRecall\\nF_beta\\n---------new doc---------\\nNone\\n\\\\(\\\\langle P(y_l, \\\\hat{y}_l) | l \\\\in L \\\\rangle\\\\)\\n\\\\(\\\\langle R(y_l, \\\\hat{y}_l) | l \\\\in L \\\\rangle\\\\)\\n\\\\(\\\\langle F_\\\\beta(y_l, \\\\hat{y}_l) | l \\\\in L \\\\rangle\\\\)\\n\\n\\n\\n\\n>>> from sklearn import metrics\\n>>> y_true = [0, 1, 2, 0, 1, 2]\\n>>> y_pred = [0, 2, 1, 0, 0, 1]\\n>>> metrics.precision_score(y_true, y_pred, average='macro')\\n0.22...\\n>>> metrics.recall_score(y_true, y_pred, average='micro')\\n0.33...\\n>>> metrics.f1_score(y_true, y_pred, average='weighted')\\n0.26...\\n>>> metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)\\n0.23...\\n>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)\\n(array([0.66..., 0.        , 0.        ]), array([1., 0., 0.]), array([0.71..., 0.        , 0.        ]), array([2, 2, 2]...))\\n---------new doc---------\\nFor multiclass classification with a “negative class”, it is possible to exclude some labels:\\n>>> metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')\\n... # excluding 0, no labels were correctly recalled\\n0.0\\n\\n\\nSimilarly, labels not present in the data sample may be accounted for in macro-averaging.\\n>>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')\\n0.166...\\n\\n\\nReferences\\n\\n\\n[OB2019]\\nOpitz, J., & Burst, S. (2019). “Macro f1 and macro f1.”\\n\\n\\n\\n\\n\\n3.4.4.10. Jaccard similarity coefficient score#\\nThe jaccard_score function computes the average of Jaccard similarity\\ncoefficients, also called the\\nJaccard index, between pairs of label sets.\\nThe Jaccard similarity coefficient with a ground truth label set \\\\(y\\\\) and\\npredicted label set \\\\(\\\\hat{y}\\\\), is defined as\\n---------new doc---------\\n3.4.4.10. Jaccard similarity coefficient score#\\nThe jaccard_score function computes the average of Jaccard similarity\\ncoefficients, also called the\\nJaccard index, between pairs of label sets.\\nThe Jaccard similarity coefficient with a ground truth label set \\\\(y\\\\) and\\npredicted label set \\\\(\\\\hat{y}\\\\), is defined as\\n\\n\\\\[J(y, \\\\hat{y}) = \\\\frac{|y \\\\cap \\\\hat{y}|}{|y \\\\cup \\\\hat{y}|}.\\\\]\\nThe jaccard_score (like precision_recall_fscore_support) applies\\nnatively to binary targets. By computing it set-wise it can be extended to apply\\nto multilabel and multiclass through the use of average (see\\nabove).\\nIn the binary case:\\n>>> import numpy as np\\n>>> from sklearn.metrics import jaccard_score\\n>>> y_true = np.array([[0, 1, 1],\\n...                    [1, 1, 0]])\\n>>> y_pred = np.array([[1, 1, 1],\\n...                    [1, 0, 0]])\\n>>> jaccard_score(y_true[0], y_pred[0])\\n0.6666...\\n\\n\\nIn the 2D comparison case (e.g. image similarity):\\n>>> jaccard_score(y_true, y_pred, average=\\\"micro\\\")\\n0.6\\n---------new doc---------\\nIn the 2D comparison case (e.g. image similarity):\\n>>> jaccard_score(y_true, y_pred, average=\\\"micro\\\")\\n0.6\\n\\n\\nIn the multilabel case with binary label indicators:\\n>>> jaccard_score(y_true, y_pred, average='samples')\\n0.5833...\\n>>> jaccard_score(y_true, y_pred, average='macro')\\n0.6666...\\n>>> jaccard_score(y_true, y_pred, average=None)\\narray([0.5, 0.5, 1. ])\\n\\n\\nMulticlass problems are binarized and treated like the corresponding\\nmultilabel problem:\\n>>> y_pred = [0, 2, 1, 2]\\n>>> y_true = [0, 1, 2, 2]\\n>>> jaccard_score(y_true, y_pred, average=None)\\narray([1. , 0. , 0.33...])\\n>>> jaccard_score(y_true, y_pred, average='macro')\\n0.44...\\n>>> jaccard_score(y_true, y_pred, average='micro')\\n0.33...\\n---------new doc---------\\nThe first [.9, .1] in y_pred denotes 90% probability that the first\\nsample has label 0.  The log loss is non-negative.\\n\\n\\n3.4.4.13. Matthews correlation coefficient#\\nThe matthews_corrcoef function computes the\\nMatthew’s correlation coefficient (MCC)\\nfor binary classes.  Quoting Wikipedia:\\n\\n“The Matthews correlation coefficient is used in machine learning as a\\nmeasure of the quality of binary (two-class) classifications. It takes\\ninto account true and false positives and negatives and is generally\\nregarded as a balanced measure which can be used even if the classes are\\nof very different sizes. The MCC is in essence a correlation coefficient\\nvalue between -1 and +1. A coefficient of +1 represents a perfect\\nprediction, 0 an average random prediction and -1 an inverse prediction.\\nThe statistic is also known as the phi coefficient.”\\n\\nIn the binary (two-class) case, \\\\(tp\\\\), \\\\(tn\\\\), \\\\(fp\\\\) and\\n\\\\(fn\\\\) are respectively the number of true positives, true negatives, false\\npositives and false negatives, the MCC is defined as\\n\\n\\\\[MCC = \\\\frac{tp \\\\times tn - fp \\\\times fn}{\\\\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.\\\\]\\nIn the multiclass case, the Matthews correlation coefficient can be defined in terms of a\\nconfusion_matrix \\\\(C\\\\) for \\\\(K\\\\) classes.  To simplify the\\ndefinition consider the following intermediate variables:\\n---------new doc---------\\n3.4.4.14. Multi-label confusion matrix#\\nThe multilabel_confusion_matrix function computes class-wise (default)\\nor sample-wise (samplewise=True) multilabel confusion matrix to evaluate\\nthe accuracy of a classification. multilabel_confusion_matrix also treats\\nmulticlass data as if it were multilabel, as this is a transformation commonly\\napplied to evaluate multiclass problems with binary classification metrics\\n(such as precision, recall, etc.).\\nWhen calculating class-wise multilabel confusion matrix \\\\(C\\\\), the\\ncount of true negatives for class \\\\(i\\\\) is \\\\(C_{i,0,0}\\\\), false\\nnegatives is \\\\(C_{i,1,0}\\\\), true positives is \\\\(C_{i,1,1}\\\\)\\nand false positives is \\\\(C_{i,0,1}\\\\).\\nHere is an example demonstrating the use of the\\nmultilabel_confusion_matrix function with\\nmultilabel indicator matrix input:\\n>>> import numpy as np\\n>>> from sklearn.metrics import multilabel_confusion_matrix\\n>>> y_true = np.array([[1, 0, 1],\\n...                    [0, 1, 0]])\\n>>> y_pred = np.array([[1, 0, 0],\\n...                    [0, 1, 1]])\\n>>> multilabel_confusion_matrix(y_true, y_pred)\\narray([[[1, 0],\\n        [0, 1]],\\n---------new doc---------\\n[[5, 0],\\n        [1, 0]],\\n\\n       [[2, 1],\\n        [1, 2]]])\\n\\n\\nHere are some examples demonstrating the use of the\\nmultilabel_confusion_matrix function to calculate recall\\n(or sensitivity), specificity, fall out and miss rate for each class in a\\nproblem with multilabel indicator matrix input.\\nCalculating\\nrecall\\n(also called the true positive rate or the sensitivity) for each class:\\n>>> y_true = np.array([[0, 0, 1],\\n...                    [0, 1, 0],\\n...                    [1, 1, 0]])\\n>>> y_pred = np.array([[0, 1, 0],\\n...                    [0, 0, 1],\\n...                    [1, 1, 0]])\\n>>> mcm = multilabel_confusion_matrix(y_true, y_pred)\\n>>> tn = mcm[:, 0, 0]\\n>>> tp = mcm[:, 1, 1]\\n>>> fn = mcm[:, 1, 0]\\n>>> fp = mcm[:, 0, 1]\\n>>> tp / (tp + fn)\\narray([1. , 0.5, 0. ])\\n---------new doc---------\\nCalculating\\nspecificity\\n(also called the true negative rate) for each class:\\n>>> tn / (tn + fp)\\narray([1. , 0. , 0.5])\\n\\n\\nCalculating fall out\\n(also called the false positive rate) for each class:\\n>>> fp / (fp + tn)\\narray([0. , 1. , 0.5])\\n\\n\\nCalculating miss rate\\n(also called the false negative rate) for each class:\\n>>> fn / (fn + tp)\\narray([0. , 0.5, 1. ])\\n\\n\\n\\n\\n3.4.4.15. Receiver operating characteristic (ROC)#\\nThe function roc_curve computes the\\nreceiver operating characteristic curve, or ROC curve.\\nQuoting Wikipedia :\\n\\n“A receiver operating characteristic (ROC), or simply ROC curve, is a\\ngraphical plot which illustrates the performance of a binary classifier\\nsystem as its discrimination threshold is varied. It is created by plotting\\nthe fraction of true positives out of the positives (TPR = true positive\\nrate) vs. the fraction of false positives out of the negatives (FPR = false\\npositive rate), at various threshold settings. TPR is also known as\\nsensitivity, and FPR is one minus the specificity or true negative rate.”\\n---------new doc---------\\nThis function requires the true binary value and the target scores, which can\\neither be probability estimates of the positive class, confidence values, or\\nbinary decisions. Here is a small example of how to use the roc_curve\\nfunction:\\n>>> import numpy as np\\n>>> from sklearn.metrics import roc_curve\\n>>> y = np.array([1, 1, 2, 2])\\n>>> scores = np.array([0.1, 0.4, 0.35, 0.8])\\n>>> fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\\n>>> fpr\\narray([0. , 0. , 0.5, 0.5, 1. ])\\n>>> tpr\\narray([0. , 0.5, 0.5, 1. , 1. ])\\n>>> thresholds\\narray([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\\n\\n\\nCompared to metrics such as the subset accuracy, the Hamming loss, or the\\nF1 score, ROC doesn’t require optimizing a threshold for each label.\\nThe roc_auc_score function, denoted by ROC-AUC or AUROC, computes the\\narea under the ROC curve. By doing so, the curve information is summarized in\\none number.\\nThe following figure shows the ROC curve and ROC-AUC score for a classifier\\naimed to distinguish the virginica flower from the rest of the species in the\\nIris plants dataset:\\n\\n\\nFor more information see the Wikipedia article on AUC.\\n---------new doc---------\\nFor more information see the Wikipedia article on AUC.\\n\\n3.4.4.15.1. Binary case#\\nIn the binary case, you can either provide the probability estimates, using\\nthe classifier.predict_proba() method, or the non-thresholded decision values\\ngiven by the classifier.decision_function() method. In the case of providing\\nthe probability estimates, the probability of the class with the\\n“greater label” should be provided. The “greater label” corresponds to\\nclassifier.classes_[1] and thus classifier.predict_proba(X)[:, 1].\\nTherefore, the y_score parameter is of size (n_samples,).\\n>>> from sklearn.datasets import load_breast_cancer\\n>>> from sklearn.linear_model import LogisticRegression\\n>>> from sklearn.metrics import roc_auc_score\\n>>> X, y = load_breast_cancer(return_X_y=True)\\n>>> clf = LogisticRegression(solver=\\\"liblinear\\\").fit(X, y)\\n>>> clf.classes_\\narray([0, 1])\\n\\n\\nWe can use the probability estimates corresponding to clf.classes_[1].\\n>>> y_score = clf.predict_proba(X)[:, 1]\\n>>> roc_auc_score(y, y_score)\\n0.99...\\n\\n\\nOtherwise, we can use the non-thresholded decision values\\n>>> roc_auc_score(y, clf.decision_function(X))\\n0.99...\\n---------new doc---------\\nWe can use the probability estimates corresponding to clf.classes_[1].\\n>>> y_score = clf.predict_proba(X)[:, 1]\\n>>> roc_auc_score(y, y_score)\\n0.99...\\n\\n\\nOtherwise, we can use the non-thresholded decision values\\n>>> roc_auc_score(y, clf.decision_function(X))\\n0.99...\\n\\n\\n\\n\\n3.4.4.15.2. Multi-class case#\\nThe roc_auc_score function can also be used in multi-class\\nclassification. Two averaging strategies are currently supported: the\\none-vs-one algorithm computes the average of the pairwise ROC AUC scores, and\\nthe one-vs-rest algorithm computes the average of the ROC AUC scores for each\\nclass against all other classes. In both cases, the predicted labels are\\nprovided in an array with values from 0 to n_classes, and the scores\\ncorrespond to the probability estimates that a sample belongs to a particular\\nclass. The OvO and OvR algorithms support weighting uniformly\\n(average='macro') and by prevalence (average='weighted').\\n\\n\\nOne-vs-one Algorithm#\\nComputes the average AUC of all possible pairwise\\ncombinations of classes. [HT2001] defines a multiclass AUC metric weighted\\nuniformly:\\n---------new doc---------\\nOne-vs-rest Algorithm#\\nComputes the AUC of each class against the rest\\n[PD2000]. The algorithm is functionally the same as the multilabel case. To\\nenable this algorithm set the keyword argument multiclass to 'ovr'.\\nAdditionally to 'macro' [F2006] and 'weighted' [F2001] averaging, OvR\\nsupports 'micro' averaging.\\nIn applications where a high false positive rate is not tolerable the parameter\\nmax_fpr of roc_auc_score can be used to summarize the ROC curve up\\nto the given limit.\\nThe following figure shows the micro-averaged ROC curve and its corresponding\\nROC-AUC score for a classifier aimed to distinguish the different species in\\nthe Iris plants dataset:\\n---------new doc---------\\n3.4.4.15.3. Multi-label case#\\nIn multi-label classification, the roc_auc_score function is\\nextended by averaging over the labels as above. In this case,\\nyou should provide a y_score of shape (n_samples, n_classes). Thus, when\\nusing the probability estimates, one needs to select the probability of the\\nclass with the greater label for each output.\\n>>> from sklearn.datasets import make_multilabel_classification\\n>>> from sklearn.multioutput import MultiOutputClassifier\\n>>> X, y = make_multilabel_classification(random_state=0)\\n>>> inner_clf = LogisticRegression(solver=\\\"liblinear\\\", random_state=0)\\n>>> clf = MultiOutputClassifier(inner_clf).fit(X, y)\\n>>> y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\\n>>> roc_auc_score(y, y_score, average=None)\\narray([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\\n\\n\\nAnd the decision values do not require such processing.\\n>>> from sklearn.linear_model import RidgeClassifierCV\\n>>> clf = RidgeClassifierCV().fit(X, y)\\n>>> y_score = clf.decision_function(X)\\n>>> roc_auc_score(y, y_score, average=None)\\narray([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\\n\\n\\nExamples\\n---------new doc---------\\n[F2006]\\nFawcett, T., 2006. An introduction to ROC analysis.\\nPattern Recognition Letters, 27(8), pp. 861-874.\\n\\n\\n[F2001]\\nFawcett, T., 2001. Using rule sets to maximize\\nROC performance\\nIn Data Mining, 2001.\\nProceedings IEEE International Conference, pp. 131-138.\\n\\n\\n\\n\\n\\n3.4.4.16. Detection error tradeoff (DET)#\\nThe function det_curve computes the\\ndetection error tradeoff curve (DET) curve [WikipediaDET2017].\\nQuoting Wikipedia:\\n\\n“A detection error tradeoff (DET) graph is a graphical plot of error rates\\nfor binary classification systems, plotting false reject rate vs. false\\naccept rate. The x- and y-axes are scaled non-linearly by their standard\\nnormal deviates (or just by logarithmic transformation), yielding tradeoff\\ncurves that are more linear than ROC curves, and use most of the image area\\nto highlight the differences of importance in the critical operating region.”\\n\\nDET curves are a variation of receiver operating characteristic (ROC) curves\\nwhere False Negative Rate is plotted on the y-axis instead of True Positive\\nRate.\\nDET curves are commonly plotted in normal deviate scale by transformation with\\n\\\\(\\\\phi^{-1}\\\\) (with \\\\(\\\\phi\\\\) being the cumulative distribution\\nfunction).\\nThe resulting performance curves explicitly visualize the tradeoff of error\\ntypes for given classification algorithms.\\nSee [Martin1997] for examples and further motivation.\\nThis figure compares the ROC and DET curves of two example classifiers on the\\nsame classification task:\\n\\n\\n\\n\\nProperties#\\n---------new doc---------\\nProperties#\\n\\nDET curves form a linear curve in normal deviate scale if the detection\\nscores are normally (or close-to normally) distributed.\\nIt was shown by [Navratil2007] that the reverse is not necessarily true and\\neven more general distributions are able to produce linear DET curves.\\nThe normal deviate scale transformation spreads out the points such that a\\ncomparatively larger space of plot is occupied.\\nTherefore curves with similar classification performance might be easier to\\ndistinguish on a DET plot.\\nWith False Negative Rate being “inverse” to True Positive Rate the point\\nof perfection for DET curves is the origin (in contrast to the top left\\ncorner for ROC curves).\\n\\n\\n\\n\\nApplications and limitations#\\nDET curves are intuitive to read and hence allow quick visual assessment of a\\nclassifier’s performance.\\nAdditionally DET curves can be consulted for threshold analysis and operating\\npoint selection.\\nThis is particularly helpful if a comparison of error types is required.\\nOn the other hand DET curves do not provide their metric as a single number.\\nTherefore for either automated evaluation or comparison to other\\nclassification tasks metrics like the derived area under ROC curve might be\\nbetter suited.\\n\\nExamples\\n\\nSee Detection error tradeoff (DET) curve\\nfor an example comparison between receiver operating characteristic (ROC)\\ncurves and Detection error tradeoff (DET) curves.\\n\\nReferences\\n\\n\\n[WikipediaDET2017]\\nWikipedia contributors. Detection error tradeoff.\\nWikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.\\nAvailable at: https://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054.\\nAccessed February 19, 2018.\\n---------new doc---------\\nReferences\\n\\n\\n[WikipediaDET2017]\\nWikipedia contributors. Detection error tradeoff.\\nWikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.\\nAvailable at: https://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054.\\nAccessed February 19, 2018.\\n\\n\\n[Martin1997]\\nA. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki,\\nThe DET Curve in Assessment of Detection Task Performance, NIST 1997.\\n\\n\\n[Navratil2007]\\nJ. Navractil and D. Klusacek,\\n“On Linear DETs”,\\n2007 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing - ICASSP ‘07, Honolulu,\\nHI, 2007, pp. IV-229-IV-232.\\n---------new doc---------\\n[Navratil2007]\\nJ. Navractil and D. Klusacek,\\n“On Linear DETs”,\\n2007 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing - ICASSP ‘07, Honolulu,\\nHI, 2007, pp. IV-229-IV-232.\\n\\n\\n\\n\\n3.4.4.17. Zero one loss#\\nThe zero_one_loss function computes the sum or the average of the 0-1\\nclassification loss (\\\\(L_{0-1}\\\\)) over \\\\(n_{\\\\text{samples}}\\\\). By\\ndefault, the function normalizes over the sample. To get the sum of the\\n\\\\(L_{0-1}\\\\), set normalize to False.\\nIn multilabel classification, the zero_one_loss scores a subset as\\none if its labels strictly match the predictions, and as a zero if there\\nare any errors.  By default, the function returns the percentage of imperfectly\\npredicted subsets.  To get the count of such subsets instead, set\\nnormalize to False\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of\\nthe \\\\(i\\\\)-th sample and \\\\(y_i\\\\) is the corresponding true value,\\nthen the 0-1 loss \\\\(L_{0-1}\\\\) is defined as:\\n---------new doc---------\\n\\\\[L_{0-1}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} 1(\\\\hat{y}_i \\\\not= y_i)\\\\]\\nwhere \\\\(1(x)\\\\) is the indicator function. The zero one\\nloss can also be computed as \\\\(zero-one loss = 1 - accuracy\\\\).\\n>>> from sklearn.metrics import zero_one_loss\\n>>> y_pred = [1, 2, 3, 4]\\n>>> y_true = [2, 2, 3, 4]\\n>>> zero_one_loss(y_true, y_pred)\\n0.25\\n>>> zero_one_loss(y_true, y_pred, normalize=False)\\n1.0\\n\\n\\nIn the multilabel case with binary label indicators, where the first label\\nset [0,1] has an error:\\n>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\\n0.5\\n\\n>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)),  normalize=False)\\n1.0\\n\\n\\nExamples\\n\\nSee Recursive feature elimination with cross-validation\\nfor an example of zero one loss usage to perform recursive feature\\nelimination with cross-validation.\\n\\n\\n\\n3.4.4.18. Brier score loss#\\nThe brier_score_loss function computes the\\nBrier score\\nfor binary classes [Brier1950]. Quoting Wikipedia:\\n---------new doc---------\\nExamples\\n\\nSee Recursive feature elimination with cross-validation\\nfor an example of zero one loss usage to perform recursive feature\\nelimination with cross-validation.\\n\\n\\n\\n3.4.4.18. Brier score loss#\\nThe brier_score_loss function computes the\\nBrier score\\nfor binary classes [Brier1950]. Quoting Wikipedia:\\n\\n“The Brier score is a proper score function that measures the accuracy of\\nprobabilistic predictions. It is applicable to tasks in which predictions\\nmust assign probabilities to a set of mutually exclusive discrete outcomes.”\\n\\nThis function returns the mean squared error of the actual outcome\\n\\\\(y \\\\in \\\\{0,1\\\\}\\\\) and the predicted probability estimate\\n\\\\(p = \\\\operatorname{Pr}(y = 1)\\\\) (predict_proba) as outputted by:\\n---------new doc---------\\nThis function returns the mean squared error of the actual outcome\\n\\\\(y \\\\in \\\\{0,1\\\\}\\\\) and the predicted probability estimate\\n\\\\(p = \\\\operatorname{Pr}(y = 1)\\\\) (predict_proba) as outputted by:\\n\\n\\\\[BS = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}} - 1}(y_i - p_i)^2\\\\]\\nThe Brier score loss is also between 0 to 1 and the lower the value (the mean\\nsquare difference is smaller), the more accurate the prediction is.\\nHere is a small example of usage of this function:\\n>>> import numpy as np\\n>>> from sklearn.metrics import brier_score_loss\\n>>> y_true = np.array([0, 1, 1, 0])\\n>>> y_true_categorical = np.array([\\\"spam\\\", \\\"ham\\\", \\\"ham\\\", \\\"spam\\\"])\\n>>> y_prob = np.array([0.1, 0.9, 0.8, 0.4])\\n>>> y_pred = np.array([0, 1, 1, 0])\\n>>> brier_score_loss(y_true, y_prob)\\n0.055\\n>>> brier_score_loss(y_true, 1 - y_prob, pos_label=0)\\n0.055\\n>>> brier_score_loss(y_true_categorical, y_prob, pos_label=\\\"ham\\\")\\n0.055\\n>>> brier_score_loss(y_true, y_prob > 0.5)\\n0.0\\n---------new doc---------\\nThe Brier score can be used to assess how well a classifier is calibrated.\\nHowever, a lower Brier score loss does not always mean a better calibration.\\nThis is because, by analogy with the bias-variance decomposition of the mean\\nsquared error, the Brier score loss can be decomposed as the sum of calibration\\nloss and refinement loss [Bella2012]. Calibration loss is defined as the mean\\nsquared deviation from empirical probabilities derived from the slope of ROC\\nsegments. Refinement loss can be defined as the expected optimal loss as\\nmeasured by the area under the optimal cost curve. Refinement loss can change\\nindependently from calibration loss, thus a lower Brier score loss does not\\nnecessarily mean a better calibrated model. “Only when refinement loss remains\\nthe same does a lower Brier score loss always mean better calibration”\\n[Bella2012], [Flach2008].\\nExamples\\n\\nSee Probability calibration of classifiers\\nfor an example of Brier score loss usage to perform probability\\ncalibration of classifiers.\\n\\nReferences\\n\\n\\n[Brier1950]\\nG. Brier, Verification of forecasts expressed in terms of probability,\\nMonthly weather review 78.1 (1950)\\n\\n\\n[Bella2012]\\n(1,2)\\nBella, Ferri, Hernández-Orallo, and Ramírez-Quintana\\n“Calibration of Machine Learning Models”\\nin Khosrow-Pour, M. “Machine learning: concepts, methodologies, tools\\nand applications.” Hershey, PA: Information Science Reference (2012).\\n---------new doc---------\\n[Bella2012]\\n(1,2)\\nBella, Ferri, Hernández-Orallo, and Ramírez-Quintana\\n“Calibration of Machine Learning Models”\\nin Khosrow-Pour, M. “Machine learning: concepts, methodologies, tools\\nand applications.” Hershey, PA: Information Science Reference (2012).\\n\\n\\n[Flach2008]\\nFlach, Peter, and Edson Matsubara. “On classification, ranking,\\nand probability estimation.”\\nDagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum fr Informatik (2008).\\n\\n\\n\\n\\n3.4.4.19. Class likelihood ratios#\\nThe class_likelihood_ratios function computes the positive and negative\\nlikelihood ratios\\n\\\\(LR_\\\\pm\\\\) for binary classes, which can be interpreted as the ratio of\\npost-test to pre-test odds as explained below. As a consequence, this metric is\\ninvariant w.r.t. the class prevalence (the number of samples in the positive\\nclass divided by the total number of samples) and can be extrapolated between\\npopulations regardless of any possible class imbalance.\\nThe \\\\(LR_\\\\pm\\\\) metrics are therefore very useful in settings where the data\\navailable to learn and evaluate a classifier is a study population with nearly\\nbalanced classes, such as a case-control study, while the target application,\\ni.e. the general population, has very low prevalence.\\nThe positive likelihood ratio \\\\(LR_+\\\\) is the probability of a classifier to\\ncorrectly predict that a sample belongs to the positive class divided by the\\nprobability of predicting the positive class for a sample belonging to the\\nnegative class:\\n---------new doc---------\\n\\\\[LR_+ = \\\\frac{\\\\text{PR}(P+|T+)}{\\\\text{PR}(P+|T-)}.\\\\]\\nThe notation here refers to predicted (\\\\(P\\\\)) or true (\\\\(T\\\\)) label and\\nthe sign \\\\(+\\\\) and \\\\(-\\\\) refer to the positive and negative class,\\nrespectively, e.g. \\\\(P+\\\\) stands for “predicted positive”.\\nAnalogously, the negative likelihood ratio \\\\(LR_-\\\\) is the probability of a\\nsample of the positive class being classified as belonging to the negative class\\ndivided by the probability of a sample of the negative class being correctly\\nclassified:\\n\\n\\\\[LR_- = \\\\frac{\\\\text{PR}(P-|T+)}{\\\\text{PR}(P-|T-)}.\\\\]\\nFor classifiers above chance \\\\(LR_+\\\\) above 1 higher is better, while\\n\\\\(LR_-\\\\) ranges from 0 to 1 and lower is better.\\nValues of \\\\(LR_\\\\pm\\\\approx 1\\\\) correspond to chance level.\\nNotice that probabilities differ from counts, for instance\\n\\\\(\\\\operatorname{PR}(P+|T+)\\\\) is not equal to the number of true positive\\ncounts tp (see the wikipedia page for\\nthe actual formulas).\\nExamples\\n\\nClass Likelihood Ratios to measure classification performance\\n\\n\\n\\nInterpretation across varying prevalence#\\nBoth class likelihood ratios are interpretable in terms of an odds ratio\\n(pre-test and post-tests):\\n\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times \\\\text{pre-test odds}.\\\\]\\nOdds are in general related to probabilities via\\n---------new doc---------\\nClass Likelihood Ratios to measure classification performance\\n\\n\\n\\nInterpretation across varying prevalence#\\nBoth class likelihood ratios are interpretable in terms of an odds ratio\\n(pre-test and post-tests):\\n\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times \\\\text{pre-test odds}.\\\\]\\nOdds are in general related to probabilities via\\n\\n\\\\[\\\\text{odds} = \\\\frac{\\\\text{probability}}{1 - \\\\text{probability}},\\\\]\\nor equivalently\\n\\n\\\\[\\\\text{probability} = \\\\frac{\\\\text{odds}}{1 + \\\\text{odds}}.\\\\]\\nOn a given population, the pre-test probability is given by the prevalence. By\\nconverting odds to probabilities, the likelihood ratios can be translated into a\\nprobability of truly belonging to either class before and after a classifier\\nprediction:\\n\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times\\n\\\\frac{\\\\text{pre-test probability}}{1 - \\\\text{pre-test probability}},\\\\]\\n\\n\\\\[\\\\text{post-test probability} = \\\\frac{\\\\text{post-test odds}}{1 + \\\\text{post-test odds}}.\\\\]\\n---------new doc---------\\n\\\\[\\\\text{post-test odds} = \\\\text{Likelihood ratio} \\\\times\\n\\\\frac{\\\\text{pre-test probability}}{1 - \\\\text{pre-test probability}},\\\\]\\n\\n\\\\[\\\\text{post-test probability} = \\\\frac{\\\\text{post-test odds}}{1 + \\\\text{post-test odds}}.\\\\]\\n\\n\\n\\nMathematical divergences#\\nThe positive likelihood ratio is undefined when \\\\(fp = 0\\\\), which can be\\ninterpreted as the classifier perfectly identifying positive cases. If \\\\(fp\\n= 0\\\\) and additionally \\\\(tp = 0\\\\), this leads to a zero/zero division. This\\nhappens, for instance, when using a DummyClassifier that always predicts the\\nnegative class and therefore the interpretation as a perfect classifier is lost.\\nThe negative likelihood ratio is undefined when \\\\(tn = 0\\\\). Such divergence\\nis invalid, as \\\\(LR_- > 1\\\\) would indicate an increase in the odds of a\\nsample belonging to the positive class after being classified as negative, as if\\nthe act of classifying caused the positive condition. This includes the case of\\na DummyClassifier that always predicts the positive class (i.e. when\\n\\\\(tn=fn=0\\\\)).\\nBoth class likelihood ratios are undefined when \\\\(tp=fn=0\\\\), which means\\nthat no samples of the positive class were present in the testing set. This can\\nalso happen when cross-validating highly imbalanced data.\\nIn all the previous cases the class_likelihood_ratios function raises by\\ndefault an appropriate warning message and returns nan to avoid pollution when\\naveraging over cross-validation folds.\\nFor a worked-out demonstration of the class_likelihood_ratios function,\\nsee the example below.\\n\\n\\n\\nReferences#\\n---------new doc---------\\nReferences#\\n\\nWikipedia entry for Likelihood ratios in diagnostic testing\\nBrenner, H., & Gefeller, O. (1997).\\nVariation of sensitivity, specificity, likelihood ratios and predictive\\nvalues with disease prevalence.\\nStatistics in medicine, 16(9), 981-991.\\n\\n\\n\\n\\n3.4.4.20. D² score for classification#\\nThe D² score computes the fraction of deviance explained.\\nIt is a generalization of R², where the squared error is generalized and replaced\\nby a classification deviance of choice \\\\(\\\\text{dev}(y, \\\\hat{y})\\\\)\\n(e.g., Log loss). D² is a form of a skill score.\\nIt is calculated as\\n\\n\\\\[D^2(y, \\\\hat{y}) = 1 - \\\\frac{\\\\text{dev}(y, \\\\hat{y})}{\\\\text{dev}(y, y_{\\\\text{null}})} \\\\,.\\\\]\\nWhere \\\\(y_{\\\\text{null}}\\\\) is the optimal prediction of an intercept-only model\\n(e.g., the per-class proportion of y_true in the case of the Log loss).\\nLike R², the best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). A constant model that always predicts\\n\\\\(y_{\\\\text{null}}\\\\), disregarding the input features, would get a D² score\\nof 0.0.\\n\\n\\nD2 log loss score#\\nThe d2_log_loss_score function implements the special case\\nof D² with the log loss, see Log loss, i.e.:\\n---------new doc---------\\n\\\\[\\\\text{dev}(y, \\\\hat{y}) = \\\\text{log_loss}(y, \\\\hat{y}).\\\\]\\nHere are some usage examples of the d2_log_loss_score function:\\n>>> from sklearn.metrics import d2_log_loss_score\\n>>> y_true = [1, 1, 2, 3]\\n>>> y_pred = [\\n...    [0.5, 0.25, 0.25],\\n...    [0.5, 0.25, 0.25],\\n...    [0.5, 0.25, 0.25],\\n...    [0.5, 0.25, 0.25],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n0.0\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [\\n...     [0.98, 0.01, 0.01],\\n...     [0.01, 0.98, 0.01],\\n...     [0.01, 0.01, 0.98],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n0.981...\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [\\n...     [0.1, 0.6, 0.3],\\n...     [0.1, 0.6, 0.3],\\n...     [0.4, 0.5, 0.1],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n---------new doc---------\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [\\n...     [0.1, 0.6, 0.3],\\n...     [0.1, 0.6, 0.3],\\n...     [0.4, 0.5, 0.1],\\n... ]\\n>>> d2_log_loss_score(y_true, y_pred)\\n-0.552...\\n---------new doc---------\\n3.4.5. Multilabel ranking metrics#\\nIn multilabel learning, each sample can have any number of ground truth labels\\nassociated with it. The goal is to give high scores and better rank to\\nthe ground truth labels.\\n\\n3.4.5.1. Coverage error#\\nThe coverage_error function computes the average number of labels that\\nhave to be included in the final prediction such that all true labels\\nare predicted. This is useful if you want to know how many top-scored-labels\\nyou have to predict in average without missing any true one. The best value\\nof this metrics is thus the average number of true labels.\\n\\nNote\\nOur implementation’s score is 1 greater than the one given in Tsoumakas\\net al., 2010. This extends it to handle the degenerate case in which an\\ninstance has 0 true labels.\\n\\nFormally, given a binary indicator matrix of the ground truth labels\\n\\\\(y \\\\in \\\\left\\\\{0, 1\\\\right\\\\}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\) and the\\nscore associated with each label\\n\\\\(\\\\hat{f} \\\\in \\\\mathbb{R}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\),\\nthe coverage is defined as\\n---------new doc---------\\n\\\\[coverage(y, \\\\hat{f}) = \\\\frac{1}{n_{\\\\text{samples}}}\\n  \\\\sum_{i=0}^{n_{\\\\text{samples}} - 1} \\\\max_{j:y_{ij} = 1} \\\\text{rank}_{ij}\\\\]\\nwith \\\\(\\\\text{rank}_{ij} = \\\\left|\\\\left\\\\{k: \\\\hat{f}_{ik} \\\\geq \\\\hat{f}_{ij} \\\\right\\\\}\\\\right|\\\\).\\nGiven the rank definition, ties in y_scores are broken by giving the\\nmaximal rank that would have been assigned to all tied values.\\nHere is a small example of usage of this function:\\n>>> import numpy as np\\n>>> from sklearn.metrics import coverage_error\\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\\n>>> coverage_error(y_true, y_score)\\n2.5\\n---------new doc---------\\n3.4.5.2. Label ranking average precision#\\nThe label_ranking_average_precision_score function\\nimplements label ranking average precision (LRAP). This metric is linked to\\nthe average_precision_score function, but is based on the notion of\\nlabel ranking instead of precision and recall.\\nLabel ranking average precision (LRAP) averages over the samples the answer to\\nthe following question: for each ground truth label, what fraction of\\nhigher-ranked labels were true labels? This performance measure will be higher\\nif you are able to give better rank to the labels associated with each sample.\\nThe obtained score is always strictly greater than 0, and the best value is 1.\\nIf there is exactly one relevant label per sample, label ranking average\\nprecision is equivalent to the mean\\nreciprocal rank.\\nFormally, given a binary indicator matrix of the ground truth labels\\n\\\\(y \\\\in \\\\left\\\\{0, 1\\\\right\\\\}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\)\\nand the score associated with each label\\n\\\\(\\\\hat{f} \\\\in \\\\mathbb{R}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\),\\nthe average precision is defined as\\n---------new doc---------\\n3.4.5.3. Ranking loss#\\nThe label_ranking_loss function computes the ranking loss which\\naverages over the samples the number of label pairs that are incorrectly\\nordered, i.e. true labels have a lower score than false labels, weighted by\\nthe inverse of the number of ordered pairs of false and true labels.\\nThe lowest achievable ranking loss is zero.\\nFormally, given a binary indicator matrix of the ground truth labels\\n\\\\(y \\\\in \\\\left\\\\{0, 1\\\\right\\\\}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\) and the\\nscore associated with each label\\n\\\\(\\\\hat{f} \\\\in \\\\mathbb{R}^{n_\\\\text{samples} \\\\times n_\\\\text{labels}}\\\\),\\nthe ranking loss is defined as\\n---------new doc---------\\nCompared with the ranking loss, NDCG can take into account relevance scores,\\nrather than a ground-truth ranking. So if the ground-truth consists only of an\\nordering, the ranking loss should be preferred; if the ground-truth consists of\\nactual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very\\nrelevant), NDCG can be used.\\nFor one sample, given the vector of continuous ground-truth values for each\\ntarget \\\\(y \\\\in \\\\mathbb{R}^{M}\\\\), where \\\\(M\\\\) is the number of outputs, and\\nthe prediction \\\\(\\\\hat{y}\\\\), which induces the ranking function \\\\(f\\\\), the\\nDCG score is\\n---------new doc---------\\n3.4.6. Regression metrics#\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions to measure regression performance. Some of those have been enhanced\\nto handle the multioutput case: mean_squared_error,\\nmean_absolute_error, r2_score,\\nexplained_variance_score, mean_pinball_loss, d2_pinball_score\\nand d2_absolute_error_score.\\nThese functions have a multioutput keyword argument which specifies the\\nway the scores or losses for each individual target should be averaged. The\\ndefault is 'uniform_average', which specifies a uniformly weighted mean\\nover outputs. If an ndarray of shape (n_outputs,) is passed, then its\\nentries are interpreted as weights and an according weighted average is\\nreturned. If multioutput is 'raw_values', then all unaltered\\nindividual scores or losses will be returned in an array of shape\\n(n_outputs,).\\nThe r2_score and explained_variance_score accept an additional\\nvalue 'variance_weighted' for the multioutput parameter. This option\\nleads to a weighting of each individual score by the variance of the\\ncorresponding target variable. This setting quantifies the globally captured\\nunscaled variance. If the target variables are of different scale, then this\\nscore puts more importance on explaining the higher variance variables.\\n---------new doc---------\\n3.4.6.1. R² score, the coefficient of determination#\\nThe r2_score function computes the coefficient of\\ndetermination,\\nusually denoted as \\\\(R^2\\\\).\\nIt represents the proportion of variance (of y) that has been explained by the\\nindependent variables in the model. It provides an indication of goodness of\\nfit and therefore a measure of how well unseen samples are likely to be\\npredicted by the model, through the proportion of explained variance.\\nAs such variance is dataset dependent, \\\\(R^2\\\\) may not be meaningfully comparable\\nacross different datasets. Best possible score is 1.0 and it can be negative\\n(because the model can be arbitrarily worse). A constant model that always\\npredicts the expected (average) value of y, disregarding the input features,\\nwould get an \\\\(R^2\\\\) score of 0.0.\\nNote: when the prediction residuals have zero mean, the \\\\(R^2\\\\) score and\\nthe Explained variance score are identical.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample\\nand \\\\(y_i\\\\) is the corresponding true value for total \\\\(n\\\\) samples,\\nthe estimated \\\\(R^2\\\\) is defined as:\\n---------new doc---------\\n\\\\[R^2(y, \\\\hat{y}) = 1 - \\\\frac{\\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2}{\\\\sum_{i=1}^{n} (y_i - \\\\bar{y})^2}\\\\]\\nwhere \\\\(\\\\bar{y} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} y_i\\\\) and \\\\(\\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2 = \\\\sum_{i=1}^{n} \\\\epsilon_i^2\\\\).\\nNote that r2_score calculates unadjusted \\\\(R^2\\\\) without correcting for\\nbias in sample variance of y.\\nIn the particular case where the true target is constant, the \\\\(R^2\\\\) score is\\nnot finite: it is either NaN (perfect predictions) or -Inf (imperfect\\npredictions). Such non-finite scores may prevent correct model optimization\\nsuch as grid-search cross-validation to be performed correctly. For this reason\\nthe default behaviour of r2_score is to replace them with 1.0 (perfect\\npredictions) or 0.0 (imperfect predictions). If force_finite\\nis set to False, this score falls back on the original \\\\(R^2\\\\) definition.\\nHere is a small example of usage of the r2_score function:\\n>>> from sklearn.metrics import r2_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> r2_score(y_true, y_pred)\\n0.948...\\n---------new doc---------\\nHere is a small example of usage of the r2_score function:\\n>>> from sklearn.metrics import r2_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> r2_score(y_true, y_pred)\\n0.948...\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> r2_score(y_true, y_pred, multioutput='variance_weighted')\\n0.938...\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> r2_score(y_true, y_pred, multioutput='uniform_average')\\n0.936...\\n>>> r2_score(y_true, y_pred, multioutput='raw_values')\\narray([0.965..., 0.908...])\\n>>> r2_score(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.925...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n>>> r2_score(y_true, y_pred)\\n1.0\\n>>> r2_score(y_true, y_pred, force_finite=False)\\nnan\\n>>> y_true = [-2, -2, -2]\\n---------new doc---------\\n0.925...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n>>> r2_score(y_true, y_pred)\\n1.0\\n>>> r2_score(y_true, y_pred, force_finite=False)\\nnan\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2 + 1e-8]\\n>>> r2_score(y_true, y_pred)\\n0.0\\n>>> r2_score(y_true, y_pred, force_finite=False)\\n-inf\\n---------new doc---------\\n\\\\[\\\\text{MAE}(y, \\\\hat{y}) = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1} \\\\left| y_i - \\\\hat{y}_i \\\\right|.\\\\]\\nHere is a small example of usage of the mean_absolute_error function:\\n>>> from sklearn.metrics import mean_absolute_error\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> mean_absolute_error(y_true, y_pred)\\n0.5\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> mean_absolute_error(y_true, y_pred)\\n0.75\\n>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\\narray([0.5, 1. ])\\n>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.85...\\n---------new doc---------\\n3.4.6.3. Mean squared error#\\nThe mean_squared_error function computes mean squared\\nerror, a risk\\nmetric corresponding to the expected value of the squared (quadratic) error or\\nloss.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample,\\nand \\\\(y_i\\\\) is the corresponding true value, then the mean squared error\\n(MSE) estimated over \\\\(n_{\\\\text{samples}}\\\\) is defined as\\n\\n\\\\[\\\\text{MSE}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples} - 1} (y_i - \\\\hat{y}_i)^2.\\\\]\\nHere is a small example of usage of the mean_squared_error\\nfunction:\\n>>> from sklearn.metrics import mean_squared_error\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> mean_squared_error(y_true, y_pred)\\n0.375\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> mean_squared_error(y_true, y_pred)\\n0.7083...\\n\\n\\nExamples\\n\\nSee Gradient Boosting regression\\nfor an example of mean squared error usage to evaluate gradient boosting regression.\\n---------new doc---------\\nThe root mean squared logarithmic error (RMSLE) is available through the\\nroot_mean_squared_log_error function.\\n\\n\\n3.4.6.5. Mean absolute percentage error#\\nThe mean_absolute_percentage_error (MAPE), also known as mean absolute\\npercentage deviation (MAPD), is an evaluation metric for regression problems.\\nThe idea of this metric is to be sensitive to relative errors. It is for example\\nnot changed by a global scaling of the target variable.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample\\nand \\\\(y_i\\\\) is the corresponding true value, then the mean absolute percentage\\nerror (MAPE) estimated over \\\\(n_{\\\\text{samples}}\\\\) is defined as\\n---------new doc---------\\n\\\\[\\\\text{MAPE}(y, \\\\hat{y}) = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1} \\\\frac{{}\\\\left| y_i - \\\\hat{y}_i \\\\right|}{\\\\max(\\\\epsilon, \\\\left| y_i \\\\right|)}\\\\]\\nwhere \\\\(\\\\epsilon\\\\) is an arbitrary small yet strictly positive number to\\navoid undefined results when y is zero.\\nThe mean_absolute_percentage_error function supports multioutput.\\nHere is a small example of usage of the mean_absolute_percentage_error\\nfunction:\\n>>> from sklearn.metrics import mean_absolute_percentage_error\\n>>> y_true = [1, 10, 1e6]\\n>>> y_pred = [0.9, 15, 1.2e6]\\n>>> mean_absolute_percentage_error(y_true, y_pred)\\n0.2666...\\n\\n\\nIn above example, if we had used mean_absolute_error, it would have ignored\\nthe small magnitude values and only reflected the error in prediction of highest\\nmagnitude value. But that problem is resolved in case of MAPE because it calculates\\nrelative percentage error with respect to actual output.\\n---------new doc---------\\nIn above example, if we had used mean_absolute_error, it would have ignored\\nthe small magnitude values and only reflected the error in prediction of highest\\nmagnitude value. But that problem is resolved in case of MAPE because it calculates\\nrelative percentage error with respect to actual output.\\n\\nNote\\nThe MAPE formula here does not represent the common “percentage” definition: the\\npercentage in the range [0, 100] is converted to a relative value in the range [0,\\n1] by dividing by 100. Thus, an error of 200% corresponds to a relative error of 2.\\nThe motivation here is to have a range of values that is more consistent with other\\nerror metrics in scikit-learn, such as accuracy_score.\\nTo obtain the mean absolute percentage error as per the Wikipedia formula,\\nmultiply the mean_absolute_percentage_error computed here by 100.\\n\\n\\n\\nReferences#\\n\\nWikipedia entry for Mean Absolute Percentage Error\\n\\n\\n\\n\\n3.4.6.6. Median absolute error#\\nThe median_absolute_error is particularly interesting because it is\\nrobust to outliers. The loss is calculated by taking the median of all absolute\\ndifferences between the target and the prediction.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample\\nand \\\\(y_i\\\\) is the corresponding true value, then the median absolute error\\n(MedAE) estimated over \\\\(n_{\\\\text{samples}}\\\\) is defined as\\n---------new doc---------\\n\\\\[\\\\text{MedAE}(y, \\\\hat{y}) = \\\\text{median}(\\\\mid y_1 - \\\\hat{y}_1 \\\\mid, \\\\ldots, \\\\mid y_n - \\\\hat{y}_n \\\\mid).\\\\]\\nThe median_absolute_error does not support multioutput.\\nHere is a small example of usage of the median_absolute_error\\nfunction:\\n>>> from sklearn.metrics import median_absolute_error\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> median_absolute_error(y_true, y_pred)\\n0.5\\n\\n\\n\\n\\n3.4.6.7. Max error#\\nThe max_error function computes the maximum residual error , a metric\\nthat captures the worst case error between the predicted value and\\nthe true value. In a perfectly fitted single output regression\\nmodel, max_error would be 0 on the training set and though this\\nwould be highly unlikely in the real world, this metric shows the\\nextent of error that the model had when it was fitted.\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample,\\nand \\\\(y_i\\\\) is the corresponding true value, then the max error is\\ndefined as\\n---------new doc---------\\n\\\\[\\\\text{Max Error}(y, \\\\hat{y}) = \\\\max(| y_i - \\\\hat{y}_i |)\\\\]\\nHere is a small example of usage of the max_error function:\\n>>> from sklearn.metrics import max_error\\n>>> y_true = [3, 2, 7, 1]\\n>>> y_pred = [9, 2, 7, 1]\\n>>> max_error(y_true, y_pred)\\n6\\n\\n\\nThe max_error does not support multioutput.\\n\\n\\n3.4.6.8. Explained variance score#\\nThe explained_variance_score computes the explained variance\\nregression score.\\nIf \\\\(\\\\hat{y}\\\\) is the estimated target output, \\\\(y\\\\) the corresponding\\n(correct) target output, and \\\\(Var\\\\) is Variance, the square of the standard deviation,\\nthen the explained variance is estimated as follow:\\n\\n\\\\[explained\\\\_{}variance(y, \\\\hat{y}) = 1 - \\\\frac{Var\\\\{ y - \\\\hat{y}\\\\}}{Var\\\\{y\\\\}}\\\\]\\nThe best possible score is 1.0, lower values are worse.\\n\\nLink to R² score, the coefficient of determination\\nThe difference between the explained variance score and the R² score, the coefficient of determination\\nis that the explained variance score does not account for\\nsystematic offset in the prediction. For this reason, the\\nR² score, the coefficient of determination should be preferred in general.\\n---------new doc---------\\nIn the particular case where the true target is constant, the Explained\\nVariance score is not finite: it is either NaN (perfect predictions) or\\n-Inf (imperfect predictions). Such non-finite scores may prevent correct\\nmodel optimization such as grid-search cross-validation to be performed\\ncorrectly. For this reason the default behaviour of\\nexplained_variance_score is to replace them with 1.0 (perfect\\npredictions) or 0.0 (imperfect predictions). You can set the force_finite\\nparameter to False to prevent this fix from happening and fallback on the\\noriginal Explained Variance score.\\nHere is a small example of usage of the explained_variance_score\\nfunction:\\n>>> from sklearn.metrics import explained_variance_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> explained_variance_score(y_true, y_pred)\\n0.957...\\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\\n>>> explained_variance_score(y_true, y_pred, multioutput='raw_values')\\narray([0.967..., 1.        ])\\n>>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.990...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n---------new doc---------\\narray([0.967..., 1.        ])\\n>>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.990...\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2]\\n>>> explained_variance_score(y_true, y_pred)\\n1.0\\n>>> explained_variance_score(y_true, y_pred, force_finite=False)\\nnan\\n>>> y_true = [-2, -2, -2]\\n>>> y_pred = [-2, -2, -2 + 1e-8]\\n>>> explained_variance_score(y_true, y_pred)\\n0.0\\n>>> explained_variance_score(y_true, y_pred, force_finite=False)\\n-inf\\n---------new doc---------\\n3.4.6.9. Mean Poisson, Gamma, and Tweedie deviances#\\nThe mean_tweedie_deviance function computes the mean Tweedie\\ndeviance error\\nwith a power parameter (\\\\(p\\\\)). This is a metric that elicits\\npredicted expectation values of regression targets.\\nFollowing special cases exist,\\n\\nwhen power=0 it is equivalent to mean_squared_error.\\nwhen power=1 it is equivalent to mean_poisson_deviance.\\nwhen power=2 it is equivalent to mean_gamma_deviance.\\n\\nIf \\\\(\\\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample,\\nand \\\\(y_i\\\\) is the corresponding true value, then the mean Tweedie\\ndeviance error (D) for power \\\\(p\\\\), estimated over \\\\(n_{\\\\text{samples}}\\\\)\\nis defined as\\n---------new doc---------\\n\\\\[\\\\begin{split}\\\\text{D}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}}\\n\\\\sum_{i=0}^{n_\\\\text{samples} - 1}\\n\\\\begin{cases}\\n(y_i-\\\\hat{y}_i)^2, & \\\\text{for }p=0\\\\text{ (Normal)}\\\\\\\\\\n2(y_i \\\\log(y_i/\\\\hat{y}_i) + \\\\hat{y}_i - y_i),  & \\\\text{for }p=1\\\\text{ (Poisson)}\\\\\\\\\\n2(\\\\log(\\\\hat{y}_i/y_i) + y_i/\\\\hat{y}_i - 1),  & \\\\text{for }p=2\\\\text{ (Gamma)}\\\\\\\\\\n2\\\\left(\\\\frac{\\\\max(y_i,0)^{2-p}}{(1-p)(2-p)}-\\n\\\\frac{y_i\\\\,\\\\hat{y}_i^{1-p}}{1-p}+\\\\frac{\\\\hat{y}_i^{2-p}}{2-p}\\\\right),\\n& \\\\text{otherwise}\\n\\\\end{cases}\\\\end{split}\\\\]\\nTweedie deviance is a homogeneous function of degree 2-power.\\nThus, Gamma distribution with power=2 means that simultaneously scaling\\ny_true and y_pred has no effect on the deviance. For Poisson\\ndistribution power=1 the deviance scales linearly, and for Normal\\ndistribution (power=0), quadratically.  In general, the higher\\npower the less weight is given to extreme deviations between true\\nand predicted targets.\\n---------new doc---------\\nTweedie deviance is a homogeneous function of degree 2-power.\\nThus, Gamma distribution with power=2 means that simultaneously scaling\\ny_true and y_pred has no effect on the deviance. For Poisson\\ndistribution power=1 the deviance scales linearly, and for Normal\\ndistribution (power=0), quadratically.  In general, the higher\\npower the less weight is given to extreme deviations between true\\nand predicted targets.\\nFor instance, let’s compare the two predictions 1.5 and 150 that are both\\n50% larger than their corresponding true value.\\nThe mean squared error (power=0) is very sensitive to the\\nprediction difference of the second point,:\\n>>> from sklearn.metrics import mean_tweedie_deviance\\n>>> mean_tweedie_deviance([1.0], [1.5], power=0)\\n0.25\\n>>> mean_tweedie_deviance([100.], [150.], power=0)\\n2500.0\\n---------new doc---------\\nIf we increase power to 1,:\\n>>> mean_tweedie_deviance([1.0], [1.5], power=1)\\n0.18...\\n>>> mean_tweedie_deviance([100.], [150.], power=1)\\n18.9...\\n\\n\\nthe difference in errors decreases. Finally, by setting, power=2:\\n>>> mean_tweedie_deviance([1.0], [1.5], power=2)\\n0.14...\\n>>> mean_tweedie_deviance([100.], [150.], power=2)\\n0.14...\\n\\n\\nwe would get identical errors. The deviance when power=2 is thus only\\nsensitive to relative errors.\\n\\n\\n3.4.6.10. Pinball loss#\\nThe mean_pinball_loss function is used to evaluate the predictive\\nperformance of quantile regression models.\\n---------new doc---------\\n3.4.6.10. Pinball loss#\\nThe mean_pinball_loss function is used to evaluate the predictive\\nperformance of quantile regression models.\\n\\n\\\\[\\\\text{pinball}(y, \\\\hat{y}) = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1}  \\\\alpha \\\\max(y_i - \\\\hat{y}_i, 0) + (1 - \\\\alpha) \\\\max(\\\\hat{y}_i - y_i, 0)\\\\]\\nThe value of pinball loss is equivalent to half of mean_absolute_error when the quantile\\nparameter alpha is set to 0.5.\\nHere is a small example of usage of the mean_pinball_loss function:\\n>>> from sklearn.metrics import mean_pinball_loss\\n>>> y_true = [1, 2, 3]\\n>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\\n0.03...\\n>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\\n0.3...\\n>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\\n0.3...\\n>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\\n0.03...\\n>>> mean_pinball_loss(y_true, y_true, alpha=0.1)\\n0.0\\n>>> mean_pinball_loss(y_true, y_true, alpha=0.9)\\n0.0\\n---------new doc---------\\nIt is possible to build a scorer object with a specific choice of alpha:\\n>>> from sklearn.metrics import make_scorer\\n>>> mean_pinball_loss_95p = make_scorer(mean_pinball_loss, alpha=0.95)\\n\\n\\nSuch a scorer can be used to evaluate the generalization performance of a\\nquantile regressor via cross-validation:\\n>>> from sklearn.datasets import make_regression\\n>>> from sklearn.model_selection import cross_val_score\\n>>> from sklearn.ensemble import GradientBoostingRegressor\\n>>>\\n>>> X, y = make_regression(n_samples=100, random_state=0)\\n>>> estimator = GradientBoostingRegressor(\\n...     loss=\\\"quantile\\\",\\n...     alpha=0.95,\\n...     random_state=0,\\n... )\\n>>> cross_val_score(estimator, X, y, cv=5, scoring=mean_pinball_loss_95p)\\narray([13.6..., 9.7..., 23.3..., 9.5..., 10.4...])\\n\\n\\nIt is also possible to build scorer objects for hyper-parameter tuning. The\\nsign of the loss must be switched to ensure that greater means better as\\nexplained in the example linked below.\\nExamples\\n\\nSee Prediction Intervals for Gradient Boosting Regression\\nfor an example of using the pinball loss to evaluate and tune the\\nhyper-parameters of quantile regression models on data with non-symmetric\\nnoise and outliers.\\n---------new doc---------\\nIt is also possible to build scorer objects for hyper-parameter tuning. The\\nsign of the loss must be switched to ensure that greater means better as\\nexplained in the example linked below.\\nExamples\\n\\nSee Prediction Intervals for Gradient Boosting Regression\\nfor an example of using the pinball loss to evaluate and tune the\\nhyper-parameters of quantile regression models on data with non-symmetric\\nnoise and outliers.\\n\\n\\n\\n3.4.6.11. D² score#\\nThe D² score computes the fraction of deviance explained.\\nIt is a generalization of R², where the squared error is generalized and replaced\\nby a deviance of choice \\\\(\\\\text{dev}(y, \\\\hat{y})\\\\)\\n(e.g., Tweedie, pinball or mean absolute error). D² is a form of a skill score.\\nIt is calculated as\\n\\n\\\\[D^2(y, \\\\hat{y}) = 1 - \\\\frac{\\\\text{dev}(y, \\\\hat{y})}{\\\\text{dev}(y, y_{\\\\text{null}})} \\\\,.\\\\]\\nWhere \\\\(y_{\\\\text{null}}\\\\) is the optimal prediction of an intercept-only model\\n(e.g., the mean of y_true for the Tweedie case, the median for absolute\\nerror and the alpha-quantile for pinball loss).\\nLike R², the best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). A constant model that always predicts\\n\\\\(y_{\\\\text{null}}\\\\), disregarding the input features, would get a D² score\\nof 0.0.\\n---------new doc---------\\nD² Tweedie score#\\nThe d2_tweedie_score function implements the special case of D²\\nwhere \\\\(\\\\text{dev}(y, \\\\hat{y})\\\\) is the Tweedie deviance, see Mean Poisson, Gamma, and Tweedie deviances.\\nIt is also known as D² Tweedie and is related to McFadden’s likelihood ratio index.\\nThe argument power defines the Tweedie power as for\\nmean_tweedie_deviance. Note that for power=0,\\nd2_tweedie_score equals r2_score (for single targets).\\nA scorer object with a specific choice of power can be built by:\\n>>> from sklearn.metrics import d2_tweedie_score, make_scorer\\n>>> d2_tweedie_score_15 = make_scorer(d2_tweedie_score, power=1.5)\\n\\n\\n\\n\\n\\nD² pinball score#\\nThe d2_pinball_score function implements the special case\\nof D² with the pinball loss, see Pinball loss, i.e.:\\n---------new doc---------\\nD² pinball score#\\nThe d2_pinball_score function implements the special case\\nof D² with the pinball loss, see Pinball loss, i.e.:\\n\\n\\\\[\\\\text{dev}(y, \\\\hat{y}) = \\\\text{pinball}(y, \\\\hat{y}).\\\\]\\nThe argument alpha defines the slope of the pinball loss as for\\nmean_pinball_loss (Pinball loss). It determines the\\nquantile level alpha for which the pinball loss and also D²\\nare optimal. Note that for alpha=0.5 (the default) d2_pinball_score\\nequals d2_absolute_error_score.\\nA scorer object with a specific choice of alpha can be built by:\\n>>> from sklearn.metrics import d2_pinball_score, make_scorer\\n>>> d2_pinball_score_08 = make_scorer(d2_pinball_score, alpha=0.8)\\n\\n\\n\\n\\n\\nD² absolute error score#\\nThe d2_absolute_error_score function implements the special case of\\nthe Mean absolute error:\\n---------new doc---------\\nD² absolute error score#\\nThe d2_absolute_error_score function implements the special case of\\nthe Mean absolute error:\\n\\n\\\\[\\\\text{dev}(y, \\\\hat{y}) = \\\\text{MAE}(y, \\\\hat{y}).\\\\]\\nHere are some usage examples of the d2_absolute_error_score function:\\n>>> from sklearn.metrics import d2_absolute_error_score\\n>>> y_true = [3, -0.5, 2, 7]\\n>>> y_pred = [2.5, 0.0, 2, 8]\\n>>> d2_absolute_error_score(y_true, y_pred)\\n0.764...\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [1, 2, 3]\\n>>> d2_absolute_error_score(y_true, y_pred)\\n1.0\\n>>> y_true = [1, 2, 3]\\n>>> y_pred = [2, 2, 2]\\n>>> d2_absolute_error_score(y_true, y_pred)\\n0.0\\n\\n\\n\\n\\n\\n3.4.6.12. Visual evaluation of regression models#\\nAmong methods to assess the quality of regression models, scikit-learn provides\\nthe PredictionErrorDisplay class. It allows to\\nvisually inspect the prediction errors of a model in two different manners.\\n---------new doc---------\\nThe plot on the left shows the actual values vs predicted values. For a\\nnoise-free regression task aiming to predict the (conditional) expectation of\\ny, a perfect regression model would display data points on the diagonal\\ndefined by predicted equal to actual values. The further away from this optimal\\nline, the larger the error of the model. In a more realistic setting with\\nirreducible noise, that is, when not all the variations of y can be explained\\nby features in X, then the best model would lead to a cloud of points densely\\narranged around the diagonal.\\nNote that the above only holds when the predicted values is the expected value\\nof y given X. This is typically the case for regression models that\\nminimize the mean squared error objective function or more generally the\\nmean Tweedie deviance for any value of its\\n“power” parameter.\\nWhen plotting the predictions of an estimator that predicts a quantile\\nof y given X, e.g. QuantileRegressor\\nor any other model minimizing the pinball loss, a\\nfraction of the points are either expected to lie above or below the diagonal\\ndepending on the estimated quantile level.\\nAll in all, while intuitive to read, this plot does not really inform us on\\nwhat to do to obtain a better model.\\nThe right-hand side plot shows the residuals (i.e. the difference between the\\nactual and the predicted values) vs. the predicted values.\\nThis plot makes it easier to visualize if the residuals follow and\\nhomoscedastic or heteroschedastic\\ndistribution.\\nIn particular, if the true distribution of y|X is Poisson or Gamma\\ndistributed, it is expected that the variance of the residuals of the optimal\\nmodel would grow with the predicted value of E[y|X] (either linearly for\\n---------new doc---------\\nactual and the predicted values) vs. the predicted values.\\nThis plot makes it easier to visualize if the residuals follow and\\nhomoscedastic or heteroschedastic\\ndistribution.\\nIn particular, if the true distribution of y|X is Poisson or Gamma\\ndistributed, it is expected that the variance of the residuals of the optimal\\nmodel would grow with the predicted value of E[y|X] (either linearly for\\nPoisson or quadratically for Gamma).\\nWhen fitting a linear least squares regression model (see\\nLinearRegression and\\nRidge), we can use this plot to check\\nif some of the model assumptions\\nare met, in particular that the residuals should be uncorrelated, their\\nexpected value should be null and that their variance should be constant\\n(homoschedasticity).\\nIf this is not the case, and in particular if the residuals plot show some\\nbanana-shaped structure, this is a hint that the model is likely mis-specified\\nand that non-linear feature engineering or switching to a non-linear regression\\nmodel might be useful.\\nRefer to the example below to see a model evaluation that makes use of this\\ndisplay.\\nExamples\\n---------new doc---------\\nExample of permutation feature importance using multiple scorers#\\nIn the example below we use a list of metrics, but more input formats are\\npossible, as documented in Using multiple metric evaluation.\\n>>> scoring = ['r2', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error']\\n>>> r_multi = permutation_importance(\\n...     model, X_val, y_val, n_repeats=30, random_state=0, scoring=scoring)\\n...\\n>>> for metric in r_multi:\\n...     print(f\\\"{metric}\\\")\\n...     r = r_multi[metric]\\n...     for i in r.importances_mean.argsort()[::-1]:\\n...         if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\\n...             print(f\\\"    {diabetes.feature_names[i]:<8}\\\"\\n...                   f\\\"{r.importances_mean[i]:.3f}\\\"\\n...                   f\\\" +/- {r.importances_std[i]:.3f}\\\")\\n...\\nr2\\n    s5      0.204 +/- 0.050\\n    bmi     0.176 +/- 0.048\\n    bp      0.088 +/- 0.033\\n---------new doc---------\\n...                   f\\\" +/- {r.importances_std[i]:.3f}\\\")\\n...\\nr2\\n    s5      0.204 +/- 0.050\\n    bmi     0.176 +/- 0.048\\n    bp      0.088 +/- 0.033\\n    sex     0.056 +/- 0.023\\nneg_mean_absolute_percentage_error\\n    s5      0.081 +/- 0.020\\n    bmi     0.064 +/- 0.015\\n    bp      0.029 +/- 0.010\\nneg_mean_squared_error\\n    s5      1013.866 +/- 246.445\\n    bmi     872.726 +/- 240.298\\n    bp      438.663 +/- 163.022\\n    sex     277.376 +/- 115.123\\n---------new doc---------\\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n>>> regr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\\nR2 score: 0.61\\n>>> raw_target_regr = LinearRegression().fit(X_train, y_train)\\n>>> print('R2 score: {0:.2f}'.format(raw_target_regr.score(X_test, y_test)))\\nR2 score: 0.59\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [4.97s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The provided text appears to be a documentation for the scikit-learn library in Python, specifically related to model evaluation and feature importance.\\n\\nHere's a summary of the main points:\\n\\n**Model Evaluation**\\n\\n* The library provides various metrics for evaluating models, including:\\n\\t+ R2 score (coefficient of determination)\\n\\t+ Mean absolute percentage error (MAPE)\\n\\t+ Mean squared error (MSE)\\n\\t+ Negated mean absolute percentage error (neg_mean_absolute_percentage_error)\\n\\t+ Negated mean squared error (neg_mean_squared_error)\\n* The `permutation_importance` function can be used to evaluate the importance of features in a model.\\n\\n**Feature Importance**\\n\\n* Feature importance is calculated using permutation feature importance, which involves randomly permuting the values of each feature and measuring the decrease in model performance.\\n* The `importances_mean` attribute of the `PermutationImportance` object contains the average importance of each feature across all permutations.\\n* The `importances_std` attribute contains the standard deviation of the importance scores.\\n\\n**Residual Plots**\\n\\n* Residual plots can be used to visualize the distribution of residuals (i.e., the difference between actual and predicted values).\\n* The plot shows the residuals vs. the predicted values, which can help identify issues with homoscedasticity (constant variance) or heteroscedasticity (variable variance).\\n\\n**Assumptions of Linear Regression**\\n\\n* The library provides a way to check if some assumptions of linear regression are met using residual plots.\\n* Specifically, it checks for:\\n\\t+ Un-correlated residuals\\n\\t+ Expected value of residuals is null\\n\\t+ Constant variance of residuals (homoscedasticity)\\n\\nOverall, the documentation provides a comprehensive overview of model evaluation and feature importance in scikit-learn, including various metrics, permutation feature importance, residual plots, and assumptions of linear regression.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.2:3b-instruct-q8_0\",\n",
      "          \"created_at\": \"2025-02-17T13:31:27.735896844Z\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"total_duration\": 4970238616,\n",
      "          \"load_duration\": 12847322,\n",
      "          \"prompt_eval_count\": 2048,\n",
      "          \"prompt_eval_duration\": 361000000,\n",
      "          \"eval_count\": 380,\n",
      "          \"eval_duration\": 4592000000,\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\",\n",
      "            \"images\": null,\n",
      "            \"tool_calls\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The provided text appears to be a documentation for the scikit-learn library in Python, specifically related to model evaluation and feature importance.\\n\\nHere's a summary of the main points:\\n\\n**Model Evaluation**\\n\\n* The library provides various metrics for evaluating models, including:\\n\\t+ R2 score (coefficient of determination)\\n\\t+ Mean absolute percentage error (MAPE)\\n\\t+ Mean squared error (MSE)\\n\\t+ Negated mean absolute percentage error (neg_mean_absolute_percentage_error)\\n\\t+ Negated mean squared error (neg_mean_squared_error)\\n* The `permutation_importance` function can be used to evaluate the importance of features in a model.\\n\\n**Feature Importance**\\n\\n* Feature importance is calculated using permutation feature importance, which involves randomly permuting the values of each feature and measuring the decrease in model performance.\\n* The `importances_mean` attribute of the `PermutationImportance` object contains the average importance of each feature across all permutations.\\n* The `importances_std` attribute contains the standard deviation of the importance scores.\\n\\n**Residual Plots**\\n\\n* Residual plots can be used to visualize the distribution of residuals (i.e., the difference between actual and predicted values).\\n* The plot shows the residuals vs. the predicted values, which can help identify issues with homoscedasticity (constant variance) or heteroscedasticity (variable variance).\\n\\n**Assumptions of Linear Regression**\\n\\n* The library provides a way to check if some assumptions of linear regression are met using residual plots.\\n* Specifically, it checks for:\\n\\t+ Un-correlated residuals\\n\\t+ Expected value of residuals is null\\n\\t+ Constant variance of residuals (homoscedasticity)\\n\\nOverall, the documentation provides a comprehensive overview of model evaluation and feature importance in scikit-learn, including various metrics, permutation feature importance, residual plots, and assumptions of linear regression.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.2:3b-instruct-q8_0\",\n",
      "              \"created_at\": \"2025-02-17T13:31:27.735896844Z\",\n",
      "              \"done\": true,\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"total_duration\": 4970238616,\n",
      "              \"load_duration\": 12847322,\n",
      "              \"prompt_eval_count\": 2048,\n",
      "              \"prompt_eval_duration\": 361000000,\n",
      "              \"eval_count\": 380,\n",
      "              \"eval_duration\": 4592000000,\n",
      "              \"message\": {\n",
      "                \"lc\": 1,\n",
      "                \"type\": \"not_implemented\",\n",
      "                \"id\": [\n",
      "                  \"ollama\",\n",
      "                  \"_types\",\n",
      "                  \"Message\"\n",
      "                ],\n",
      "                \"repr\": \"Message(role='assistant', content='', images=None, tool_calls=None)\"\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-d16aee84-d44e-42fb-9c09-a68a628d369b-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 2048,\n",
      "              \"output_tokens\": 380,\n",
      "              \"total_tokens\": 2428\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The provided text appears to be a documentation for the scikit-learn library in Python, specifically related to model evaluation and feature importance.\\n\\nHere's a summary of the main points:\\n\\n**Model Evaluation**\\n\\n* The library provides various metrics for evaluating models, including:\\n\\t+ R2 score (coefficient of determination)\\n\\t+ Mean absolute percentage error (MAPE)\\n\\t+ Mean squared error (MSE)\\n\\t+ Negated mean absolute percentage error (neg_mean_absolute_percentage_error)\\n\\t+ Negated mean squared error (neg_mean_squared_error)\\n* The `permutation_importance` function can be used to evaluate the importance of features in a model.\\n\\n**Feature Importance**\\n\\n* Feature importance is calculated using permutation feature importance, which involves randomly permuting the values of each feature and measuring the decrease in model performance.\\n* The `importances_mean` attribute of the `PermutationImportance` object contains the average importance of each feature across all permutations.\\n* The `importances_std` attribute contains the standard deviation of the importance scores.\\n\\n**Residual Plots**\\n\\n* Residual plots can be used to visualize the distribution of residuals (i.e., the difference between actual and predicted values).\\n* The plot shows the residuals vs. the predicted values, which can help identify issues with homoscedasticity (constant variance) or heteroscedasticity (variable variance).\\n\\n**Assumptions of Linear Regression**\\n\\n* The library provides a way to check if some assumptions of linear regression are met using residual plots.\\n* Specifically, it checks for:\\n\\t+ Un-correlated residuals\\n\\t+ Expected value of residuals is null\\n\\t+ Constant variance of residuals (homoscedasticity)\\n\\nOverall, the documentation provides a comprehensive overview of model evaluation and feature importance in scikit-learn, including various metrics, permutation feature importance, residual plots, and assumptions of linear regression.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [4.98s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The provided text appears to be a documentation for the scikit-learn library in Python, specifically related to model evaluation and feature importance.\\n\\nHere's a summary of the main points:\\n\\n**Model Evaluation**\\n\\n* The library provides various metrics for evaluating models, including:\\n\\t+ R2 score (coefficient of determination)\\n\\t+ Mean absolute percentage error (MAPE)\\n\\t+ Mean squared error (MSE)\\n\\t+ Negated mean absolute percentage error (neg_mean_absolute_percentage_error)\\n\\t+ Negated mean squared error (neg_mean_squared_error)\\n* The `permutation_importance` function can be used to evaluate the importance of features in a model.\\n\\n**Feature Importance**\\n\\n* Feature importance is calculated using permutation feature importance, which involves randomly permuting the values of each feature and measuring the decrease in model performance.\\n* The `importances_mean` attribute of the `PermutationImportance` object contains the average importance of each feature across all permutations.\\n* The `importances_std` attribute contains the standard deviation of the importance scores.\\n\\n**Residual Plots**\\n\\n* Residual plots can be used to visualize the distribution of residuals (i.e., the difference between actual and predicted values).\\n* The plot shows the residuals vs. the predicted values, which can help identify issues with homoscedasticity (constant variance) or heteroscedasticity (variable variance).\\n\\n**Assumptions of Linear Regression**\\n\\n* The library provides a way to check if some assumptions of linear regression are met using residual plots.\\n* Specifically, it checks for:\\n\\t+ Un-correlated residuals\\n\\t+ Expected value of residuals is null\\n\\t+ Constant variance of residuals (homoscedasticity)\\n\\nOverall, the documentation provides a comprehensive overview of model evaluation and feature importance in scikit-learn, including various metrics, permutation feature importance, residual plots, and assumptions of linear regression.\"\n",
      "}\n",
      "Global cluster: 1, has 465 documents.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m leaf_1 \u001b[38;5;241m=\u001b[39m \u001b[43mraptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarization_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_embedding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m, in \u001b[0;36mraptor\u001b[0;34m(documents, summarization_chain, desired_embedding_size, global_max_clusters, local_max_clusters)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal cluster: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_cluster_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(global_cluster_documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# recluster into local cluster\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m local_clusters \u001b[38;5;241m=\u001b[39m \u001b[43mgmm_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_cluster_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_max_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_fold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m local_cluster_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(local_clusters):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Select all documents of this local cluster\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     local_cluster_documents \u001b[38;5;241m=\u001b[39m global_cluster_documents[local_clusters\u001b[38;5;241m==\u001b[39mlocal_cluster_index]\n",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m, in \u001b[0;36mgmm_clustering\u001b[0;34m(X, K, k, cv_fold)\u001b[0m\n\u001b[1;32m     19\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mrange\u001b[39m(k, max_cluster_trial),\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtied\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiag\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspherical\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     23\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     24\u001b[0m     GaussianMixture(), param_grid\u001b[38;5;241m=\u001b[39mparam_grid, scoring\u001b[38;5;241m=\u001b[39mgmm_bic_score, cv\u001b[38;5;241m=\u001b[39mcv_fold\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected params for clustering: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, with BIC score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    967\u001b[0m         )\n\u001b[1;32m    968\u001b[0m     )\n\u001b[0;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:864\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/mixture/_base.py:180\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/mixture/_base.py:246\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    244\u001b[0m     prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[0;32m--> 246\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_m_step(X, log_resp)\n\u001b[1;32m    248\u001b[0m     lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(log_resp, log_prob_norm)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/mixture/_base.py:305\u001b[0m, in \u001b[0;36mBaseMixture._e_step\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_e_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    290\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"E step.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m        the point of each sample in X.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_log_prob_resp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(log_prob_norm), log_resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/mixture/_base.py:525\u001b[0m, in \u001b[0;36mBaseMixture._estimate_log_prob_resp\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_estimate_log_prob_resp\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate log probabilities and responsibilities for each sample.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    Compute the log probabilities, weighted log probabilities per\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m        logarithm of the responsibilities\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 525\u001b[0m     weighted_log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_weighted_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m     log_prob_norm \u001b[38;5;241m=\u001b[39m logsumexp(weighted_log_prob, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;66;03m# ignore underflow\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/mixture/_base.py:478\u001b[0m, in \u001b[0;36mBaseMixture._estimate_weighted_log_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_estimate_weighted_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate the weighted log-probabilities, log P(X | Z) + log weights.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m    weighted_log_prob : array, shape (n_samples, n_component)\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_log_weights()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/mixture/_gaussian_mixture.py:820\u001b[0m, in \u001b[0;36mGaussianMixture._estimate_log_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_estimate_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_estimate_log_gaussian_prob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeans_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecisions_cholesky_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance_type\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/rag/lib/python3.10/site-packages/sklearn/mixture/_gaussian_mixture.py:480\u001b[0m, in \u001b[0;36m_estimate_log_gaussian_prob\u001b[0;34m(X, means, precisions_chol, covariance_type)\u001b[0m\n\u001b[1;32m    478\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((n_samples, n_components))\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, (mu, prec_chol) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(means, precisions_chol)):\n\u001b[0;32m--> 480\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprec_chol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(mu, prec_chol)\n\u001b[1;32m    481\u001b[0m         log_prob[:, k] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39msquare(y), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m covariance_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtied\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "leaf_1 = raptor(texts_split, summarization_chain, desired_embedding_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32611.25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "130445 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
